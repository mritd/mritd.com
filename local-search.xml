<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>编写一个动态准入控制来实现自动化</title>
    <link href="/2020/08/19/write-a-dynamic-admission-control-webhook/"/>
    <url>/2020/08/19/write-a-dynamic-admission-control-webhook/</url>
    
    <content type="html"><![CDATA[<h2 id="一、准入控制介绍"><a href="#一、准入控制介绍" class="headerlink" title="一、准入控制介绍"></a>一、准入控制介绍</h2><p>在 Kubernetes 整个请求链路中，请求通过认证和授权之后、对象被持久化之前需要通过一连串的 “准入控制拦截器”；这些准入控制器负载验证请求的合法性，必要情况下也可以对请求进行修改；默认准入控制器编写在 kube-apiserver 的代码中，针对于当前 kube-apiserver 默认启用的准入控制器你可以通过以下命令查看:</p><pre><code class="hljs sh">kube-apiserver -h | grep <span class="hljs-built_in">enable</span>-admission-plugins</code></pre><p>具体每个准入控制器的作用可以通过 <a href="https://kubernetes.io/zh/docs/reference/access-authn-authz/admission-controllers/" target="_blank" rel="noopener">Using Admission Controllers</a> 文档查看。在这些准入控制器中有两个特殊的准入控制器 <code>MutatingAdmissionWebhook</code> 和 <code>ValidatingAdmissionWebhook</code>。<strong>这两个准入控制器以 WebHook 的方式提供扩展能力，从而我们可以实现自定义的一些功能。当我们在集群中创建相关 WebHook 配置后，我们配置中描述的想要关注的资源在集群中创建、修改等都会触发 WebHook，我们再编写具体的应用来响应 WebHook 即可完成特定功能。</strong></p><h2 id="二、动态准入控制"><a href="#二、动态准入控制" class="headerlink" title="二、动态准入控制"></a>二、动态准入控制</h2><p>动态准入控制实际上指的就是上面所说的两个 WebHook，在使用动态准入控制时需要一些先决条件:</p><ul><li>确保 Kubernetes 集群版本至少为 v1.16 (以便使用 <code>admissionregistration.k8s.io/v1 API</code>)或者 v1.9 (以便使用 <code>admissionregistration.k8s.io/v1beta1</code> API)。</li><li>确保启用 MutatingAdmissionWebhook 和 ValidatingAdmissionWebhook 控制器。 </li><li>确保启用 <code>admissionregistration.k8s.io/v1</code> 或 <code>admissionregistration.k8s.io/v1beta1</code> API。</li></ul><p>如果要使用 Mutating Admission Webhook，在满足先决条件后，需要在系统中 create 一个 MutatingWebhookConfiguration:</p><pre><code class="hljs yaml"><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">admissionregistration.k8s.io/v1</span><span class="hljs-attr">kind:</span> <span class="hljs-string">MutatingWebhookConfiguration</span><span class="hljs-attr">metadata:</span>  <span class="hljs-attr">name:</span> <span class="hljs-string">"mutating-webhook.mritd.me"</span>  <span class="hljs-attr">namespace:</span> <span class="hljs-string">kube-addons</span><span class="hljs-attr">webhooks:</span>  <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">"mutating-webhook.mritd.me"</span>    <span class="hljs-attr">rules:</span>      <span class="hljs-bullet">-</span> <span class="hljs-attr">apiGroups:</span>   <span class="hljs-string">[""]</span>        <span class="hljs-attr">apiVersions:</span> <span class="hljs-string">["v1"]</span>        <span class="hljs-attr">operations:</span>  <span class="hljs-string">["CREATE","UPDATE"]</span>        <span class="hljs-attr">resources:</span>   <span class="hljs-string">["pods"]</span>        <span class="hljs-attr">scope:</span>       <span class="hljs-string">"Namespaced"</span>    <span class="hljs-attr">clientConfig:</span>      <span class="hljs-attr">service:</span>        <span class="hljs-attr">name:</span> <span class="hljs-string">"mutating-webhook"</span>        <span class="hljs-attr">namespace:</span> <span class="hljs-string">"kube-addons"</span>        <span class="hljs-attr">path:</span> <span class="hljs-string">/print</span>      <span class="hljs-attr">caBundle:</span> <span class="hljs-string">$&#123;CA_BUNDLE&#125;</span>    <span class="hljs-attr">admissionReviewVersions:</span> <span class="hljs-string">["v1",</span> <span class="hljs-string">"v1beta1"</span><span class="hljs-string">]</span>    <span class="hljs-attr">sideEffects:</span> <span class="hljs-string">None</span>    <span class="hljs-attr">timeoutSeconds:</span> <span class="hljs-number">5</span>    <span class="hljs-attr">failurePolicy:</span> <span class="hljs-string">Ignore</span>    <span class="hljs-attr">namespaceSelector:</span>      <span class="hljs-attr">matchLabels:</span>        <span class="hljs-attr">mutating-webhook.mritd.me:</span> <span class="hljs-string">"true"</span></code></pre><p>同样要使用 Validating Admission Webhook 也需要类似的配置:</p><pre><code class="hljs yaml"><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">admissionregistration.k8s.io/v1</span><span class="hljs-attr">kind:</span> <span class="hljs-string">ValidatingWebhookConfiguration</span><span class="hljs-attr">metadata:</span>  <span class="hljs-attr">name:</span> <span class="hljs-string">"validating-webhook.mritd.me"</span><span class="hljs-attr">webhooks:</span>  <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">"validating-webhook.mritd.me"</span>    <span class="hljs-attr">rules:</span>      <span class="hljs-bullet">-</span> <span class="hljs-attr">apiGroups:</span>   <span class="hljs-string">[""]</span>        <span class="hljs-attr">apiVersions:</span> <span class="hljs-string">["v1"]</span>        <span class="hljs-attr">operations:</span>  <span class="hljs-string">["CREATE","UPDATE"]</span>        <span class="hljs-attr">resources:</span>   <span class="hljs-string">["pods"]</span>        <span class="hljs-attr">scope:</span>       <span class="hljs-string">"Namespaced"</span>    <span class="hljs-attr">clientConfig:</span>      <span class="hljs-attr">service:</span>        <span class="hljs-attr">name:</span> <span class="hljs-string">"validating-webhook"</span>        <span class="hljs-attr">namespace:</span> <span class="hljs-string">"kube-addons"</span>        <span class="hljs-attr">path:</span> <span class="hljs-string">/print</span>      <span class="hljs-attr">caBundle:</span> <span class="hljs-string">$&#123;CA_BUNDLE&#125;</span>    <span class="hljs-attr">admissionReviewVersions:</span> <span class="hljs-string">["v1",</span> <span class="hljs-string">"v1beta1"</span><span class="hljs-string">]</span>    <span class="hljs-attr">sideEffects:</span> <span class="hljs-string">None</span>    <span class="hljs-attr">timeoutSeconds:</span> <span class="hljs-number">5</span>    <span class="hljs-attr">failurePolicy:</span> <span class="hljs-string">Ignore</span>    <span class="hljs-attr">namespaceSelector:</span>      <span class="hljs-attr">matchLabels:</span>        <span class="hljs-attr">validating-webhook.mritd.me:</span> <span class="hljs-string">"true"</span></code></pre><p>从配置文件中可以看到，<code>webhooks.rules</code> 段落中具体指定了我们想要关注的资源及其行为，<code>webhooks.clientConfig</code> 中指定了 webhook 触发后将其发送到那个地址以及证书配置等，这些具体字段的含义可以通过官方文档 <a href="https://kubernetes.io/zh/docs/reference/access-authn-authz/extensible-admission-controllers/" target="_blank" rel="noopener">Dynamic Admission Control</a> 来查看。</p><p><strong>值得注意的是 Mutating Admission Webhook 会在 Validating Admission Webhook 之前触发；Mutating Admission Webhook 可以修改用户的请求，比如自动调整镜像名称、增加注解等，而 Validating Admission Webhook 只能做校验(true or false)，不可以进行修改操作。</strong></p><h2 id="三、编写一个-WebHook"><a href="#三、编写一个-WebHook" class="headerlink" title="三、编写一个 WebHook"></a>三、编写一个 WebHook</h2><blockquote><p><strong>郑重提示: 本部分文章请结合 <a href="https://github.com/mritd/goadmission" target="_blank" rel="noopener">goadmission</a> 框架源码进行阅读。</strong></p></blockquote><h3 id="3-1、大体思路"><a href="#3-1、大体思路" class="headerlink" title="3.1、大体思路"></a>3.1、大体思路</h3><p>在编写之前一般我们先大体了解一下流程并制订方案再去实现，边写边思考适合在细节实现上，对于整体的把控需要提前作好预习。针对于这个准入控制的 WebHook 来说，根据其官方文档大致总结重点如下:</p><ul><li>WebHook 接收者就是一个标准的 HTTP Server，请求方式是 POST + JSON</li><li>请求响应都是一个 AdmissionReview 对象</li><li>响应时需要请求时的 UID(<code>request.uid</code>)</li><li>响应时 Mutating Admission Webhook 可以包含对请求的修改信息，格式为 JSONPatch</li></ul><p>有了以上信息以后便可以知道编写 WebHook 需要的东西，根据这些信息目前我作出的大体方案如下:</p><ul><li>最起码我们要有个 HTTP Server，考虑到后续可能会同时处理多种 WebHook，所以需要一个带有路径匹配的 HTTP 框架，Gin 什么的虽然不错但是太重，最终选择简单轻量的 <code>gorilla/mux</code>。</li><li>应该做好适当的抽象，因为对于响应需要包含的 UID 等限制在每个请求都有可以提取出来自动化完成。</li><li>针对于 Mutating Admission Webhook 响应的 JSONPatch 可以弄个结构体然后直接反序列化。</li></ul><h3 id="3-2、AdmissionReview-对象"><a href="#3-2、AdmissionReview-对象" class="headerlink" title="3.2、AdmissionReview 对象"></a>3.2、AdmissionReview 对象</h3><p>基于 3.1 部分的分析可以知道，WebHook 接收和响应都是一个 AdmissionReview 对象，在查看源码以后可以看到 AdmissionReview 结构如下:</p><p><img src="https://cdn.oss.link/markdown/jro62.png" srcset="/img/loading.gif" alt="AdmissionReview"></p><p>从代码的命名中可以很清晰的看出，在请求发送到 WebHook 时我们只需要关注内部的 AdmissionRequest(实际入参)，在我们编写的 WebHook 处理完成后只需要返回包含有 AdmissionResponse(实际返回体) 的 AdmissionReview 对象即可；总的来说 <strong>AdmissionReview 对象是个套壳，请求是里面的 AdmissionRequest，响应是里面的 AdmissionResponse</strong>。</p><h3 id="3-3、Hello-World"><a href="#3-3、Hello-World" class="headerlink" title="3.3、Hello World"></a>3.3、Hello World</h3><p>有了上面的一些基础知识，我们就可以简单的实行一个什么也不干的 WebHook 方法(本地无法直接运行，重点在于思路):</p><pre><code class="hljs go"><span class="hljs-comment">// printRequest 接收 AdmissionRequest 对象并将其打印到到控制台，接着不做任何处理直接返回一个 AdmissionResponse 对象</span><span class="hljs-function"><span class="hljs-keyword">func</span> <span class="hljs-title">printRequest</span><span class="hljs-params">(request *admissionv1.AdmissionRequest)</span> <span class="hljs-params">(*admissionv1.AdmissionResponse, error)</span></span> &#123;bs, err := jsoniter.MarshalIndent(request, <span class="hljs-string">""</span>, <span class="hljs-string">"    "</span>)<span class="hljs-keyword">if</span> err != <span class="hljs-literal">nil</span> &#123;<span class="hljs-keyword">return</span> <span class="hljs-literal">nil</span>, err&#125;logger.Infof(<span class="hljs-string">"print request: %s"</span>, <span class="hljs-keyword">string</span>(bs))<span class="hljs-keyword">return</span> &amp;admissionv1.AdmissionResponse&#123;Allowed: <span class="hljs-literal">true</span>,Result: &amp;metav1.Status&#123;Code:    http.StatusOK,Message: <span class="hljs-string">"Hello World"</span>,&#125;,&#125;, <span class="hljs-literal">nil</span>&#125;</code></pre><p>上面这个 <code>printRequest</code> 方法最细粒度的控制到只面向我们的实际请求和响应；而对于 WebHook Server 来说其接到的是 http 请求，<strong>所以我们还需要在外面包装一下，将 http 请求转换为 AdmissionReview 并提取 AdmissionRequest 再调用上面的 <code>printRequest</code> 来处理，最后将返回结果重新包装为 AdmissionReview 重新返回；整体的代码如下</strong></p><pre><code class="hljs go"><span class="hljs-comment">// 通用的错误返回方法</span><span class="hljs-function"><span class="hljs-keyword">func</span> <span class="hljs-title">responseErr</span><span class="hljs-params">(handlePath, msg <span class="hljs-keyword">string</span>, httpCode <span class="hljs-keyword">int</span>, w http.ResponseWriter)</span></span> &#123;logger.Errorf(<span class="hljs-string">"handle func [%s] response err: %s"</span>, handlePath, msg)review := &amp;admissionv1.AdmissionReview&#123;Response: &amp;admissionv1.AdmissionResponse&#123;Allowed: <span class="hljs-literal">false</span>,Result: &amp;metav1.Status&#123;Message: msg,&#125;,&#125;,&#125;bs, err := jsoniter.Marshal(review)<span class="hljs-keyword">if</span> err != <span class="hljs-literal">nil</span> &#123;logger.Errorf(<span class="hljs-string">"failed to marshal response: %v"</span>, err)w.WriteHeader(http.StatusInternalServerError)_, _ = w.Write([]<span class="hljs-keyword">byte</span>(fmt.Sprintf(<span class="hljs-string">"failed to marshal response: %s"</span>, err)))&#125;w.WriteHeader(httpCode)_, err = w.Write(bs)logger.Debugf(<span class="hljs-string">"write err response: %d: %v: %v"</span>, httpCode, review, err)&#125;<span class="hljs-comment">// printRequest 接收 AdmissionRequest 对象并将其打印到到控制台，接着不做任何处理直接返回一个 AdmissionResponse 对象</span><span class="hljs-function"><span class="hljs-keyword">func</span> <span class="hljs-title">printRequest</span><span class="hljs-params">(request *admissionv1.AdmissionRequest)</span> <span class="hljs-params">(*admissionv1.AdmissionResponse, error)</span></span> &#123;bs, err := jsoniter.MarshalIndent(request, <span class="hljs-string">""</span>, <span class="hljs-string">"    "</span>)<span class="hljs-keyword">if</span> err != <span class="hljs-literal">nil</span> &#123;<span class="hljs-keyword">return</span> <span class="hljs-literal">nil</span>, err&#125;logger.Infof(<span class="hljs-string">"print request: %s"</span>, <span class="hljs-keyword">string</span>(bs))<span class="hljs-keyword">return</span> &amp;admissionv1.AdmissionResponse&#123;Allowed: <span class="hljs-literal">true</span>,Result: &amp;metav1.Status&#123;Code:    http.StatusOK,Message: <span class="hljs-string">"Hello World"</span>,&#125;,&#125;, <span class="hljs-literal">nil</span>&#125;<span class="hljs-comment">// http server 的处理方法</span><span class="hljs-function"><span class="hljs-keyword">func</span> <span class="hljs-title">headler</span><span class="hljs-params">(w http.ResponseWriter, r *http.Request)</span></span> &#123;<span class="hljs-keyword">defer</span> <span class="hljs-function"><span class="hljs-keyword">func</span><span class="hljs-params">()</span></span> &#123; _ = r.Body.Close() &#125;()w.Header().Set(<span class="hljs-string">"Content-Type"</span>, <span class="hljs-string">"application/json"</span>)<span class="hljs-comment">// 读取 body，出错直接返回</span>reqBs, err := ioutil.ReadAll(r.Body)<span class="hljs-keyword">if</span> err != <span class="hljs-literal">nil</span> &#123;responseErr(handlePath, err.Error(), http.StatusInternalServerError, w)<span class="hljs-keyword">return</span>&#125;<span class="hljs-keyword">if</span> reqBs == <span class="hljs-literal">nil</span> || <span class="hljs-built_in">len</span>(reqBs) == <span class="hljs-number">0</span> &#123;responseErr(handlePath, <span class="hljs-string">"request body is empty"</span>, http.StatusBadRequest, w)<span class="hljs-keyword">return</span>&#125;logger.Debugf(<span class="hljs-string">"request body: %s"</span>, <span class="hljs-keyword">string</span>(reqBs))<span class="hljs-comment">// 将 body 反序列化为 AdmissionReview</span>reqReview := admissionv1.AdmissionReview&#123;&#125;<span class="hljs-keyword">if</span> _, _, err := deserializer.Decode(reqBs, <span class="hljs-literal">nil</span>, &amp;reqReview); err != <span class="hljs-literal">nil</span> &#123;responseErr(handlePath, fmt.Sprintf(<span class="hljs-string">"failed to decode req: %s"</span>, err), http.StatusInternalServerError, w)<span class="hljs-keyword">return</span>&#125;<span class="hljs-keyword">if</span> reqReview.Request == <span class="hljs-literal">nil</span> &#123;responseErr(handlePath, <span class="hljs-string">"admission review request is empty"</span>, http.StatusBadRequest, w)<span class="hljs-keyword">return</span>&#125;<span class="hljs-comment">// 提取 AdmissionRequest 并调用 printRequest 处理</span>resp, err := printRequest(reqReview.Request)<span class="hljs-keyword">if</span> err != <span class="hljs-literal">nil</span> &#123;responseErr(handlePath, fmt.Sprintf(<span class="hljs-string">"admission func response: %s"</span>, err), http.StatusForbidden, w)<span class="hljs-keyword">return</span>&#125;<span class="hljs-keyword">if</span> resp == <span class="hljs-literal">nil</span> &#123;responseErr(handlePath, <span class="hljs-string">"admission func response is empty"</span>, http.StatusInternalServerError, w)<span class="hljs-keyword">return</span>&#125;<span class="hljs-comment">// 复制 AdmissionRequest 中的 UID 到 AdmissionResponse 中(必须进行，否则会导致响应无效)</span>resp.UID = reqReview.Request.UID<span class="hljs-comment">// 复制 reqReview.TypeMeta 到新的响应 AdmissionReview 中</span>respReview := admissionv1.AdmissionReview&#123;TypeMeta: reqReview.TypeMeta,Response: resp,&#125;<span class="hljs-comment">// 重新序列化响应并返回</span>respBs, err := jsoniter.Marshal(respReview)<span class="hljs-keyword">if</span> err != <span class="hljs-literal">nil</span> &#123;responseErr(handlePath, fmt.Sprintf(<span class="hljs-string">"failed to marshal response: %s"</span>, err), http.StatusInternalServerError, w)logger.Errorf(<span class="hljs-string">"the expected response is: %v"</span>, respReview)<span class="hljs-keyword">return</span>&#125;w.WriteHeader(http.StatusOK)_, err = w.Write(respBs)logger.Debugf(<span class="hljs-string">"write response: %d: %s: %v"</span>, http.StatusOK, <span class="hljs-keyword">string</span>(respBs), err)&#125;</code></pre><h3 id="3-4、抽象出框架"><a href="#3-4、抽象出框架" class="headerlink" title="3.4、抽象出框架"></a>3.4、抽象出框架</h3><p>编写了简单的 Hello World 以后可以看出，真正在编写时我们需要实现的都是处理 AdmissionRequest 并返回 AdmissionResponse 这部份(printRequest)；外部的包装为 AdmissionReview、复制 UID、复制 TypeMeta 等都是通用的方法，所以基于这一点我们可以进行适当的抽象:</p><h4 id="3-4-1、AdmissionFunc"><a href="#3-4-1、AdmissionFunc" class="headerlink" title="3.4.1、AdmissionFunc"></a>3.4.1、AdmissionFunc</h4><p>针对每一个贴合业务的 WebHook 来说，其大致有三大属性:</p><ul><li>WebHook 的类型(Mutating/Validating)</li><li>WebHook 拦截的 URL 路径(/print_request)</li><li>WebHook 核心的处理逻辑(处理 Request 和返回 Response)</li></ul><p>我们将其抽象为 AdmissionFunc 结构体以后如下所示</p><pre><code class="hljs go"><span class="hljs-comment">// WebHook 类型</span><span class="hljs-keyword">const</span> (Mutating   AdmissionFuncType = <span class="hljs-string">"Mutating"</span>Validating AdmissionFuncType = <span class="hljs-string">"Validating"</span>)<span class="hljs-comment">// 每一个对应到我们业务的 WebHook 抽象的 struct</span><span class="hljs-keyword">type</span> AdmissionFunc <span class="hljs-keyword">struct</span> &#123;Type AdmissionFuncTypePath <span class="hljs-keyword">string</span>Func <span class="hljs-function"><span class="hljs-keyword">func</span><span class="hljs-params">(request *admissionv1.AdmissionRequest)</span> <span class="hljs-params">(*admissionv1.AdmissionResponse, error)</span></span>&#125;</code></pre><h4 id="3-4-2、HandleFunc"><a href="#3-4-2、HandleFunc" class="headerlink" title="3.4.2、HandleFunc"></a>3.4.2、HandleFunc</h4><p>我们知道 WebHook 是基于 HTTP 的，所以上面抽象出的 AdmissionFunc 还不能直接用在 HTTP 请求代码中；如果直接偶合到 HTTP 请求代码中，我们就没法为 HTTP 代码再增加其他拦截路径等等特殊的底层设置；<strong>所以站在 HTTP 层面来说还需要抽象一个 “更高层面的且包含 AdmissionFunc 全部能力的 HandleFunc” 来使用；HandleFunc 抽象 HTTP 层面的需求:</strong></p><ul><li>HTTP 请求方法</li><li>HTTP 请求路径</li><li>HTTP 处理方法</li></ul><p>以下为 HandleFunc 的抽象:</p><pre><code class="hljs go"><span class="hljs-keyword">type</span> HandleFunc <span class="hljs-keyword">struct</span> &#123;Path   <span class="hljs-keyword">string</span>Method <span class="hljs-keyword">string</span>Func   <span class="hljs-function"><span class="hljs-keyword">func</span><span class="hljs-params">(w http.ResponseWriter, r *http.Request)</span></span>&#125;</code></pre><h3 id="3-5、goadmission-框架"><a href="#3-5、goadmission-框架" class="headerlink" title="3.5、goadmission 框架"></a>3.5、goadmission 框架</h3><p>有了以上两个角度的抽象，再结合 命令行参数解析、日志处理、配置文件读取等等，我揉合出了一个 <a href="https://github.com/mritd/goadmission" target="_blank" rel="noopener">goadmission</a> 框架，以方便动态准入控制的快速开发。</p><h4 id="3-5-1、基本结构"><a href="#3-5-1、基本结构" class="headerlink" title="3.5.1、基本结构"></a>3.5.1、基本结构</h4><pre><code class="hljs sh">.├── main.go└── pkg    ├── adfunc    │   ├── adfuncs.go    │   ├── adfuncs_json.go    │   ├── func_check_deploy_time.go    │   ├── func_disable_service_links.go    │   ├── func_image_rename.go    │   └── func_print_request.go    ├── conf    │   └── conf.go    ├── route    │   ├── route_available.go    │   ├── route_health.go    │   └── router.go    └── zaplogger        ├── config.go        └── logger.go5 directories, 13 files</code></pre><ul><li>main.go 为程序运行入口，在此设置命令行 flag 参数等</li><li>pkg/conf 为框架配置包，所有的配置读取只读取这个包即可</li><li>pkg/zaplogger zap log 库的日志抽象和处理(copy 自 operator-sdk)</li><li>pkg/route http 级别的路由抽象(HandleFunc)</li><li>pkg/adfunc 动态准入控制 WebHook 级别的抽(AdmissionFunc)</li></ul><h4 id="3-5-2、增加动态准入控制"><a href="#3-5-2、增加动态准入控制" class="headerlink" title="3.5.2、增加动态准入控制"></a>3.5.2、增加动态准入控制</h4><p>由于框架已经作好了路由注册等相关抽象，所以只需要新建 go 文件，然后通过 init 方法注册到全局 WebHook 组中即可，新编写的 WebHook 对已有代码不会有任何侵入:</p><p><img src="https://cdn.oss.link/markdown/lg6zc.png" srcset="/img/loading.gif" alt="add_adfunc"></p><p><strong>需要注意的是所有 validating 类型的 WebHook 会在 URL 路径前自动拼接 <code>/validating</code> 路径，mutating 类型的 WebHook 会在 URL 路径前自动拼接 <code>/mutating</code> 路径；</strong>这么做是为了避免在更高层级的 HTTP Route 上添加冲突的路由。</p><p><img src="https://cdn.oss.link/markdown/nd5ez.png" srcset="/img/loading.gif" alt="auto_fix_url"></p><h4 id="3-5-3、实现-image-自动修改"><a href="#3-5-3、实现-image-自动修改" class="headerlink" title="3.5.3、实现 image 自动修改"></a>3.5.3、实现 image 自动修改</h4><p>所以一切准备就绪以后，就需要 “不忘初心”，撸一个自动修改镜像名称的 WebHook:</p><pre><code class="hljs go"><span class="hljs-keyword">package</span> adfunc<span class="hljs-keyword">import</span> (<span class="hljs-string">"fmt"</span><span class="hljs-string">"net/http"</span><span class="hljs-string">"strings"</span><span class="hljs-string">"sync"</span><span class="hljs-string">"time"</span><span class="hljs-string">"github.com/mritd/goadmission/pkg/conf"</span>jsoniter <span class="hljs-string">"github.com/json-iterator/go"</span>corev1 <span class="hljs-string">"k8s.io/api/core/v1"</span>metav1 <span class="hljs-string">"k8s.io/apimachinery/pkg/apis/meta/v1"</span><span class="hljs-string">"github.com/mritd/goadmission/pkg/route"</span>admissionv1 <span class="hljs-string">"k8s.io/api/admission/v1"</span>)<span class="hljs-comment">// 只初始化一次 renameMap</span><span class="hljs-keyword">var</span> renameOnce sync.Once<span class="hljs-comment">// renameMap 保存镜像名称的替换规则，目前粗略实现为纯文本替换</span><span class="hljs-keyword">var</span> renameMap <span class="hljs-keyword">map</span>[<span class="hljs-keyword">string</span>]<span class="hljs-keyword">string</span><span class="hljs-function"><span class="hljs-keyword">func</span> <span class="hljs-title">init</span><span class="hljs-params">()</span></span> &#123;route.Register(route.AdmissionFunc&#123;Type: route.Mutating,Path: <span class="hljs-string">"/rename"</span>,Func: <span class="hljs-function"><span class="hljs-keyword">func</span><span class="hljs-params">(request *admissionv1.AdmissionRequest)</span> <span class="hljs-params">(*admissionv1.AdmissionResponse, error)</span></span> &#123;<span class="hljs-comment">// init rename rules map</span>renameOnce.Do(<span class="hljs-function"><span class="hljs-keyword">func</span><span class="hljs-params">()</span></span> &#123;renameMap = <span class="hljs-built_in">make</span>(<span class="hljs-keyword">map</span>[<span class="hljs-keyword">string</span>]<span class="hljs-keyword">string</span>, <span class="hljs-number">10</span>)<span class="hljs-comment">// 将镜像重命名规则初始化到 renameMap 中，方便后续读取</span><span class="hljs-comment">// rename rule example: k8s.gcr.io/=gcrxio/k8s.gcr.io_</span><span class="hljs-keyword">for</span> _, s := <span class="hljs-keyword">range</span> conf.ImageRename &#123;ss := strings.Split(s, <span class="hljs-string">"="</span>)<span class="hljs-keyword">if</span> <span class="hljs-built_in">len</span>(ss) != <span class="hljs-number">2</span> &#123;logger.Fatalf(<span class="hljs-string">"failed to parse image name rename rules: %s"</span>, s)&#125;renameMap[ss[<span class="hljs-number">0</span>]] = ss[<span class="hljs-number">1</span>]&#125;&#125;)<span class="hljs-comment">// 这个准入控制的 WebHook 只针对 Pod 处理，非 Pod 类请求直接返回错误</span><span class="hljs-keyword">switch</span> request.Kind.Kind &#123;<span class="hljs-keyword">case</span> <span class="hljs-string">"Pod"</span>:<span class="hljs-comment">// 从 request 中反序列化出 Pod 实例</span><span class="hljs-keyword">var</span> pod corev1.Poderr := jsoniter.Unmarshal(request.Object.Raw, &amp;pod)<span class="hljs-keyword">if</span> err != <span class="hljs-literal">nil</span> &#123;errMsg := fmt.Sprintf(<span class="hljs-string">"[route.Mutating] /rename: failed to unmarshal object: %v"</span>, err)logger.Error(errMsg)<span class="hljs-keyword">return</span> &amp;admissionv1.AdmissionResponse&#123;Allowed: <span class="hljs-literal">false</span>,Result: &amp;metav1.Status&#123;Code:    http.StatusBadRequest,Message: errMsg,&#125;,&#125;, <span class="hljs-literal">nil</span>&#125;<span class="hljs-comment">// 后来我发现带有下面这个注解的 Pod 是没法更改成功的，这种 Pod 是由 kubelet 直接</span><span class="hljs-comment">// 启动的 static pod，在 api server 中只能看到它的 "mirror"，不能改的</span><span class="hljs-comment">// skip static pod</span><span class="hljs-keyword">for</span> k := <span class="hljs-keyword">range</span> pod.Annotations &#123;<span class="hljs-keyword">if</span> k == <span class="hljs-string">"kubernetes.io/config.mirror"</span> &#123;errMsg := fmt.Sprintf(<span class="hljs-string">"[route.Mutating] /rename: pod %s has kubernetes.io/config.mirror annotation, skip image rename"</span>, pod.Name)logger.Warn(errMsg)<span class="hljs-keyword">return</span> &amp;admissionv1.AdmissionResponse&#123;Allowed: <span class="hljs-literal">true</span>,Result: &amp;metav1.Status&#123;Code:    http.StatusOK,Message: errMsg,&#125;,&#125;, <span class="hljs-literal">nil</span>&#125;&#125;<span class="hljs-comment">// 遍历所有 Pod，然后生成 JSONPatch</span><span class="hljs-comment">// 注意: 返回结果必须是 JSONPatch，k8s api server 再将 JSONPatch 应用到 Pod 上 </span><span class="hljs-comment">// 由于有多个 Pod，所以最终会产生一个补丁数组</span><span class="hljs-keyword">var</span> patches []Patch<span class="hljs-keyword">for</span> i, c := <span class="hljs-keyword">range</span> pod.Spec.Containers &#123;<span class="hljs-keyword">for</span> s, t := <span class="hljs-keyword">range</span> renameMap &#123;<span class="hljs-keyword">if</span> strings.HasPrefix(c.Image, s) &#123;patches = <span class="hljs-built_in">append</span>(patches, Patch&#123;<span class="hljs-comment">// 指定 JSONPatch 动作为 replace </span>Option: PatchOptionReplace,<span class="hljs-comment">// 打补丁的绝对位置</span>Path:   fmt.Sprintf(<span class="hljs-string">"/spec/containers/%d/image"</span>, i),<span class="hljs-comment">// replace 为处理过的镜像名</span>Value:  strings.Replace(c.Image, s, t, <span class="hljs-number">1</span>),&#125;)<span class="hljs-comment">// 为了后期调试和留存历史，我们再为修改过的 Pod 加个注解</span>patches = <span class="hljs-built_in">append</span>(patches, Patch&#123;Option: PatchOptionAdd,Path:   <span class="hljs-string">"/metadata/annotations"</span>,Value: <span class="hljs-keyword">map</span>[<span class="hljs-keyword">string</span>]<span class="hljs-keyword">string</span>&#123;fmt.Sprintf(<span class="hljs-string">"rename-mutatingwebhook-%d.mritd.me"</span>, time.Now().Unix()): fmt.Sprintf(<span class="hljs-string">"%d-%s-%s"</span>, i, strings.ReplaceAll(s, <span class="hljs-string">"/"</span>, <span class="hljs-string">"_"</span>), strings.ReplaceAll(t, <span class="hljs-string">"/"</span>, <span class="hljs-string">"_"</span>)),&#125;,&#125;)<span class="hljs-keyword">break</span>&#125;&#125;&#125;<span class="hljs-comment">// 将所有 JSONPatch 序列化成 json，然后返回即可</span>patch, err := jsoniter.Marshal(patches)<span class="hljs-keyword">if</span> err != <span class="hljs-literal">nil</span> &#123;errMsg := fmt.Sprintf(<span class="hljs-string">"[route.Mutating] /rename: failed to marshal patch: %v"</span>, err)logger.Error(errMsg)<span class="hljs-keyword">return</span> &amp;admissionv1.AdmissionResponse&#123;Allowed: <span class="hljs-literal">false</span>,Result: &amp;metav1.Status&#123;Code:    http.StatusInternalServerError,Message: errMsg,&#125;,&#125;, <span class="hljs-literal">nil</span>&#125;logger.Infof(<span class="hljs-string">"[route.Mutating] /rename: patches: %s"</span>, <span class="hljs-keyword">string</span>(patch))<span class="hljs-keyword">return</span> &amp;admissionv1.AdmissionResponse&#123;Allowed:   <span class="hljs-literal">true</span>,Patch:     patch,PatchType: JSONPatch(),Result: &amp;metav1.Status&#123;Code:    http.StatusOK,Message: <span class="hljs-string">"success"</span>,&#125;,&#125;, <span class="hljs-literal">nil</span><span class="hljs-keyword">default</span>:errMsg := fmt.Sprintf(<span class="hljs-string">"[route.Mutating] /rename: received wrong kind request: %s, Only support Kind: Pod"</span>, request.Kind.Kind)logger.Error(errMsg)<span class="hljs-keyword">return</span> &amp;admissionv1.AdmissionResponse&#123;Allowed: <span class="hljs-literal">false</span>,Result: &amp;metav1.Status&#123;Code:    http.StatusForbidden,Message: errMsg,&#125;,&#125;, <span class="hljs-literal">nil</span>&#125;&#125;,&#125;)&#125;</code></pre><h2 id="四、总结"><a href="#四、总结" class="headerlink" title="四、总结"></a>四、总结</h2><ul><li>动态准入控制其实就是个 WebHook，我们弄个 HTTP Server 接收 AdmissionRequest 响应 AdmissionResponse 就行。</li><li>Request、Response 会包装到 AdmissionReview 中，我们还需要做一些边缘处理，比如复制 UID、TypeMeta 等</li><li>MutatingWebHook 想要修改东西时，要返回描述修改操作的 JSONPatch 补丁</li><li>单个 WebHook 很简单，写多个的时候要自己抽好框架，尽量优雅的作好复用和封装</li></ul>]]></content>
    
    
    <categories>
      
      <category>Kubernetes</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Admission</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>使用 etcdadm 三分钟搭建 etcd 集群</title>
    <link href="/2020/08/19/use-etcdadm-to-build-etcd-cluster-in-3-minutes/"/>
    <url>/2020/08/19/use-etcdadm-to-build-etcd-cluster-in-3-minutes/</url>
    
    <content type="html"><![CDATA[<h2 id="一、介绍"><a href="#一、介绍" class="headerlink" title="一、介绍"></a>一、介绍</h2><p>在搭建 Kubernetes 集群的过程中首先要搞定 Etcd 集群，虽然说 kubeadm 工具已经提供了默认和 master 节点绑定的 Etcd 集群自动搭建方式，但是我个人一直是手动将 Etcd 集群搭建在宿主机；<strong>因为这个玩意太重要了，毫不夸张的说 kubernetes 所有组件崩溃我们都能在一定时间以后排查问题恢复，但是一旦 Etcd 集群没了那么 Kubernetes 集群也就真没了。</strong></p><p>在很久以前我创建了 <a href="https://github.com/Gozap/edep" target="_blank" rel="noopener">edep</a> 工具来实现 Etcd 集群的辅助部署，再后来由于我们的底层系统偶合了 Ubuntu，所以创建了 <a href="https://github.com/mritd/etcd-deb" target="_blank" rel="noopener">etcd-deb</a> 项目来自动打 deb 包来直接安装；最近逛了一下 Kubernetes 的相关项目，发现跟我的 edep 差不多的项目 <a href="https://github.com/kubernetes-sigs/etcdadm" target="_blank" rel="noopener">etcdadm</a>，试了一下 “真香”。</p><h2 id="二、安装"><a href="#二、安装" class="headerlink" title="二、安装"></a>二、安装</h2><p><a href="https://github.com/kubernetes-sigs/etcdadm" target="_blank" rel="noopener">etcdadm</a> 项目是使用 go 编写的，所以很明显只有一个二进制下载下来就能用:</p><pre><code class="hljs sh">wget https://github.com/kubernetes-sigs/etcdadm/releases/download/v0.1.3/etcdadm-linux-amd64chmod +x etcdadm-linux-amd64</code></pre><h2 id="三、使用"><a href="#三、使用" class="headerlink" title="三、使用"></a>三、使用</h2><h3 id="3-1、启动引导节点"><a href="#3-1、启动引导节点" class="headerlink" title="3.1、启动引导节点"></a>3.1、启动引导节点</h3><p>类似 kubeadm 一样，etcdadm 也是先启动第一个节点，然后后续节点直接 join 即可；第一个节点启动只需要执行 <code>etcdadm init</code> 命令即可:</p><pre><code class="hljs sh">k1.node ➜  ~ ./etcdadm-linux-amd64 initINFO[0000] [install] extracting etcd archive /var/cache/etcdadm/etcd/v3.3.8/etcd-v3.3.8-linux-amd64.tar.gz to /tmp/etcd664686683INFO[0001] [install] verifying etcd 3.3.8 is installed <span class="hljs-keyword">in</span> /opt/bin/INFO[0001] [certificates] creating PKI assetsINFO[0001] creating a self signed etcd CA certificate and key files[certificates] Generated ca certificate and key.INFO[0001] creating a new server certificate and key files <span class="hljs-keyword">for</span> etcd[certificates] Generated server certificate and key.[certificates] server serving cert is signed <span class="hljs-keyword">for</span> DNS names [k1.node] and IPs [127.0.0.1 172.16.10.21]INFO[0002] creating a new certificate and key files <span class="hljs-keyword">for</span> etcd peering[certificates] Generated peer certificate and key.[certificates] peer serving cert is signed <span class="hljs-keyword">for</span> DNS names [k1.node] and IPs [172.16.10.21]INFO[0002] creating a new client certificate <span class="hljs-keyword">for</span> the etcdctl[certificates] Generated etcdctl-etcd-client certificate and key.INFO[0002] creating a new client certificate <span class="hljs-keyword">for</span> the apiserver calling etcd[certificates] Generated apiserver-etcd-client certificate and key.[certificates] valid certificates and keys now exist <span class="hljs-keyword">in</span> <span class="hljs-string">"/etc/etcd/pki"</span>INFO[0006] [health] Checking <span class="hljs-built_in">local</span> etcd endpoint healthINFO[0006] [health] Local etcd endpoint is healthyINFO[0006] To add another member to the cluster, copy the CA cert/key to its certificate dir and run:INFO[0006]      etcdadm join https://172.16.10.21:2379</code></pre><p>从命令行输出可以看到不同阶段 etcdadm 的相关日志输出；在 <code>init</code> 命令时可以指定一些特定参数来覆盖默认行为，比如版本号、安装目录等:</p><pre><code class="hljs sh">k1.node ➜  ~ ./etcdadm-linux-amd64 init --<span class="hljs-built_in">help</span>Initialize a new etcd clusterUsage:  etcdadm init [flags]Flags:      --certs-dir string                    certificates directory (default <span class="hljs-string">"/etc/etcd/pki"</span>)      --disk-priorities stringArray         Setting etcd disk priority (default [Nice=-10,IOSchedulingClass=best-effort,IOSchedulingPriority=2])      --download-connect-timeout duration   Maximum time <span class="hljs-keyword">in</span> seconds that you allow the connection to the server to take. (default 10s)  -h, --<span class="hljs-built_in">help</span>                                <span class="hljs-built_in">help</span> <span class="hljs-keyword">for</span> init      --install-dir string                  install directory (default <span class="hljs-string">"/opt/bin/"</span>)      --name string                         etcd member name      --release-url string                  URL used to download etcd (default <span class="hljs-string">"https://github.com/coreos/etcd/releases/download"</span>)      --server-cert-extra-sans strings      optional extra Subject Alternative Names <span class="hljs-keyword">for</span> the etcd server signing cert, can be multiple comma separated DNS names or IPs      --skip-hash-check                     Ignore snapshot integrity <span class="hljs-built_in">hash</span> value (required <span class="hljs-keyword">if</span> copied from data directory)      --snapshot string                     Etcd v3 snapshot file used to initialize member      --version string                      etcd version (default <span class="hljs-string">"3.3.8"</span>)Global Flags:  -l, --<span class="hljs-built_in">log</span>-level string   <span class="hljs-built_in">set</span> <span class="hljs-built_in">log</span> level <span class="hljs-keyword">for</span> output, permitted values debug, info, warn, error, fatal and panic (default <span class="hljs-string">"info"</span>)</code></pre><h3 id="3-2、其他节点加入"><a href="#3-2、其他节点加入" class="headerlink" title="3.2、其他节点加入"></a>3.2、其他节点加入</h3><p>在首个节点启动完成后，将集群 ca 证书复制到其他节点然后执行 <code>etcdadm join ENDPOINT_ADDRESS</code> 即可:</p><pre><code class="hljs sh"><span class="hljs-comment"># 复制 ca 证书</span>k1.node ➜  ~ rsync -avR /etc/etcd/pki/ca.* 172.16.10.22:/root@172.16.10.22<span class="hljs-string">'s password:</span><span class="hljs-string">sending incremental file list</span><span class="hljs-string">/etc/etcd/</span><span class="hljs-string">/etc/etcd/pki/</span><span class="hljs-string">/etc/etcd/pki/ca.crt</span><span class="hljs-string">/etc/etcd/pki/ca.key</span><span class="hljs-string"></span><span class="hljs-string">sent 2,932 bytes  received 67 bytes  856.86 bytes/sec</span><span class="hljs-string">total size is 2,684  speedup is 0.89</span><span class="hljs-string"></span><span class="hljs-string"># 执行 join</span><span class="hljs-string">k2.node ➜  ~ ./etcdadm-linux-amd64 join https://172.16.10.21:2379</span><span class="hljs-string">INFO[0000] [certificates] creating PKI assets</span><span class="hljs-string">INFO[0000] creating a self signed etcd CA certificate and key files</span><span class="hljs-string">[certificates] Using the existing ca certificate and key.</span><span class="hljs-string">INFO[0000] creating a new server certificate and key files for etcd</span><span class="hljs-string">[certificates] Generated server certificate and key.</span><span class="hljs-string">[certificates] server serving cert is signed for DNS names [k2.node] and IPs [172.16.10.22 127.0.0.1]</span><span class="hljs-string">INFO[0000] creating a new certificate and key files for etcd peering</span><span class="hljs-string">[certificates] Generated peer certificate and key.</span><span class="hljs-string">[certificates] peer serving cert is signed for DNS names [k2.node] and IPs [172.16.10.22]</span><span class="hljs-string">INFO[0000] creating a new client certificate for the etcdctl</span><span class="hljs-string">[certificates] Generated etcdctl-etcd-client certificate and key.</span><span class="hljs-string">INFO[0001] creating a new client certificate for the apiserver calling etcd</span><span class="hljs-string">[certificates] Generated apiserver-etcd-client certificate and key.</span><span class="hljs-string">[certificates] valid certificates and keys now exist in "/etc/etcd/pki"</span><span class="hljs-string">INFO[0001] [membership] Checking if this member was added</span><span class="hljs-string">INFO[0001] [membership] Member was not added</span><span class="hljs-string">INFO[0001] Removing existing data dir "/var/lib/etcd"</span><span class="hljs-string">INFO[0001] [membership] Adding member</span><span class="hljs-string">INFO[0001] [membership] Checking if member was started</span><span class="hljs-string">INFO[0001] [membership] Member was not started</span><span class="hljs-string">INFO[0001] [membership] Removing existing data dir "/var/lib/etcd"</span><span class="hljs-string">INFO[0001] [install] extracting etcd archive /var/cache/etcdadm/etcd/v3.3.8/etcd-v3.3.8-linux-amd64.tar.gz to /tmp/etcd315786364</span><span class="hljs-string">INFO[0003] [install] verifying etcd 3.3.8 is installed in /opt/bin/</span><span class="hljs-string">INFO[0006] [health] Checking local etcd endpoint health</span><span class="hljs-string">INFO[0006] [health] Local etcd endpoint is healthy</span></code></pre><h2 id="四、细节分析"><a href="#四、细节分析" class="headerlink" title="四、细节分析"></a>四、细节分析</h2><h3 id="4-1、默认配置"><a href="#4-1、默认配置" class="headerlink" title="4.1、默认配置"></a>4.1、默认配置</h3><p>在目前 etcdadm 尚未支持配置文件，目前所有默认配置存放在 <a href="https://github.com/kubernetes-sigs/etcdadm/blob/master/constants/constants.go#L22" target="_blank" rel="noopener">constants.go</a> 中，这里面包含了默认安装位置、systemd 配置、环境变量配置等，限于篇幅请自行查看代码；下面简单介绍一些一些刚须的配置:</p><h4 id="4-1-1、etcdctl"><a href="#4-1-1、etcdctl" class="headerlink" title="4.1.1、etcdctl"></a>4.1.1、etcdctl</h4><p>etcdctl 默认安装在 <code>/opt/bin</code> 目录下，同时你会发现该目录下还存在一个 <code>etcdctl.sh</code> 脚本，<strong>这个脚本将会自动读取 etcdctl 配置文件(<code>/etc/etcd/etcdctl.env</code>)，所以推荐使用这个脚本来替代 etcdctl 命令。</strong></p><h4 id="4-1-2、数据目录"><a href="#4-1-2、数据目录" class="headerlink" title="4.1.2、数据目录"></a>4.1.2、数据目录</h4><p>默认的数据目录存储在 <code>/var/lib/etcd</code> 目录，目前 etcdadm 尚未提供任何可配置方式，当然你可以自己改源码。</p><h4 id="4-2-3、配置文件"><a href="#4-2-3、配置文件" class="headerlink" title="4.2.3、配置文件"></a>4.2.3、配置文件</h4><p>配置文件总共有两个，一个是 <code>/etc/etcd/etcdctl.env</code> 用于 <code>/opt/bin/etcdctl.sh</code> 读取；另一个是 <code>/etc/etcd/etcd.env</code> 用于 systemd 读取并启动 etcd server。</p><h3 id="4-2、Join-流程"><a href="#4-2、Join-流程" class="headerlink" title="4.2、Join 流程"></a>4.2、Join 流程</h3><blockquote><p>其实很久以前由于我自己部署方式导致了我一直以来理解的一个错误，我一直以为 etcd server 证书要包含所有 server 地址，当然这个想法是怎么来的我也不知道，但是当我看了以下 Join 操作源码以后突然意识到 “为什么要包含所有？包含当前 server 不就行了么。”；当然对于 HTTPS 证书的理解一直是明白的，但是很奇怪就是不知道怎么就产生了这个想法(哈哈，我自己都觉的不可思议)…</p></blockquote><ul><li>由于预先拷贝了 ca 证书，所以 join 开始前 etcdadm 使用这个 ca 证书会签发自己需要的所有证书。</li><li>接下来 etcdadmin 通过 etcdctl-etcd-client 证书创建 client，然后调用 <code>MemberAdd</code> 添加新集群</li><li>最后老套路下载安装+启动就完成了</li></ul><h3 id="4-3、目前不足"><a href="#4-3、目前不足" class="headerlink" title="4.3、目前不足"></a>4.3、目前不足</h3><p>目前 etcdadm 虽然已经基本生产可用，但是仍有些不足的地方:</p><ul><li>不支持配置文件，很多东西无法定制</li><li>join 加入集群是在内部 api 完成，并未持久化到物理配置文件，后续重建可能忘记节点 ip</li><li>集群证书目前不支持自动续期，默认证书为 1 年很容易过期</li><li>下载动作调用了系统命令(curl)依赖性有点强</li><li>日志格式有点不友好，比如 level 和日期</li></ul>]]></content>
    
    
    <categories>
      
      <category>Kubernetes</category>
      
    </categories>
    
    
    <tags>
      
      <tag>etcd</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>如何编写 CSI 插件</title>
    <link href="/2020/08/19/how-to-write-a-csi-driver-for-kubernetes/"/>
    <url>/2020/08/19/how-to-write-a-csi-driver-for-kubernetes/</url>
    
    <content type="html"><![CDATA[<h2 id="一、为什么需要-CSI"><a href="#一、为什么需要-CSI" class="headerlink" title="一、为什么需要 CSI"></a>一、为什么需要 CSI</h2><p>在 Kubernetes 以前的版本中，其所有受官方支持的存储驱动全部在 Kubernetes 的主干代码中，其他第三方开发的自定义插件通过 <a href="https://github.com/kubernetes/community/blob/master/contributors/devel/sig-storage/flexvolume.md" target="_blank" rel="noopener">FlexVolume</a> 插件的形势提供服务；<strong>相对于 kubernetes 的源码树来说，内置的存储我们称之为 “树内存储”，外部第三方实现我们称之为 “树外存储”；</strong>在很长一段时间里树内存储和树外存储并行开发和使用，但是随着时间推移渐渐的就出现了很严重的问题:</p><ul><li>想要添加官方支持的存储必须在树内修改，这意味着需要 Kubernetes 发版</li><li>如果树内存储出现问题则也必须等待 Kubernetes 发版才能修复</li></ul><p>为了解决这种尴尬的问题，Kubernetes 必须抽象出一个合适的存储接口，并将所有存储驱动全部适配到这个接口上，存储驱动最好与 Kubernetes 之间进行 RPC 调用完成解耦，这样就造就了 CSI(Container Storage Interface)。</p><h2 id="二、CSI-基础知识"><a href="#二、CSI-基础知识" class="headerlink" title="二、CSI 基础知识"></a>二、CSI 基础知识</h2><h3 id="2-1、CSI-Sidecar-Containers"><a href="#2-1、CSI-Sidecar-Containers" class="headerlink" title="2.1、CSI Sidecar Containers"></a>2.1、CSI Sidecar Containers</h3><p>在开发 CSI 之前我们最好熟悉一下 CSI 开发中的一些常识；了解过 Kubernetes API 开发的朋友应该清楚，所有的资源定义(Deployment、Service…)在 Kubernetes 中其实就是一个 Object，此时可以将 Kubernetes 看作是一个 Database，无论是 Operator 还是 CSI 其核心本质都是不停的 Watch 特定的 Object，一但 kubectl 或者其他客户端 “动了” 这个 Object，我们的对应实现程序就 Watch 到变更然后作出相应的响应；<strong>对于 CSI 编写者来说，这些 Watch 动作已经不必自己实现 <a href="https://kubernetes.io/docs/concepts/extend-kubernetes/api-extension/custom-resources/#custom-controllers" target="_blank" rel="noopener">Custom Controller</a>，官方为我们提供了 <a href="https://kubernetes-csi.github.io/docs/sidecar-containers.html" target="_blank" rel="noopener">CSI Sidecar Containers</a>；</strong>并且在新版本中这些 Sidecar Containers 实现极其完善，比如自动的多节点 HA(Etcd 选举)等。</p><p><strong>所以到迄今为止，所谓的 CSI 插件开发事实上并非面向 Kubernetes API 开发，而是面向 Sidecar Containers 的 gRPC 开发，Sidecar Containers 一般会和我们自己开发的 CSI 驱动程序在同一个 Pod 中启动，然后 Sidecar Containers Watch API 中 CSI 相关 Object 的变动，接着通过本地 unix 套接字调用我们编写的 CSI 驱动：</strong></p><p><img src="https://cdn.oss.link/markdown/10w5g.png" srcset="/img/loading.gif" alt="CSI_Sidecar_Containers"></p><p>目前官方提供的 Sidecar Containers 如下:</p><ul><li><a href="https://kubernetes-csi.github.io/docs/external-provisioner.html" target="_blank" rel="noopener">external-provisioner</a></li><li><a href="https://kubernetes-csi.github.io/docs/external-attacher.html" target="_blank" rel="noopener">external-attacher</a></li><li><a href="https://kubernetes-csi.github.io/docs/external-snapshotter.html" target="_blank" rel="noopener">external-snapshotter</a></li><li><a href="https://kubernetes-csi.github.io/docs/external-resizer.html" target="_blank" rel="noopener">external-resizer</a></li><li><a href="https://kubernetes-csi.github.io/docs/node-driver-registrar.html" target="_blank" rel="noopener">node-driver-registrar</a></li><li><a href="https://kubernetes-csi.github.io/docs/cluster-driver-registrar.html" target="_blank" rel="noopener">cluster-driver-registrar (deprecated)</a></li><li><a href="https://kubernetes-csi.github.io/docs/livenessprobe.html" target="_blank" rel="noopener">livenessprobe</a></li></ul><p>每个 Sidecar Container 的作用可以通过对应链接查看，需要注意的是 cluster-driver-registrar 已经停止维护，请改用 node-driver-registrar。</p><h3 id="2-2、CSI-处理阶段"><a href="#2-2、CSI-处理阶段" class="headerlink" title="2.2、CSI 处理阶段"></a>2.2、CSI 处理阶段</h3><blockquote><p>在理解了 CSI Sidecar Containers 以后，我们仍需要大致的了解 CSI 挂载过程中的大致流程，以此来针对性的实现每个阶段所需要的功能；CSI 整个流程实际上大致分为以下三大阶段:</p></blockquote><h4 id="2-2-1、Provisioning-and-Deleting"><a href="#2-2-1、Provisioning-and-Deleting" class="headerlink" title="2.2.1、Provisioning and Deleting"></a>2.2.1、Provisioning and Deleting</h4><p><a href="https://github.com/kubernetes/community/blob/master/contributors/design-proposals/storage/container-storage-interface.md#provisioning-and-deleting" target="_blank" rel="noopener">Provisioning and Deleting</a> 阶段实现与外部存储供应商协调卷的创建/删除处理，简单地说就是需要实现 CreateVolume 和 DeleteVolume；假设外部存储供应商为阿里云存储那么此阶段应该完成在阿里云存储商创建一个指定大小的块设备，或者在用户删除 volume 时完成在阿里云存储上删除这个块设备；除此之外此阶段还应当响应存储拓扑分布从而保证 volume 分布在正确的集群拓扑上(此处描述不算清晰，推荐查看设计文档)。</p><h4 id="2-2-2、Attaching-and-Detaching"><a href="#2-2-2、Attaching-and-Detaching" class="headerlink" title="2.2.2、Attaching and Detaching"></a>2.2.2、Attaching and Detaching</h4><p><a href="https://github.com/kubernetes/community/blob/master/contributors/design-proposals/storage/container-storage-interface.md#attaching-and-detaching" target="_blank" rel="noopener">Attaching and Detaching</a> 阶段实现将外部存储供应商提供好的卷设备挂载到本地或者从本地卸载，简单地说就是实现 ControllerPublishVolume 和 ControllerUnpublishVolume；同样以外部存储供应商为阿里云存储为例，在 Provisioning 阶段创建好的卷的块设备，在此阶段应该实现将其挂载到服务器本地或从本地卸载，在必要的情况下还需要进行格式化等操作。</p><h4 id="2-2-3、Mount-and-Umount"><a href="#2-2-3、Mount-and-Umount" class="headerlink" title="2.2.3、Mount and Umount"></a>2.2.3、Mount and Umount</h4><p>这个阶段在 CSI 设计文档中没有做详细描述，在前两个阶段完成后，当一个目标 Pod 在某个 Node 节点上调度时，kubelet 会根据前两个阶段返回的结果来创建这个 Pod；同样以外部存储供应商为阿里云存储为例，此阶段将会把已经 Attaching 的本地块设备以目录形式挂载到 Pod 中或者从 Pod 中卸载这个块设备。</p><h3 id="2-3、CSI-gRPC-Server"><a href="#2-3、CSI-gRPC-Server" class="headerlink" title="2.3、CSI gRPC Server"></a>2.3、CSI gRPC Server</h3><p>CSI 的三大阶段实际上更细粒度的划分到 CSI Sidecar Containers 中，上面已经说过我们开发 CSI 实际上是面向 CSI Sidecar Containers 编程，针对于 CSI Sidecar Containers 我们主要需要实现以下三个 gRPC Server:</p><h4 id="2-3-1、Identity-Server"><a href="#2-3-1、Identity-Server" class="headerlink" title="2.3.1、Identity Server"></a>2.3.1、Identity Server</h4><p>在当前 CSI Spec v1.3.0 中 IdentityServer 定义如下:</p><pre><code class="hljs go"><span class="hljs-comment">// IdentityServer is the server API for Identity service.</span><span class="hljs-keyword">type</span> IdentityServer <span class="hljs-keyword">interface</span> &#123;GetPluginInfo(context.Context, *GetPluginInfoRequest) (*GetPluginInfoResponse, error)GetPluginCapabilities(context.Context, *GetPluginCapabilitiesRequest) (*GetPluginCapabilitiesResponse, error)Probe(context.Context, *ProbeRequest) (*ProbeResponse, error)&#125;</code></pre><p>从代码上可以看出 IdentityServer 主要负责像 Kubernetes 提供 CSI 插件名称可选功能等，所以此 Server 是必须实现的。</p><h4 id="2-3-2、Node-Server"><a href="#2-3-2、Node-Server" class="headerlink" title="2.3.2、Node Server"></a>2.3.2、Node Server</h4><p>同样当前 CSI v1.3.0 Spec 中 NodeServer 定义如下:</p><pre><code class="hljs go"><span class="hljs-comment">// NodeServer is the server API for Node service.</span><span class="hljs-keyword">type</span> NodeServer <span class="hljs-keyword">interface</span> &#123;NodeStageVolume(context.Context, *NodeStageVolumeRequest) (*NodeStageVolumeResponse, error)NodeUnstageVolume(context.Context, *NodeUnstageVolumeRequest) (*NodeUnstageVolumeResponse, error)NodePublishVolume(context.Context, *NodePublishVolumeRequest) (*NodePublishVolumeResponse, error)NodeUnpublishVolume(context.Context, *NodeUnpublishVolumeRequest) (*NodeUnpublishVolumeResponse, error)NodeGetVolumeStats(context.Context, *NodeGetVolumeStatsRequest) (*NodeGetVolumeStatsResponse, error)NodeExpandVolume(context.Context, *NodeExpandVolumeRequest) (*NodeExpandVolumeResponse, error)NodeGetCapabilities(context.Context, *NodeGetCapabilitiesRequest) (*NodeGetCapabilitiesResponse, error)NodeGetInfo(context.Context, *NodeGetInfoRequest) (*NodeGetInfoResponse, error)&#125;</code></pre><p>在最小化的实现中，NodeServer 中仅仅需要实现 <code>NodePublishVolume</code>、<code>NodeUnpublishVolume</code>、<code>NodeGetCapabilities</code> 三个方法，在 Mount 阶段 kubelet 会通过 <a href="https://kubernetes-csi.github.io/docs/node-driver-registrar.html" target="_blank" rel="noopener">node-driver-registrar</a> 容器调用这三个方法。</p><h4 id="2-3-3、Controller-Server"><a href="#2-3-3、Controller-Server" class="headerlink" title="2.3.3、Controller Server"></a>2.3.3、Controller Server</h4><p>在当前 CSI Spec v1.3.0 ControllerServer 定义如下:</p><pre><code class="hljs go"><span class="hljs-comment">// ControllerServer is the server API for Controller service.</span><span class="hljs-keyword">type</span> ControllerServer <span class="hljs-keyword">interface</span> &#123;CreateVolume(context.Context, *CreateVolumeRequest) (*CreateVolumeResponse, error)DeleteVolume(context.Context, *DeleteVolumeRequest) (*DeleteVolumeResponse, error)ControllerPublishVolume(context.Context, *ControllerPublishVolumeRequest) (*ControllerPublishVolumeResponse, error)ControllerUnpublishVolume(context.Context, *ControllerUnpublishVolumeRequest) (*ControllerUnpublishVolumeResponse, error)ValidateVolumeCapabilities(context.Context, *ValidateVolumeCapabilitiesRequest) (*ValidateVolumeCapabilitiesResponse, error)ListVolumes(context.Context, *ListVolumesRequest) (*ListVolumesResponse, error)GetCapacity(context.Context, *GetCapacityRequest) (*GetCapacityResponse, error)ControllerGetCapabilities(context.Context, *ControllerGetCapabilitiesRequest) (*ControllerGetCapabilitiesResponse, error)CreateSnapshot(context.Context, *CreateSnapshotRequest) (*CreateSnapshotResponse, error)DeleteSnapshot(context.Context, *DeleteSnapshotRequest) (*DeleteSnapshotResponse, error)ListSnapshots(context.Context, *ListSnapshotsRequest) (*ListSnapshotsResponse, error)ControllerExpandVolume(context.Context, *ControllerExpandVolumeRequest) (*ControllerExpandVolumeResponse, error)ControllerGetVolume(context.Context, *ControllerGetVolumeRequest) (*ControllerGetVolumeResponse, error)&#125;</code></pre><p>从这些方法上可以看出，大部分的核心逻辑应该在 ControllerServer 中实现，比如创建/销毁 Volume，创建/销毁 Snapshot 等；在一般情况下我们自己编写的 CSI 都会实现 <code>CreateVolume</code> 和 <code>DeleteVolume</code>，至于其他方法根据业务需求以及外部存储供应商实际情况来决定是否进行实现。</p><h4 id="2-3-4、整体部署加构图"><a href="#2-3-4、整体部署加构图" class="headerlink" title="2.3.4、整体部署加构图"></a>2.3.4、整体部署加构图</h4><p><img src="https://cdn.oss.link/markdown/vopox.jpg" srcset="/img/loading.gif" alt="CSI Deploy Mechanism"></p><p><strong>从这个部署架构图上可以看出在实际上 CSI 部署时，Mount and Umount 阶段(对应 Node Server 实现)以 Daemonset 方式保证其部署到每个节点，当 Volume 创建完成后由其挂载到 Pod 中；其他阶段(Provisioning and Deleting 和 Attaching and Detaching) 只要部署多个实例保证 HA 即可(最新版本的 Sidecar Containers 已经实现了多节点自动选举)；每次 PV 创建时首先由其他两个阶段的 Sidecar Containers 做处理，处理完成后信息返回给 Kubernetes 再传递到 Node Driver(Node Server) 上，然后 Node Driver 将其 Mount 到 Pod 中。</strong></p><h2 id="三、编写一个-NFS-CSI-插件"><a href="#三、编写一个-NFS-CSI-插件" class="headerlink" title="三、编写一个 NFS CSI 插件"></a>三、编写一个 NFS CSI 插件</h2><h3 id="3-1、前置准备及分析"><a href="#3-1、前置准备及分析" class="headerlink" title="3.1、前置准备及分析"></a>3.1、前置准备及分析</h3><p>根据以上文档的描述，针对于需要编写一个 NFS CSI 插件这个需求，大致我们可以作出如下分析:</p><ul><li>三大阶段中我们只需要实现 Provisioning and Deleting 和 Mount and Umount；因为以 NFS 作为外部存储供应商来说我们并非是块设备，所以也不需要挂载到宿主机(Attaching and Detaching)。</li><li>Provisioning and Deleting 阶段我们需要实现 <code>CreateVolume</code> 和 <code>DeleteVolume</code> 逻辑，其核心逻辑应该是针对每个 PV 在 NFS Server 目录下执行 <code>mkdir</code>，并将生成的目录名称等信息返回给 Kubernetes。</li><li>Mount and Umount 阶段需要实现 Node Server 的 <code>NodePublishVolume</code> 和 <code>NodeUnpublishVolume</code> 方法，然后将上一阶段提供的目录名称等信息组合成挂载命令 Mount 到 Pod 即可。</li></ul><p>在明确了这个需求以后我们需要开始编写 gRPC Server，当然不能盲目的自己乱造轮子，<strong>因为这些 gRPC Server 需要是 <code>NonBlocking</code> 的，</strong>所以最佳实践就是参考官方给出的样例项目 <a href="https://github.com/kubernetes-csi/csi-driver-host-path" target="_blank" rel="noopener">csi-driver-host-path</a>，这是一名合格的 CCE 必备的技能(CCE = Ctrl C + Ctrl V + Engineer)。</p><h3 id="3-2、Hostpath-CSI-源码分析"><a href="#3-2、Hostpath-CSI-源码分析" class="headerlink" title="3.2、Hostpath CSI 源码分析"></a>3.2、Hostpath CSI 源码分析</h3><p>针对官方给出的 CSI 样例，首先把源码弄到本地，然后通过 IDE 打开；这里默认为读者熟悉 Go 语言相关语法以及 go mod 等依赖配置，开发 IDE 默认为 GoLand</p><p><img src="https://cdn.oss.link/markdown/jlsdg.png" srcset="/img/loading.gif" alt="source tree"></p><p>从源码树上可以看到，hostpath 的 CSI 实现非常简单；首先是 <code>cmd</code> 包下的命令行部分，main 方法在这里定义，然后就是 <code>pkg/hostpath</code> 包的具体实现部分，CSI 需要实现的三大 gRPC Server 全部在此。</p><h4 id="3-2-1、命令行解析"><a href="#3-2-1、命令行解析" class="headerlink" title="3.2.1、命令行解析"></a>3.2.1、命令行解析</h4><p><code>cmd</code> 包下主要代码就是一些命令行解析，方便从外部传入一些参数供 CSI 使用；针对于 NFS CSI 我们需要从外部传入 NFS Server 地址、挂载目录等参数，如果外部存储供应商为其他云存储可能就需要从命令行传入 AccessKey、AccessToken 等参数。</p><p><img src="https://cdn.oss.link/markdown/t4mje.png" srcset="/img/loading.gif" alt="flag_parse"></p><p>目前 go 原生的命令行解析非常弱鸡，所以更推荐使用 <a href="https://github.com/spf13/cobra" target="_blank" rel="noopener">cobra</a> 命令行库完成解析。</p><h4 id="3-2-2、Hostpath-结构体"><a href="#3-2-2、Hostpath-结构体" class="headerlink" title="3.2.2、Hostpath 结构体"></a>3.2.2、Hostpath 结构体</h4><p>从上面命令行解析的图中可以看到，在完成命令行解析后交由 <code>handle</code> 方法处理；<code>handle</code> 方法很简单，通过命令行拿到的参数创建一个 <code>hostpath</code> 结构体指针，然后 <code>Run</code> 起来就行了，所以接下来要着重看一下这个结构体</p><p><img src="https://cdn.oss.link/markdown/0dc0j.png" srcset="/img/loading.gif" alt="hostpath_struct"></p><p>从代码上可以看到，<code>hostpath</code> 结构体内有一系列的字段用来存储命令行传入的特定参数，然后还有三个 gRPC Server 的引用；命令行参数解析完成后通过 <code>NewHostPathDriver</code> 方法设置到 <code>hostpath</code> 结构体内，然后通过调用结构体的 <code>Run</code> 方法创建三个 gRPC Server 并运行</p><p><img src="https://cdn.oss.link/markdown/wt4ha.png" srcset="/img/loading.gif" alt="hostpath_run"></p><h4 id="3-2-3、代码分布"><a href="#3-2-3、代码分布" class="headerlink" title="3.2.3、代码分布"></a>3.2.3、代码分布</h4><p>经过这么简单的一看，基本上一个最小化的 CSI 代码分布已经可以出来了:</p><ul><li>首先需要做命令行解析，一般放在 <code>cmd</code> 包</li><li>然后需要一个一般与 CSI 插件名称相同的结构体用来承载参数</li><li>结构体内持有三个 gRPC Server 引用，并通过适当的方法使用内部参数还初始化这个三个 gRPC Server</li><li>有了这些 gRPC Server 以后通过 <code>server.go</code> 中的 <code>NewNonBlockingGRPCServer</code> 方法将其启动(这里也可以看出 server.go 里面的方法我们后面可以 copy 直接用)</li></ul><h3 id="3-3、创建-CSI-插件骨架"><a href="#3-3、创建-CSI-插件骨架" class="headerlink" title="3.3、创建 CSI 插件骨架"></a>3.3、创建 CSI 插件骨架</h3><blockquote><p>项目骨架已经提交到 Github <a href="https://github.com/mritd/csi-archetype" target="_blank" rel="noopener">mritd/csi-archetype</a> 项目，可直接 clone 并使用。</p></blockquote><p>大致的研究完 Hostpath 的 CSI 源码，我们就可以根据其实现细节抽象出一个项目 CSI 骨架:</p><p><img src="https://cdn.oss.link/markdown/7y8qu.png" srcset="/img/loading.gif" alt="csi_archetype"></p><p>在这个骨架中我们采用 <a href="https://github.com/spf13/cobra" target="_blank" rel="noopener">corba</a> 完成命令行参数解析，同时使用 <a href="github.com/sirupsen/logrus">logrus</a> 作为日志输出库，这两个库都是 Kubernetes 以及 docker 比较常用的库；我们创建了一个叫 <code>archetype</code> 的结构体作为 CSI 的主承载类，这个结构体需要定义一些参数(parameter1…)方便后面初始化相关 gRPC Server 实现相关调用。</p><pre><code class="hljs go"><span class="hljs-keyword">type</span> archetype <span class="hljs-keyword">struct</span> &#123;name     <span class="hljs-keyword">string</span>nodeID   <span class="hljs-keyword">string</span>version  <span class="hljs-keyword">string</span>endpoint <span class="hljs-keyword">string</span><span class="hljs-comment">// Add CSI plugin parameters here</span>parameter1 <span class="hljs-keyword">string</span>parameter2 <span class="hljs-keyword">int</span>parameter3 time.Duration<span class="hljs-built_in">cap</span>   []*csi.VolumeCapability_AccessModecscap []*csi.ControllerServiceCapability&#125;</code></pre><p>与 Hostpath CSI 实现相同，我们创建一个 <code>NewCSIDriver</code> 方法来返回 <code>archetype</code> 结构体实例，在 <code>NewCSIDriver</code> 方法中将命令行解析得到的相关参数设置进结构体中并添加一些 <code>AccessModes</code> 和 <code>ServiceCapabilities</code> 方便后面 <code>Identity Server</code> 调用。</p><pre><code class="hljs go"><span class="hljs-function"><span class="hljs-keyword">func</span> <span class="hljs-title">NewCSIDriver</span><span class="hljs-params">(version, nodeID, endpoint, parameter1 <span class="hljs-keyword">string</span>, parameter2 <span class="hljs-keyword">int</span>, parameter3 time.Duration)</span> *<span class="hljs-title">archetype</span></span> &#123;logrus.Infof(<span class="hljs-string">"Driver: %s version: %s"</span>, driverName, version)<span class="hljs-comment">// Add some check here</span><span class="hljs-keyword">if</span> parameter1 == <span class="hljs-string">""</span> &#123;logrus.Fatal(<span class="hljs-string">"parameter1 is empty"</span>)&#125;n := &amp;archetype&#123;name:     driverName,nodeID:   nodeID,version:  version,endpoint: endpoint,parameter1: parameter1,parameter2: parameter2,parameter3: parameter3,&#125;<span class="hljs-comment">// Add access modes for CSI here</span>n.AddVolumeCapabilityAccessModes([]csi.VolumeCapability_AccessMode_Mode&#123;csi.VolumeCapability_AccessMode_MULTI_NODE_MULTI_WRITER,&#125;)<span class="hljs-comment">// Add service capabilities for CSI here</span>n.AddControllerServiceCapabilities([]csi.ControllerServiceCapability_RPC_Type&#123;csi.ControllerServiceCapability_RPC_CREATE_DELETE_VOLUME,csi.ControllerServiceCapability_RPC_CREATE_DELETE_SNAPSHOT,&#125;)<span class="hljs-keyword">return</span> n&#125;</code></pre><p><strong>整个骨架源码树中，命令行解析自己重构使用一些更加方便的命令行解析、日志输出库；结构体部分参考 Hostpath 结构体自己调整，<code>server.go</code> 用来创建 <code>NonBlocking</code> 的 gRPC Server(直接从 Hotspath 样例项目 copy 即可)；然后就是三大 gRPC Server 的实现，由于是 “项目骨架” 所以相关方法我们都返回未实现，后续我们主要来实现这些方法就能让自己写的这个 CSI 插件 work。</strong></p><p><img src="https://cdn.oss.link/markdown/876sk.png" srcset="/img/loading.gif" alt="Unimplemented_gRPC_Server"></p><h3 id="3-4、创建-NFS-CSI-插件骨架"><a href="#3-4、创建-NFS-CSI-插件骨架" class="headerlink" title="3.4、创建 NFS CSI 插件骨架"></a>3.4、创建 NFS CSI 插件骨架</h3><p>有了 CSI 的项目骨架以后，我们只需要简单地修改名字将其重命名为 NFS CSI 插件即可；由于这篇文章是先实现好了 NFS CSI(已经 work) 再来写的，所以 NFS CSI 的源码可以直接参考 <a href="https://github.com/Gozap/csi-nfs" target="_blank" rel="noopener">Gozap/csi-nfs</a> 即可，下面的部分主要介绍三大 gRPC Server 的实现</p><p><img src="https://cdn.oss.link/markdown/kk42j.png" srcset="/img/loading.gif" alt="csi-nfs"></p><h3 id="3-5、实现-Identity-Server"><a href="#3-5、实现-Identity-Server" class="headerlink" title="3.5、实现 Identity Server"></a>3.5、实现 Identity Server</h3><p><img src="https://cdn.oss.link/markdown/r8etm.png" srcset="/img/loading.gif" alt="Identity Server"></p><p>Identity Server 实现相对简单，总共就三个接口；<code>GetPluginInfo</code> 接口返回插件名称版本即可(注意版本号好像只能是 <code>1.1.1</code> 这种，<code>v1.1.1</code> 好像会报错)；<code>Probe</code> 接口用来做健康检测可以直接返回空 response 即可，当然最理想的情况应该是做一些业务逻辑判活；<code>GetPluginCapabilities</code> 接口看起来简单但是要清楚返回的 <code>Capabilities</code> 含义，由于我们的 NFS 插件必然需要响应 <code>CreateVolume</code> 等请求(实现 Controller Server)，所以 cap 必须给予 <code>PluginCapability_Service_CONTROLLER_SERVICE</code>，除此之外如果节点不支持均匀的创建外部存储供应商的 Volume，那么应当同时返回 <code>PluginCapability_Service_VOLUME_ACCESSIBILITY_CONSTRAINTS</code> 以表示 CSI 处理时需要根据集群拓扑作调整；具体的可以查看 gRPC 注释:</p><pre><code class="hljs go"><span class="hljs-keyword">const</span> (PluginCapability_Service_UNKNOWN PluginCapability_Service_Type = <span class="hljs-number">0</span><span class="hljs-comment">// CONTROLLER_SERVICE indicates that the Plugin provides RPCs for</span><span class="hljs-comment">// the ControllerService. Plugins SHOULD provide this capability.</span><span class="hljs-comment">// In rare cases certain plugins MAY wish to omit the</span><span class="hljs-comment">// ControllerService entirely from their implementation, but such</span><span class="hljs-comment">// SHOULD NOT be the common case.</span><span class="hljs-comment">// The presence of this capability determines whether the CO will</span><span class="hljs-comment">// attempt to invoke the REQUIRED ControllerService RPCs, as well</span><span class="hljs-comment">// as specific RPCs as indicated by ControllerGetCapabilities.</span>PluginCapability_Service_CONTROLLER_SERVICE PluginCapability_Service_Type = <span class="hljs-number">1</span><span class="hljs-comment">// VOLUME_ACCESSIBILITY_CONSTRAINTS indicates that the volumes for</span><span class="hljs-comment">// this plugin MAY NOT be equally accessible by all nodes in the</span><span class="hljs-comment">// cluster. The CO MUST use the topology information returned by</span><span class="hljs-comment">// CreateVolumeRequest along with the topology information</span><span class="hljs-comment">// returned by NodeGetInfo to ensure that a given volume is</span><span class="hljs-comment">// accessible from a given node when scheduling workloads.</span>PluginCapability_Service_VOLUME_ACCESSIBILITY_CONSTRAINTS PluginCapability_Service_Type = <span class="hljs-number">2</span>)</code></pre><h3 id="3-6、实现-Controller-Server"><a href="#3-6、实现-Controller-Server" class="headerlink" title="3.6、实现 Controller Server"></a>3.6、实现 Controller Server</h3><p>Controller Server 实际上对应着 Provisioning and Deleting 阶段；换句话说核心的创建/删除卷、快照等都应在此做实现，针对于本次编写的 NFS 插件仅做最小实现(创建/删除卷)；需要注意的是除了核心的创建删除卷要实现以外还需要实现 <code>ControllerGetCapabilities</code> 方法，该方法返回 Controller Server 的 cap:</p><p><img src="https://cdn.oss.link/markdown/pl0n3.png" srcset="/img/loading.gif" alt="ControllerGetCapabilities"></p><p><code>ControllerGetCapabilities</code> 返回的实际上是在创建驱动时设置的 cscap:</p><pre><code class="hljs go">n.AddControllerServiceCapabilities([]csi.ControllerServiceCapability_RPC_Type&#123;csi.ControllerServiceCapability_RPC_CREATE_DELETE_VOLUME,csi.ControllerServiceCapability_RPC_CREATE_DELETE_SNAPSHOT,&#125;)</code></pre><p><code>ControllerServiceCapability_RPC_CREATE_DELETE_VOLUME</code> 表示这个 Controller Server 支持创建/删除卷，<code>ControllerServiceCapability_RPC_CREATE_DELETE_SNAPSHOT</code> 表示支持创建/删除快照(快照功能是后来闲的没事加的)；<strong>应该明确的是我们返回了特定的 cap 那就要针对特定方法做实现，因为你一旦声明了这些 cap Kubernetes 就认为有相应请求可以让你处理(你不能吹完牛逼然后关键时刻掉链子)。</strong>针对于可以返回哪些 cscap 可以通过这些 gRPC 常量来查看:</p><pre><code class="hljs go"><span class="hljs-keyword">const</span> (ControllerServiceCapability_RPC_UNKNOWN                  ControllerServiceCapability_RPC_Type = <span class="hljs-number">0</span>ControllerServiceCapability_RPC_CREATE_DELETE_VOLUME     ControllerServiceCapability_RPC_Type = <span class="hljs-number">1</span>ControllerServiceCapability_RPC_PUBLISH_UNPUBLISH_VOLUME ControllerServiceCapability_RPC_Type = <span class="hljs-number">2</span>ControllerServiceCapability_RPC_LIST_VOLUMES             ControllerServiceCapability_RPC_Type = <span class="hljs-number">3</span>ControllerServiceCapability_RPC_GET_CAPACITY             ControllerServiceCapability_RPC_Type = <span class="hljs-number">4</span><span class="hljs-comment">// Currently the only way to consume a snapshot is to create</span><span class="hljs-comment">// a volume from it. Therefore plugins supporting</span><span class="hljs-comment">// CREATE_DELETE_SNAPSHOT MUST support creating volume from</span><span class="hljs-comment">// snapshot.</span>ControllerServiceCapability_RPC_CREATE_DELETE_SNAPSHOT ControllerServiceCapability_RPC_Type = <span class="hljs-number">5</span>ControllerServiceCapability_RPC_LIST_SNAPSHOTS         ControllerServiceCapability_RPC_Type = <span class="hljs-number">6</span><span class="hljs-comment">// Plugins supporting volume cloning at the storage level MAY</span><span class="hljs-comment">// report this capability. The source volume MUST be managed by</span><span class="hljs-comment">// the same plugin. Not all volume sources and parameters</span><span class="hljs-comment">// combinations MAY work.</span>ControllerServiceCapability_RPC_CLONE_VOLUME ControllerServiceCapability_RPC_Type = <span class="hljs-number">7</span><span class="hljs-comment">// Indicates the SP supports ControllerPublishVolume.readonly</span><span class="hljs-comment">// field.</span>ControllerServiceCapability_RPC_PUBLISH_READONLY ControllerServiceCapability_RPC_Type = <span class="hljs-number">8</span><span class="hljs-comment">// See VolumeExpansion for details.</span>ControllerServiceCapability_RPC_EXPAND_VOLUME ControllerServiceCapability_RPC_Type = <span class="hljs-number">9</span><span class="hljs-comment">// Indicates the SP supports the</span><span class="hljs-comment">// ListVolumesResponse.entry.published_nodes field</span>ControllerServiceCapability_RPC_LIST_VOLUMES_PUBLISHED_NODES ControllerServiceCapability_RPC_Type = <span class="hljs-number">10</span><span class="hljs-comment">// Indicates that the Controller service can report volume</span><span class="hljs-comment">// conditions.</span><span class="hljs-comment">// An SP MAY implement `VolumeCondition` in only the Controller</span><span class="hljs-comment">// Plugin, only the Node Plugin, or both.</span><span class="hljs-comment">// If `VolumeCondition` is implemented in both the Controller and</span><span class="hljs-comment">// Node Plugins, it SHALL report from different perspectives.</span><span class="hljs-comment">// If for some reason Controller and Node Plugins report</span><span class="hljs-comment">// misaligned volume conditions, CO SHALL assume the worst case</span><span class="hljs-comment">// is the truth.</span><span class="hljs-comment">// Note that, for alpha, `VolumeCondition` is intended be</span><span class="hljs-comment">// informative for humans only, not for automation.</span>ControllerServiceCapability_RPC_VOLUME_CONDITION ControllerServiceCapability_RPC_Type = <span class="hljs-number">11</span><span class="hljs-comment">// Indicates the SP supports the ControllerGetVolume RPC.</span><span class="hljs-comment">// This enables COs to, for example, fetch per volume</span><span class="hljs-comment">// condition after a volume is provisioned.</span>ControllerServiceCapability_RPC_GET_VOLUME ControllerServiceCapability_RPC_Type = <span class="hljs-number">12</span>)</code></pre><p>当声明了 <code>ControllerServiceCapability_RPC_CREATE_DELETE_VOLUME</code> 以后针对创建删除卷方法 <code>CreateVolume</code>、<code>DeleteVolume</code> 做实现即可；这两个方法实现就是常规的业务逻辑层面没什么技术含量，对于外部存储供应商是 NFS 来说无非就是接到一个 <code>CreateVolumeRequest</code> ，然后根据 request 给的 volume name 啥的信息自己执行一下在 NFS Server 上 <code>mkdir</code> ，删除卷处理就是反向的 <code>rm -rf dir</code>；在两个方法的处理中可能额外掺杂一些校验等其他的辅助实现。</p><p><img src="https://cdn.oss.link/markdown/jkhb6.png" srcset="/img/loading.gif" alt="CreateVolume"></p><p><img src="https://cdn.oss.link/markdown/96ij8.png" srcset="/img/loading.gif" alt="DeleteVolume"></p><p><strong>最后有几点需要注意的地方:</strong></p><ul><li><strong>幂等性: Kubernetes 可能由于一些其他原因会重复发出请求(比如超时重试)，此时一定要保证创建/删除卷实现的幂等性，简单地说 Kubernetes 连续两次调用同一个卷创建 CSI 插件应当实现自动去重过滤，不能调用两次返回两个新卷。</strong></li><li><strong>数据回写: 要明白的是 Controller Server 是 Provisioning and Deleting 阶段，此时还没有真正挂载到 Pod，所以就本地使用 NFS 作为存储后端来说 <code>mkdir</code> 以后要把目录、NFS Server 地址等必要信息通过 VolumeContext 返回给 Kubernetes，Kubernetes 接下来会传递给 Node Driver(Mount/Umount)用。</strong></li><li><strong>预挂载: 当然这个问题目前只存在在 NFS 作为存储后端中，问题核心在于在创建卷进行 <code>mkdir</code> 之前，NFS 应该已经确保 mount 到了 Controller Server 容器本地，所以目前的做法就是启动 Controller Server 时就执行 NFS 挂载；如果用其他的后端存储比如阿里云存储时也要考虑在创建卷之前相关的 API Client 是否可用。</strong></li></ul><h3 id="3-7、实现-Node-Server"><a href="#3-7、实现-Node-Server" class="headerlink" title="3.7、实现 Node Server"></a>3.7、实现 Node Server</h3><p>Node Server 实际上就是 Node Driver，简单地说当 Controller Server 完成一个卷的创建，并且已经 Attach 到 Node 以后(当然这里的 NFS 不需要 Attach)，Node Server 就需要实现根据给定的信息将卷 Mount 到 Pod 或者从 Pod Umount 掉卷；同样的 Node Server 也许要返回一些信息来告诉 Kubernetes 自己的详细情况，这部份由两个方法完成 <code>NodeGetInfo</code> 和 <code>NodeGetCapabilities</code></p><p><img src="https://cdn.oss.link/markdown/ts3l9.png" srcset="/img/loading.gif" alt="NodeGetInfo_NodeGetCapabilities"></p><p><code>NodeGetInfo</code> 中返回节点的常规信息，比如 Node ID、最大允许的 Volume 数量、集群拓扑信息等；<code>NodeGetCapabilities</code> 返回这个 Node 的 cap，由于我们的 NFS 是真的啥也不支持，所以只好返回 <code>NodeServiceCapability_RPC_UNKNOWN</code>，至于其他的 cap 如下(含义自己看注释):</p><pre><code class="hljs go"><span class="hljs-keyword">const</span> (NodeServiceCapability_RPC_UNKNOWN              NodeServiceCapability_RPC_Type = <span class="hljs-number">0</span>NodeServiceCapability_RPC_STAGE_UNSTAGE_VOLUME NodeServiceCapability_RPC_Type = <span class="hljs-number">1</span><span class="hljs-comment">// If Plugin implements GET_VOLUME_STATS capability</span><span class="hljs-comment">// then it MUST implement NodeGetVolumeStats RPC</span><span class="hljs-comment">// call for fetching volume statistics.</span>NodeServiceCapability_RPC_GET_VOLUME_STATS NodeServiceCapability_RPC_Type = <span class="hljs-number">2</span><span class="hljs-comment">// See VolumeExpansion for details.</span>NodeServiceCapability_RPC_EXPAND_VOLUME NodeServiceCapability_RPC_Type = <span class="hljs-number">3</span><span class="hljs-comment">// Indicates that the Node service can report volume conditions.</span><span class="hljs-comment">// An SP MAY implement `VolumeCondition` in only the Node</span><span class="hljs-comment">// Plugin, only the Controller Plugin, or both.</span><span class="hljs-comment">// If `VolumeCondition` is implemented in both the Node and</span><span class="hljs-comment">// Controller Plugins, it SHALL report from different</span><span class="hljs-comment">// perspectives.</span><span class="hljs-comment">// If for some reason Node and Controller Plugins report</span><span class="hljs-comment">// misaligned volume conditions, CO SHALL assume the worst case</span><span class="hljs-comment">// is the truth.</span><span class="hljs-comment">// Note that, for alpha, `VolumeCondition` is intended to be</span><span class="hljs-comment">// informative for humans only, not for automation.</span>NodeServiceCapability_RPC_VOLUME_CONDITION NodeServiceCapability_RPC_Type = <span class="hljs-number">4</span>)</code></pre><p>剩下的核心方法 <code>NodePublishVolume</code> 和 <code>NodeUnpublishVolume</code> 挂载/卸载卷同 Controller Server 创建删除卷一样都是业务处理，没啥可说的，按步就班的调用一下 Mount 上就行；<strong>唯一需要注意的点就是这里也要保证幂等性，同时由于要操作 Pod 目录，所以要把宿主机的 <code>/var/lib/kubelet/pods</code> 目录挂载到 Node Server 容器里。</strong></p><h3 id="3-8、部署测试-NFS-插件"><a href="#3-8、部署测试-NFS-插件" class="headerlink" title="3.8、部署测试 NFS 插件"></a>3.8、部署测试 NFS 插件</h3><p>NFS 插件写完以后就可以实体环境做测试了，测试方法不同插件可能并不相同，本 NFS 插件可以直接使用源码项目的 <code>deploy</code> 目录创建相关容器做测试(需要根据自己的 NFS Server 修改一些参数)。针对于如何部署下面做一下简单说明:</p><p>三大阶段笼统的其实对应着三个 Sidecar Container:</p><ul><li>Provisioning and Deleting: external-provisioner</li><li>Attaching and Detaching: external-attacher</li><li>Mount and Umount: node-driver-registrar</li></ul><p><strong>我们的 NFS CSI 插件不需要 Attach，所以 external-attacher 也不需要部署；external-provisioner 只响应创建删除卷请求，所以通过 Deployment 部署足够多的复本保证 HA 就行；由于 Pod 不一定会落到那个节点上，理论上任意 Node 都可能有 Mount/Umount 行为，所以 node-driver-registrar 要以 Daemonset 方式部署保证每个节点都有一个。</strong></p><h2 id="四、其他说明"><a href="#四、其他说明" class="headerlink" title="四、其他说明"></a>四、其他说明</h2><h3 id="4-1、前期调试"><a href="#4-1、前期调试" class="headerlink" title="4.1、前期调试"></a>4.1、前期调试</h3><p>在前期代码编写时一般都是 “盲狙”，就是按照自己的理解无脑实现，这时候可能离实际部署还很远，但是只是单纯的想知道某个 Request 里面到底是什么个东西，这时候你可以利用 <code>mritd/socket2tcp</code> 容器模拟监听 socket 文件，然后将请求转发到你的 IDE 监听端口上，然后再进行 Debug。</p><p>可能有人会问: “我直接在 Sidecar Containers 里写个 tcp 地址不就行了，还转发毛线，这不是脱裤子放屁多此一举么？”，但是这里我友情提醒一下，Sidecar Containers 指定 CSI 地址时填写非 socket 类型的地址是不好使的，会直接启动失败。</p><h3 id="4-2、后期调试"><a href="#4-2、后期调试" class="headerlink" title="4.2、后期调试"></a>4.2、后期调试</h3><p>等到代码编写到后期其实就开始 “真机” 调试了，这时候其实不必使用原始的打日志调试方法，NFS CSI 的项目源码中的 <code>Dockerfile.debug</code> 提供了使用 dlv 做远程调试的样例；具体怎么配合 IDE 做远程调试请自行 Google。</p><h3 id="4-3、其他功能实现"><a href="#4-3、其他功能实现" class="headerlink" title="4.3、其他功能实现"></a>4.3、其他功能实现</h3><p>其他功能根据需要可以自己酌情实现，比如创建/删除快照功能；对于 NFS 插件来说 NFS Server 又没有 API，所以最简单最 low 的办法当然是 <code>tar -zcvf</code> 了(哈哈哈(超大声))，当然性能么就不要提了。</p><h2 id="五、总结"><a href="#五、总结" class="headerlink" title="五、总结"></a>五、总结</h2><p><strong>CSI 开发其实是针对 Kubernetes CSI Sidecar Containers 的 gRPC 开发，根据自己需求实现三大阶段中对应三大 gRPC Server 相应方法即可；相关功能要保证幂等性，cap 要看文档根据实际情况返回。</strong></p><h2 id="六、参考文档"><a href="#六、参考文档" class="headerlink" title="六、参考文档"></a>六、参考文档</h2><ul><li><a href="https://kubernetes-csi.github.io/docs/introduction.html" target="_blank" rel="noopener">https://kubernetes-csi.github.io/docs/introduction.html</a></li><li><a href="https://github.com/container-storage-interface/spec" target="_blank" rel="noopener">https://github.com/container-storage-interface/spec</a></li></ul>]]></content>
    
    
    <categories>
      
      <category>Kubernetes</category>
      
    </categories>
    
    
    <tags>
      
      <tag>CSI</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>树莓派4 Manjaro 系统定制</title>
    <link href="/2020/08/19/make-a-custom-manjaro-image-for-rpi4/"/>
    <url>/2020/08/19/make-a-custom-manjaro-image-for-rpi4/</url>
    
    <content type="html"><![CDATA[<h2 id="一、目前的系统现状"><a href="#一、目前的系统现状" class="headerlink" title="一、目前的系统现状"></a>一、目前的系统现状</h2><p>截止本文编写时间，树莓派4 官方系统仍然不支持 64bit；但是当我在 3b+ 上使用 arch 64bit 以后我发现 32bit 系统和 64bit 系统装在同一个树莓派上在使用时那就是两个完全不一样的树莓派…所以对于这个新的 rpi4 那么必需要用 64bit 的系统；而当前我大致查看到支持 64bit 的系统只有 Ubuntu20、Manjaro 两个，Ubuntu 对我来说太重了(虽然服务器上我一直是 Ubuntu，但是 rpi 上我选择说 “不”)，Manjaro 基于 Arch 这种非常轻量的系统非常适合树莓派这种开发板，所以最终我选择了 Manjaro。但是万万没想到的是 Manjaro 都是带 KDE 什么的图形化的，而我的树莓派只想仍在角落里跑东西，所以说图形化这东西对我来说也没啥用，最后迫于无奈只能自己通过 Manjaro 的工具自己定制了。</p><h2 id="二、manjaro-arm-tools"><a href="#二、manjaro-arm-tools" class="headerlink" title="二、manjaro-arm-tools"></a>二、manjaro-arm-tools</h2><p>经过几经查找各种 Google，发现了 Manjaro 官方提供了自定义创建 arm 镜像的工具 <a href="https://gitlab.manjaro.org/manjaro-arm/applications/manjaro-arm-tools" target="_blank" rel="noopener">manjaro-arm-tools</a>，这个工具简单使用如下:</p><ul><li>首先准备一个 Manjaro 系统(虚拟机 x86 即可)</li><li>然后安装 manjaro-arm-tool 所需<a href="https://gitlab.manjaro.org/manjaro-arm/applications/manjaro-arm-tools#dependencies" target="_blank" rel="noopener">依赖工具</a></li><li>添加 Manjaro 的<a href="https://gitlab.manjaro.org/manjaro-arm/applications/manjaro-arm-tools#git-version-from-manjaro-strit-repo" target="_blank" rel="noopener">软件源</a></li><li>安装 manjaro-arm-tool <code>sudo pacman -Syyu manjaro-strit-keyring &amp;&amp; sudo pacman -S manjaro-arm-tools-git</code></li></ul><p>当工具都准备完成后，只需要执行 <code>sudo buildarmimg -d rpi4 -e minimal</code> 即可创建 manjaro 的 rpi4 最小镜像。</p><h2 id="三、系统定制"><a href="#三、系统定制" class="headerlink" title="三、系统定制"></a>三、系统定制</h2><p>在使用 manjaro-arm-tool 创建系统以后发现一些细微的东西需要自己调整，比如网络设置常用软件包等，而 manjaro-arm-tool 工具又没有提供太好的自定义处理的一些 hook，所以最后萌生了自己根据 manjaro-arm-tool 来创建自己的 rpi4 系统定制工具的想法。</p><h3 id="3-1、常用软件包安装"><a href="#3-1、常用软件包安装" class="headerlink" title="3.1、常用软件包安装"></a>3.1、常用软件包安装</h3><p>在查看了 manjaro-arm-tool 的源码后可以看到实际上软件安装就是利用 systemd-nspawn 进入到 arm 系统执行 pacman 安装，自己依葫芦画瓢增加一些常用的软件包安装:</p><pre><code class="hljs sh">systemd-nspawn -q --resolv-conf=copy-host --timezone=off -D <span class="hljs-variable">$&#123;ROOTFS_DIR&#125;</span> pacman -Syyu zsh htop vim wget <span class="hljs-built_in">which</span> git make net-tools dnsutils inetutils iproute2 sysstat nload lsof --noconfirm</code></pre><h3 id="3-2、pacman-镜像"><a href="#3-2、pacman-镜像" class="headerlink" title="3.2、pacman 镜像"></a>3.2、pacman 镜像</h3><p>在安装软件包时发现安装速读奇慢，研究以后发现是没有使用国内的镜像源，故增加了国内镜像源的处理:</p><pre><code class="hljs sh">systemd-nspawn -q --resolv-conf=copy-host --timezone=off -D <span class="hljs-variable">$&#123;ROOTFS_DIR&#125;</span> pacman-mirrors -c China</code></pre><h3 id="3-3、网络处理"><a href="#3-3、网络处理" class="headerlink" title="3.3、网络处理"></a>3.3、网络处理</h3><h4 id="3-3-1、有线连接"><a href="#3-3-1、有线连接" class="headerlink" title="3.3.1、有线连接"></a>3.3.1、有线连接</h4><p>默认的 manjaro-arm-tool 创建的系统网络部分采用 dhspcd 做 dhcp 处理，但是我个人感觉一切尽量精简统一还是比较好的；所以准备网络部分完全由 systemd 接管处理，即直接使用 systemd-networkd 和 systemd-resolved；systemd-networkd 处理相对简单，编写一个配置文件然后 enable systemd-networkd 服务即可:</p><p><strong>/etc/systemd/network/10-eth-dhcp.network</strong></p><pre><code class="hljs sh">[Match]Name=eth*[Network]DHCP=yes</code></pre><p><strong>让 systemd-networkd 开机自启动</strong></p><pre><code class="hljs sh">systemd-nspawn -q --resolv-conf=copy-host --timezone=off -D <span class="hljs-variable">$&#123;ROOTFS_DIR&#125;</span> systemctl <span class="hljs-built_in">enable</span> systemd-networkd.service</code></pre><p>一开始以为 systemd-resolved 同样 enable 一下就行，后来发现每次开机初始化以后 systemd-resolved 都会被莫明其妙的 disable 掉；经过几经寻找和开 issue 问作者，发现这个操作是被 manjaro-arm-oem-install 包下的脚本执行的，作者的回复意思是大部分带有图形化的版本网络管理工具都会与 systemd-resolved 冲突，所以默认关闭了，这时候我们就要针对 manjaro-arm-oem-install 单独处理一下:</p><pre><code class="hljs sh">systemd-nspawn -q --resolv-conf=copy-host --timezone=off -D <span class="hljs-variable">$&#123;ROOTFS_DIR&#125;</span> systemctl <span class="hljs-built_in">enable</span> systemd-resolved.servicesed -i <span class="hljs-string">'s@systemctl disable systemd-resolved.service 1&gt; /dev/null 2&gt;&amp;1@@g'</span> <span class="hljs-variable">$&#123;ROOTFS_DIR&#125;</span>/usr/share/manjaro-arm-oem-install/manjaro-arm-oem-install</code></pre><h4 id="3-3-2、无限连接"><a href="#3-3-2、无限连接" class="headerlink" title="3.3.2、无限连接"></a>3.3.2、无限连接</h4><p>有线连接只要 systemd-networkd 处理好就能很好的工作，而无线连接目前有很多方案，我一开始想用 <a href="https://wiki.archlinux.org/index.php/Netctl_(%E7%AE%80%E4%BD%93%E4%B8%AD%E6%96%87)" target="_blank" rel="noopener">netctl</a>，后来发现这东西虽然是 Arch 亲儿子，但是在系统定制时采用 systemd-nspawn 调用不兼容(因为里面调用了 systemd 的一些命令，这些命令一般只有在开机时才可用)，而且只用 netctl 来管理 wifi 还感觉怪怪的，后来我的想法是要么用就全都用，要么就纯手动不要用这些东西，所以最后的方案是 wpa_supplicant + systemd-networkd 一把梭:</p><p><strong>/etc/systemd/network/10-wlan-dhcp.network.example</strong></p><pre><code class="hljs sh"><span class="hljs-comment"># 1. Generate wifi configuration (don't modify the name of wpa_supplicant-wlan0.conf file)</span><span class="hljs-comment"># $ wpa_passphrase MyNetwork SuperSecretPassphrase &gt; /etc/wpa_supplicant/wpa_supplicant-wlan0.conf</span><span class="hljs-comment">#</span><span class="hljs-comment"># 2. Connect to wifi automatically after booting</span><span class="hljs-comment"># $ systemctl enable wpa_supplicant@wlan0</span><span class="hljs-comment">#</span><span class="hljs-comment"># 3.Systemd automatically makes dhcp request</span><span class="hljs-comment"># $ cp /etc/systemd/network/10-wlan-dhcp.network.example /etc/systemd/network/10-wlan-dhcp.network</span>[Match]Name=wlan*[Network]DHCP=yes</code></pre><h3 id="3-4、内核调整"><a href="#3-4、内核调整" class="headerlink" title="3.4、内核调整"></a>3.4、内核调整</h3><p>在上面的一些调整完成后我就启动系统实体机测试了，测试过程中发现安装 docker 以后会有两个警告，大致意思就是不支持 swap limit 和 cpu limit；查询资料以后发现是内核有两个参数没开启(<code>CONFIG_MEMCG_SWAP</code>、<code>CONFIG_CFS_BANDWIDTH</code>)…当然我这种强迫症是不能忍的，没办法就自己在 rpi4 上重新编译了内核(后来我想想还不如用 arch 32bit 然后自己编译 64bit 内核了):</p><pre><code class="hljs sh">git <span class="hljs-built_in">clone</span> https://github.com/mritd/linux-rpi4.git<span class="hljs-built_in">cd</span> linux-rpi4MAKEFLAGS=<span class="hljs-string">'-j4'</span> makepkg</code></pre><h3 id="3-5、外壳驱动"><a href="#3-5、外壳驱动" class="headerlink" title="3.5、外壳驱动"></a>3.5、外壳驱动</h3><p>由于我的 rpi4 配的是 ARGON ONE 的外壳，所以电源按钮还有风扇需要驱动才能完美工作，没办法我又编译了 ARGON ONE 外壳的驱动:</p><pre><code class="hljs sh">git <span class="hljs-built_in">clone</span> https://github.com/mritd/argonone.git<span class="hljs-built_in">cd</span> argononemakepkg</code></pre><h2 id="四、定制脚本"><a href="#四、定制脚本" class="headerlink" title="四、定制脚本"></a>四、定制脚本</h2><p>综合以上的各种修改以后，我从 manjaro-arm-tool 提取出了定制化的 rpi4 的编译脚本，该脚本目前存放在 <a href="https://github.com/mritd/manjaro-rpi4" target="_blank" rel="noopener">mritd/manjaro-rpi4</a> 仓库中；目前使用此脚本编译的系统镜像默认进行了以下处理:</p><ul><li>调整 pacman mirror 为中国</li><li>安装常用软件包(zsh htop vim wget which…)</li><li>有线网络完全的 systemd-networkd 接管，resolv.conf 由 systemd-resolved 接管</li><li>无线网络由 wpa_supplicant 和 systemd-networkd 接管</li><li>安装自行编译的内核以消除 docker 警告(<strong>自编译内核不影响升级，升级/覆盖安装后自动恢复</strong>)</li></ul><p>至于 ARGON ONE 的外壳驱动只在 resources 目录下提供了安装包，并未默认安装到系统。</p>]]></content>
    
    
    <categories>
      
      <category>Linux</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Linux</tag>
      
      <tag>Manjaro</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>如何在 Filebeat 端进行日志处理</title>
    <link href="/2020/08/19/how-to-modify-filebeat-source-code-to-processing-logs/"/>
    <url>/2020/08/19/how-to-modify-filebeat-source-code-to-processing-logs/</url>
    
    <content type="html"><![CDATA[<h2 id="一、起因"><a href="#一、起因" class="headerlink" title="一、起因"></a>一、起因</h2><p>目前某项目组日志需要做切割处理，针对日志信息进行分割并提取 k/v 放入 es 中方便查询。这种需求在传统 ELK 中应当由 logstash 组件完成，通过 <code>gork</code> 等操作对日志进行过滤、切割等处理。不过很尴尬的是我并不会 ruby，logstash pipeline 的一些配置我也是极其头疼，而且还不想学…更不凑巧的是我会写点 go，<strong>那么理所应当的此时的我对 filebeat 源码产生了一些想法，比如我直接在 filebeat 端完成日志处理，然后直接发 es/logstash，这样似乎更方便，而且还能分摊 logstash 的压力，我感觉这个操作并不过分😂…</strong></p><h2 id="二、需求"><a href="#二、需求" class="headerlink" title="二、需求"></a>二、需求</h2><p>目前某项目组 java 日志格式如下:</p><pre><code class="hljs sh">2020-04-30 21:56:30.117$<span class="hljs-variable">$api</span>-<span class="hljs-built_in">test</span>-65c8c7cf7f-lng7h$<span class="hljs-variable">$http</span>-nio-8080-exec-3$<span class="hljs-variable">$INFO</span>$<span class="hljs-variable">$com</span>.example.api.common.filter.GlobalDataFilter$<span class="hljs-variable">$GlobalDataFilter</span>.java$<span class="hljs-variable">$95</span>$<span class="hljs-variable">$test</span>build commonData from header :&#123;<span class="hljs-string">"romVersion"</span>:<span class="hljs-string">"W_V2.1.4"</span>,<span class="hljs-string">"softwareVersion"</span>:<span class="hljs-string">"15"</span>,<span class="hljs-string">"token"</span>:<span class="hljs-string">"aFxANNM3pnRYpohvLMSmENydgFSfsmFMgCbFWAosIE="</span>&#125;$$$$</code></pre><p>目前开发约定格式为日志通过 <code>$$</code> 进行分割，日志格式比较简单，但是 logstash 共用(nginx 等各种日志都会往这个 logstash 输出)，不想去折腾 logstash 配置的情况下，只需要让 filebeat 能够直接切割并设置好 k/v 对应既可。</p><h2 id="三、filebeat-module"><a href="#三、filebeat-module" class="headerlink" title="三、filebeat module"></a>三、filebeat module</h2><blockquote><p>module 部份只做简介，以为实际上依托 es 完成，意义不大。</p></blockquote><p>当然在考虑修改 filebeat 源码后，我第一想到的是 filebeat 的 module，这个 module 在官方文档中是个很神奇的东西；通过开启一个 module 就可以对某种日志直接做处理，这种东西似乎就是我想要的；比如我写一个 “项目名” module，然后 filebeat 直接开启这个 module，这个项目的日志就直接自动处理好(听起来就很 “上流”)…</p><p>针对于自定义 module，官方给出了文档: <a href="https://www.elastic.co/guide/en/beats/devguide/current/filebeat-modules-devguide.html" target="_blank" rel="noopener">Creating a New Filebeat Module</a></p><p>按照文档操作如下(假设我们的项目名为 cdm):</p><pre><code class="hljs sh"><span class="hljs-comment"># 克隆源码</span>git <span class="hljs-built_in">clone</span> git@github.com:elastic/beats.git<span class="hljs-comment"># 切换到稳定分支</span><span class="hljs-built_in">cd</span> bests &amp;&amp; git checkout -b v7.6.2 v7.6.2-module<span class="hljs-comment"># 创建 module，GO111MODULE 需要设置为 off</span><span class="hljs-comment"># 在 7.6.2 版本官方尚未开始支持 go mod</span><span class="hljs-built_in">cd</span> filebeatGO111MODULE=off make create-module MODULE=cdm</code></pre><p>创建完成后目录结构如下</p><pre><code class="hljs sh">➜  filebeat git:(v7.6.2-module) ✗ tree module/cdmmodule/cdm├── _meta│   ├── config.yml│   ├── docs.asciidoc│   └── fields.yml└── module.yml1 directory, 4 files</code></pre><p>这几个文件具体作用<a href="https://www.elastic.co/guide/en/beats/devguide/current/filebeat-modules-devguide.html" target="_blank" rel="noopener">官方文档</a>都有详细的描述；但是根据文档描述光有这几个文件是不够的，<strong>module 只是一个处理集合的定义，尚未包含任何处理，针对真正的处理需要继续创建 fileset，fileset 简单的理解就是针对具体的一组文件集合的处理；</strong>例如官方 nginx module 中包含两个 fileset: <code>access</code> 和 <code>error</code>，这两个一个针对 access 日志处理一个针对 error 日志进行处理；在 fileset 中可以设置默认文件位置、处理方式。</p><p><strong>But… 我翻了 nginx module 的样例配置才发现，module 这个东西实质上只做定义和存储处理表达式，具体的切割处理实际上交由 es 的 <a href="https://www.elastic.co/guide/en/elasticsearch/reference/current/ingest.html" target="_blank" rel="noopener">Ingest Node</a> 处理；表达式里仍需要定义 <code>grok</code> 等操作，而且这东西最终会编译到 go 静态文件里；</strong>此时的我想说一句 “MMP”，本来我是不像写 grok 啥的才来折腾 filebeat，结果这个 module 折腾一圈还是要写 grok 啥的，而且这东西直接借助 es 完成导致压力回到了 es 同时每次修改还得重新编译 filebeat… 所以折腾到这我就放弃了，这已经违背了当初的目的，有兴趣的可以参考以下文档继续折腾:</p><ul><li><a href="https://www.elastic.co/guide/en/beats/devguide/current/filebeat-modules-devguide.html" target="_blank" rel="noopener">Creating a New Filebeat Module</a></li><li><a href="https://www.elastic.co/guide/en/elasticsearch/reference/current/ingest.html" target="_blank" rel="noopener">Ingest nodeedit</a></li><li><a href="https://www.elastic.co/guide/en/elasticsearch/reference/current/ingest-apis.html" target="_blank" rel="noopener">Ingest APIs</a></li><li><a href="https://www.elastic.co/guide/en/elasticsearch/reference/current/ingest-processors.html" target="_blank" rel="noopener">Processors</a></li></ul><h2 id="四、filebeat-processors"><a href="#四、filebeat-processors" class="headerlink" title="四、filebeat processors"></a>四、filebeat processors</h2><p>经历了 module 的失望以后，我把目光对准了 processors；processors 是 filebeat 一个强大的功能，顾名思义它可以对 filbeat 收集到的日志进行一些处理；从官方 <a href="https://www.elastic.co/guide/en/beats/filebeat/current/filtering-and-enhancing-data.html" target="_blank" rel="noopener">Processors</a> 页面可以看到其内置了大量的 processor；这些 processor 大部份都是直接对日志进行 “写” 操作，所以理论上我们自己写一个 processor 就可以 “为所欲为+为所欲为=为所欲为”。</p><p>不过不幸的是关于 processor 的开发官方并未给出文档，官方认为这是一个 <code>high level</code> 的东西，不过也找到了一个 issue 对其做了相关回答: <a href="https://github.com/elastic/beats/issues/6760" target="_blank" rel="noopener">How do I write a processor plugin by myself</a>；所以最好的办法就是直接看已有 processor 的源码抄一个。</p><p>理所应当的找了一个软柿子捏: <code>add_host_metadata</code>，add_host_metadata processor 顾名思义在每个日志事件(以下简称为 event)中加入宿主机的信息，比如 hostname 啥的；以下为 add_host_metadata processor 的文件结构(processors 代码存储在 <code>libbeat/processors</code> 目录下)。</p><p><img src="https://cdn.oss.link/markdown/axucc.jpg" srcset="/img/loading.gif" alt="dir_tree"></p><p>通过阅读源码和 issue 的回答可以看出，我们自定义的 processor 只需要实现 <a href="https://godoc.org/github.com/elastic/beats/libbeat/processors#Processor" target="_blank" rel="noopener">Processor interface</a> 既可，这个接口定义如下:</p><p><img src="https://cdn.oss.link/markdown/xuja6.png" srcset="/img/loading.gif" alt="Processor interface"></p><p>通过查看 add_host_metadata 的源码，<code>String() string</code> 方法只需要返回这个 processor 名称既可(可以包含必要的配置信息)；<strong>而 <code>Run(event *beat.Event) (*beat.Event, error)</code> 方法表示在每一条日志被读取后都会转换为一个 event 对象，我们在方法内进行处理然后把 event 返回既可(其他 processor 可能也要处理)。</strong></p><p><img src="https://cdn.oss.link/markdown/jhtnx.png" srcset="/img/loading.gif" alt="add_host_metadata source"></p><p>有了这些信息就简单得多了，毕竟作为<strong>一名合格的 CCE(Ctrl C + Ctrl V + Engineer)</strong> 抄这种操作还是很简单的，直接照猫画虎写一个就行了</p><p>config.go</p><pre><code class="hljs go"><span class="hljs-keyword">package</span> cmd<span class="hljs-comment">// Config for cdm processor.</span><span class="hljs-keyword">type</span> Config <span class="hljs-keyword">struct</span> &#123;Name           <span class="hljs-keyword">string</span>          <span class="hljs-string">`config:"name"`</span>&#125;<span class="hljs-function"><span class="hljs-keyword">func</span> <span class="hljs-title">defaultConfig</span><span class="hljs-params">()</span> <span class="hljs-title">Config</span></span> &#123;<span class="hljs-keyword">return</span> Config&#123;&#125;&#125;</code></pre><p>cdm.go</p><pre><code class="hljs go"><span class="hljs-keyword">package</span> cmd<span class="hljs-keyword">import</span> (<span class="hljs-string">"strings"</span><span class="hljs-string">"github.com/elastic/beats/libbeat/logp"</span><span class="hljs-string">"github.com/pkg/errors"</span><span class="hljs-string">"github.com/elastic/beats/libbeat/beat"</span><span class="hljs-string">"github.com/elastic/beats/libbeat/common"</span><span class="hljs-string">"github.com/elastic/beats/libbeat/processors"</span>jsprocessor <span class="hljs-string">"github.com/elastic/beats/libbeat/processors/script/javascript/module/processor"</span>)<span class="hljs-function"><span class="hljs-keyword">func</span> <span class="hljs-title">init</span><span class="hljs-params">()</span></span> &#123;processors.RegisterPlugin(<span class="hljs-string">"cdm"</span>, New)jsprocessor.RegisterPlugin(<span class="hljs-string">"CDM"</span>, New)&#125;<span class="hljs-keyword">type</span> cdm <span class="hljs-keyword">struct</span> &#123;config Configfields []<span class="hljs-keyword">string</span>log    *logp.Logger&#125;<span class="hljs-keyword">const</span> (processorName = <span class="hljs-string">"cdm"</span>logName       = <span class="hljs-string">"processor.cdm"</span>)<span class="hljs-comment">// New constructs a new cdm processor.</span><span class="hljs-function"><span class="hljs-keyword">func</span> <span class="hljs-title">New</span><span class="hljs-params">(cfg *common.Config)</span> <span class="hljs-params">(processors.Processor, error)</span></span> &#123;<span class="hljs-comment">// 配置文件里就一个 Name 字段，结构体留着以后方便扩展</span>config := defaultConfig()<span class="hljs-keyword">if</span> err := cfg.Unpack(&amp;config); err != <span class="hljs-literal">nil</span> &#123;<span class="hljs-keyword">return</span> <span class="hljs-literal">nil</span>, errors.Wrapf(err, <span class="hljs-string">"fail to unpack the %v configuration"</span>, processorName)&#125;p := &amp;cdm&#123;config: config,<span class="hljs-comment">// 待分割的每段日志对应的 key</span>fields: []<span class="hljs-keyword">string</span>&#123;<span class="hljs-string">"timestamp"</span>, <span class="hljs-string">"hostname"</span>, <span class="hljs-string">"thread"</span>, <span class="hljs-string">"level"</span>, <span class="hljs-string">"logger"</span>, <span class="hljs-string">"file"</span>, <span class="hljs-string">"line"</span>, <span class="hljs-string">"serviceName"</span>, <span class="hljs-string">"traceId"</span>, <span class="hljs-string">"feTraceId"</span>, <span class="hljs-string">"msg"</span>, <span class="hljs-string">"exception"</span>&#125;,log:    logp.NewLogger(logName),&#125;<span class="hljs-keyword">return</span> p, <span class="hljs-literal">nil</span>&#125;<span class="hljs-comment">// 真正的日志处理逻辑</span><span class="hljs-comment">// 为了保证后面的 processor 正常处理，这里面没有 return 任何 error，只是简单的打印</span><span class="hljs-function"><span class="hljs-keyword">func</span> <span class="hljs-params">(p *cdm)</span> <span class="hljs-title">Run</span><span class="hljs-params">(event *beat.Event)</span> <span class="hljs-params">(*beat.Event, error)</span></span> &#123;<span class="hljs-comment">// 尝试获取 message，理论上这一步不应该出现问题</span>msg, err := event.GetValue(<span class="hljs-string">"message"</span>)<span class="hljs-keyword">if</span> err != <span class="hljs-literal">nil</span> &#123;p.log.Error(err)<span class="hljs-keyword">return</span> event, <span class="hljs-literal">nil</span>&#125;message, ok := msg.(<span class="hljs-keyword">string</span>)<span class="hljs-keyword">if</span> !ok &#123;p.log.Error(<span class="hljs-string">"failed to parse message"</span>)<span class="hljs-keyword">return</span> event, <span class="hljs-literal">nil</span>&#125;<span class="hljs-comment">// 分割日志</span>fieldsValue := strings.Split(message, <span class="hljs-string">"$$"</span>)p.log.Debugf(<span class="hljs-string">"message fields: %v"</span>, fieldsVaule)<span class="hljs-comment">// 为了保证不会出现数组越界需要判断一下(万一弄出个格式不正常的日志过来保证不崩)</span><span class="hljs-keyword">if</span> <span class="hljs-built_in">len</span>(fieldsValue) &lt; <span class="hljs-built_in">len</span>(p.fields) &#123;p.log.Errorf(<span class="hljs-string">"incorrect field length: %d, expected length: %d"</span>, <span class="hljs-built_in">len</span>(fieldsValue), <span class="hljs-built_in">len</span>(p.fields))<span class="hljs-keyword">return</span> event, <span class="hljs-literal">nil</span>&#125;<span class="hljs-comment">// 这里遍历然后赛会到 event 既可</span>data := common.MapStr&#123;&#125;<span class="hljs-keyword">for</span> i, k := <span class="hljs-keyword">range</span> p.fields &#123;_, _ = event.PutValue(k, strings.TrimSpace(fieldsValue[i]))&#125;event.Fields.DeepUpdate(data)<span class="hljs-keyword">return</span> event, <span class="hljs-literal">nil</span>&#125;<span class="hljs-function"><span class="hljs-keyword">func</span> <span class="hljs-params">(p *cdm)</span> <span class="hljs-title">String</span><span class="hljs-params">()</span> <span class="hljs-title">string</span></span> &#123;<span class="hljs-keyword">return</span> processorName&#125;</code></pre><p>写好代码以后就可以编译一个自己的 filebeat 了(开心ing)</p><pre><code class="hljs sh"><span class="hljs-built_in">cd</span> filebeat<span class="hljs-comment"># 如果想交叉编译 linux 需要增加 GOOS=linux 变量 </span>GO111MODULE=off make</code></pre><p>然后编写配置文件进行测试，日志相关字段已经成功塞到了 event 中，这样我直接发到 es 或者 logstash 就行了。</p><pre><code class="hljs yaml"><span class="hljs-attr">filebeat.inputs:</span><span class="hljs-bullet">-</span> <span class="hljs-attr">type:</span> <span class="hljs-string">log</span>  <span class="hljs-attr">enabled:</span> <span class="hljs-literal">true</span>  <span class="hljs-attr">paths:</span>    <span class="hljs-bullet">-</span> <span class="hljs-string">/Users/natural/tmp/cdm.log</span>  <span class="hljs-attr">processors:</span>    <span class="hljs-bullet">-</span> <span class="hljs-attr">cdm:</span> <span class="hljs-string">~</span>  <span class="hljs-attr">multiline.pattern:</span> <span class="hljs-string">^\d&#123;4&#125;-\d&#123;1,2&#125;-\d&#123;1,2&#125;</span>  <span class="hljs-attr">multiline.match:</span> <span class="hljs-string">after</span>  <span class="hljs-attr">multiline.negate:</span> <span class="hljs-literal">true</span>  <span class="hljs-attr">multiline.timeout:</span> <span class="hljs-string">5s</span></code></pre><h2 id="五、script-processor"><a href="#五、script-processor" class="headerlink" title="五、script processor"></a>五、script processor</h2><p>在我折腾完源码以后，反思一下其实这种方式需要自己编译 filebeat，而且每次规则修改也很不方便，唯一的好处真的就是用代码可以 “为所欲为”；反过来一想 “filebeat 有没有 processor 的扩展呢？脚本热加载那种？” 答案是使用 script processor，<strong>script processor 虽然名字上是个 processor，实际上其包含了完整的 ECMA 5.1 js 规范实现；结论就是我们可以写一些 js 脚本来处理日志，然后 filebeat 每次启动后加载这些脚本既可。</strong></p><p>script processor 的使用方式很简单，js 文件中只需要包含一个 <code>function process(event)</code> 方法既可，与自己用 go 实现的 processor 类似，每行日志也会形成一个 event 对象然后调用这个方法进行处理；目前 event 对象可用的 api 需要参考<a href="https://www.elastic.co/guide/en/beats/filebeat/current/processor-script.html#_event_api" target="_blank" rel="noopener">官方文档</a>；<strong>需要注意的是 script processor 目前只支持 ECMA 5.1 语法规范，超过这个范围的语法是不被支持；</strong>实际上其根本是借助了 <a href="https://github.com/dop251/goja" target="_blank" rel="noopener">https://github.com/dop251/goja</a> 这个库来实现的。同时为了方便开发调试，script processor 也增加了一些 nodejs 的兼容 module，比如 <code>console.log</code> 等方法是可用的；以下为 js 处理上面日志的逻辑:</p><pre><code class="hljs js"><span class="hljs-keyword">var</span> <span class="hljs-built_in">console</span> = <span class="hljs-built_in">require</span>(<span class="hljs-string">'console'</span>);<span class="hljs-keyword">var</span> fileds = <span class="hljs-keyword">new</span> <span class="hljs-built_in">Array</span>(<span class="hljs-string">"timestamp"</span>, <span class="hljs-string">"hostname"</span>, <span class="hljs-string">"thread"</span>, <span class="hljs-string">"level"</span>, <span class="hljs-string">"logger"</span>, <span class="hljs-string">"file"</span>, <span class="hljs-string">"line"</span>, <span class="hljs-string">"serviceName"</span>, <span class="hljs-string">"traceId"</span>, <span class="hljs-string">"feTraceId"</span>, <span class="hljs-string">"msg"</span>, <span class="hljs-string">"exception"</span>)<span class="hljs-function"><span class="hljs-keyword">function</span> <span class="hljs-title">process</span>(<span class="hljs-params">event</span>) </span>&#123;    <span class="hljs-keyword">var</span> message = event.Get(<span class="hljs-string">"message"</span>);    <span class="hljs-keyword">if</span> (message == <span class="hljs-literal">null</span> || message == <span class="hljs-literal">undefined</span> || message == <span class="hljs-string">''</span>) &#123;        <span class="hljs-built_in">console</span>.log(<span class="hljs-string">"failed to get message"</span>);        <span class="hljs-keyword">return</span>    &#125;    <span class="hljs-keyword">var</span> fieldValues = message.split(<span class="hljs-string">"$$"</span>);    <span class="hljs-keyword">if</span> (fieldValues.length&lt;fileds.length) &#123;        <span class="hljs-built_in">console</span>.log(<span class="hljs-string">"incorrect field length"</span>);        <span class="hljs-keyword">return</span>    &#125;    <span class="hljs-keyword">for</span> (<span class="hljs-keyword">var</span> i = <span class="hljs-number">0</span>; i &lt; fileds.length; ++i) &#123;        event.Put(fileds[i],fieldValues[i].trim())    &#125;&#125;</code></pre><p>写好脚本后调整配置测试既可，如果 js 编写有问题，可以通过 <code>console.log</code> 来打印日志进行不断的调试</p><pre><code class="hljs yaml"><span class="hljs-attr">filebeat.inputs:</span><span class="hljs-bullet">-</span> <span class="hljs-attr">type:</span> <span class="hljs-string">log</span>  <span class="hljs-attr">enabled:</span> <span class="hljs-literal">true</span>  <span class="hljs-attr">paths:</span>    <span class="hljs-bullet">-</span> <span class="hljs-string">/Users/natural/tmp/cdm.log</span>  <span class="hljs-attr">processors:</span>    <span class="hljs-bullet">-</span> <span class="hljs-attr">script:</span>        <span class="hljs-attr">lang:</span> <span class="hljs-string">js</span>        <span class="hljs-attr">id:</span> <span class="hljs-string">cdm</span>        <span class="hljs-attr">file:</span> <span class="hljs-string">cdm.js</span>  <span class="hljs-attr">multiline.pattern:</span> <span class="hljs-string">^\d&#123;4&#125;-\d&#123;1,2&#125;-\d&#123;1,2&#125;</span>  <span class="hljs-attr">multiline.match:</span> <span class="hljs-string">after</span>  <span class="hljs-attr">multiline.negate:</span> <span class="hljs-literal">true</span>  <span class="hljs-attr">multiline.timeout:</span> <span class="hljs-string">5s</span></code></pre><p><strong>需要注意的是目前 <code>lang</code> 的值只能为 <code>javascript</code> 和 <code>js</code>(官方文档写的只能是 <code>javascript</code>)；根据代码来看后续 script processor 有可能支持其他脚本语言，个人认为主要取决于其他脚本语言有没有纯 go 实现的 runtime，如果有的话未来很有可能被整合到 script processor 中。</strong></p><p><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1gegg80j1gmj31nc0u0wpa.jpg" srcset="/img/loading.gif" alt="script processor"></p><h2 id="六、其他-processor"><a href="#六、其他-processor" class="headerlink" title="六、其他 processor"></a>六、其他 processor</h2><p>研究完 script processor 后我顿时对其他 processor 也产生了兴趣，随着更多的查看processor 文档，我发现其实大部份过滤分割能力已经有很多 processor 进行了实现，<strong>其完善程度外加可扩展的 script processor 实际能力已经足矣替换掉 logstash 的日志分割过滤处理了。</strong>比如上面的日志切割其实使用 dissect processor 实现更加简单(这个配置并不完善，只是样例):</p><pre><code class="hljs yaml"><span class="hljs-attr">processors:</span>  <span class="hljs-bullet">-</span> <span class="hljs-attr">dissect:</span>      <span class="hljs-attr">field:</span> <span class="hljs-string">"message"</span>      <span class="hljs-attr">tokenizer:</span> <span class="hljs-string">"<span class="hljs-template-variable">%&#123;timestamp&#125;</span>$$<span class="hljs-template-variable">%&#123;hostname&#125;</span>$$<span class="hljs-template-variable">%&#123;thread&#125;</span>$$<span class="hljs-template-variable">%&#123;level&#125;</span>$$<span class="hljs-template-variable">%&#123;logger&#125;</span>$$<span class="hljs-template-variable">%&#123;file&#125;</span>$$<span class="hljs-template-variable">%&#123;line&#125;</span>$$<span class="hljs-template-variable">%&#123;serviceName&#125;</span>$$<span class="hljs-template-variable">%&#123;traceId&#125;</span>$$<span class="hljs-template-variable">%&#123;feTraceId&#125;</span>$$<span class="hljs-template-variable">%&#123;msg&#125;</span>$$<span class="hljs-template-variable">%&#123;exception&#125;</span>$$"</span></code></pre><p>除此之外还有很多 processor，例如 <code>drop_event</code>、<code>drop_fields</code>、<code>timestamp</code> 等等，感兴趣的可以自行研究。</p><h2 id="七、总结"><a href="#七、总结" class="headerlink" title="七、总结"></a>七、总结</h2><p>基本上折腾完以后做了一个总结:</p><ul><li><strong>filebeat module</strong>: 这就是个华而不实的东西，每次修改需要重新编译且扩展能力几近于零，最蛋疼的是实际逻辑通过 es 来完成；我能想到的是唯一应用场景就是官方给我们弄一些 demo 来炫耀用的，比如 nginx module；实际生产中 nginx 日志格式保持原封不动的人我相信少之又少。</li><li><strong>filebeat custom processor</strong>: 每次修改也需要重新编译且需要会 go 语言还有相关工具链，但是好处就是完全通过代码实现真正的为所欲为；扩展性取决于外部是否对特定位置做了可配置化，比如预留可以配置切割用正则表达式的变量等，最终取决于代码编写者(怎么为所欲为的问题)。</li><li><strong>filebeat script processor</strong>: 完整 ECMA 5.1 js 规范支持，代码化对日志进行为所欲为，修改不需要重新编译；普通用户我个人觉得是首选，当然同时会写 go 和 js 的就看你想用哪个了。</li><li><strong>filebeat other processor</strong>: 基本上实现了很多 logstash 的功能，简单用用很舒服，复杂场景还是得撸代码；但是一些特定的 processor 很实用，比如加入宿主机信息的 add_host_metadata processor 等。</li></ul>]]></content>
    
    
    <categories>
      
      <category>Golang</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Kubernetes</tag>
      
      <tag>Golang</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>如何不通过 docker 下载 docker image</title>
    <link href="/2020/03/31/how-to-download-docker-image-without-docker/"/>
    <url>/2020/03/31/how-to-download-docker-image-without-docker/</url>
    
    <content type="html"><![CDATA[<blockquote><p>这是一个比较骚的动作，但是事实上确实有这个需求，折腾半天找工具看源码，这里记录一下(不想看源码分析啥的请直接跳转到第五部份)。</p></blockquote><h2 id="一、起因"><a href="#一、起因" class="headerlink" title="一、起因"></a>一、起因</h2><p>由于最近某个爬虫业务需要抓取微信公众号的一些文章，某开发小伙伴想到了通过启动安卓虚拟机然后抓包的方式实现；经过几番寻找最终我们选择采用 docker 的方式启动安卓虚拟机，docker 里安卓虚拟机比较成熟的项目我们找到了 <a href="https://github.com/budtmo/docker-android" target="_blank" rel="noopener">https://github.com/budtmo/docker-android</a> 这个项目；但是由于众所周知的原因这个 2G+ 的镜像国内拉取是非常慢的，于是我想到了通过国外 VPS 拉取然后 scp 回来… 由于贫穷的原因，当我实际操作的时候遇到了比较尴尬的问题: <strong>VPS 磁盘空间 25G，镜像拉取后解压接近 10G，我需要 <code>docker save</code> 成 tar 包再进行打包成 <code>tar.gz</code> 格式 scp 回来，这个时候空间不够用了…</strong>所以我当时就在想有没有办法让 docker daemon 拉取镜像时不解压？或者说自己通过 HTTP 下载镜像直接存储为 tar？</p><h2 id="二、尝试造轮子"><a href="#二、尝试造轮子" class="headerlink" title="二、尝试造轮子"></a>二、尝试造轮子</h2><p>当出现了上面的问题后，我第一反应就是:</p><ul><li>1、docker 拆分为 moby</li><li>2、moby 模块化，大部份开源到 <a href="https://github.com/containers" target="_blank" rel="noopener">containers</a></li><li>3、<a href="https://github.com/containers/image" target="_blank" rel="noopener">containers/image</a> 项目是镜像部份源码</li><li>4、看 <a href="https://github.com/containers/image" target="_blank" rel="noopener">containers/image</a> 源码造轮子</li><li>5、不确定是否需要 <a href="https://github.com/containers/storage" target="_blank" rel="noopener">containers/storage</a> 做存储</li></ul><h2 id="三、猜测源码"><a href="#三、猜测源码" class="headerlink" title="三、猜测源码"></a>三、猜测源码</h2><p>当我查看 <a href="https://github.com/containers/image" target="_blank" rel="noopener">containers/image</a> README 文档时发现其提到了 <a href="https://github.com/containers/skopeo" target="_blank" rel="noopener">skopeo</a> 项目，并且很明确的说了</p><blockquote><p>The containers/image project is only a library with no user interface; you can either incorporate it into your Go programs, or use the skopeo tool:<br>The skopeo tool uses the containers/image library and takes advantage of many of its features, e.g. skopeo copy exposes the containers/image/copy.Image functionality.</p></blockquote><p>那么也就是说镜像下载这块很大可能应该调用 <code>containers/image/copy.Image</code> 完成，随即就看了下源码文档</p><p><img src="https://cdn.oss.link/markdown/tv7iy.png" srcset="/img/loading.gif" alt=""></p><p>很明显，<code>types.ImageReference</code>、<code>Options</code> 里面的属性啥的我完全看不懂… 😂😂😂</p><h2 id="四、看-skopeo-源码"><a href="#四、看-skopeo-源码" class="headerlink" title="四、看 skopeo 源码"></a>四、看 skopeo 源码</h2><p>当 <a href="https://github.com/containers/image" target="_blank" rel="noopener">containers/image</a> 源码看不懂时，突然想到 <a href="https://github.com/containers/skopeo" target="_blank" rel="noopener">skopeo</a> 调用的是这个玩意，那么依葫芦画瓢看 <a href="https://github.com/containers/skopeo" target="_blank" rel="noopener">skopeo</a> 源码应该能行；接下来常规操作 clone skopeo 源码然后编译运行测试；编译后 skopeo 支持命令如下</p><pre><code class="hljs sh">NAME:   skopeo - Various operations with container images and container image registriesUSAGE:   skopeo [global options] <span class="hljs-built_in">command</span> [<span class="hljs-built_in">command</span> options] [arguments...]VERSION:   0.1.42-dev commit: 018a0108b103341526b41289c434b59d65783f6fCOMMANDS:   copy               Copy an IMAGE-NAME from one location to another   inspect            Inspect image IMAGE-NAME   delete             Delete image IMAGE-NAME   manifest-digest    Compute a manifest digest of a file   sync               Synchronize one or more images from one location to another   standalone-sign    Create a signature using <span class="hljs-built_in">local</span> files   standalone-verify  Verify a signature using <span class="hljs-built_in">local</span> files   list-tags          List tags <span class="hljs-keyword">in</span> the transport/repository specified by the REPOSITORY-NAME   <span class="hljs-built_in">help</span>, h            Shows a list of commands or <span class="hljs-built_in">help</span> <span class="hljs-keyword">for</span> one <span class="hljs-built_in">command</span>GLOBAL OPTIONS:   --debug                     <span class="hljs-built_in">enable</span> debug output   --policy value              Path to a trust policy file   --insecure-policy           run the tool without any policy check   --registries.d DIR          use registry configuration files <span class="hljs-keyword">in</span> DIR (e.g. <span class="hljs-keyword">for</span> container signature storage)   --override-arch ARCH        use ARCH instead of the architecture of the machine <span class="hljs-keyword">for</span> choosing images   --override-os OS            use OS instead of the running OS <span class="hljs-keyword">for</span> choosing images   --override-variant VARIANT  use VARIANT instead of the running architecture variant <span class="hljs-keyword">for</span> choosing images   --<span class="hljs-built_in">command</span>-timeout value     timeout <span class="hljs-keyword">for</span> the <span class="hljs-built_in">command</span> execution (default: 0s)   --<span class="hljs-built_in">help</span>, -h                  show <span class="hljs-built_in">help</span>   --version, -v               <span class="hljs-built_in">print</span> the version</code></pre><p><strong>我掐指一算调用 copy 命令应该是我要找的那个它</strong>，所以常规操作打开源码直接看</p><p><img src="https://cdn.oss.link/markdown/urn3l.png" srcset="/img/loading.gif" alt="copy_cmd"></p><p>通过继续追踪 <code>alltransports.ParseImageName</code> 方法最终可以得知 copy 命令的 <code>SOURCE-IMAGE</code> 和 <code>DESTINATION-IMAGE</code> 都支持哪些写法</p><p><img src="https://cdn.oss.link/markdown/ush4t.png" srcset="/img/loading.gif" alt="tp_register"></p><p><strong>每一个 Transport 的实现都提供了 Name 方法，其名称即为 src 或 dest 镜像名称的前缀，例如 <code>docker://nginx:1.17.6</code></strong></p><p><img src="https://cdn.oss.link/markdown/7fpap.png" srcset="/img/loading.gif" alt="tp_docker"></p><p><strong>经过测试不同的 Transport 格式并不完全一致(具体看源码)，比如 <code>docker://nginx:1.17.6</code> 和 <code>dir:/tmp/nginx</code>；同时这些 Transport 并非完全都适用与 src 与 dest，比如 <code>tarball:/tmp/nginx.tar</code> 支持 src 而不支持 dest；</strong>其判断核心依据为 <code>ImageReference.NewImageSource</code> 和 <code>ImageReference.NewImageDestination</code> 方法实现是否返回 error</p><p><img src="https://cdn.oss.link/markdown/jb087.png" srcset="/img/loading.gif" alt="NewImageDestination"></p><p>当我看了一会各种 Transport 源码后我发现一件事: <strong>这特么不就是我要造的轮子么！😱😱😱</strong></p><h2 id="五、skopeo-copy-使用"><a href="#五、skopeo-copy-使用" class="headerlink" title="五、skopeo copy 使用"></a>五、skopeo copy 使用</h2><h3 id="5-1、不借助-docker-下载镜像"><a href="#5-1、不借助-docker-下载镜像" class="headerlink" title="5.1、不借助 docker 下载镜像"></a>5.1、不借助 docker 下载镜像</h3><pre><code class="hljs sh">skopeo --insecure-policy copy docker://nginx:1.17.6 docker-archive:/tmp/nginx.tar</code></pre><p><code>--insecure-policy</code> 选项用于忽略安全策略配置文件，该命令将会直接通过 http 下载目标镜像并存储为 <code>/tmp/nginx.tar</code>，此文件可以直接通过 <code>docker load</code> 命令导入</p><h3 id="5-2、从-docker-daemon-导出镜像"><a href="#5-2、从-docker-daemon-导出镜像" class="headerlink" title="5.2、从 docker daemon 导出镜像"></a>5.2、从 docker daemon 导出镜像</h3><pre><code class="hljs sh">skopeo --insecure-policy copy docker-daemon:nginx:1.17.6 docker-archive:/tmp/nginx.tar</code></pre><p>该命令将会从 docker daemon 导出镜像到 <code>/tmp/nginx.tar</code>；为什么不用 <code>docker save</code>？因为我是偷懒 dest 也是 docker-archive，实际上 skopeo 可以导出为其他格式比如 <code>oci</code>、<code>oci-archive</code>、<code>ostree</code> 等</p><h3 id="5-3、其他命令"><a href="#5-3、其他命令" class="headerlink" title="5.3、其他命令"></a>5.3、其他命令</h3><p>skopeo 还有一些其他的实用命令，比如 <code>sync</code> 可以在两个位置之间同步镜像(😂早知道我还写个鸡儿 gcrsync)，<code>inspect</code> 可以查看镜像信息等，迫于本人太懒，剩下的请自行查阅文档、<code>--help</code> 以及源码(没错，整篇文章都没写 skopeo 怎么安装)。</p>]]></content>
    
    
    <categories>
      
      <category>Docker</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Linux</tag>
      
      <tag>Docker</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>kubeadm 证书期限调整</title>
    <link href="/2020/01/21/how-to-extend-the-validity-of-your-kubeadm-certificate/"/>
    <url>/2020/01/21/how-to-extend-the-validity-of-your-kubeadm-certificate/</url>
    
    <content type="html"><![CDATA[<h2 id="一、证书管理"><a href="#一、证书管理" class="headerlink" title="一、证书管理"></a>一、证书管理</h2><p>kubeadm 集群安装完成后，证书管理上实际上大致是两大类型:</p><ul><li>自动滚动续期</li><li>手动定期续期</li></ul><p>自动滚动续期类型的证书目前从我所阅读文档和实际测试中目前只有 kubelet 的 client 证书；kubelet client 证书自动滚动涉及到了 <a href="https://kubernetes.io/docs/reference/command-line-tools-reference/kubelet-tls-bootstrapping/" target="_blank" rel="noopener">TLS bootstrapping</a> 部份，<strong>其核心由两个 ClusterRole 完成(<code>system:certificates.k8s.io:certificatesigningrequests:nodeclient</code> 和 <code>system:certificates.k8s.io:certificatesigningrequests:selfnodeclient</code>)，针对这两个 ClusterRole kubeadm 在引导期间创建了 <a href="https://kubernetes.io/docs/reference/setup-tools/kubeadm/implementation-details/#create-a-bootstrap-token" target="_blank" rel="noopener">bootstrap token</a> 来完成引导期间证书签发(该 Token 24h 失效)，后续通过预先创建的 ClusterRoleBinding(<code>kubeadm:node-autoapprove-bootstrap</code> 和 <code>kubeadm:node-autoapprove-certificate-rotation</code>) 完成自动的 node 证书续期；</strong>kubelet client 证书续期部份涉及到 TLS bootstrapping 太多了，有兴趣的可以仔细查看(最后还是友情提醒: <strong>用 kubeadm 一定要看看 <a href="https://kubernetes.io/docs/reference/setup-tools/kubeadm/implementation-details" target="_blank" rel="noopener">Implementation details</a></strong>)。</p><p>手动续期的证书目前需要在到期前使用 kubeadm 命令自行续期，这些证书目前可以通过以下命令列出</p><pre><code class="hljs sh"><span class="hljs-comment"># 不要在意我的证书过期时间是 10 年，下面会说</span>k1.node ➜ kubeadm alpha certs check-expiration[check-expiration] Reading configuration from the cluster...[check-expiration] FYI: You can look at this config file with <span class="hljs-string">'kubectl -n kube-system get cm kubeadm-config -oyaml'</span>CERTIFICATE                EXPIRES                  RESIDUAL TIME   CERTIFICATE AUTHORITY   EXTERNALLY MANAGEDadmin.conf                 Dec 06, 2029 20:58 UTC   9y                                      noapiserver                  Dec 06, 2029 20:59 UTC   9y              ca                      noapiserver-kubelet-client   Dec 06, 2029 20:59 UTC   9y              ca                      nocontroller-manager.conf    Dec 06, 2029 20:59 UTC   9y                                      nofront-proxy-client         Dec 06, 2029 20:59 UTC   9y              front-proxy-ca          noscheduler.conf             Dec 06, 2029 20:59 UTC   9y                                      noCERTIFICATE AUTHORITY   EXPIRES                  RESIDUAL TIME   EXTERNALLY MANAGEDca                      Jan 13, 2030 08:45 UTC   9y              nofront-proxy-ca          Jan 13, 2030 08:45 UTC   9y              no</code></pre><h2 id="二、证书期限调整"><a href="#二、证书期限调整" class="headerlink" title="二、证书期限调整"></a>二、证书期限调整</h2><p>上面已经提到了，手动管理部份的证书需要自己用命令续签(<code>kubeadm alpha certs renew all</code>)，而且你会发现续签以后有效期还是 1 年；kubeadm 的初衷是 <strong>“为快速创建 kubernetes 集群的最佳实践”</strong>，当然最佳实践包含确保证书安全性，毕竟 Let’s Encrypt 的证书有效期只有 3 个月的情况下 kubeadm 有效期有 1 年已经很不错了；但是对于最佳实践来说，我们公司的集群安全性并不需要那么高，一年续期一次无疑在增加运维人员心智负担(它并不最佳)，所以我们迫切需要一种 “一劳永逸” 的解决方案；当然我目前能想到的就是找到证书签发时在哪设置的有效期，然后想办法改掉它。</p><h3 id="2-1、源码分析"><a href="#2-1、源码分析" class="headerlink" title="2.1、源码分析"></a>2.1、源码分析</h3><p>目前通过宏观角度看整个 kubeadm 集群搭建过程，其中涉及到证书签署大致有两大部份: init  阶段和后期 renew，下面开始分析两个阶段的源码</p><h4 id="2-1-1、init-阶段"><a href="#2-1-1、init-阶段" class="headerlink" title="2.1.1、init 阶段"></a>2.1.1、init 阶段</h4><p>由于 kubernetes 整个命令行都是通过 cobra 库构建的，那么根据这个库的习惯首先直接从 <code>cmd</code> 包开始翻，而 kubernetes 源码组织的又比较清晰进而直接定位到 kubeadm 命令包下面；接着打开 <code>app</code> 目录一眼就看到了 <code>phases</code>… <code>phases</code> 顾名思义啊，整个 init 都是通过不同的 <code>phases</code> 完成的，那么直接去 <code>phases</code> 包下面找证书阶段的源码既可</p><p><img src="http://cdn.oss.link/markdown/ssdo7.jpg" srcset="/img/loading.gif" alt="init_source"></p><p>进入到这个 <code>certs.go</code> 里面，直接列出所有方法，go 的规范里只有首字母大写才会被暴露出去，那么我们直接查看这些方法名既可；从名字上很轻松的看到了这个方法…基本上就是它了</p><p><img src="http://cdn.oss.link/markdown/uoqx4.jpg" srcset="/img/loading.gif" alt="certs.go"></p><p>通过这个方法的代码会发现最终还是调用了 <code>certSpec.CreateFromCA(cfg, caCert, caKey)</code>，那么接着看看这个方法</p><p><img src="http://cdn.oss.link/markdown/psrho.jpg" srcset="/img/loading.gif" alt="pkiutil.NewCertAndKey"></p><p>通过这个方法继续往下翻发现调用了 <code>pkiutil.NewCertAndKey(caCert, caKey, cfg)</code>，这个方法里最终调用了 <code>NewSignedCert(config, key, caCert, caKey)</code></p><p><img src="http://cdn.oss.link/markdown/nel5u.jpg" srcset="/img/loading.gif" alt="NewSignedCert"></p><p>从 <code>NewSignedCert</code> 方法里看到证书有效期实际上是个常量，<strong>那也就意味着我改了这个常量 init 阶段的证书有效期八九不离十的就变了，再通过包名看这个是个 <code>pkiutil</code>… <code>xxxxxutil</code> 明显是公共的，所以推测改了它 renew 阶段应该也会变</strong></p><p><img src="http://cdn.oss.link/markdown/t3amy.jpg" srcset="/img/loading.gif" alt="CertificateValidity"></p><h4 id="2-1-2、renew-阶段"><a href="#2-1-2、renew-阶段" class="headerlink" title="2.1.2、renew 阶段"></a>2.1.2、renew 阶段</h4><p>renew 阶段也是老套路，不过稳妥点先从 cmd 找起来，所以先看 <code>alpha</code> 包下的 <code>certs.go</code>；这时候方法名语义清晰就很有好处，一下就能找到 <code>newCmdCertsRenewal</code> 方法</p><p><img src="http://cdn.oss.link/markdown/amupo.jpg" srcset="/img/loading.gif" alt="alpha_certs.go"></p><p>而这个 <code>newCmdCertsRenewal</code> 方法实际上没啥实现，所以目测实现是从 <code>getRenewSubCommands</code> 实现的</p><p><img src="http://cdn.oss.link/markdown/8c38y.jpg" srcset="/img/loading.gif" alt="getRenewSubCommands"></p><p>看了 <code>getRenewSubCommands</code> 以后发现上面全是命令行库、配置文件参数啥的处理，核心在 <code>renewCert</code> 上，从这个方法里发现还有意外收获: <strong>renew 时实际上分两种情况处理，一种是使用了 <code>--use-api</code> 选项，另一种是未使用</strong>；当然根据上面的命令来说我们没使用，那么看 else 部份就行了(没看源码之前我特么居然没看 <code>--help</code> 不知道有这个选项)</p><p><img src="http://cdn.oss.link/markdown/9zsgp.jpg" srcset="/img/loading.gif" alt="renewCert"></p><p>else 部份源码最终还是调用了 <code>RenewUsingLocalCA</code> 方法，这个方法一直往下跟会有一个 <code>Renew</code> 方法</p><p><img src="http://cdn.oss.link/markdown/s3a5c.jpg" srcset="/img/loading.gif" alt="Renew"></p><p>这个方法一点进去… <strong>我上面的想法是对的</strong></p><p><img src="http://cdn.oss.link/markdown/08cnb.jpg" srcset="/img/loading.gif" alt="FileRenewer_Renew"></p><h4 id="2-1-3、其他推测"><a href="#2-1-3、其他推测" class="headerlink" title="2.1.3、其他推测"></a>2.1.3、其他推测</h4><p>根据刚刚查看代码可以看到在 renew 阶段判断了 <code>--use-api</code> 选项是否使用，通过跟踪源码发现最终会调用到 <code>RenewUsingCSRAPI</code> 方法上，<code>RenewUsingCSRAPI</code> 会调用集群 CSR Api 执行证书签署</p><p><img src="http://cdn.oss.link/markdown/xivs9.png" srcset="/img/loading.gif" alt="RenewUsingCSRAPI"></p><p>有了这个发现后基本上可以推测出这一步通过集群完成，那么按理说是应该受到 <code>kube-controller-manager</code> 组件的 <code>--experimental-cluster-signing-duration</code> 影响。</p><h3 id="2-2、测试验证"><a href="#2-2、测试验证" class="headerlink" title="2.2、测试验证"></a>2.2、测试验证</h3><h4 id="2-2-1、验证修改源码"><a href="#2-2-1、验证修改源码" class="headerlink" title="2.2.1、验证修改源码"></a>2.2.1、验证修改源码</h4><p>想验证修改源码是否有效只需要修改源码重新 build 出 kubeadm 命令，然后使用这个特定版本的 kubeadm renew 证书测试既可，源码调整的位置如下</p><p><img src="http://cdn.oss.link/markdown/qaavr.png" srcset="/img/loading.gif" alt="update_source"></p><p>然后命令行下执行 <code>make cross</code> 进行跨平台交叉编译(如果过你在 linux amd64 平台下则直接 <code>make</code> 既可)</p><pre><code class="hljs sh">➜  kubernetes git:(v1.17.4) ✗ make crossgrep: /proc/meminfo: No such file or directorygrep: /proc/meminfo: No such file or directory+++ [0116 23:43:19] Multiple platforms requested and available 64G &gt;= threshold 40G, building platforms <span class="hljs-keyword">in</span> parallel+++ [0116 23:43:19] Building go targets <span class="hljs-keyword">for</span> &#123;linux/amd64 linux/arm linux/arm64 linux/s390x linux/ppc64le&#125; <span class="hljs-keyword">in</span> parallel (output will appear <span class="hljs-keyword">in</span> a burst when complete):    cmd/kube-proxy    cmd/kube-apiserver    cmd/kube-controller-manager    cmd/kubelet    cmd/kubeadm    cmd/kube-scheduler    vendor/k8s.io/apiextensions-apiserver    cluster/gce/gci/mounter+++ [0116 23:43:19] linux/amd64: build started+++ [0116 23:47:24] linux/amd64: build finished+++ [0116 23:43:19] linux/arm: build started+++ [0116 23:47:23] linux/arm: build finished+++ [0116 23:43:19] linux/arm64: build started+++ [0116 23:47:23] linux/arm64: build finished+++ [0116 23:43:19] linux/s390x: build started+++ [0116 23:47:24] linux/s390x: build finished+++ [0116 23:43:19] linux/ppc64le: build started+++ [0116 23:47:24] linux/ppc64le: build finishedgrep: /proc/meminfo: No such file or directorygrep: /proc/meminfo: No such file or directory+++ [0116 23:47:52] Multiple platforms requested and available 64G &gt;= threshold 40G, building platforms <span class="hljs-keyword">in</span> parallel+++ [0116 23:47:52] Building go targets <span class="hljs-keyword">for</span> &#123;linux/amd64 linux/arm<span class="hljs-comment"># ... 省略编译日志</span></code></pre><p>编译完成后能够在 <code>_output/local/bin/linux/amd64</code> 下找到刚刚编译成功的 <code>kubeadm</code> 文件，将编译好的 kubeadm scp 到已经存在集群上执行 renew，然后查看证书时间</p><p><img src="http://cdn.oss.link/markdown/i3laa.png" srcset="/img/loading.gif" alt="kubeadm_renew"></p><p><strong>经过测试后确认源码修改方式有效</strong></p><h4 id="2-2-2、验证调整-CSR-API"><a href="#2-2-2、验证调整-CSR-API" class="headerlink" title="2.2.2、验证调整 CSR API"></a>2.2.2、验证调整 CSR API</h4><p>根据推测当使用 <code>--use-api</code> 会受到 <code>kube-controller-manager</code> 组件的 <code>--experimental-cluster-signing-duration</code> 影响，从而从集群中下发证书；所以首先在启动集群时需要将 <code>--experimental-cluster-signing-duration</code> 调整为 10 年，然后再进行测试</p><pre><code class="hljs yaml"><span class="hljs-attr">controllerManager:</span>  <span class="hljs-attr">extraArgs:</span>    <span class="hljs-attr">v:</span> <span class="hljs-string">"4"</span>    <span class="hljs-attr">node-cidr-mask-size:</span> <span class="hljs-string">"19"</span>    <span class="hljs-attr">deployment-controller-sync-period:</span> <span class="hljs-string">"10s"</span>    <span class="hljs-comment"># 在 kubeadm 配置文件中设置证书有效期为 10 年</span>    <span class="hljs-attr">experimental-cluster-signing-duration:</span> <span class="hljs-string">"86700h"</span>    <span class="hljs-attr">node-monitor-grace-period:</span> <span class="hljs-string">"20s"</span>    <span class="hljs-attr">pod-eviction-timeout:</span> <span class="hljs-string">"2m"</span>    <span class="hljs-attr">terminated-pod-gc-threshold:</span> <span class="hljs-string">"30"</span></code></pre><p>然后使用 <code>--use-api</code> 选项进行 renew</p><pre><code class="hljs sh">kubeadm alpha certs renew all --use-api</code></pre><p>此时会发现日志中打印出 <code>[certs] Certificate request &quot;kubeadm-cert-kubernetes-admin-648w4&quot; created</code> 字样，接下来从 <code>kube-system</code> 的 namespace 中能够看到相关 csr</p><p><img src="https://cdn.oss.link/markdown/54awl.png" srcset="/img/loading.gif" alt="list_csr"></p><p>这时我们开始手动批准证书，每次批准完成一个 csr，紧接着 kubeadm 会创建另一个 csr</p><p><img src="https://cdn.oss.link/markdown/tdde7.png" srcset="/img/loading.gif" alt="approve_csr"></p><p>当所有 csr 被批准后，再次查看集群证书发现证书期限确实被调整了</p><p><img src="https://cdn.oss.link/markdown/081qe.png" srcset="/img/loading.gif" alt="success"></p><h2 id="三、总结"><a href="#三、总结" class="headerlink" title="三、总结"></a>三、总结</h2><p>总结一下，调整 kubeadm 证书期限有两种方案；第一种直接修改源码，耗时耗力还得会 go，最后还要跑跨平台编译(很耗时)；第二种在启动集群时调整 <code>kube-controller-manager</code> 组件的 <code>--experimental-cluster-signing-duration</code> 参数，集群创建好后手动 renew 一下并批准相关 csr。</p><p>两种方案各有利弊，修改源码方式意味着在 client 端签发处理，不会对集群产生永久性影响，也就是说哪天你想 “反悔了” 你不需要修改集群什么配置，直接用官方 kubeadm renew 一下就会变回一年期限的证书；改集群参数实现的方式意味着你不需要懂 go 代码，只需要常规的集群配置既可实现，同时你也不需要跑几十分钟的交叉编译，不需要为编译过程中的网络问题而烦恼；所以最后使用哪种方案因人因情况而定吧。</p>]]></content>
    
    
    <categories>
      
      <category>Kubernetes</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Kubernetes</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>kubeadm 集群升级</title>
    <link href="/2020/01/21/how-to-upgrade-kubeadm-cluster/"/>
    <url>/2020/01/21/how-to-upgrade-kubeadm-cluster/</url>
    
    <content type="html"><![CDATA[<h2 id="一、升级前准备"><a href="#一、升级前准备" class="headerlink" title="一、升级前准备"></a>一、升级前准备</h2><ul><li>确保你的集群是 kubeadm 搭建的(等同于废话)</li><li>确保当前集群已经完成 HA(多个 master 节点)</li><li>确保在夜深人静的时候(无大量业务流量)</li><li>确保集群版本大于 v1.16.0</li><li>确保已经仔细阅读了目标版本 CHANGELOG</li><li>确保做好了完整地集群备份</li></ul><h2 id="二、升级注意事项"><a href="#二、升级注意事项" class="headerlink" title="二、升级注意事项"></a>二、升级注意事项</h2><ul><li>升级后所有集群组件 Pod 会重启(hash 变更)</li><li><strong>升级时 <code>kubeadm</code> 版本必须大于或等于目标版本</strong></li><li><strong>升级期间所有 <code>kube-proxy</code> 组件会有一次全节点滚动更新</strong></li><li><strong>升级只支持顺次进行，不支持跨版本升级(You only can upgrade from one MINOR version to the next MINOR version, or between PATCH versions of the same MINOR. That is, you cannot skip MINOR versions when you upgrade. For example, you can upgrade from 1.y to 1.y+1, but not from 1.y to 1.y+2.)</strong></li></ul><p>关于升级版本问题…虽然是这么说的，但是官方文档样例代码里是从 <code>v1.16.0</code> 升级到 <code>v1.17.0</code>；可能是我理解有误，跨大版本升级好像官方没提，具体啥后果不清楚…</p><h2 id="三、升级-Master"><a href="#三、升级-Master" class="headerlink" title="三、升级 Master"></a>三、升级 Master</h2><blockquote><p>事实上所有升级工作主要是针对 master 节点做的，所以整个升级流程中最重要的是如何把 master 升级好。</p></blockquote><h3 id="3-1、升级-kubeadm、kubectl"><a href="#3-1、升级-kubeadm、kubectl" class="headerlink" title="3.1、升级 kubeadm、kubectl"></a>3.1、升级 kubeadm、kubectl</h3><p>首先由于升级限制，必须先将 <code>kubeadm</code> 和 <code>kubectl</code> 升级到大于等于目标版本</p><pre><code class="hljs sh"><span class="hljs-comment"># replace x in 1.17.x-00 with the latest patch version</span>apt-mark unhold kubeadm kubectlapt-get updateapt-get install -y kubeadm=1.17.x-00 kubectl=1.17.x-00apt-mark hold kubeadm kubectl</code></pre><p>当然如果你之前没有 <code>hold</code> 住这几个软件包的版本，那么就不需要 <code>unhold</code>；我的做法可能比较极端…一般为了防止后面的误升级安装完成后我会直接 <code>rename</code> 掉相关软件包的 <code>apt source</code> 配置(从根本上防止手贱)。</p><h3 id="3-2、升级前准备"><a href="#3-2、升级前准备" class="headerlink" title="3.2、升级前准备"></a>3.2、升级前准备</h3><h4 id="3-2-1、配置修改"><a href="#3-2-1、配置修改" class="headerlink" title="3.2.1、配置修改"></a>3.2.1、配置修改</h4><p>对于高级玩家一般安装集群时都会自定义很多组件参数，此时不可避免的会采用配置文件；所以安装完新版本的 <code>kubeadm</code> 后就要着手修改配置文件中的 <code>kubernetesVersion</code> 字段为目标集群版本，当然有其他变更也可以一起修改。</p><h4 id="3-2-2、节点驱逐"><a href="#3-2-2、节点驱逐" class="headerlink" title="3.2.2、节点驱逐"></a>3.2.2、节点驱逐</h4><p>如果你的 master 节点也当作 node 在跑一些工作负载，则需要执行以下命令驱逐这些 pod 并使节点进入维护模式(禁止调度)。</p><pre><code class="hljs sh"><span class="hljs-comment"># 将 NODE_NAME 换成 Master 节点名称</span>kubectl drain NODE_NAME --ignore-daemonsets</code></pre><h4 id="3-2-3、查看升级计划"><a href="#3-2-3、查看升级计划" class="headerlink" title="3.2.3、查看升级计划"></a>3.2.3、查看升级计划</h4><p>完成节点驱逐以后，可以通过以下命令查看升级计划；<strong>升级计划中列出了升级期间要执行的所有步骤以及相关警告，一定要仔细查看。</strong></p><pre><code class="hljs sh">k8s16.node ➜  ~ kubeadm upgrade plan --config /etc/kubernetes/kubeadm.yamlW0115 10:59:52.586204     983 validation.go:28] Cannot validate kube-proxy config - no validator is availableW0115 10:59:52.586241     983 validation.go:28] Cannot validate kubelet config - no validator is available[upgrade/config] Making sure the configuration is correct:W0115 10:59:52.605458     983 common.go:94] WARNING: Usage of the --config flag <span class="hljs-keyword">for</span> reconfiguring the cluster during upgrade is not recommended!W0115 10:59:52.607258     983 validation.go:28] Cannot validate kube-proxy config - no validator is availableW0115 10:59:52.607274     983 validation.go:28] Cannot validate kubelet config - no validator is available[preflight] Running pre-flight checks.[upgrade] Making sure the cluster is healthy:[upgrade] Fetching available versions to upgrade to[upgrade/versions] Cluster version: v1.17.0[upgrade/versions] kubeadm version: v1.17.1External components that should be upgraded manually before you upgrade the control plane with <span class="hljs-string">'kubeadm upgrade apply'</span>:COMPONENT   CURRENT   AVAILABLEEtcd        3.3.18    3.4.3-0Components that must be upgraded manually after you have upgraded the control plane with <span class="hljs-string">'kubeadm upgrade apply'</span>:COMPONENT   CURRENT       AVAILABLEKubelet     5 x v1.17.0   v1.17.1Upgrade to the latest version <span class="hljs-keyword">in</span> the v1.17 series:COMPONENT            CURRENT   AVAILABLEAPI Server           v1.17.0   v1.17.1Controller Manager   v1.17.0   v1.17.1Scheduler            v1.17.0   v1.17.1Kube Proxy           v1.17.0   v1.17.1CoreDNS              1.6.5     1.6.5You can now apply the upgrade by executing the following <span class="hljs-built_in">command</span>:        kubeadm upgrade apply v1.17.1_____________________________________________________________________</code></pre><h3 id="3-3、执行升级"><a href="#3-3、执行升级" class="headerlink" title="3.3、执行升级"></a>3.3、执行升级</h3><p>确认好升级计划以后，只需要一条命令既可将当前 master 节点升级到目标版本</p><pre><code class="hljs sh">kubeadm upgrade apply v1.17.1 --config /etc/kubernetes/kubeadm.yaml</code></pre><p>升级期间会打印很详细的日志，在日志中可以实时观察到升级流程，建议仔细关注升级流程；<strong>在最后一步会有一条日志 <code>[addons] Applied essential addon: kube-proxy</code>，这意味着集群开始更新 <code>kube-proxy</code> 组件，该组件目前是通过 <code>daemonset</code> 方式启动的；这会意味着此时会造成全节点的 <code>kube-proxy</code> 更新；</strong>理论上不会有很大影响，但是升级是还是需要注意一下这一步操作，在我的观察中似乎 <code>kube-proxy</code> 也是通过滚动更新完成的，所以问题应该不大。</p><h3 id="3-4、升级-kubelet"><a href="#3-4、升级-kubelet" class="headerlink" title="3.4、升级 kubelet"></a>3.4、升级 kubelet</h3><p>在单个 master 上升级完成后，<strong>只会升级本节点的 master 相关组件和全节点的 <code>kube-proxy</code> 组件；</strong>由于 kubelet 是在宿主机安装的，所以需要通过包管理器手动升级 kubelet</p><pre><code class="hljs sh"><span class="hljs-comment"># replace x in 1.17.x-00 with the latest patch version</span>apt-mark unhold kubeletapt-get install -y kubelet=1.17.x-00apt-mark hold kubelet</code></pre><p>更新完成后执行 <code>systemctl restart kubelet</code> 重启，并等待启动成功既可；最后不要忘记解除当前节点的维护模式(<code>uncordon</code>)。</p><h3 id="3-5、升级其他-Master"><a href="#3-5、升级其他-Master" class="headerlink" title="3.5、升级其他 Master"></a>3.5、升级其他 Master</h3><p>当其中一个 master 节点升级完成后，其他的 master 升级就会相对简单的多；<strong>首先国际惯例升级一下 <code>kubeadm</code> 和 <code>kubectl</code> 软件包，然后直接在其他 master 节点执行 <code>kubeadm upgrade node</code> 既可。</strong>由于 apiserver 等组件配置已经在升级第一个 master 时上传到了集群的 configMap 中，所以事实上其他 master 节点只是正常拉取然后重启相关组件既可；这一步同样会输出详细日志，可以仔细观察进度，<strong>最后不要忘记升级之前先进入维护模式，升级完成后重新安装 <code>kubelet</code> 并关闭节点维护模式。</strong></p><h2 id="四、升级-Node"><a href="#四、升级-Node" class="headerlink" title="四、升级 Node"></a>四、升级 Node</h2><p>node 节点的升级实际上在升级完 master 节点以后不需要什么特殊操作，node 节点唯一需要升级的就是 <code>kubelet</code> 组件；<strong>首先在 node 节点执行 <code>kubeadm upgrade node</code> 命令，该命令会拉取集群内的 <code>kubelet</code> 配置文件，然后重新安装 <code>kubelet</code> 重启既可；</strong>同样升级 node 节点时不要忘记开启维护模式。针对于 CNI 组件请按需手动升级，并且确认好 CNI 组件的兼容版本。</p><h2 id="五、验证集群"><a href="#五、验证集群" class="headerlink" title="五、验证集群"></a>五、验证集群</h2><p>所有组件升级完成后，可以通过 <code>kubectl describe POD_NAME</code> 的方式验证 master 组件是否都升级到了最新版本；通过 <code>kuebctl version</code> 命令验证 api 相关信息(HA rr 轮训模式下可以多执行几遍)；还有就是通过 <code>kubectl get node -o wide</code> 查看相关 node 的信息，确保 <code>kubelet</code> 都升级成功，同时全部节点维护模式都已经关闭，其他细节可以参考<a href="https://kubernetes.io/docs/tasks/administer-cluster/kubeadm/kubeadm-upgrade" target="_blank" rel="noopener">官方文档</a>。</p>]]></content>
    
    
    <categories>
      
      <category>Kubernetes</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Kubernetes</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>kubeadm 搭建 HA kubernetes 集群</title>
    <link href="/2020/01/21/set-up-kubernetes-ha-cluster-by-kubeadm/"/>
    <url>/2020/01/21/set-up-kubernetes-ha-cluster-by-kubeadm/</url>
    
    <content type="html"><![CDATA[<h2 id="一、环境准备"><a href="#一、环境准备" class="headerlink" title="一、环境准备"></a>一、环境准备</h2><p>搭建环境为 5 台虚拟机，每台虚拟机配置为 4 核心 8G 内存，虚拟机 IP 范围为 <code>172.16.10.21~25</code>，其他软件配置如下</p><ul><li>os version: ubuntu 18.04</li><li>kubeadm version: 1.17.0</li><li>kubernetes version: 1.17.0</li><li>etcd version: 3.3.18</li><li>docker version: 19.03.5</li></ul><h2 id="二、HA-方案"><a href="#二、HA-方案" class="headerlink" title="二、HA 方案"></a>二、HA 方案</h2><p>目前的 HA 方案与官方的不同，官方 HA 方案推荐使用类似 haproxy 等工具进行 4 层代理 apiserver，但是同样会有一个问题就是我们还需要对这个 haproxy 做 HA；由于目前我们实际生产环境都是多个独立的小集群，所以单独弄 2 台 haproxy + keeplived 去维持这个 apiserver LB 的 HA 有点不划算；所以还是准备延续老的 HA 方案，将外部 apiserver 的 4 层 LB 前置到每个 node 节点上；<strong>目前是采用在每个 node 节点上部署 nginx 4 层代理所有 apiserver，nginx 本身资源消耗低而且请求量不大，综合来说对宿主机影响很小；</strong>以下为 HA 的大致方案图</p><p><img src="https://cdn.oss.link/markdown/mktld.png" srcset="/img/loading.gif" alt="ha"></p><h2 id="三、环境初始化"><a href="#三、环境初始化" class="headerlink" title="三、环境初始化"></a>三、环境初始化</h2><h3 id="3-1、系统环境"><a href="#3-1、系统环境" class="headerlink" title="3.1、系统环境"></a>3.1、系统环境</h3><p>由于个人操作习惯原因，目前已经将常用的初始化环境整理到一个小脚本里了，脚本具体参见 <a href="https://github.com/mritd/shell_scripts/blob/master/init_ubuntu.sh" target="_blank" rel="noopener">mritd/shell_scripts</a> 仓库，基本上常用的初始化内容为: </p><ul><li>设置 locale(en_US.UTF-8)</li><li>设置时区(Asia/Shanghai)</li><li>更新所有系统软件包(system update)</li><li>配置 vim(vim8 + 常用插件、配色)</li><li>ohmyzsh(别跟我说不兼容 bash 脚本，我就是喜欢)</li><li>docker</li><li>ctop(一个 docker 的辅助工具)</li><li>docker-compose</li></ul><p><strong>在以上初始化中，实际对 kubernetes 安装产生影响的主要有三个地方:</strong></p><ul><li><strong>docker 的 cgroup driver 调整为 systemd，具体参考 <a href="https://github.com/mritd/config/blob/master/docker/docker.service" target="_blank" rel="noopener">docker.service</a></strong></li><li><strong>docker 一定要限制 conatiner 日志大小，防止 apiserver 等日志大量输出导致磁盘占用过大</strong></li><li><strong>安装 <code>conntrack</code> 和 <code>ipvsadm</code>，后面可能需要借助其排查问题</strong></li></ul><h3 id="3-2、配置-ipvs"><a href="#3-2、配置-ipvs" class="headerlink" title="3.2、配置 ipvs"></a>3.2、配置 ipvs</h3><p>由于后面 kube-proxy 需要使用 ipvs 模式，所以需要对内核参数、模块做一些调整，调整命令如下:</p><pre><code class="hljs sh">cat &gt;&gt; /etc/sysctl.conf &lt;&lt;EOFnet.ipv4.ip_forward=1net.bridge.bridge-nf-call-iptables=1net.bridge.bridge-nf-call-ip6tables=1EOFsysctl -pcat &gt;&gt; /etc/modules &lt;&lt;EOFip_vsip_vs_lcip_vs_wlcip_vs_rrip_vs_wrrip_vs_lblcip_vs_lblcrip_vs_dhip_vs_ship_vs_foip_vs_nqip_vs_sedip_vs_ftpEOF</code></pre><p><strong>配置完成后切记需要重启，重启完成后使用 <code>lsmod | grep ip_vs</code> 验证相关 ipvs 模块加载是否正常，本文将主要使用 <code>ip_vs_wrr</code>，所以目前只关注这个模块既可。</strong></p><p><img src="https://cdn.oss.link/markdown/4irz1.png" srcset="/img/loading.gif" alt="ipvs_mode"></p><h2 id="四、安装-Etcd"><a href="#四、安装-Etcd" class="headerlink" title="四、安装 Etcd"></a>四、安装 Etcd</h2><h3 id="4-1、方案选择"><a href="#4-1、方案选择" class="headerlink" title="4.1、方案选择"></a>4.1、方案选择</h3><p>官方对于集群 HA 给出了两种有关于 Etcd 的部署方案: </p><ul><li>一种是深度耦合到 <code>control plane</code> 上，即每个 <code>control plane</code> 一个 etcd</li><li>另一种是使用外部的 Etcd 集群，通过在配置中指定外部集群让 apiserver 等组件连接</li></ul><p>在测试深度耦合 <code>control plane</code> 方案后，发现一些比较恶心的问题；比如说开始创建第二个 <code>control plane</code> 时配置写错了需要重建，此时你一旦删除第二个 <code>control plane</code> 会导致第一个 <code>control plane</code> 也会失败，原因是<strong>创建第二个 <code>control plane</code> 时 kubeadm 已经自动完成了 etcd 的集群模式，当删除第二个 <code>control plane</code> 的时候由于集群可用原因会导致第一个 <code>control plane</code> 下的 etcd 发现节点失联从而也不提供服务；</strong>所以综合考虑到后续迁移、灾备等因素，这里选择了将 etcd 放置在外部集群中；同样也方便我以后各种折腾应对一些极端情况啥的。</p><h3 id="4-2、部署-Etcd"><a href="#4-2、部署-Etcd" class="headerlink" title="4.2、部署 Etcd"></a>4.2、部署 Etcd</h3><p>确定了需要在外部部署 etcd 集群后，只需要开干就完事了；查了一下 ubuntu 官方源已经有了 etcd 安装包，但是版本比较老，测试了一下 golang 的 build 版本是 1.10；所以我还是选择了从官方 release 下载最新的版本安装；当然最后还是因为懒，我自己打了一个 deb 包… deb 包可以从这个项目 <a href="https://github.com/mritd/etcd-deb/releases" target="_blank" rel="noopener">mritd/etcd-deb</a> 下载，担心安全性的可以利用项目脚本自己打包，以下是安装过程:</p><pre><code class="hljs sh"><span class="hljs-comment"># 下载软件包</span>wget https://github.com/mritd/etcd-deb/releases/download/v3.3.18/etcd_3.3.18_amd64.debwget https://github.com/mritd/etcd-deb/releases/download/v3.3.18/cfssl_1.4.1_amd64.deb<span class="hljs-comment"># 安装 etcd(至少在 3 台节点上执行)</span>dpkg -i etcd_3.3.18_amd64.deb cfssl_1.4.1_amd64.deb</code></pre><p><strong>既然自己部署 etcd，那么证书签署啥的还得自己来了，证书签署这里借助 cfssl 工具，cfssl 目前提供了 deb 的 make target，但是没找到 deb 包，所以也自己 build 了(担心安全性的可自行去官方下载)；</strong>接着编辑一下 <code>/etc/etcd/cfssl/etcd-csr.json</code> 文件，用 <code>/etc/etcd/cfssl/create.sh</code> 脚本创建证书，并将证书复制到指定目录</p><pre><code class="hljs sh"><span class="hljs-comment"># 创建证书</span><span class="hljs-built_in">cd</span> /etc/etcd/cfssl &amp;&amp; ./create.sh<span class="hljs-comment"># 复制证书</span>mv /etc/etcd/cfssl/*.pem /etc/etcd/ssl</code></pre><p>最后在 3 台节点上修改配置，并将刚刚创建的证书同步到其他两台节点启动既可；下面是单台节点的配置样例</p><pre><code class="hljs sh"><span class="hljs-comment"># /etc/etcd/etcd.conf</span><span class="hljs-comment"># [member]</span>ETCD_NAME=etcd1ETCD_DATA_DIR=<span class="hljs-string">"/var/lib/etcd/data"</span>ETCD_WAL_DIR=<span class="hljs-string">"/var/lib/etcd/wal"</span>ETCD_SNAPSHOT_COUNT=<span class="hljs-string">"100"</span>ETCD_HEARTBEAT_INTERVAL=<span class="hljs-string">"100"</span>ETCD_ELECTION_TIMEOUT=<span class="hljs-string">"1000"</span>ETCD_LISTEN_PEER_URLS=<span class="hljs-string">"https://172.16.10.21:2380"</span>ETCD_LISTEN_CLIENT_URLS=<span class="hljs-string">"https://172.16.10.21:2379,http://127.0.0.1:2379"</span>ETCD_MAX_SNAPSHOTS=<span class="hljs-string">"5"</span>ETCD_MAX_WALS=<span class="hljs-string">"5"</span><span class="hljs-comment">#ETCD_CORS=""</span><span class="hljs-comment"># [cluster]</span>ETCD_INITIAL_ADVERTISE_PEER_URLS=<span class="hljs-string">"https://172.16.10.21:2380"</span><span class="hljs-comment"># if you use different ETCD_NAME (e.g. test), set ETCD_INITIAL_CLUSTER value for this name, i.e. "test=http://..."</span>ETCD_INITIAL_CLUSTER=<span class="hljs-string">"etcd1=https://172.16.10.21:2380,etcd2=https://172.16.10.22:2380,etcd3=https://172.16.10.23:2380"</span>ETCD_INITIAL_CLUSTER_STATE=<span class="hljs-string">"new"</span>ETCD_INITIAL_CLUSTER_TOKEN=<span class="hljs-string">"etcd-cluster"</span>ETCD_ADVERTISE_CLIENT_URLS=<span class="hljs-string">"https://172.16.10.21:2379"</span><span class="hljs-comment">#ETCD_DISCOVERY=""</span><span class="hljs-comment">#ETCD_DISCOVERY_SRV=""</span><span class="hljs-comment">#ETCD_DISCOVERY_FALLBACK="proxy"</span><span class="hljs-comment">#ETCD_DISCOVERY_PROXY=""</span><span class="hljs-comment">#ETCD_STRICT_RECONFIG_CHECK="false"</span>ETCD_AUTO_COMPACTION_RETENTION=<span class="hljs-string">"24"</span><span class="hljs-comment"># [proxy]</span><span class="hljs-comment">#ETCD_PROXY="off"</span><span class="hljs-comment">#ETCD_PROXY_FAILURE_WAIT="5000"</span><span class="hljs-comment">#ETCD_PROXY_REFRESH_INTERVAL="30000"</span><span class="hljs-comment">#ETCD_PROXY_DIAL_TIMEOUT="1000"</span><span class="hljs-comment">#ETCD_PROXY_WRITE_TIMEOUT="5000"</span><span class="hljs-comment">#ETCD_PROXY_READ_TIMEOUT="0"</span><span class="hljs-comment"># [security]</span>ETCD_CERT_FILE=<span class="hljs-string">"/etc/etcd/ssl/etcd.pem"</span>ETCD_KEY_FILE=<span class="hljs-string">"/etc/etcd/ssl/etcd-key.pem"</span>ETCD_CLIENT_CERT_AUTH=<span class="hljs-string">"true"</span>ETCD_TRUSTED_CA_FILE=<span class="hljs-string">"/etc/etcd/ssl/etcd-root-ca.pem"</span>ETCD_AUTO_TLS=<span class="hljs-string">"true"</span>ETCD_PEER_CERT_FILE=<span class="hljs-string">"/etc/etcd/ssl/etcd.pem"</span>ETCD_PEER_KEY_FILE=<span class="hljs-string">"/etc/etcd/ssl/etcd-key.pem"</span>ETCD_PEER_CLIENT_CERT_AUTH=<span class="hljs-string">"true"</span>ETCD_PEER_TRUSTED_CA_FILE=<span class="hljs-string">"/etc/etcd/ssl/etcd-root-ca.pem"</span>ETCD_PEER_AUTO_TLS=<span class="hljs-string">"true"</span><span class="hljs-comment"># [logging]</span><span class="hljs-comment">#ETCD_DEBUG="false"</span><span class="hljs-comment"># examples for -log-package-levels etcdserver=WARNING,security=DEBUG</span><span class="hljs-comment">#ETCD_LOG_PACKAGE_LEVELS=""</span><span class="hljs-comment"># [performance]</span>ETCD_QUOTA_BACKEND_BYTES=<span class="hljs-string">"5368709120"</span>ETCD_AUTO_COMPACTION_RETENTION=<span class="hljs-string">"3"</span></code></pre><p><strong>注意: 其他两台节点请调整 <code>ETCD_NAME</code> 为不重复的其他名称，调整 <code>ETCD_LISTEN_PEER_URLS</code>、<code>ETCD_LISTEN_CLIENT_URLS</code>、<code>ETCD_INITIAL_ADVERTISE_PEER_URLS</code>、<code>ETCD_ADVERTISE_CLIENT_URLS</code> 为其他节点对应的 IP；同时生产环境请将 <code>ETCD_INITIAL_CLUSTER_TOKEN</code> 替换为复杂的 token</strong></p><pre><code class="hljs sh"><span class="hljs-comment"># 同步证书</span>scp -r /etc/etcd/ssl 172.16.10.22:/etc/etcd/sslscp -r /etc/etcd/ssl 172.16.10.23:/etc/etcd/ssl<span class="hljs-comment"># 修复权限(3台节点都要执行)</span>chown -R etcd:etcd /etc/etcd<span class="hljs-comment"># 最后每个节点依次启动既可</span>systemctl start etcd</code></pre><p>启动完成后可以通过以下命令测试是否正常</p><pre><code class="hljs sh"><span class="hljs-comment"># 查看集群成员</span>k1.node ➜ etcdctl member list3cbbaf77904c6153, started, etcd2, https://172.16.10.22:2380, https://172.16.10.22:23798eb7652b6bd99c30, started, etcd1, https://172.16.10.21:2380, https://172.16.10.21:237991f4e10726460d8c, started, etcd3, https://172.16.10.23:2380, https://172.16.10.23:2379<span class="hljs-comment"># 检测集群健康状态</span>k1.node ➜ etcdctl endpoint health --cacert /etc/etcd/ssl/etcd-root-ca.pem --cert /etc/etcd/ssl/etcd.pem --key /etc/etcd/ssl/etcd-key.pem --endpoints https://172.16.10.21:2379,https://172.16.10.22:2379,https://172.16.10.23:2379https://172.16.10.21:2379 is healthy: successfully committed proposal: took = 16.632246mshttps://172.16.10.23:2379 is healthy: successfully committed proposal: took = 21.122603mshttps://172.16.10.22:2379 is healthy: successfully committed proposal: took = 22.592005ms</code></pre><h2 id="五、部署-Kubernetes"><a href="#五、部署-Kubernetes" class="headerlink" title="五、部署 Kubernetes"></a>五、部署 Kubernetes</h2><h3 id="5-1、安装-kueadm"><a href="#5-1、安装-kueadm" class="headerlink" title="5.1、安装 kueadm"></a>5.1、安装 kueadm</h3><p>安装 kubeadm 没什么好说的，国内被墙用阿里的源既可</p><pre><code class="hljs sh">apt-get install -y apt-transport-httpscurl https://mirrors.aliyun.com/kubernetes/apt/doc/apt-key.gpg | apt-key add -cat &lt;&lt;EOF &gt;/etc/apt/sources.list.d/kubernetes.listdeb https://mirrors.aliyun.com/kubernetes/apt/ kubernetes-xenial mainEOFapt update<span class="hljs-comment"># ebtables、ethtool kubelet 可能会用，具体忘了，反正从官方文档上看到的</span>apt install kubelet kubeadm kubectl ebtables ethtool -y</code></pre><h3 id="5-2、部署-Nginx"><a href="#5-2、部署-Nginx" class="headerlink" title="5.2、部署 Nginx"></a>5.2、部署 Nginx</h3><p>从上面的 HA 架构图上可以看到，为了维持 apiserver 的 HA，需要在每个机器上部署一个 nginx 做 4 层的 LB；为保证后续的 node 节点正常加入，需要首先行部署 nginx；nginx 安装同样喜欢偷懒，直接 docker 跑了…毕竟都开始 kubeadm 了，那么也没必要去纠结 docker 是否稳定的问题了；以下为 nginx 相关配置</p><p><strong>apiserver-proxy.conf</strong></p><pre><code class="hljs sh">error_log stderr notice;worker_processes auto;events &#123;multi_accept on;use epoll;worker_connections 1024;&#125;stream &#123;    upstream kube_apiserver &#123;        least_conn;        <span class="hljs-comment"># 后端为三台 master 节点的 apiserver 地址</span>        server 172.16.10.21:5443;        server 172.16.10.22:5443;        server 172.16.10.23:5443;    &#125;        server &#123;        listen        0.0.0.0:6443;        proxy_pass    kube_apiserver;        proxy_timeout 10m;        proxy_connect_timeout 1s;    &#125;&#125;</code></pre><p><strong>kube-apiserver-proxy.service</strong></p><pre><code class="hljs sh">[Unit]Description=kubernetes apiserver docker wrapperWants=docker.socketAfter=docker.service[Service]User=rootPermissionsStartOnly=<span class="hljs-literal">true</span>ExecStart=/usr/bin/docker run -p 6443:6443 \                          -v /etc/kubernetes/apiserver-proxy.conf:/etc/nginx/nginx.conf \                          --name kube-apiserver-proxy \                          --net=host \                          --restart=on-failure:5 \                          --memory=512M \                          nginx:1.17.6-alpineExecStartPre=-/usr/bin/docker rm -f kube-apiserver-proxyExecStop=/usr/bin/docker rm -rf kube-apiserver-proxyRestart=alwaysRestartSec=15sTimeoutStartSec=30s[Install]WantedBy=multi-user.target</code></pre><p>启动 nginx 代理(每台机器都要启动，包括 master 节点)</p><pre><code class="hljs sh">cp apiserver-proxy.conf /etc/kubernetescp kube-apiserver-proxy.service /lib/systemd/systemsystemctl daemon-reloadsystemctl <span class="hljs-built_in">enable</span> kube-apiserver-proxy.service &amp;&amp; systemctl start kube-apiserver-proxy.service</code></pre><h3 id="5-3、启动-control-plane"><a href="#5-3、启动-control-plane" class="headerlink" title="5.3、启动 control plane"></a>5.3、启动 control plane</h3><h4 id="5-3-1、关于-Swap"><a href="#5-3-1、关于-Swap" class="headerlink" title="5.3.1、关于 Swap"></a>5.3.1、关于 Swap</h4><p>目前 kubelet 为了保证内存 limit，需要在每个节点上关闭 swap；但是说实话我看了这篇文章 <a href="https://chrisdown.name/2018/01/02/in-defence-of-swap.html" target="_blank" rel="noopener">In defence of swap: common misconceptions</a> 以后还是不想关闭 swap；更确切的说其实我们生产环境比较 “富”，pod 都不 limit 内存，所以下面的部署我忽略了 swap 错误检测</p><h4 id="5-3-2、kubeadm-配置"><a href="#5-3-2、kubeadm-配置" class="headerlink" title="5.3.2、kubeadm 配置"></a>5.3.2、kubeadm 配置</h4><p>当前版本的 kubeadm 已经支持了完善的配置管理(当然细节部分还有待支持)，以下为我目前使用的配置，相关位置已经做了注释，更具体的配置自行查阅官方文档</p><p><strong>kubeadm.yaml</strong></p><pre><code class="hljs yaml"><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">kubeadm.k8s.io/v1beta2</span><span class="hljs-attr">kind:</span> <span class="hljs-string">InitConfiguration</span><span class="hljs-attr">localAPIEndpoint:</span>  <span class="hljs-comment"># 第一个 master 节点 IP</span>  <span class="hljs-attr">advertiseAddress:</span> <span class="hljs-string">"172.16.10.21"</span>  <span class="hljs-comment"># 6443 留给了 nginx，apiserver 换到 5443</span>  <span class="hljs-attr">bindPort:</span> <span class="hljs-number">5443</span><span class="hljs-comment"># 这个 token 使用以下命令生成</span><span class="hljs-comment"># kubeadm alpha certs certificate-key</span><span class="hljs-attr">certificateKey:</span> <span class="hljs-string">7373f829c733b46fb78f0069f90185e0f00254381641d8d5a7c5984b2cf17cd3</span> <span class="hljs-meta">---</span><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">kubeadm.k8s.io/v1beta2</span><span class="hljs-attr">kind:</span> <span class="hljs-string">ClusterConfiguration</span><span class="hljs-comment"># 使用外部 etcd 配置</span><span class="hljs-attr">etcd:</span>  <span class="hljs-attr">external:</span>    <span class="hljs-attr">endpoints:</span>    <span class="hljs-bullet">-</span> <span class="hljs-string">"https://172.16.10.21:2379"</span>    <span class="hljs-bullet">-</span> <span class="hljs-string">"https://172.16.10.22:2379"</span>    <span class="hljs-bullet">-</span> <span class="hljs-string">"https://172.16.10.23:2379"</span>    <span class="hljs-attr">caFile:</span> <span class="hljs-string">"/etc/etcd/ssl/etcd-root-ca.pem"</span>    <span class="hljs-attr">certFile:</span> <span class="hljs-string">"/etc/etcd/ssl/etcd.pem"</span>    <span class="hljs-attr">keyFile:</span> <span class="hljs-string">"/etc/etcd/ssl/etcd-key.pem"</span><span class="hljs-comment"># 网络配置</span><span class="hljs-attr">networking:</span>  <span class="hljs-attr">serviceSubnet:</span> <span class="hljs-string">"10.25.0.0/16"</span>  <span class="hljs-attr">podSubnet:</span> <span class="hljs-string">"10.30.0.1/16"</span>  <span class="hljs-attr">dnsDomain:</span> <span class="hljs-string">"cluster.local"</span><span class="hljs-attr">kubernetesVersion:</span> <span class="hljs-string">"v1.17.0"</span><span class="hljs-comment"># 全局 apiserver LB 地址，由于采用了 nginx 负载，所以直接指向本地既可</span><span class="hljs-attr">controlPlaneEndpoint:</span> <span class="hljs-string">"127.0.0.1:6443"</span><span class="hljs-attr">apiServer:</span>  <span class="hljs-comment"># apiserver 的自定义扩展参数</span>  <span class="hljs-attr">extraArgs:</span>    <span class="hljs-attr">v:</span> <span class="hljs-string">"4"</span>    <span class="hljs-attr">alsologtostderr:</span> <span class="hljs-string">"true"</span>    <span class="hljs-comment"># 审计日志相关配置</span>    <span class="hljs-attr">audit-log-maxage:</span> <span class="hljs-string">"20"</span>    <span class="hljs-attr">audit-log-maxbackup:</span> <span class="hljs-string">"10"</span>    <span class="hljs-attr">audit-log-maxsize:</span> <span class="hljs-string">"100"</span>    <span class="hljs-attr">audit-log-path:</span> <span class="hljs-string">"/var/log/kube-audit/audit.log"</span>    <span class="hljs-attr">audit-policy-file:</span> <span class="hljs-string">"/etc/kubernetes/audit-policy.yaml"</span>    <span class="hljs-attr">authorization-mode:</span> <span class="hljs-string">"Node,RBAC"</span>    <span class="hljs-attr">event-ttl:</span> <span class="hljs-string">"720h"</span>    <span class="hljs-attr">runtime-config:</span> <span class="hljs-string">"api/all=true"</span>    <span class="hljs-attr">service-node-port-range:</span> <span class="hljs-string">"30000-50000"</span>    <span class="hljs-attr">service-cluster-ip-range:</span> <span class="hljs-string">"10.25.0.0/16"</span>  <span class="hljs-comment"># 由于自行定义了审计日志配置，所以需要将宿主机上的审计配置</span>  <span class="hljs-comment"># 挂载到 kube-apiserver 的 pod 容器中</span>  <span class="hljs-attr">extraVolumes:</span>  <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">"audit-config"</span>    <span class="hljs-attr">hostPath:</span> <span class="hljs-string">"/etc/kubernetes/audit-policy.yaml"</span>    <span class="hljs-attr">mountPath:</span> <span class="hljs-string">"/etc/kubernetes/audit-policy.yaml"</span>    <span class="hljs-attr">readOnly:</span> <span class="hljs-literal">true</span>    <span class="hljs-attr">pathType:</span> <span class="hljs-string">"File"</span>  <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">"audit-log"</span>    <span class="hljs-attr">hostPath:</span> <span class="hljs-string">"/var/log/kube-audit"</span>    <span class="hljs-attr">mountPath:</span> <span class="hljs-string">"/var/log/kube-audit"</span>    <span class="hljs-attr">pathType:</span> <span class="hljs-string">"DirectoryOrCreate"</span>  <span class="hljs-comment"># 这里是 apiserver 的证书地址配置</span>  <span class="hljs-comment"># 为了防止以后出特殊情况，我增加了一个泛域名</span>  <span class="hljs-attr">certSANs:</span>  <span class="hljs-bullet">-</span> <span class="hljs-string">"*.kubernetes.node"</span>  <span class="hljs-bullet">-</span> <span class="hljs-string">"172.16.10.21"</span>  <span class="hljs-bullet">-</span> <span class="hljs-string">"172.16.10.22"</span>  <span class="hljs-bullet">-</span> <span class="hljs-string">"172.16.10.23"</span>  <span class="hljs-attr">timeoutForControlPlane:</span> <span class="hljs-string">5m</span><span class="hljs-attr">controllerManager:</span>  <span class="hljs-attr">extraArgs:</span>    <span class="hljs-attr">v:</span> <span class="hljs-string">"4"</span>    <span class="hljs-comment"># 宿主机 ip 掩码</span>    <span class="hljs-attr">node-cidr-mask-size:</span> <span class="hljs-string">"19"</span>    <span class="hljs-attr">deployment-controller-sync-period:</span> <span class="hljs-string">"10s"</span>    <span class="hljs-attr">experimental-cluster-signing-duration:</span> <span class="hljs-string">"87600h"</span>    <span class="hljs-attr">node-monitor-grace-period:</span> <span class="hljs-string">"20s"</span>    <span class="hljs-attr">pod-eviction-timeout:</span> <span class="hljs-string">"2m"</span>    <span class="hljs-attr">terminated-pod-gc-threshold:</span> <span class="hljs-string">"30"</span><span class="hljs-attr">scheduler:</span>  <span class="hljs-attr">extraArgs:</span>    <span class="hljs-attr">v:</span> <span class="hljs-string">"4"</span><span class="hljs-attr">certificatesDir:</span> <span class="hljs-string">"/etc/kubernetes/pki"</span><span class="hljs-comment"># gcr.io 被墙，换成微软的镜像地址</span><span class="hljs-attr">imageRepository:</span> <span class="hljs-string">"gcr.azk8s.cn/google_containers"</span><span class="hljs-attr">clusterName:</span> <span class="hljs-string">"kuberentes"</span><span class="hljs-meta">---</span><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">kubelet.config.k8s.io/v1beta1</span><span class="hljs-attr">kind:</span> <span class="hljs-string">KubeletConfiguration</span><span class="hljs-comment"># kubelet specific options here</span><span class="hljs-comment"># 此配置保证了 kubelet 能在 swap 开启的情况下启动</span><span class="hljs-attr">failSwapOn:</span> <span class="hljs-literal">false</span><span class="hljs-attr">nodeStatusUpdateFrequency:</span> <span class="hljs-string">5s</span><span class="hljs-comment"># 一些驱逐阀值，具体自行查文档修改</span><span class="hljs-attr">evictionSoft:</span>  <span class="hljs-attr">"imagefs.available":</span> <span class="hljs-string">"15%"</span>  <span class="hljs-attr">"memory.available":</span> <span class="hljs-string">"512Mi"</span>  <span class="hljs-attr">"nodefs.available":</span> <span class="hljs-string">"15%"</span>  <span class="hljs-attr">"nodefs.inodesFree":</span> <span class="hljs-string">"10%"</span><span class="hljs-attr">evictionSoftGracePeriod:</span>  <span class="hljs-attr">"imagefs.available":</span> <span class="hljs-string">"3m"</span>  <span class="hljs-attr">"memory.available":</span> <span class="hljs-string">"1m"</span>  <span class="hljs-attr">"nodefs.available":</span> <span class="hljs-string">"3m"</span>  <span class="hljs-attr">"nodefs.inodesFree":</span> <span class="hljs-string">"1m"</span><span class="hljs-attr">evictionHard:</span>  <span class="hljs-attr">"imagefs.available":</span> <span class="hljs-string">"10%"</span>  <span class="hljs-attr">"memory.available":</span> <span class="hljs-string">"256Mi"</span>  <span class="hljs-attr">"nodefs.available":</span> <span class="hljs-string">"10%"</span>  <span class="hljs-attr">"nodefs.inodesFree":</span> <span class="hljs-string">"5%"</span><span class="hljs-attr">evictionMaxPodGracePeriod:</span> <span class="hljs-number">30</span><span class="hljs-attr">imageGCLowThresholdPercent:</span> <span class="hljs-number">70</span><span class="hljs-attr">imageGCHighThresholdPercent:</span> <span class="hljs-number">80</span><span class="hljs-attr">kubeReserved:</span>  <span class="hljs-attr">"cpu":</span> <span class="hljs-string">"500m"</span>  <span class="hljs-attr">"memory":</span> <span class="hljs-string">"512Mi"</span>  <span class="hljs-attr">"ephemeral-storage":</span> <span class="hljs-string">"1Gi"</span><span class="hljs-attr">rotateCertificates:</span> <span class="hljs-literal">true</span><span class="hljs-meta">---</span><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">kubeproxy.config.k8s.io/v1alpha1</span><span class="hljs-attr">kind:</span> <span class="hljs-string">KubeProxyConfiguration</span><span class="hljs-comment"># kube-proxy specific options here</span><span class="hljs-attr">clusterCIDR:</span> <span class="hljs-string">"10.30.0.1/16"</span><span class="hljs-comment"># 启用 ipvs 模式</span><span class="hljs-attr">mode:</span> <span class="hljs-string">"ipvs"</span><span class="hljs-attr">ipvs:</span>  <span class="hljs-attr">minSyncPeriod:</span> <span class="hljs-string">5s</span>  <span class="hljs-attr">syncPeriod:</span> <span class="hljs-string">5s</span>  <span class="hljs-comment"># ipvs 负载策略</span>  <span class="hljs-attr">scheduler:</span> <span class="hljs-string">"wrr"</span></code></pre><p><strong>关于这个配置配置文件的文档还是很不完善，对于不懂 golang 的人来说很难知道具体怎么配置，以下做一下简要说明(请确保你已经拉取了 kubernetes 源码和安装了 Goland)</strong></p><p><strong>kubeadm 配置中每个配置段都会有个 <code>kind</code> 字段，<code>kind</code> 实际上对应了 go 代码中的 <code>struct</code> 结构体；同时从 <code>apiVersion</code> 字段中能够看到具体的版本，比如 <code>v1alpha1</code> 等；有了这两个信息事实上你就可以直接在源码中去找到对应的结构体</strong></p><p><img src="https://cdn.oss.link/markdown/dwo5h.png" srcset="/img/loading.gif" alt="struct_search"></p><p>在结构体中所有的配置便可以一目了然</p><p><img src="https://cdn.oss.link/markdown/0jc9b.png" srcset="/img/loading.gif" alt="struct_detail"></p><p>关于数据类型，如果是 <code>string</code> 的类型，那么意味着你要在 yaml 里写 <code>&quot;xxxx&quot;</code> 带引号这种，当然有些时候不写能兼容，有些时候不行比如 <code>extraArgs</code> 字段是一个 <code>map[string]string</code> 如果 value 不带引号就报错；<strong>如果数据类型为 <code>metav1.Duration</code>(实际上就是 <code>time.Duration</code>)，那么你看着它是个 <code>int64</code> 但实际上你要写 <code>1h2m3s</code> 这种人类可读的格式，这是 go 的特色…</strong></p><p><strong>audit-policy.yaml</strong></p><pre><code class="hljs yaml"><span class="hljs-comment"># Log all requests at the Metadata level.</span><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">audit.k8s.io/v1</span><span class="hljs-attr">kind:</span> <span class="hljs-string">Policy</span><span class="hljs-attr">rules:</span><span class="hljs-bullet">-</span> <span class="hljs-attr">level:</span> <span class="hljs-string">Metadata</span></code></pre><p>可能 <code>Metadata</code> 级别的审计日志比较多，想自行调整审计日志级别的可以参考<a href="https://kubernetes.io/docs/tasks/debug-application-cluster/audit/#audit-policy" target="_blank" rel="noopener">官方文档</a></p><h4 id="5-3-3、拉起-control-plane"><a href="#5-3-3、拉起-control-plane" class="headerlink" title="5.3.3、拉起 control plane"></a>5.3.3、拉起 control plane</h4><p>有了完整的 <code>kubeadm.yaml</code> 和 <code>audit-policy.yaml</code> 配置后，直接一条命令拉起 control plane 既可</p><pre><code class="hljs sh"><span class="hljs-comment"># 先将审计配置放到目标位置(3 台 master 都要执行)</span>cp audit-policy.yaml /etc/kubernetes<span class="hljs-comment"># 拉起 control plane</span>kubeadm init --config kubeadm.yaml --upload-certs --ignore-preflight-errors=Swap</code></pre><p><strong>control plane 拉起以后注意要保存屏幕输出，方便后续添加其他集群节点</strong></p><pre><code class="hljs sh">Your Kubernetes control-plane has initialized successfully!To start using your cluster, you need to run the following as a regular user:  mkdir -p <span class="hljs-variable">$HOME</span>/.kube  sudo cp -i /etc/kubernetes/admin.conf <span class="hljs-variable">$HOME</span>/.kube/config  sudo chown $(id -u):$(id -g) <span class="hljs-variable">$HOME</span>/.kube/configYou should now deploy a pod network to the cluster.Run <span class="hljs-string">"kubectl apply -f [podnetwork].yaml"</span> with one of the options listed at:  https://kubernetes.io/docs/concepts/cluster-administration/addons/You can now join any number of the control-plane node running the following <span class="hljs-built_in">command</span> on each as root:  kubeadm join 127.0.0.1:6443 --token r4t3l3.14mmuivm7xbtaeoj \    --discovery-token-ca-cert-hash sha256:06f49f1f29d08b797fbf04d87b9b0fd6095a4693e9b1d59c429745cfa082b31d \    --control-plane --certificate-key 7373f829c733b46fb78f0069f90185e0f00254381641d8d5a7c5984b2cf17cd3Please note that the certificate-key gives access to cluster sensitive data, keep it secret!As a safeguard, uploaded-certs will be deleted <span class="hljs-keyword">in</span> two hours; If necessary, you can use<span class="hljs-string">"kubeadm init phase upload-certs --upload-certs"</span> to reload certs afterward.Then you can join any number of worker nodes by running the following on each as root:kubeadm join 127.0.0.1:6443 --token r4t3l3.14mmuivm7xbtaeoj \    --discovery-token-ca-cert-hash sha256:06f49f1f29d08b797fbf04d87b9b0fd6095a4693e9b1d59c429745cfa082b31d</code></pre><p><strong>根据屏幕提示配置 kubectl</strong></p><pre><code class="hljs sh">mkdir -p <span class="hljs-variable">$HOME</span>/.kubesudo cp -i /etc/kubernetes/admin.conf <span class="hljs-variable">$HOME</span>/.kube/configsudo chown $(id -u):$(id -g) <span class="hljs-variable">$HOME</span>/.kube/config</code></pre><h3 id="5-4、部署-CNI"><a href="#5-4、部署-CNI" class="headerlink" title="5.4、部署 CNI"></a>5.4、部署 CNI</h3><p>关于网络插件的选择，以前一直喜欢 Calico，因为其性能确实好；到后来 flannel 出了 <code>host-gw</code> 以后现在两者性能也差不多了；但是 <strong>flannel 好处是一个工具通吃所有环境(云环境+裸机2层直通)，坏处是 flannel 缺乏比较好的策略管理(当然可以使用两者结合的 Canal)；</strong>后来思来想去其实我们生产倒是很少需要策略管理，所以这回怂回到 flannel 了(逃…)</p><p>Flannel 部署非常简单，根据官方文档下载配置，根据情况调整 <code>backend</code> 和 pod 的 CIDR，然后 apply 一下既可</p><pre><code class="hljs sh"><span class="hljs-comment"># 下载配置文件</span>wget https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml<span class="hljs-comment"># 调整 backend 为 host-gw(测试环境 2 层直连)</span>k1.node ➜  grep -A 35 ConfigMap kube-flannel.ymlkind: ConfigMapapiVersion: v1metadata:  name: kube-flannel-cfg  namespace: kube-system  labels:    tier: node    app: flanneldata:  cni-conf.json: |    &#123;      <span class="hljs-string">"name"</span>: <span class="hljs-string">"cbr0"</span>,      <span class="hljs-string">"cniVersion"</span>: <span class="hljs-string">"0.3.1"</span>,      <span class="hljs-string">"plugins"</span>: [        &#123;          <span class="hljs-string">"type"</span>: <span class="hljs-string">"flannel"</span>,          <span class="hljs-string">"delegate"</span>: &#123;            <span class="hljs-string">"hairpinMode"</span>: <span class="hljs-literal">true</span>,            <span class="hljs-string">"isDefaultGateway"</span>: <span class="hljs-literal">true</span>          &#125;        &#125;,        &#123;          <span class="hljs-string">"type"</span>: <span class="hljs-string">"portmap"</span>,          <span class="hljs-string">"capabilities"</span>: &#123;            <span class="hljs-string">"portMappings"</span>: <span class="hljs-literal">true</span>          &#125;        &#125;      ]    &#125;  net-conf.json: |    &#123;      <span class="hljs-string">"Network"</span>: <span class="hljs-string">"10.30.0.0/16"</span>,      <span class="hljs-string">"Backend"</span>: &#123;        <span class="hljs-string">"Type"</span>: <span class="hljs-string">"host-gw"</span>      &#125;    &#125;<span class="hljs-comment"># 调整完成后 apply 一下</span>kubectl apply -f kube-flannel.yml</code></pre><h3 id="5-5、启动其他-control-plane"><a href="#5-5、启动其他-control-plane" class="headerlink" title="5.5、启动其他 control plane"></a>5.5、启动其他 control plane</h3><p>为了保证 HA 架构，还需要在另外两台 master 上启动 control plane；<strong>在启动之前请确保另外两台 master 节点节点上 <code>/etc/kubernetes/audit-policy.yaml</code> 审计配置已经分发完成，确保 <code>127.0.0.1:6443</code> 上监听的 4 层 LB 工作正常(可尝试使用 <code>curl -k https://127.0.0.1:6443</code> 测试)；</strong>根据第一个 control plane 终端输出，其他 control plane 加入命令如下</p><pre><code class="hljs sh">kubeadm join 127.0.0.1:6443 --token r4t3l3.14mmuivm7xbtaeoj \    --discovery-token-ca-cert-hash sha256:06f49f1f29d08b797fbf04d87b9b0fd6095a4693e9b1d59c429745cfa082b31d \    --control-plane --certificate-key 7373f829c733b46fb78f0069f90185e0f00254381641d8d5a7c5984b2cf17cd3</code></pre><p><strong>由于在使用 <code>kubeadm join</code> 时相关选项(<code>--discovery-token-ca-cert-hash</code>、<code>--control-plane</code>)无法与 <code>--config</code> 一起使用，这也就意味着我们必须增加一些附加指令来提供 <code>kubeadm.yaml</code> 配置文件中的一些属性</strong>；最终完整的 control plane 加入命令如下，在其他 master 直接执行既可(<strong><code>--apiserver-advertise-address</code> 的 IP 地址是目标 master 的 IP</strong>)</p><pre><code class="hljs sh">kubeadm join 127.0.0.1:6443 --token r4t3l3.14mmuivm7xbtaeoj \    --discovery-token-ca-cert-hash sha256:06f49f1f29d08b797fbf04d87b9b0fd6095a4693e9b1d59c429745cfa082b31d \    --control-plane --certificate-key 7373f829c733b46fb78f0069f90185e0f00254381641d8d5a7c5984b2cf17cd3 \    --apiserver-advertise-address 172.16.10.22 \    --apiserver-bind-port 5443 \    --ignore-preflight-errors=Swap</code></pre><p><strong>所有 control plane 启动完成后应当通过在每个节点上运行 <code>kubectl get cs</code> 验证各个组件运行状态</strong></p><pre><code class="hljs sh">k2.node ➜ kubectl get csNAME                 STATUS    MESSAGE             ERRORscheduler            Healthy   okcontroller-manager   Healthy   oketcd-1               Healthy   &#123;<span class="hljs-string">"health"</span>:<span class="hljs-string">"true"</span>&#125;etcd-0               Healthy   &#123;<span class="hljs-string">"health"</span>:<span class="hljs-string">"true"</span>&#125;etcd-2               Healthy   &#123;<span class="hljs-string">"health"</span>:<span class="hljs-string">"true"</span>&#125;k2.node ➜ kubectl get node -o wideNAME      STATUS   ROLES    AGE   VERSION   INTERNAL-IP    EXTERNAL-IP   OS-IMAGE             KERNEL-VERSION      CONTAINER-RUNTIMEk1.node   Ready    master   28m   v1.17.0   172.16.10.21   &lt;none&gt;        Ubuntu 18.04.3 LTS   4.15.0-74-generic   docker://19.3.5k2.node   Ready    master   10m   v1.17.0   172.16.10.22   &lt;none&gt;        Ubuntu 18.04.3 LTS   4.15.0-74-generic   docker://19.3.5k3.node   Ready    master   3m    v1.17.0   172.16.10.23   &lt;none&gt;        Ubuntu 18.04.3 LTS   4.15.0-74-generic   docker://19.3.5</code></pre><h3 id="5-6、启动-Node"><a href="#5-6、启动-Node" class="headerlink" title="5.6、启动 Node"></a>5.6、启动 Node</h3><p>node 节点的启动相较于 master 来说要简单得多，只需要增加一个防止 <code>swap</code> 开启拒绝启动的参数既可</p><pre><code class="hljs sh">kubeadm join 127.0.0.1:6443 --token r4t3l3.14mmuivm7xbtaeoj \    --discovery-token-ca-cert-hash sha256:06f49f1f29d08b797fbf04d87b9b0fd6095a4693e9b1d59c429745cfa082b31d \    --ignore-preflight-errors=Swap</code></pre><p>启动成功后在 master 上可以看到所有 node 信息</p><pre><code class="hljs sh">k1.node ➜ kubectl get node -o wideNAME      STATUS   ROLES    AGE     VERSION   INTERNAL-IP    EXTERNAL-IP   OS-IMAGE             KERNEL-VERSION      CONTAINER-RUNTIMEk1.node   Ready    master   32m     v1.17.0   172.16.10.21   &lt;none&gt;        Ubuntu 18.04.3 LTS   4.15.0-74-generic   docker://19.3.5k2.node   Ready    master   14m     v1.17.0   172.16.10.22   &lt;none&gt;        Ubuntu 18.04.3 LTS   4.15.0-74-generic   docker://19.3.5k3.node   Ready    master   6m35s   v1.17.0   172.16.10.23   &lt;none&gt;        Ubuntu 18.04.3 LTS   4.15.0-74-generic   docker://19.3.5k4.node   Ready    &lt;none&gt;   72s     v1.17.0   172.16.10.24   &lt;none&gt;        Ubuntu 18.04.3 LTS   4.15.0-74-generic   docker://19.3.5k5.node   Ready    &lt;none&gt;   66s     v1.17.0   172.16.10.25   &lt;none&gt;        Ubuntu 18.04.3 LTS   4.15.0-74-generic   docker://19.3.5</code></pre><h3 id="5-7、调整及测试"><a href="#5-7、调整及测试" class="headerlink" title="5.7、调整及测试"></a>5.7、调整及测试</h3><p>集群搭建好以后，如果想让 master 节点也参与调度任务，需要在任意一台 master 节点执行以下命令</p><pre><code class="hljs sh"><span class="hljs-comment"># node 节点报错属于正常情况</span>k1.node ➜ kubectl taint nodes --all node-role.kubernetes.io/master-node/k1.node untaintednode/k2.node untaintednode/k3.node untaintedtaint <span class="hljs-string">"node-role.kubernetes.io/master"</span> not foundtaint <span class="hljs-string">"node-role.kubernetes.io/master"</span> not found</code></pre><p>最后创建一个 deployment 和一个 service，并在不同主机上 ping pod IP 测试网络联通性，在 pod 内直接 curl service 名称测试 dns 解析既可</p><p><strong>test-nginx.deploy.yaml</strong></p><pre><code class="hljs yaml"><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">apps/v1</span><span class="hljs-attr">kind:</span> <span class="hljs-string">Deployment</span><span class="hljs-attr">metadata:</span>  <span class="hljs-attr">name:</span> <span class="hljs-string">test-nginx</span>  <span class="hljs-attr">labels:</span>    <span class="hljs-attr">app:</span> <span class="hljs-string">test-nginx</span><span class="hljs-attr">spec:</span>  <span class="hljs-attr">replicas:</span> <span class="hljs-number">3</span>  <span class="hljs-attr">selector:</span>    <span class="hljs-attr">matchLabels:</span>      <span class="hljs-attr">app:</span> <span class="hljs-string">test-nginx</span>  <span class="hljs-attr">template:</span>    <span class="hljs-attr">metadata:</span>      <span class="hljs-attr">labels:</span>        <span class="hljs-attr">app:</span> <span class="hljs-string">test-nginx</span>    <span class="hljs-attr">spec:</span>      <span class="hljs-attr">containers:</span>      <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">test-nginx</span>        <span class="hljs-attr">image:</span> <span class="hljs-string">nginx:1.17.6-alpine</span>        <span class="hljs-attr">ports:</span>        <span class="hljs-bullet">-</span> <span class="hljs-attr">containerPort:</span> <span class="hljs-number">80</span></code></pre><p><strong>test-nginx.svc.yaml</strong></p><pre><code class="hljs yaml"><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">v1</span><span class="hljs-attr">kind:</span> <span class="hljs-string">Service</span><span class="hljs-attr">metadata:</span>  <span class="hljs-attr">name:</span> <span class="hljs-string">test-nginx</span><span class="hljs-attr">spec:</span>  <span class="hljs-attr">selector:</span>    <span class="hljs-attr">app:</span> <span class="hljs-string">test-nginx</span>  <span class="hljs-attr">ports:</span>    <span class="hljs-bullet">-</span> <span class="hljs-attr">protocol:</span> <span class="hljs-string">TCP</span>      <span class="hljs-attr">port:</span> <span class="hljs-number">80</span>      <span class="hljs-attr">targetPort:</span> <span class="hljs-number">80</span></code></pre><h2 id="六、后续处理"><a href="#六、后续处理" class="headerlink" title="六、后续处理"></a>六、后续处理</h2><blockquote><p>说实话使用 kubeadm 后，我更关注的是集群后续的扩展性调整是否能达到目标；搭建其实很简单，大部份时间都在测试后续调整上</p></blockquote><h3 id="6-1、Etcd-迁移"><a href="#6-1、Etcd-迁移" class="headerlink" title="6.1、Etcd 迁移"></a>6.1、Etcd 迁移</h3><p>由于我们采用的是外部的 Etcd，所以迁移起来比较简单怎么折腾都行；需要注意的是换 IP 的时候注意保证老的 3 个节点至少有一个可用，否则可能导致集群崩溃；调整完成后记得分发相关 Etcd 节点的证书，重启时顺序一个一个重启，不要并行操作</p><h3 id="6-2、Master-配置修改"><a href="#6-2、Master-配置修改" class="headerlink" title="6.2、Master 配置修改"></a>6.2、Master 配置修改</h3><p>如果需要修改 conrol plane 上 apiserver、scheduler 等配置，直接修改 <code>kubeadm.yaml</code> 配置文件(<strong>所以集群搭建好后务必保存好</strong>)，然后执行 <code>kubeadm upgrade apply --config kubeadm.yaml</code> 升级集群既可，升级前一定作好相关备份工作；我只在测试环境测试这个命令工作还可以，生产环境还是需要谨慎</p><h3 id="6-3、证书续期"><a href="#6-3、证书续期" class="headerlink" title="6.3、证书续期"></a>6.3、证书续期</h3><p>目前根据我测试的结果，controller manager 的 <strong>experimental-cluster-signing-duration</strong> 参数在 init 的签发证书阶段似乎并未生效；<strong>目前根据文档描述 <code>kubelet</code> client 的证书会自动滚动，其他证书默认 1 年有效期，需要自己使用命令续签；</strong>续签命令如下</p><pre><code class="hljs sh"><span class="hljs-comment"># 查看证书过期时间</span>k1.node ➜ kubeadm alpha certs check-expiration[check-expiration] Reading configuration from the cluster...[check-expiration] FYI: You can look at this config file with <span class="hljs-string">'kubectl -n kube-system get cm kubeadm-config -oyaml'</span>CERTIFICATE                EXPIRES                  RESIDUAL TIME   CERTIFICATE AUTHORITY   EXTERNALLY MANAGEDadmin.conf                 Jan 11, 2021 10:06 UTC   364d                                    noapiserver                  Jan 11, 2021 10:06 UTC   364d            ca                      noapiserver-kubelet-client   Jan 11, 2021 10:06 UTC   364d            ca                      nocontroller-manager.conf    Jan 11, 2021 10:06 UTC   364d                                    nofront-proxy-client         Jan 11, 2021 10:06 UTC   364d            front-proxy-ca          noscheduler.conf             Jan 11, 2021 10:06 UTC   364d                                    noCERTIFICATE AUTHORITY   EXPIRES                  RESIDUAL TIME   EXTERNALLY MANAGEDca                      Jan 09, 2030 10:06 UTC   9y              nofront-proxy-ca          Jan 09, 2030 10:06 UTC   9y              no<span class="hljs-comment"># 续签证书</span>k1.node ➜ kubeadm alpha certs renew all[renew] Reading configuration from the cluster...[renew] FYI: You can look at this config file with <span class="hljs-string">'kubectl -n kube-system get cm kubeadm-config -oyaml'</span>certificate embedded <span class="hljs-keyword">in</span> the kubeconfig file <span class="hljs-keyword">for</span> the admin to use and <span class="hljs-keyword">for</span> kubeadm itself renewedcertificate <span class="hljs-keyword">for</span> serving the Kubernetes API renewedcertificate <span class="hljs-keyword">for</span> the API server to connect to kubelet renewedcertificate embedded <span class="hljs-keyword">in</span> the kubeconfig file <span class="hljs-keyword">for</span> the controller manager to use renewedcertificate <span class="hljs-keyword">for</span> the front proxy client renewedcertificate embedded <span class="hljs-keyword">in</span> the kubeconfig file <span class="hljs-keyword">for</span> the scheduler manager to use renewed</code></pre><h3 id="6-4、Node-重加入"><a href="#6-4、Node-重加入" class="headerlink" title="6.4、Node 重加入"></a>6.4、Node 重加入</h3><p>默认的 bootstrap token 会在 24h 后失效，所以后续增加新节点需要重新创建 token，重新创建 token 可以通过以下命令完成</p><pre><code class="hljs sh"><span class="hljs-comment"># 列出 token</span>k1.node ➜ kubeadm token listTOKEN                     TTL         EXPIRES                     USAGES                   DESCRIPTION                                                EXTRA GROUPSr4t3l3.14mmuivm7xbtaeoj   22h         2020-01-13T18:06:54+08:00   authentication,signing   &lt;none&gt;                                                     system:bootstrappers:kubeadm:default-node-tokenzady4i.57f9i2o6zl9vf9hy   45m         2020-01-12T20:06:53+08:00   &lt;none&gt;                   Proxy <span class="hljs-keyword">for</span> managing TTL <span class="hljs-keyword">for</span> the kubeadm-certs secret        &lt;none&gt;<span class="hljs-comment"># 创建新 token</span>k1.node ➜ kubeadm token create --<span class="hljs-built_in">print</span>-join-commandW0112 19:21:15.174765   26626 validation.go:28] Cannot validate kube-proxy config - no validator is availableW0112 19:21:15.174836   26626 validation.go:28] Cannot validate kubelet config - no validator is availablekubeadm join 127.0.0.1:6443 --token 2dz4dc.mobzgjbvu0bkxz7j     --discovery-token-ca-cert-hash sha256:06f49f1f29d08b797fbf04d87b9b0fd6095a4693e9b1d59c429745cfa082b31d</code></pre><p>如果忘记了 certificate-key 可以通过一下命令重新 upload 并查看</p><pre><code class="hljs sh">k1.node ➜ kubeadm init --config kubeadm.yaml phase upload-certs --upload-certsW0112 19:23:06.466711   28637 validation.go:28] Cannot validate kubelet config - no validator is availableW0112 19:23:06.466778   28637 validation.go:28] Cannot validate kube-proxy config - no validator is available[upload-certs] Storing the certificates <span class="hljs-keyword">in</span> Secret <span class="hljs-string">"kubeadm-certs"</span> <span class="hljs-keyword">in</span> the <span class="hljs-string">"kube-system"</span> Namespace[upload-certs] Using certificate key:7373f829c733b46fb78f0069f90185e0f00254381641d8d5a7c5984b2cf17cd3</code></pre><h3 id="6-5、调整-kubelet"><a href="#6-5、调整-kubelet" class="headerlink" title="6.5、调整 kubelet"></a>6.5、调整 kubelet</h3><p>node 节点一旦启动完成后，kubelet 配置便不可再修改；如果想要修改 kubelet 配置，可以通过调整 <code>/etc/systemd/system/kubelet.service.d/10-kubeadm.conf</code> 配置文件完成</p><h2 id="七、其他"><a href="#七、其他" class="headerlink" title="七、其他"></a>七、其他</h2><p>本文参考了许多官方文档，以下是一些个人认为比较有价值并且在使用 kubeadm 后应该阅读的文档</p><ul><li><a href="https://kubernetes.io/docs/reference/setup-tools/kubeadm/implementation-details" target="_blank" rel="noopener">Implementation details</a></li><li><a href="https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/kubelet-integration/" target="_blank" rel="noopener">Configuring each kubelet in your cluster using kubeadm</a></li><li><a href="https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/control-plane-flags/" target="_blank" rel="noopener">Customizing control plane configuration with kubeadm</a></li><li><a href="https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/high-availability/" target="_blank" rel="noopener">Creating Highly Available clusters with kubeadm</a></li><li><a href="https://kubernetes.io/docs/tasks/administer-cluster/kubeadm/kubeadm-certs/" target="_blank" rel="noopener">Certificate Management with kubeadm</a></li><li><a href="https://kubernetes.io/docs/tasks/administer-cluster/kubeadm/kubeadm-upgrade/" target="_blank" rel="noopener">Upgrading kubeadm clusters</a></li><li><a href="https://kubernetes.io/docs/tasks/administer-cluster/reconfigure-kubelet/" target="_blank" rel="noopener">Reconfigure a Node’s Kubelet in a Live Cluster</a></li></ul>]]></content>
    
    
    <categories>
      
      <category>Kubernetes</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Kubernetes</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>云服务器下 Ubuntu 18 正确的 DNS 修改</title>
    <link href="/2020/01/21/how-to-modify-dns-on-ubuntu18-server/"/>
    <url>/2020/01/21/how-to-modify-dns-on-ubuntu18-server/</url>
    
    <content type="html"><![CDATA[<h2 id="一、起因"><a href="#一、起因" class="headerlink" title="一、起因"></a>一、起因</h2><p>Netflix DNS 分流实际上我目前的方案是通过 CoreDNS 作为主 DNS Server，然后在 CoreDNS 上针对 Netflix 全部域名解析 forward 到一台国外可以解锁 Netflix 机器上；如果直接将 CoreDNS 暴露在公网，那么无疑是在作死，为 DNS 反射 DDos 提供肉鸡；所以想到的方案是自己编写一个不可描述的工具，本地 Client 到 Server 端以后，Server 端再去设置到 CoreDNS 做分流；其中不可避免的需要调整 Server 端默认 DNS。</p><h2 id="二、已废弃修改方式"><a href="#二、已废弃修改方式" class="headerlink" title="二、已废弃修改方式"></a>二、已废弃修改方式</h2><p>目前大部份人还是习惯修改 <code>/etc/resolv.conf</code> 配置文件，这个配置文件上面已经明确标注了不要去修改它；<strong>因为自 Systemd 一统江山以后，系统 DNS 已经被 <code>systemd-resolved</code> 服务接管；一但修改了 <code>/etc/resolv.conf</code>，机器重启后就会被恢复；</strong>所以根源解决方案还是需要修改 <code>systemd-resolved</code> 的配置。</p><h2 id="三、netplan-的调整"><a href="#三、netplan-的调整" class="headerlink" title="三、netplan 的调整"></a>三、netplan 的调整</h2><p>在调整完 <code>systemd-resolved</code> 配置后其实有些地方仍然是不生效的；<strong>原因是 Ubuntu 18 开始网络已经被 netplan 接管，所以问题又回到了如何修改 netplan；</strong>由于云服务器初始化全部是由 cloud-init 完成的，netplan 配置里 IP 全部是由 DHCP 完成；那么直接修改 netplan 为 static IP 理论上可行，但是事实上还是不够优雅；后来研究了一下其实更优雅的方式是覆盖掉 DHCP 的某些配置，比如 DNS 配置；在阿里云上配置如下(<code>/etc/netplan/99-netcfg.yaml</code>)</p><pre><code class="hljs yaml"><span class="hljs-attr">network:</span>  <span class="hljs-attr">version:</span> <span class="hljs-number">2</span>  <span class="hljs-attr">renderer:</span> <span class="hljs-string">networkd</span>  <span class="hljs-attr">ethernets:</span>    <span class="hljs-attr">eth0:</span>      <span class="hljs-attr">dhcp4:</span> <span class="hljs-literal">yes</span>      <span class="hljs-attr">dhcp4-overrides:</span>        <span class="hljs-attr">use-dns:</span> <span class="hljs-literal">no</span>      <span class="hljs-attr">dhcp6:</span> <span class="hljs-literal">no</span>      <span class="hljs-attr">nameservers:</span>        <span class="hljs-attr">search:</span> <span class="hljs-string">[local,node]</span>        <span class="hljs-comment"># 我自己的 CoreDNS 服务器</span>        <span class="hljs-attr">addresses:</span> <span class="hljs-string">[172.17.3.17]</span></code></pre><p>修改完成后执行 <code>netplan try</code> 等待几秒钟，如果屏幕的读秒倒计时一直在动，说明修改没问题，接着回车既可(尽量不要 <code>netplan apply</code>，一旦修改错误你就再也连不上了…)</p><h2 id="四、DNS-分流"><a href="#四、DNS-分流" class="headerlink" title="四、DNS 分流"></a>四、DNS 分流</h2><p>顺便贴一下 CoreDNS 配置吧，可能有些人也需要；第一部分的域名是目前我整理的 Netflix 全部访问域名，针对这些域名的流量转发到自己其他可解锁 Netflix 的机器既可</p><pre><code class="hljs sh">netflix.com nflxext.com nflximg.net nflxso.net nflxvideo.net &#123;    <span class="hljs-built_in">bind</span> 172.17.3.17    cache 30 . &#123;        success 4096    &#125;    forward . 158.1.1.1 &#123;        max_fails 2        prefer_udp        expire 20s        policy random        health_check 0.2s    &#125;    errors    <span class="hljs-built_in">log</span> . <span class="hljs-string">"&#123;remote&#125;:&#123;port&#125; - &#123;&gt;id&#125; \"&#123;type&#125; &#123;class&#125; &#123;name&#125; &#123;proto&#125; &#123;size&#125; &#123;&gt;do&#125; &#123;&gt;bufsize&#125;\" &#123;rcode&#125; &#123;&gt;rflags&#125; &#123;rsize&#125; &#123;duration&#125;"</span>&#125;.:53 &#123;    <span class="hljs-built_in">bind</span> 172.17.3.17    cache 30 . &#123;        success 4096    &#125;    forward . 8.8.8.8 1.1.1.1 &#123;        except netflix.com nflxext.com nflximg.net nflxso.net nflxvideo.net        max_fails 2        expire 20s        policy random        health_check 0.2s    &#125;    errors    <span class="hljs-built_in">log</span> . <span class="hljs-string">"&#123;remote&#125;:&#123;port&#125; - &#123;&gt;id&#125; \"&#123;type&#125; &#123;class&#125; &#123;name&#125; &#123;proto&#125; &#123;size&#125; &#123;&gt;do&#125; &#123;&gt;bufsize&#125;\" &#123;rcode&#125; &#123;&gt;rflags&#125; &#123;rsize&#125; &#123;duration&#125;"</span></code></pre><h2 id="五、关于-docker"><a href="#五、关于-docker" class="headerlink" title="五、关于 docker"></a>五、关于 docker</h2><p>当 netplan 修改完成后，只需要重启 docker 既可保证 docker 内所有容器 DNS 请求全部发送到自己定义的 DNS 服务器上；<strong>请不要尝试将自己的 CoreDNS 监听到 <code>127.*</code> 或者 <code>::1</code> 上，这两个地址会导致 docker 中的 DNS 无效</strong>，因为在 <a href="https://github.com/docker/libnetwork/blob/fec6476dfa21380bf8ee4d74048515d968c1ee63/resolvconf/resolvconf.go#L148" target="_blank" rel="noopener">libnetwork</a> 中针对这两个地址做了过滤，并且 <code>FilterResolvDNS</code> 方法在剔除这两种地址时不会给予任何警告日志</p>]]></content>
    
    
    <categories>
      
      <category>Linux</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Linux</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Percona MySQL 搭建</title>
    <link href="/2020/01/20/set-up-percona-server/"/>
    <url>/2020/01/20/set-up-percona-server/</url>
    
    <content type="html"><![CDATA[<h2 id="一、版本信息"><a href="#一、版本信息" class="headerlink" title="一、版本信息"></a>一、版本信息</h2><p>目前采用 MySQL fork 版本 Percona Server 5.7.28，监控方面选择 Percona Monitoring and Management 2.1.0，对应监控 Client 版本为 2.1.0</p><h2 id="二、Percona-Server-安装"><a href="#二、Percona-Server-安装" class="headerlink" title="二、Percona Server 安装"></a>二、Percona Server 安装</h2><p>为保证兼容以及稳定性，MySQL 宿主机系统选择 CentOS 7，Percona Server 安装方式为 rpm 包，安装后由 Systemd 守护</p><h3 id="2-1、下载安装包"><a href="#2-1、下载安装包" class="headerlink" title="2.1、下载安装包"></a>2.1、下载安装包</h3><p>安装包下载地址为 <a href="https://www.percona.com/downloads/Percona-Server-5.7/LATEST/" target="_blank" rel="noopener">https://www.percona.com/downloads/Percona-Server-5.7/LATEST/</a>，下载时选择 <code>Download All Packages Together</code>，下载后是所有组件全量的压缩 tar 包。</p><h3 id="2-2、安装前准备"><a href="#2-2、安装前准备" class="headerlink" title="2.2、安装前准备"></a>2.2、安装前准备</h3><p>针对 CentOS 7 系统，安装前升级所有系统组件库，执行 <code>yum update</code> 既可；大部份 <strong>CentOS 7 安装后可能会附带 <code>mariadb-libs</code> 包，这个包会默认创建一些配置文件，导致后面的 Percona Server 无法覆盖它(例如 <code>/etc/my.cnf</code>)，所以安装 Percona Server 之前需要卸载它 <code>yum remove mariadb-libs</code></strong></p><p>针对于数据存储硬盘，目前统一为 SSD 硬盘，挂载点为 <code>/data</code>，挂载方式可以采用 <code>fstab</code>、<code>systemd-mount</code>，分区格式目前采用 <code>xfs</code> 格式。</p><p><strong>SSD 优化有待补充…</strong></p><h3 id="2-3、安装-Percona-Server"><a href="#2-3、安装-Percona-Server" class="headerlink" title="2.3、安装 Percona Server"></a>2.3、安装 Percona Server</h3><p>Percona Server tar 包解压后会有 9 个 rpm 包，实际安装时只需要安装其中 4 个既可</p><pre><code class="hljs sh">yum install Percona-Server-client-57-5.7.28-31.1.el7.x86_64.rpm Percona-Server-server-57-5.7.28-31.1.el7.x86_64.rpm Percona-Server-shared-57-5.7.28-31.1.el7.x86_64.rpm Percona-Server-shared-compat-57-5.7.28-31.1.el7.x86_64.rpm</code></pre><h3 id="2-4、安装后调整"><a href="#2-4、安装后调整" class="headerlink" title="2.4、安装后调整"></a>2.4、安装后调整</h3><h4 id="2-4-1、硬盘调整"><a href="#2-4-1、硬盘调整" class="headerlink" title="2.4.1、硬盘调整"></a>2.4.1、硬盘调整</h4><p>目前 MySQL 数据会统一存放到 <code>/data</code> 目录下，所以需要将单独的数据盘挂载到 <code>/data</code> 目录；<strong>如果是 SSD 硬盘还需要调整系统 I/O 调度器等其他优化。</strong></p><h4 id="2-4-2、目录预创建"><a href="#2-4-2、目录预创建" class="headerlink" title="2.4.2、目录预创建"></a>2.4.2、目录预创建</h4><p>Percona Server 安装完成后，由于配置调整原因，还会用到一些其他的数据目录，这些目录可以预先创建并授权</p><pre><code class="hljs sh">mkdir -p /var/<span class="hljs-built_in">log</span>/mysql /data/mysql_tmpchown -R mysql:mysql /var/<span class="hljs-built_in">log</span>/mysql /data/mysql_tmp</code></pre><p><code>/var/log/mysql</code> 目录用来存放 MySQL 相关的日志(不包括 binlog)，<code>/data/mysql_tmp</code> 用来存放 MySQL 运行时产生的缓存文件。</p><h4 id="2-4-3、文件描述符调整"><a href="#2-4-3、文件描述符调整" class="headerlink" title="2.4.3、文件描述符调整"></a>2.4.3、文件描述符调整</h4><p>由于 rpm 安装的 Percona Server 会采用 Systemd 守护，所以如果想修改文件描述符配置应当调整 Systemd 配置文件</p><pre><code class="hljs sh">vim /usr/lib/systemd/system/mysqld.service<span class="hljs-comment"># Sets open_files_limit</span><span class="hljs-comment"># 注意 infinity = 65536</span>LimitCORE = infinityLimitNOFILE = infinityLimitNPROC = infinity</code></pre><p>然后执行 <code>systemctl daemon-reload</code> 重载既可。</p><h4 id="2-4-4、配置文件调整"><a href="#2-4-4、配置文件调整" class="headerlink" title="2.4.4、配置文件调整"></a>2.4.4、配置文件调整</h4><p>Percona Server 安装完成后也会生成 <code>/etc/my.cnf</code> 配置文件，不过不建议直接修改该文件；修改配置文件需要进入到 <code>/etc/percona-server.conf.d/</code> 目录调整相应配置；以下为配置样例(<strong>生产环境 mysqld 配置需要优化调整</strong>)</p><p><strong>mysql.cnf</strong></p><pre><code class="hljs ini"><span class="hljs-section">[mysql]</span>auto-rehash<span class="hljs-attr">default_character_set</span>=utf8mb4</code></pre><p><strong>mysqld.cnf</strong></p><pre><code class="hljs ini"><span class="hljs-comment"># Percona Server template configuration</span><span class="hljs-section">[mysqld]</span><span class="hljs-comment">#</span><span class="hljs-comment"># Remove leading # and set to the amount of RAM for the most important data</span><span class="hljs-comment"># cache in MySQL. Start at 70% of total RAM for dedicated server, else 10%.</span><span class="hljs-comment"># innodb_buffer_pool_size = 128M</span><span class="hljs-comment">#</span><span class="hljs-comment"># Remove leading # to turn on a very important data integrity option: logging</span><span class="hljs-comment"># changes to the binary log between backups.</span><span class="hljs-comment"># log_bin</span><span class="hljs-comment">#</span><span class="hljs-comment"># Remove leading # to set options mainly useful for reporting servers.</span><span class="hljs-comment"># The server defaults are faster for transactions and fast SELECTs.</span><span class="hljs-comment"># Adjust sizes as needed, experiment to find the optimal values.</span><span class="hljs-comment"># join_buffer_size = 128M</span><span class="hljs-comment"># sort_buffer_size = 2M</span><span class="hljs-comment"># read_rnd_buffer_size = 2M</span><span class="hljs-attr">port</span>=<span class="hljs-number">3306</span><span class="hljs-attr">datadir</span>=/data/mysql<span class="hljs-attr">socket</span>=/data/mysql/mysql.sock<span class="hljs-attr">pid_file</span>=/data/mysql/mysqld.pid<span class="hljs-comment"># 服务端编码</span><span class="hljs-attr">character_set_server</span>=utf8mb4<span class="hljs-comment"># 服务端排序</span><span class="hljs-attr">collation_server</span>=utf8mb4_general_ci<span class="hljs-comment"># 强制使用 utf8mb4 编码集，忽略客户端设置</span><span class="hljs-attr">skip_character_set_client_handshake</span>=<span class="hljs-number">1</span><span class="hljs-comment"># 日志输出到文件</span><span class="hljs-attr">log_output</span>=FILE<span class="hljs-comment"># 开启常规日志输出</span><span class="hljs-attr">general_log</span>=<span class="hljs-number">1</span><span class="hljs-comment"># 常规日志输出文件位置</span><span class="hljs-attr">general_log_file</span>=/var/log/mysql/mysqld.log<span class="hljs-comment"># 错误日志位置</span><span class="hljs-attr">log_error</span>=/var/log/mysql/mysqld-error.log<span class="hljs-comment"># 记录慢查询</span><span class="hljs-attr">slow_query_log</span>=<span class="hljs-number">1</span><span class="hljs-comment"># 慢查询时间(大于 1s 被视为慢查询)</span><span class="hljs-attr">long_query_time</span>=<span class="hljs-number">1</span><span class="hljs-comment"># 慢查询日志文件位置</span><span class="hljs-attr">slow_query_log_file</span>=/var/log/mysql/mysqld-slow.log<span class="hljs-comment"># 临时文件位置</span><span class="hljs-attr">tmpdir</span>=/data/mysql_tmp<span class="hljs-comment"># 线程池缓存(refs https://my.oschina.net/realfighter/blog/363853)</span><span class="hljs-attr">thread_cache_size</span>=<span class="hljs-number">30</span><span class="hljs-comment"># The number of open tables for all threads.(refs https://dev.mysql.com/doc/refman/5.7/en/server-system-variables.html#sysvar_table_open_cache)</span><span class="hljs-attr">table_open_cache</span>=<span class="hljs-number">16384</span><span class="hljs-comment"># 文件描述符(此处修改不生效，请修改 systemd service 配置) </span><span class="hljs-comment"># refs https://www.percona.com/blog/2017/10/12/open_files_limit-mystery/</span><span class="hljs-comment"># refs https://www.cnblogs.com/wxxjianchi/p/10370419.html</span><span class="hljs-comment">#open_files_limit=65535</span><span class="hljs-comment"># 表定义缓存(5.7 以后自动调整)</span><span class="hljs-comment"># refs https://dev.mysql.com/doc/refman/5.6/en/server-system-variables.html#sysvar_table_definition_cache</span><span class="hljs-comment"># refs http://mysql.taobao.org/monthly/2015/08/10/</span><span class="hljs-comment">#table_definition_cache=16384</span><span class="hljs-attr">sort_buffer_size</span>=<span class="hljs-number">1</span>M<span class="hljs-attr">join_buffer_size</span>=<span class="hljs-number">1</span>M<span class="hljs-comment"># MyiSAM 引擎专用(内部临时磁盘表可能会用)</span><span class="hljs-attr">read_buffer_size</span>=<span class="hljs-number">1</span>M<span class="hljs-attr">read_rnd_buffer_size</span>=<span class="hljs-number">1</span>M<span class="hljs-comment"># MyiSAM 引擎专用(内部临时磁盘表可能会用)</span><span class="hljs-attr">key_buffer_size</span>=<span class="hljs-number">32</span>M<span class="hljs-comment"># MyiSAM 引擎专用(内部临时磁盘表可能会用)</span><span class="hljs-attr">bulk_insert_buffer_size</span>=<span class="hljs-number">16</span>M<span class="hljs-comment"># myisam_sort_buffer_size 与 sort_buffer_size 区别请参考(https://stackoverflow.com/questions/7871027/myisam-sort-buffer-size-vs-sort-buffer-size)</span><span class="hljs-attr">myisam_sort_buffer_size</span>=<span class="hljs-number">64</span>M<span class="hljs-comment"># 内部内存临时表大小</span><span class="hljs-attr">tmp_table_size</span>=<span class="hljs-number">32</span>M<span class="hljs-comment"># 用户创建的 MEMORY 表最大大小(tmp_table_size 受此值影响)</span><span class="hljs-attr">max_heap_table_size</span>=<span class="hljs-number">32</span>M<span class="hljs-comment"># 开启查询缓存</span><span class="hljs-attr">query_cache_type</span>=<span class="hljs-number">1</span><span class="hljs-comment"># 查询缓存大小</span><span class="hljs-attr">query_cache_size</span>=<span class="hljs-number">32</span>M<span class="hljs-comment"># sql mode</span><span class="hljs-attr">sql_mode</span>=<span class="hljs-string">'STRICT_TRANS_TABLES,NO_ZERO_IN_DATE,NO_ZERO_DATE,ERROR_FOR_DIVISION_BY_ZERO,NO_AUTO_CREATE_USER,NO_ENGINE_SUBSTITUTION'</span><span class="hljs-comment">########### Network ###########</span><span class="hljs-comment"># 最大连接数(该参数受到最大文件描述符影响，如果不生效请检查最大文件描述符设置)</span><span class="hljs-comment"># refs https://stackoverflow.com/questions/39976756/the-max-connections-in-mysql-5-7</span><span class="hljs-attr">max_connections</span>=<span class="hljs-number">1500</span><span class="hljs-comment"># mysql 堆栈内暂存的链接数量</span><span class="hljs-comment"># 当短时间内链接数量超过 max_connections 时，部分链接会存储在堆栈内，存储数量受此参数控制</span><span class="hljs-attr">back_log</span>=<span class="hljs-number">256</span><span class="hljs-comment"># 最大链接错误，针对于 client 主机，超过此数量的链接错误将会导致 mysql server 针对此主机执行锁定(禁止链接 ERROR 1129 )</span><span class="hljs-comment"># 此错误计数仅在 mysql 链接握手失败才会计算，一般出现问题时都是网络故障</span><span class="hljs-comment"># refs https://www.cnblogs.com/kerrycode/p/8405862.html</span><span class="hljs-attr">max_connect_errors</span>=<span class="hljs-number">100000</span><span class="hljs-comment"># mysql server 允许的最大数据包大小</span><span class="hljs-attr">max_allowed_packet</span>=<span class="hljs-number">64</span>M<span class="hljs-comment"># 交互式客户端链接超时(30分钟自动断开)</span><span class="hljs-attr">interactive_timeout</span>=<span class="hljs-number">1800</span><span class="hljs-comment"># 非交互式链接超时时间(10分钟)</span><span class="hljs-comment"># 如果客户端有连接池，则需要协商此参数(refs https://database.51cto.com/art/201909/603519.htm)</span><span class="hljs-attr">wait_timeout</span>=<span class="hljs-number">600</span><span class="hljs-comment"># 跳过外部文件系统锁定</span><span class="hljs-comment"># If you run multiple servers that use the same database directory (not recommended), </span><span class="hljs-comment"># each server must have external locking enabled.</span><span class="hljs-comment"># refs https://dev.mysql.com/doc/refman/5.7/en/external-locking.html</span><span class="hljs-attr">skip_external_locking</span>=<span class="hljs-number">1</span><span class="hljs-comment"># 跳过链接的域名解析(开启此选项后 mysql 用户授权的 host 方式失效)</span><span class="hljs-attr">skip_name_resolve</span>=<span class="hljs-number">0</span><span class="hljs-comment"># 禁用主机名缓存，每次都会走 DNS</span><span class="hljs-attr">host_cache_size</span>=<span class="hljs-number">0</span><span class="hljs-comment">########### REPL ###########</span><span class="hljs-comment"># 开启 binlog</span><span class="hljs-attr">log_bin</span>=mysql-bin<span class="hljs-comment"># 作为从库时，同步信息依然写入 binlog，方便此从库再作为其他从库的主库</span><span class="hljs-attr">log_slave_updates</span>=<span class="hljs-number">1</span><span class="hljs-comment"># server id，默认为 ipv4 地址去除第一段</span><span class="hljs-comment"># eg: 172.16.10.11 =&gt; 161011</span><span class="hljs-attr">server_id</span>=<span class="hljs-number">161011</span><span class="hljs-comment"># 每次次事务 binlog 刷新到磁盘</span><span class="hljs-comment"># refs http://liyangliang.me/posts/2014/03/innodb_flush_log_at_trx_commit-and-sync_binlog/</span><span class="hljs-attr">sync_binlog</span>=<span class="hljs-number">100</span><span class="hljs-comment"># binlog 格式(refs https://zhuanlan.zhihu.com/p/33504555)</span><span class="hljs-attr">binlog_format</span>=row<span class="hljs-comment"># binlog 自动清理时间</span><span class="hljs-attr">expire_logs_days</span>=<span class="hljs-number">10</span><span class="hljs-comment"># 开启 relay-log，一般作为 slave 时开启</span><span class="hljs-attr">relay_log</span>=mysql-replay<span class="hljs-comment"># 主从复制时跳过 test 库</span><span class="hljs-attr">replicate_ignore_db</span>=test<span class="hljs-comment"># 每个 session binlog 缓存</span><span class="hljs-attr">binlog_cache_size</span>=<span class="hljs-number">4</span>M<span class="hljs-comment"># binlog 滚动大小</span><span class="hljs-attr">max_binlog_size</span>=<span class="hljs-number">1024</span>M<span class="hljs-comment"># GTID 相关(refs https://keithlan.github.io/2016/06/23/gtid/)</span><span class="hljs-comment">#gtid_mode=1</span><span class="hljs-comment">#enforce_gtid_consistency=1</span><span class="hljs-comment">########### InnoDB ###########</span><span class="hljs-comment"># 永久表默认存储引擎</span><span class="hljs-attr">default_storage_engine</span>=InnoDB<span class="hljs-comment"># 系统表空间数据文件大小(初始化为 1G，并且自动增长)</span><span class="hljs-attr">innodb_data_file_path</span>=ibdata1:<span class="hljs-number">1</span>G:autoextend<span class="hljs-comment"># InnoDB 缓存池大小</span><span class="hljs-comment"># innodb_buffer_pool_size 必须等于 innodb_buffer_pool_chunk_size*innodb_buffer_pool_instances，或者是其整数倍</span><span class="hljs-comment"># refs https://dev.mysql.com/doc/refman/5.7/en/innodb-buffer-pool-resize.html</span><span class="hljs-comment"># refs https://zhuanlan.zhihu.com/p/60089484</span><span class="hljs-attr">innodb_buffer_pool_size</span>=<span class="hljs-number">7680</span>M<span class="hljs-attr">innodb_buffer_pool_instances</span>=<span class="hljs-number">10</span><span class="hljs-attr">innodb_buffer_pool_chunk_size</span>=<span class="hljs-number">128</span>M<span class="hljs-comment"># InnoDB 强制恢复(refs https://www.askmaclean.com/archives/mysql-innodb-innodb_force_recovery.html)</span><span class="hljs-attr">innodb_force_recovery</span>=<span class="hljs-number">0</span><span class="hljs-comment"># InnoDB buffer 预热(refs http://www.dbhelp.net/2017/01/12/mysql-innodb-buffer-pool-warmup.html)</span><span class="hljs-attr">innodb_buffer_pool_dump_at_shutdown</span>=<span class="hljs-number">1</span><span class="hljs-attr">innodb_buffer_pool_load_at_startup</span>=<span class="hljs-number">1</span><span class="hljs-comment"># InnoDB 日志组中的日志文件数</span><span class="hljs-attr">innodb_log_files_in_group</span>=<span class="hljs-number">2</span><span class="hljs-comment"># InnoDB redo 日志大小</span><span class="hljs-comment"># refs https://www.percona.com/blog/2017/10/18/chose-mysql-innodb_log_file_size/</span><span class="hljs-attr">innodb_log_file_size</span>=<span class="hljs-number">256</span>MB<span class="hljs-comment"># 缓存还未提交的事务的缓冲区大小</span><span class="hljs-attr">innodb_log_buffer_size</span>=<span class="hljs-number">16</span>M<span class="hljs-comment"># InnoDB 在事务提交后的日志写入频率</span><span class="hljs-comment"># refs http://liyangliang.me/posts/2014/03/innodb_flush_log_at_trx_commit-and-sync_binlog/</span><span class="hljs-attr">innodb_flush_log_at_trx_commit</span>=<span class="hljs-number">2</span><span class="hljs-comment"># InnoDB DML 操作行级锁等待时间</span><span class="hljs-comment"># 超时返回 ERROR 1205 (HY000): Lock wait timeout exceeded; try restarting transaction</span><span class="hljs-comment"># refs https://ningyu1.github.io/site/post/75-mysql-lock-wait-timeout-exceeded/</span><span class="hljs-attr">innodb_lock_wait_timeout</span>=<span class="hljs-number">30</span><span class="hljs-comment"># InnoDB 行级锁超时是否回滚整个事务，默认为 OFF 仅回滚上一条语句</span><span class="hljs-comment"># 此时应用程序可以接受到错误后选择是否继续提交事务(并没有违反 ACID 原子性)</span><span class="hljs-comment"># refs https://www.cnblogs.com/hustcat/archive/2012/11/18/2775487.html</span><span class="hljs-comment">#innodb_rollback_on_timeout=ON</span><span class="hljs-comment"># InnoDB 数据写入磁盘的方式，具体见博客文章</span><span class="hljs-comment"># refs https://www.cnblogs.com/gomysql/p/3595806.html</span><span class="hljs-attr">innodb_flush_method</span>=O_DIRECT<span class="hljs-comment"># InnoDB 缓冲池脏页刷新百分比</span><span class="hljs-comment"># refs https://dbarobin.com/2015/08/29/mysql-optimization-under-ssd</span><span class="hljs-attr">innodb_max_dirty_pages_pct</span>=<span class="hljs-number">50</span><span class="hljs-comment"># InnoDB 每秒执行的写IO量</span><span class="hljs-comment"># refs https://www.centos.bz/2016/11/mysql-performance-tuning-15-config-item/#10.INNODB_IO_CAPACITY,%20INNODB_IO_CAPACITY_MAX</span><span class="hljs-attr">innodb_io_capacity</span>=<span class="hljs-number">500</span><span class="hljs-attr">innodb_io_capacity_max</span>=<span class="hljs-number">1000</span><span class="hljs-comment"># 请求并发 InnoDB 线程数</span><span class="hljs-comment"># refs https://www.cnblogs.com/xinysu/p/6439715.html#_lab2_1_0</span><span class="hljs-attr">innodb_thread_concurrency</span>=<span class="hljs-number">60</span><span class="hljs-comment"># 再使用多个 InnoDB 表空间时，允许打开的最大 ".ibd" 文件个数，不设置默认 300，</span><span class="hljs-comment"># 并且取与 table_open_cache 相比较大的一个，此选项独立于 open_files_limit</span><span class="hljs-comment"># refs https://dev.mysql.com/doc/refman/5.7/en/innodb-parameters.html#sysvar_innodb_open_files</span><span class="hljs-attr">innodb_open_files</span>=<span class="hljs-number">65535</span><span class="hljs-comment"># 每个 InnoDB 表都存储在独立的表空间(.ibd)中</span><span class="hljs-attr">innodb_file_per_table</span>=<span class="hljs-number">1</span><span class="hljs-comment"># 事务级别(可重复读，会出幻读)</span><span class="hljs-attr">transaction_isolation</span>=REPEATABLE-READ<span class="hljs-comment"># 是否在搜索和索引扫描中使用间隙锁(gap locking)，不建议使用未来将删除</span><span class="hljs-attr">innodb_locks_unsafe_for_binlog</span>=<span class="hljs-number">0</span><span class="hljs-comment"># InnoDB 后台清理线程数，更大的值有助于 DML 执行性能，&gt;= 5.7.8 默认为 4</span><span class="hljs-attr">innodb_purge_threads</span>=<span class="hljs-number">4</span></code></pre><p><strong>mysqld_safe.cnf</strong></p><pre><code class="hljs ini"><span class="hljs-comment">#</span><span class="hljs-comment"># The Percona Server 5.7 configuration file.</span><span class="hljs-comment">#</span><span class="hljs-comment"># One can use all long options that the program supports.</span><span class="hljs-comment"># Run program with --help to get a list of available options and with</span><span class="hljs-comment"># --print-defaults to see which it would actually understand and use.</span><span class="hljs-comment">#</span><span class="hljs-comment"># For explanations see</span><span class="hljs-comment"># http://dev.mysql.com/doc/mysql/en/server-system-variables.html</span><span class="hljs-section">[mysqld_safe]</span><span class="hljs-attr">pid-file</span> = /var/run/mysqld/mysqld.pid<span class="hljs-attr">socket</span>   = /var/run/mysqld/mysqld.sock<span class="hljs-attr">nice</span>     = <span class="hljs-number">0</span></code></pre><p><strong>mysqldump.cnf</strong></p><pre><code class="hljs ini"><span class="hljs-section">[mysqldump]</span>quick<span class="hljs-attr">default-character-set</span>=utf8mb4<span class="hljs-attr">max_allowed_packet</span>=<span class="hljs-number">256</span>M</code></pre><h3 id="2-5、启动"><a href="#2-5、启动" class="headerlink" title="2.5、启动"></a>2.5、启动</h3><p>配置文件调整完成后启动既可</p><pre><code class="hljs sh">systemctl start mysqld</code></pre><p>启动完成后默认 root 密码会自动生成，通过 <code>grep &#39;temporary password&#39; /var/log/mysql/*</code> 查看默认密码；获得默认密码后可以通过 <code>mysqladmin -S /data/mysql/mysql.sock -u root -p password</code> 修改 root 密码。</p><h2 id="三、Percona-Monitoring-and-Management"><a href="#三、Percona-Monitoring-and-Management" class="headerlink" title="三、Percona Monitoring and Management"></a>三、Percona Monitoring and Management</h2><p>数据库创建成功后需要增加 pmm 监控，后续将会通过监控信息来调优数据库，所以数据库监控必不可少。</p><h3 id="3-1、安装前准备"><a href="#3-1、安装前准备" class="headerlink" title="3.1、安装前准备"></a>3.1、安装前准备</h3><p>pmm 监控需要使用特定用户来监控数据信息，所以需要预先为 pmm 创建用户</p><pre><code class="hljs sql"><span class="hljs-keyword">USE</span> mysql;<span class="hljs-keyword">GRANT</span> <span class="hljs-keyword">ALL</span> <span class="hljs-keyword">PRIVILEGES</span> <span class="hljs-keyword">ON</span> *.* <span class="hljs-keyword">TO</span> <span class="hljs-string">'pmm'</span>@<span class="hljs-string">'%'</span> <span class="hljs-keyword">IDENTIFIED</span> <span class="hljs-keyword">BY</span> <span class="hljs-string">'pmm12345'</span> <span class="hljs-keyword">WITH</span> <span class="hljs-keyword">GRANT</span> <span class="hljs-keyword">OPTION</span>;<span class="hljs-keyword">FLUSH</span> <span class="hljs-keyword">PRIVILEGES</span>;</code></pre><h3 id="3-2、安装-PMM-Server"><a href="#3-2、安装-PMM-Server" class="headerlink" title="3.2、安装 PMM Server"></a>3.2、安装 PMM Server</h3><p>pmm server 端推荐直接使用 docker 启动，以下为样例 docker compose</p><pre><code class="hljs yaml"><span class="hljs-attr">version:</span> <span class="hljs-string">'3.7'</span><span class="hljs-attr">services:</span>  <span class="hljs-attr">pmm:</span>    <span class="hljs-attr">image:</span> <span class="hljs-string">percona/pmm-server:2.1.0</span>    <span class="hljs-attr">container_name:</span> <span class="hljs-string">pmm</span>    <span class="hljs-attr">restart:</span> <span class="hljs-string">always</span>    <span class="hljs-attr">volumes:</span>      <span class="hljs-bullet">-</span> <span class="hljs-string">data:/srv</span>    <span class="hljs-attr">ports:</span>      <span class="hljs-bullet">-</span> <span class="hljs-string">"80:80"</span>      <span class="hljs-bullet">-</span> <span class="hljs-string">"443:443"</span><span class="hljs-attr">volumes:</span>  <span class="hljs-attr">data:</span></code></pre><p><strong>如果想要自定义证书，请将证书复制到 volume 内的 nginx 目录下，自定义证书需要以下证书文件</strong></p><pre><code class="hljs sh">pmmserver.node ➜ tree.├── ca-certs.pem├── certificate.conf  <span class="hljs-comment"># 此文件是 pmm 默认生成自签证书的配置文件，不需要关注</span>├── certificate.crt├── certificate.key└── dhparam.pem</code></pre><p><strong>pmm server 启动后访问 <code>http(s)://IP_ADDRESS</code> 既可进入 granafa 面板，默认账户名和密码都是 <code>admin</code></strong></p><h3 id="3-3、安装-PMM-Client"><a href="#3-3、安装-PMM-Client" class="headerlink" title="3.3、安装 PMM Client"></a>3.3、安装 PMM Client</h3><p>PMM Client 同样采用 rpm 安装，下载地址 <a href="https://www.percona.com/downloads/pmm2/" target="_blank" rel="noopener">https://www.percona.com/downloads/pmm2/</a>，当前采用最新的 2.1.0 版本；rpm 下载完成后直接 <code>yum install</code> 既可。</p><p>rpm 安装完成后使用 <code>pmm-admin</code> 命令配置服务端地址，并添加当前 mysql 实例监控</p><pre><code class="hljs sh"><span class="hljs-comment"># 配置服务端地址</span>pmm-admin config --server-url https://admin:admin@pmm.mysql.node 172.16.0.11 generic mysql<span class="hljs-comment"># 配置当前 mysql 实例</span>pmm-admin add mysql --username=pmm --password=pmm12345 mysql 172.16.0.11:3306</code></pre><p>完成后稍等片刻既可在 pmm server 端的 granafa 中看到相关数据。</p><h2 id="四、数据导入"><a href="#四、数据导入" class="headerlink" title="四、数据导入"></a>四、数据导入</h2><p>从原始数据库 dump 相关库，并导入到新数据库既可</p><pre><code class="hljs sh"><span class="hljs-comment"># dump</span>mysqldump -h 172.16.1.10 -u root -p --master-data=2 --routines --triggers --single_transaction --databases DATABASE_NAME &gt; dump.sql<span class="hljs-comment"># load</span>mysql -S /data/mysql/mysql.sock -u root -p &lt; dump.sql</code></pre><p>数据导入后重建业务用户既可</p><pre><code class="hljs sql"><span class="hljs-keyword">USE</span> mysql;<span class="hljs-keyword">GRANT</span> <span class="hljs-keyword">ALL</span> <span class="hljs-keyword">PRIVILEGES</span> <span class="hljs-keyword">ON</span> *.* <span class="hljs-keyword">TO</span> <span class="hljs-string">'test_user'</span>@<span class="hljs-string">'%'</span> <span class="hljs-keyword">IDENTIFIED</span> <span class="hljs-keyword">BY</span> <span class="hljs-string">'test_user'</span> <span class="hljs-keyword">WITH</span> <span class="hljs-keyword">GRANT</span> <span class="hljs-keyword">OPTION</span>;<span class="hljs-keyword">FLUSH</span> <span class="hljs-keyword">PRIVILEGES</span>;</code></pre><h2 id="五、数据备份"><a href="#五、数据备份" class="headerlink" title="五、数据备份"></a>五、数据备份</h2><h3 id="5-1、安装-xtrabackup"><a href="#5-1、安装-xtrabackup" class="headerlink" title="5.1、安装 xtrabackup"></a>5.1、安装 xtrabackup</h3><p>目前数据备份采用 Perconra xtrabackup 工具，xtrabackup 可以实现高速、压缩带增量的备份；xtrabackup 安装同样采用 rpm 方式，下载地址为 <a href="https://www.percona.com/downloads/Percona-XtraBackup-2.4/LATEST/" target="_blank" rel="noopener">https://www.percona.com/downloads/Percona-XtraBackup-2.4/LATEST/</a>，下载完成后执行 <code>yum install</code> 既可</p><h3 id="5-2、备份工具"><a href="#5-2、备份工具" class="headerlink" title="5.2、备份工具"></a>5.2、备份工具</h3><p>目前备份工具开源在 <a href="https://github.com/gozap/mybak" target="_blank" rel="noopener">GitHub</a> 上，每次全量备份会写入 <code>.full-backup</code> 文件，增量备份会写入 <code>.inc-backup</code> 文件</p><h3 id="5-3、配置-systemd"><a href="#5-3、配置-systemd" class="headerlink" title="5.3、配置 systemd"></a>5.3、配置 systemd</h3><p>为了使备份自动运行，目前将定时任务配置到 systemd 中，由 systemd 调度并执行；以下为相关 systemd 配置文件</p><p><strong>mysql-backup-full.service</strong></p><pre><code class="hljs sh">[Unit]Description=mysql full backupAfter=network.target[Service]Type=simpleRestart=on-failureExecStart=/usr/<span class="hljs-built_in">local</span>/bin/mybak --backup-dir /data/mysql_backup --prefix mysql full[Install]WantedBy=multi-user.target</code></pre><p><strong>mysql-backup-inc.service</strong></p><pre><code class="hljs sh">[Unit]Description=mysql incremental backupAfter=network.target[Service]Type=simpleRestart=on-failureExecStart=/usr/<span class="hljs-built_in">local</span>/bin/mybak --backup-dir /data/mysql_backup --prefix mysql inc[Install]WantedBy=multi-user.target</code></pre><p><strong>mysql-backup-compress.service</strong></p><pre><code class="hljs sh">[Unit]Description=mysql backup compressAfter=network.target[Service]Type=simpleRestart=on-failureExecStart=/usr/<span class="hljs-built_in">local</span>/bin/mybak --backup-dir /data/mysql_backup --prefix mysql compress --clean[Install]WantedBy=multi-user.target</code></pre><p><strong>mysql-backup-full.timer</strong></p><pre><code class="hljs sh">[Unit]Description=mysql weekly full backup<span class="hljs-comment"># 备份之前依赖相关目录的挂载</span>After=data.mountAfter=data-mysql_backup.mount[Timer]<span class="hljs-comment"># 目前每周日一个全量备份</span>OnCalendar=Sun *-*-* 3:00Persistent=<span class="hljs-literal">true</span>[Install]WantedBy=timers.target</code></pre><p><strong>mysql-backup-inc.timer</strong></p><pre><code class="hljs sh">[Unit]Description=mysql weekly full backupAfter=data.mountAfter=data-mysql_backup.mount[Timer]<span class="hljs-comment"># 每天三个增量备份</span>OnCalendar=*-*-* 9:00OnCalendar=*-*-* 13:00OnCalendar=*-*-* 18:00Persistent=<span class="hljs-literal">true</span>[Install]WantedBy=timers.target</code></pre><p><strong>mysql-backup-compress.timer</strong></p><pre><code class="hljs sh">[Unit]Description=mysql weekly backup compress<span class="hljs-comment"># 备份之前依赖相关目录的挂载</span>After=data.mountAfter=data-mysql_backup.mount[Timer]<span class="hljs-comment"># 目前每周日一个全量备份，自动压缩后同时完成清理</span>OnCalendar=Sun *-*-* 5:00Persistent=<span class="hljs-literal">true</span>[Install]WantedBy=timers.target</code></pre><p>创建好相关文件后启动相关定时器既可</p><pre><code class="hljs sh">cp *.timer *.service /lib/systemd/systemsystemctl daemon-reloadsystemctl start mysql-backup-full.timer mysql-backup-inc.timer mysql-backup-compress.timersystemctl <span class="hljs-built_in">enable</span> mysql-backup-full.timer mysql-backup-inc.timer mysql-backup-compress.timer</code></pre><h2 id="六、数据恢复"><a href="#六、数据恢复" class="headerlink" title="六、数据恢复"></a>六、数据恢复</h2><h3 id="6-1、全量备份恢复"><a href="#6-1、全量备份恢复" class="headerlink" title="6.1、全量备份恢复"></a>6.1、全量备份恢复</h3><p>针对于全量备份，只需要按照官方文档的还原顺序进行还原既可</p><pre><code class="hljs sh"><span class="hljs-comment"># 由于备份时进行了压缩，所以先解压备份文件</span>xtrabackup --decompress --parallel 4 --target-dir /data/mysql_backup/mysql-20191205230502<span class="hljs-comment"># 执行预处理</span>xtrabackup --prepare --target-dir /data/mysql_backup/mysql-20191205230502<span class="hljs-comment"># 执行恢复(恢复时自动根据 my.cnf 将数据覆盖到 data 数据目录)</span>xtrabackup --copy-back --target-dir /data/mysql_backup/mysql-20191205230502<span class="hljs-comment"># 修复数据目录权限</span>chown -R mysql:mysql /data/mysql<span class="hljs-comment"># 启动 mysql</span>systemctl start mysqld</code></pre><h3 id="6-2、增量备份恢复"><a href="#6-2、增量备份恢复" class="headerlink" title="6.2、增量备份恢复"></a>6.2、增量备份恢复</h3><p>对于增量备份恢复，其与全量备份恢复的根本区别在于: <strong>对于非最后一个增量文件的预处理必须使用 <code>--apply-log-only</code> 选项防止运行回滚阶段的处理</strong></p><pre><code class="hljs sh"><span class="hljs-comment"># 对所有备份文件进行解压处理</span><span class="hljs-keyword">for</span> dir <span class="hljs-keyword">in</span> `ls`; <span class="hljs-keyword">do</span> xtrabackup --decompress --parallel 4 --target-dir <span class="hljs-variable">$dir</span>; <span class="hljs-keyword">done</span><span class="hljs-comment"># 对全量备份文件执行预处理(注意增加 --apply-log-only 选项)</span>xtrabackup --prepare --apply-log-only --target-dir /data/mysql_backup/mysql-20191205230502<span class="hljs-comment"># 对非最后一个增量备份执行预处理</span>xtrabackup --prepare --apply-log-only --target-dir /data/mysql_backup/mysql-20191205230502 --incremental-dir /data/mysql_backup/mysql-inc-20191206230802<span class="hljs-comment"># 对最后一个增量备份执行预处理(不需要 --apply-log-only)</span>xtrabackup --prepare --target-dir /data/mysql_backup/mysql-20191205230502 --incremental-dir /data/mysql_backup/mysql-inc-20191207031005<span class="hljs-comment"># 执行恢复(恢复时自动根据 my.cnf 将数据覆盖到 data 数据目录)</span>xtrabackup --copy-back --target-dir /data/mysql_backup/mysql-20191205230502<span class="hljs-comment"># 修复数据目录权限</span>chown -R mysql:mysql /data/mysql<span class="hljs-comment"># 启动 mysql</span>systemctl start mysqld</code></pre><h3 id="6-3、创建-slave"><a href="#6-3、创建-slave" class="headerlink" title="6.3、创建 slave"></a>6.3、创建 slave</h3><p>针对 xtrabackup 备份的数据可以直接恢复成 slave 节点，具体步骤如下:</p><p>首先将备份文件复制到目标机器，然后执行解压(默认备份工具采用 lz4 压缩)</p><pre><code class="hljs sh">xtrabackup --decompress --target-dir=xxxxxx</code></pre><p>解压完成后执行预处理操作(<strong>在执行预处理之前请确保 slave 机器上相关配置文件与 master 相同，并且处理好数据目录存放等</strong>)</p><pre><code class="hljs sh">xtrabackup --user=root --password=xxxxxxx --prepare --target-dir=xxxx</code></pre><p>预处理成功后便可执行恢复，以下命令将自动读取 <code>my.cnf</code> 配置，自动识别数据目录位置并将数据文件移动到该位置</p><pre><code class="hljs sh">xtrabackup --move-back --target-dir=xxxxx</code></pre><p>所由准备就绪后需要进行权限修复</p><pre><code class="hljs sh">chown -R mysql:mysql MYSQL_DATA_DIR</code></pre><p>最后在 mysql 内启动 slave 既可，slave 信息可通过从数据备份目录的 <code>xtrabackup_binlog_info</code> 中获取</p><pre><code class="hljs sh"><span class="hljs-comment"># 获取备份 POS 信息</span>cat xxxxxx/xtrabackup_binlog_info<span class="hljs-comment"># 创建 slave 节点</span>CHANGE MASTER TO    MASTER_HOST=<span class="hljs-string">'192.168.2.48'</span>,    MASTER_USER=<span class="hljs-string">'repl'</span>,    MASTER_PASSWORD=<span class="hljs-string">'xxxxxxx'</span>,    MASTER_LOG_FILE=<span class="hljs-string">'mysql-bin.000005'</span>,    MASTER_LOG_POS=52500595;<span class="hljs-comment"># 启动 slave</span>start slave;show slave status \G;</code></pre><h2 id="七、生产处理"><a href="#七、生产处理" class="headerlink" title="七、生产处理"></a>七、生产处理</h2><h3 id="7-1、数据目录"><a href="#7-1、数据目录" class="headerlink" title="7.1、数据目录"></a>7.1、数据目录</h3><p>目前生产环境数据目录位置调整到 <code>/home/mysql</code>，所以目录权限处理也要做对应调整</p><pre><code class="hljs sh">mkdir -p /var/<span class="hljs-built_in">log</span>/mysql /home/mysql_tmpchown -R mysql:mysql /var/<span class="hljs-built_in">log</span>/mysql /home/mysql_tmp</code></pre><h3 id="7-2、配置文件"><a href="#7-2、配置文件" class="headerlink" title="7.2、配置文件"></a>7.2、配置文件</h3><p>生产环境目前节点配置如下</p><ul><li>CPU: <code>Intel(R) Xeon(R) CPU E5-2620 v4 @ 2.10GHz</code></li><li>RAM: <code>128G</code></li></ul><p>所以配置文件也需要做相应的优化调整</p><p><strong>mysql.cnf</strong></p><pre><code class="hljs ini"><span class="hljs-section">[mysql]</span>auto-rehash<span class="hljs-attr">default_character_set</span>=utf8mb4</code></pre><p><strong>mysqld.cnf</strong></p><pre><code class="hljs ini"><span class="hljs-comment"># Percona Server template configuration</span><span class="hljs-section">[mysqld]</span><span class="hljs-comment">#</span><span class="hljs-comment"># Remove leading # and set to the amount of RAM for the most important data</span><span class="hljs-comment"># cache in MySQL. Start at 70% of total RAM for dedicated server, else 10%.</span><span class="hljs-comment"># innodb_buffer_pool_size = 128M</span><span class="hljs-comment">#</span><span class="hljs-comment"># Remove leading # to turn on a very important data integrity option: logging</span><span class="hljs-comment"># changes to the binary log between backups.</span><span class="hljs-comment"># log_bin</span><span class="hljs-comment">#</span><span class="hljs-comment"># Remove leading # to set options mainly useful for reporting servers.</span><span class="hljs-comment"># The server defaults are faster for transactions and fast SELECTs.</span><span class="hljs-comment"># Adjust sizes as needed, experiment to find the optimal values.</span><span class="hljs-comment"># join_buffer_size = 128M</span><span class="hljs-comment"># sort_buffer_size = 2M</span><span class="hljs-comment"># read_rnd_buffer_size = 2M</span><span class="hljs-attr">port</span>=<span class="hljs-number">3306</span><span class="hljs-attr">datadir</span>=/home/mysql/mysql<span class="hljs-attr">socket</span>=/home/mysql/mysql/mysql.sock<span class="hljs-attr">pid_file</span>=/home/mysql/mysql/mysqld.pid<span class="hljs-comment"># 服务端编码</span><span class="hljs-attr">character_set_server</span>=utf8mb4<span class="hljs-comment"># 服务端排序</span><span class="hljs-attr">collation_server</span>=utf8mb4_general_ci<span class="hljs-comment"># 强制使用 utf8mb4 编码集，忽略客户端设置</span><span class="hljs-attr">skip_character_set_client_handshake</span>=<span class="hljs-number">1</span><span class="hljs-comment"># 日志输出到文件</span><span class="hljs-attr">log_output</span>=FILE<span class="hljs-comment"># 开启常规日志输出</span><span class="hljs-attr">general_log</span>=<span class="hljs-number">1</span><span class="hljs-comment"># 常规日志输出文件位置</span><span class="hljs-attr">general_log_file</span>=/var/log/mysql/mysqld.log<span class="hljs-comment"># 错误日志位置</span><span class="hljs-attr">log_error</span>=/var/log/mysql/mysqld-error.log<span class="hljs-comment"># 记录慢查询</span><span class="hljs-attr">slow_query_log</span>=<span class="hljs-number">1</span><span class="hljs-comment"># 慢查询时间(大于 1s 被视为慢查询)</span><span class="hljs-attr">long_query_time</span>=<span class="hljs-number">1</span><span class="hljs-comment"># 慢查询日志文件位置</span><span class="hljs-attr">slow_query_log_file</span>=/var/log/mysql/mysqld-slow.log<span class="hljs-comment"># 临时文件位置</span><span class="hljs-attr">tmpdir</span>=/home/mysql/mysql_tmp<span class="hljs-comment"># The number of open tables for all threads.(refs https://dev.mysql.com/doc/refman/5.7/en/server-system-variables.html#sysvar_table_open_cache)</span><span class="hljs-attr">table_open_cache</span>=<span class="hljs-number">16384</span><span class="hljs-comment"># 文件描述符(此处修改不生效，请修改 systemd service 配置) </span><span class="hljs-comment"># refs https://www.percona.com/blog/2017/10/12/open_files_limit-mystery/</span><span class="hljs-comment"># refs https://www.cnblogs.com/wxxjianchi/p/10370419.html</span><span class="hljs-comment">#open_files_limit=65535</span><span class="hljs-comment"># 表定义缓存(5.7 以后自动调整)</span><span class="hljs-comment"># refs https://dev.mysql.com/doc/refman/5.6/en/server-system-variables.html#sysvar_table_definition_cache</span><span class="hljs-comment"># refs http://mysql.taobao.org/monthly/2015/08/10/</span><span class="hljs-comment">#table_definition_cache=16384</span><span class="hljs-attr">sort_buffer_size</span>=<span class="hljs-number">1</span>M<span class="hljs-attr">join_buffer_size</span>=<span class="hljs-number">1</span>M<span class="hljs-comment"># MyiSAM 引擎专用(内部临时磁盘表可能会用)</span><span class="hljs-attr">read_buffer_size</span>=<span class="hljs-number">1</span>M<span class="hljs-attr">read_rnd_buffer_size</span>=<span class="hljs-number">1</span>M<span class="hljs-comment"># MyiSAM 引擎专用(内部临时磁盘表可能会用)</span><span class="hljs-attr">key_buffer_size</span>=<span class="hljs-number">32</span>M<span class="hljs-comment"># MyiSAM 引擎专用(内部临时磁盘表可能会用)</span><span class="hljs-attr">bulk_insert_buffer_size</span>=<span class="hljs-number">16</span>M<span class="hljs-comment"># myisam_sort_buffer_size 与 sort_buffer_size 区别请参考(https://stackoverflow.com/questions/7871027/myisam-sort-buffer-size-vs-sort-buffer-size)</span><span class="hljs-attr">myisam_sort_buffer_size</span>=<span class="hljs-number">64</span>M<span class="hljs-comment"># 内部内存临时表大小</span><span class="hljs-attr">tmp_table_size</span>=<span class="hljs-number">32</span>M<span class="hljs-comment"># 用户创建的 MEMORY 表最大大小(tmp_table_size 受此值影响)</span><span class="hljs-attr">max_heap_table_size</span>=<span class="hljs-number">32</span>M<span class="hljs-comment"># 开启查询缓存</span><span class="hljs-attr">query_cache_type</span>=<span class="hljs-number">1</span><span class="hljs-comment"># 查询缓存大小</span><span class="hljs-attr">query_cache_size</span>=<span class="hljs-number">32</span>M<span class="hljs-comment"># sql mode</span><span class="hljs-attr">sql_mode</span>=<span class="hljs-string">'STRICT_TRANS_TABLES,NO_ZERO_IN_DATE,NO_ZERO_DATE,ERROR_FOR_DIVISION_BY_ZERO,NO_AUTO_CREATE_USER,NO_ENGINE_SUBSTITUTION'</span><span class="hljs-comment">########### Network ###########</span><span class="hljs-comment"># 最大连接数(该参数受到最大文件描述符影响，如果不生效请检查最大文件描述符设置)</span><span class="hljs-comment"># refs https://stackoverflow.com/questions/39976756/the-max-connections-in-mysql-5-7</span><span class="hljs-attr">max_connections</span>=<span class="hljs-number">1500</span><span class="hljs-comment"># mysql 堆栈内暂存的链接数量</span><span class="hljs-comment"># 当短时间内链接数量超过 max_connections 时，部分链接会存储在堆栈内，存储数量受此参数控制</span><span class="hljs-attr">back_log</span>=<span class="hljs-number">256</span><span class="hljs-comment"># 最大链接错误，针对于 client 主机，超过此数量的链接错误将会导致 mysql server 针对此主机执行锁定(禁止链接 ERROR 1129 )</span><span class="hljs-comment"># 此错误计数仅在 mysql 链接握手失败才会计算，一般出现问题时都是网络故障</span><span class="hljs-comment"># refs https://www.cnblogs.com/kerrycode/p/8405862.html</span><span class="hljs-attr">max_connect_errors</span>=<span class="hljs-number">100000</span><span class="hljs-comment"># mysql server 允许的最大数据包大小</span><span class="hljs-attr">max_allowed_packet</span>=<span class="hljs-number">64</span>M<span class="hljs-comment"># 交互式客户端链接超时(30分钟自动断开)</span><span class="hljs-attr">interactive_timeout</span>=<span class="hljs-number">1800</span><span class="hljs-comment"># 非交互式链接超时时间(10分钟)</span><span class="hljs-comment"># 如果客户端有连接池，则需要协商此参数(refs https://database.51cto.com/art/201909/603519.htm)</span><span class="hljs-attr">wait_timeout</span>=<span class="hljs-number">28800</span><span class="hljs-comment"># 跳过外部文件系统锁定</span><span class="hljs-comment"># If you run multiple servers that use the same database directory (not recommended), </span><span class="hljs-comment"># each server must have external locking enabled.</span><span class="hljs-comment"># refs https://dev.mysql.com/doc/refman/5.7/en/external-locking.html</span><span class="hljs-attr">skip_external_locking</span>=<span class="hljs-number">1</span><span class="hljs-comment"># 跳过链接的域名解析(开启此选项后 mysql 用户授权的 host 方式失效)</span><span class="hljs-attr">skip_name_resolve</span>=<span class="hljs-number">0</span><span class="hljs-comment"># 禁用主机名缓存，每次都会走 DNS</span><span class="hljs-attr">host_cache_size</span>=<span class="hljs-number">0</span><span class="hljs-comment">########### REPL ###########</span><span class="hljs-comment"># 开启 binlog</span><span class="hljs-attr">log_bin</span>=mysql-bin<span class="hljs-comment"># 作为从库时，同步信息依然写入 binlog，方便此从库再作为其他从库的主库</span><span class="hljs-attr">log_slave_updates</span>=<span class="hljs-number">1</span><span class="hljs-comment"># server id，默认为 ipv4 地址去除第一段</span><span class="hljs-comment"># eg: 192.168.2.48 =&gt; 168248</span><span class="hljs-attr">server_id</span>=<span class="hljs-number">168248</span><span class="hljs-comment"># 每 n 次事务 binlog 刷新到磁盘</span><span class="hljs-comment"># refs http://liyangliang.me/posts/2014/03/innodb_flush_log_at_trx_commit-and-sync_binlog/</span><span class="hljs-attr">sync_binlog</span>=<span class="hljs-number">100</span><span class="hljs-comment"># binlog 格式(refs https://zhuanlan.zhihu.com/p/33504555)</span><span class="hljs-attr">binlog_format</span>=row<span class="hljs-comment"># binlog 自动清理时间</span><span class="hljs-attr">expire_logs_days</span>=<span class="hljs-number">20</span><span class="hljs-comment"># 开启 relay-log，一般作为 slave 时开启</span><span class="hljs-attr">relay_log</span>=mysql-replay<span class="hljs-comment"># 主从复制时跳过 test 库</span><span class="hljs-attr">replicate_ignore_db</span>=test<span class="hljs-comment"># 每个 session binlog 缓存</span><span class="hljs-attr">binlog_cache_size</span>=<span class="hljs-number">4</span>M<span class="hljs-comment"># binlog 滚动大小</span><span class="hljs-attr">max_binlog_size</span>=<span class="hljs-number">1024</span>M<span class="hljs-comment"># GTID 相关(refs https://keithlan.github.io/2016/06/23/gtid/)</span><span class="hljs-comment">#gtid_mode=1</span><span class="hljs-comment">#enforce_gtid_consistency=1</span><span class="hljs-comment">########### InnoDB ###########</span><span class="hljs-comment"># 永久表默认存储引擎</span><span class="hljs-attr">default_storage_engine</span>=InnoDB<span class="hljs-comment"># 系统表空间数据文件大小(初始化为 1G，并且自动增长)</span><span class="hljs-attr">innodb_data_file_path</span>=ibdata1:<span class="hljs-number">1</span>G:autoextend<span class="hljs-comment"># InnoDB 缓存池大小(资源充足，为所欲为)</span><span class="hljs-comment"># innodb_buffer_pool_size 必须等于 innodb_buffer_pool_chunk_size*innodb_buffer_pool_instances，或者是其整数倍</span><span class="hljs-comment"># refs https://dev.mysql.com/doc/refman/5.7/en/innodb-buffer-pool-resize.html</span><span class="hljs-comment"># refs https://zhuanlan.zhihu.com/p/60089484</span><span class="hljs-attr">innodb_buffer_pool_size</span>=<span class="hljs-number">61440</span>M<span class="hljs-attr">innodb_buffer_pool_instances</span>=<span class="hljs-number">16</span><span class="hljs-comment"># 默认 128M</span><span class="hljs-attr">innodb_buffer_pool_chunk_size</span>=<span class="hljs-number">128</span>M<span class="hljs-comment"># InnoDB 强制恢复(refs https://www.askmaclean.com/archives/mysql-innodb-innodb_force_recovery.html)</span><span class="hljs-attr">innodb_force_recovery</span>=<span class="hljs-number">0</span><span class="hljs-comment"># InnoDB buffer 预热(refs http://www.dbhelp.net/2017/01/12/mysql-innodb-buffer-pool-warmup.html)</span><span class="hljs-attr">innodb_buffer_pool_dump_at_shutdown</span>=<span class="hljs-number">1</span><span class="hljs-attr">innodb_buffer_pool_load_at_startup</span>=<span class="hljs-number">1</span><span class="hljs-comment"># InnoDB 日志组中的日志文件数</span><span class="hljs-attr">innodb_log_files_in_group</span>=<span class="hljs-number">2</span><span class="hljs-comment"># InnoDB redo 日志大小</span><span class="hljs-comment"># refs https://www.percona.com/blog/2017/10/18/chose-mysql-innodb_log_file_size/</span><span class="hljs-attr">innodb_log_file_size</span>=<span class="hljs-number">256</span>MB<span class="hljs-comment"># 缓存还未提交的事务的缓冲区大小</span><span class="hljs-attr">innodb_log_buffer_size</span>=<span class="hljs-number">16</span>M<span class="hljs-comment"># InnoDB 在事务提交后的日志写入频率</span><span class="hljs-comment"># refs http://liyangliang.me/posts/2014/03/innodb_flush_log_at_trx_commit-and-sync_binlog/</span><span class="hljs-attr">innodb_flush_log_at_trx_commit</span>=<span class="hljs-number">2</span><span class="hljs-comment"># InnoDB DML 操作行级锁等待时间</span><span class="hljs-comment"># 超时返回 ERROR 1205 (HY000): Lock wait timeout exceeded; try restarting transaction</span><span class="hljs-comment"># refs https://ningyu1.github.io/site/post/75-mysql-lock-wait-timeout-exceeded/</span><span class="hljs-attr">innodb_lock_wait_timeout</span>=<span class="hljs-number">30</span><span class="hljs-comment"># InnoDB 行级锁超时是否回滚整个事务，默认为 OFF 仅回滚上一条语句</span><span class="hljs-comment"># 此时应用程序可以接受到错误后选择是否继续提交事务(并没有违反 ACID 原子性)</span><span class="hljs-comment"># refs https://www.cnblogs.com/hustcat/archive/2012/11/18/2775487.html</span><span class="hljs-comment">#innodb_rollback_on_timeout=ON</span><span class="hljs-comment"># InnoDB 数据写入磁盘的方式，具体见博客文章</span><span class="hljs-comment"># refs https://www.cnblogs.com/gomysql/p/3595806.html</span><span class="hljs-attr">innodb_flush_method</span>=O_DIRECT<span class="hljs-comment"># InnoDB 缓冲池脏页刷新百分比</span><span class="hljs-comment"># refs https://dbarobin.com/2015/08/29/mysql-optimization-under-ssd</span><span class="hljs-attr">innodb_max_dirty_pages_pct</span>=<span class="hljs-number">50</span><span class="hljs-comment"># InnoDB 每秒执行的写IO量</span><span class="hljs-comment"># refs https://www.centos.bz/2016/11/mysql-performance-tuning-15-config-item/#10.INNODB_IO_CAPACITY,%20INNODB_IO_CAPACITY_MAX</span><span class="hljs-comment"># refs https://www.alibabacloud.com/blog/testing-io-performance-with-sysbench_594709</span><span class="hljs-attr">innodb_io_capacity</span>=<span class="hljs-number">8000</span><span class="hljs-attr">innodb_io_capacity_max</span>=<span class="hljs-number">16000</span><span class="hljs-comment"># 请求并发 InnoDB 线程数</span><span class="hljs-comment"># refs https://www.cnblogs.com/xinysu/p/6439715.html#_lab2_1_0</span><span class="hljs-attr">innodb_thread_concurrency</span>=<span class="hljs-number">0</span><span class="hljs-comment"># 再使用多个 InnoDB 表空间时，允许打开的最大 ".ibd" 文件个数，不设置默认 300，</span><span class="hljs-comment"># 并且取与 table_open_cache 相比较大的一个，此选项独立于 open_files_limit</span><span class="hljs-comment"># refs https://dev.mysql.com/doc/refman/5.7/en/innodb-parameters.html#sysvar_innodb_open_files</span><span class="hljs-attr">innodb_open_files</span>=<span class="hljs-number">65535</span><span class="hljs-comment"># 每个 InnoDB 表都存储在独立的表空间(.ibd)中</span><span class="hljs-attr">innodb_file_per_table</span>=<span class="hljs-number">1</span><span class="hljs-comment"># 事务级别(可重复读，会出幻读)</span><span class="hljs-attr">transaction_isolation</span>=REPEATABLE-READ<span class="hljs-comment"># 是否在搜索和索引扫描中使用间隙锁(gap locking)，不建议使用未来将删除</span><span class="hljs-attr">innodb_locks_unsafe_for_binlog</span>=<span class="hljs-number">0</span><span class="hljs-comment"># InnoDB 后台清理线程数，更大的值有助于 DML 执行性能，&gt;= 5.7.8 默认为 4</span><span class="hljs-attr">innodb_purge_threads</span>=<span class="hljs-number">4</span></code></pre><p><strong>mysqld_safe.cnf</strong></p><pre><code class="hljs ini"><span class="hljs-comment">#</span><span class="hljs-comment"># The Percona Server 5.7 configuration file.</span><span class="hljs-comment">#</span><span class="hljs-comment"># One can use all long options that the program supports.</span><span class="hljs-comment"># Run program with --help to get a list of available options and with</span><span class="hljs-comment"># --print-defaults to see which it would actually understand and use.</span><span class="hljs-comment">#</span><span class="hljs-comment"># For explanations see</span><span class="hljs-comment"># http://dev.mysql.com/doc/mysql/en/server-system-variables.html</span><span class="hljs-section">[mysqld_safe]</span><span class="hljs-attr">pid-file</span> = /var/run/mysqld/mysqld.pid<span class="hljs-attr">socket</span>   = /var/run/mysqld/mysqld.sock<span class="hljs-attr">nice</span>     = <span class="hljs-number">0</span></code></pre><p><strong>mysqldump.cnf</strong></p><pre><code class="hljs ini"><span class="hljs-section">[mysqldump]</span>quick<span class="hljs-attr">default-character-set</span>=utf8mb4<span class="hljs-attr">max_allowed_packet</span>=<span class="hljs-number">256</span>M</code></pre><h2 id="八、常用诊断"><a href="#八、常用诊断" class="headerlink" title="八、常用诊断"></a>八、常用诊断</h2><h3 id="8-1、动态配置-diff"><a href="#8-1、动态配置-diff" class="headerlink" title="8.1、动态配置 diff"></a>8.1、动态配置 diff</h3><p>mysql 默认允许在实例运行后使用 <code>set global VARIABLES=VALUE</code> 的方式动态调整一些配置，这可能导致在运行一段时间后(运维动态修改)实例运行配置和配置文件中配置不一致；所以<strong>建议定期 diff 运行时配置与配置文件配置差异，防制特殊情况下 mysql 重启后运行期配置丢失</strong></p><pre><code class="hljs sh">pt-config-diff /etc/percona-server.conf.d/mysqld.cnf h=127.0.0.1 --user root --ask-pass --report-width 100Enter MySQL password:2 config differencesVariable                  /etc/percona-server.conf.d/mysqld.cnf mysql47.test.com========================= ===================================== ==================innodb_max_dirty_pages... 50                                    50.000000skip_name_resolve         0                                     ON</code></pre><h3 id="8-2、配置优化建议"><a href="#8-2、配置优化建议" class="headerlink" title="8.2、配置优化建议"></a>8.2、配置优化建议</h3><p>Percona Toolkit 提供了一个诊断工具，用于对 mysql 内的配置进行扫描并给出优化建议，在初始化时可以使用此工具评估 mysql 当前配置的具体情况</p><pre><code class="hljs sh">pt-variable-advisor 127.0.0.1 --user root --ask-pass | grep -v <span class="hljs-string">'^$'</span>Enter password: <span class="hljs-comment"># WARN delay_key_write: MyISAM index blocks are never flushed until necessary.</span><span class="hljs-comment"># WARN innodb_flush_log_at_trx_commit-1: InnoDB is not configured in strictly ACID mode.</span><span class="hljs-comment"># NOTE innodb_max_dirty_pages_pct: The innodb_max_dirty_pages_pct is lower than the default.</span><span class="hljs-comment"># WARN max_connections: If the server ever really has more than a thousand threads running, then the system is likely to spend more time scheduling threads than really doing useful work.</span><span class="hljs-comment"># NOTE read_buffer_size-1: The read_buffer_size variable should generally be left at its default unless an expert determines it is necessary to change it.</span><span class="hljs-comment"># NOTE read_rnd_buffer_size-1: The read_rnd_buffer_size variable should generally be left at its default unless an expert determines it is necessary to change it.</span><span class="hljs-comment"># NOTE sort_buffer_size-1: The sort_buffer_size variable should generally be left at its default unless an expert determines it is necessary to change it.</span><span class="hljs-comment"># NOTE innodb_data_file_path: Auto-extending InnoDB files can consume a lot of disk space that is very difficult to reclaim later.</span><span class="hljs-comment"># WARN myisam_recover_options: myisam_recover_options should be set to some value such as BACKUP,FORCE to ensure that table corruption is noticed.</span><span class="hljs-comment"># WARN sync_binlog: Binary logging is enabled, but sync_binlog isn't configured so that every transaction is flushed to the binary log for durability.</span></code></pre><h3 id="8-3、死锁诊断"><a href="#8-3、死锁诊断" class="headerlink" title="8.3、死锁诊断"></a>8.3、死锁诊断</h3><p>使用 pt-deadlock-logger 工具可以诊断当前的死锁状态，以下为对死锁检测的测试</p><p>首先创建测试数据库和表</p><pre><code class="hljs sql"><span class="hljs-comment"># 创建测试库</span><span class="hljs-keyword">CREATE</span> <span class="hljs-keyword">DATABASE</span> dbatest <span class="hljs-built_in">CHARACTER</span> <span class="hljs-keyword">SET</span> utf8mb4 <span class="hljs-keyword">COLLATE</span> utf8mb4_unicode_ci;<span class="hljs-comment"># 切换到测试库并建立测试表</span><span class="hljs-keyword">USE</span> dbatest;<span class="hljs-keyword">CREATE</span> <span class="hljs-keyword">TABLE</span> <span class="hljs-keyword">IF</span> <span class="hljs-keyword">NOT</span> <span class="hljs-keyword">EXISTS</span> <span class="hljs-keyword">test</span> (<span class="hljs-keyword">id</span> <span class="hljs-built_in">INT</span> AUTO_INCREMENT PRIMARY <span class="hljs-keyword">KEY</span>, <span class="hljs-keyword">value</span> <span class="hljs-built_in">VARCHAR</span>(<span class="hljs-number">255</span>), createtime <span class="hljs-built_in">TIMESTAMP</span> <span class="hljs-keyword">DEFAULT</span> <span class="hljs-keyword">CURRENT_TIMESTAMP</span>) <span class="hljs-keyword">ENGINE</span>=<span class="hljs-keyword">INNODB</span>;</code></pre><p>在一个其他终端上开启 pt-deadlock-logger 检测</p><pre><code class="hljs sh">pt-deadlock-logger 127.0.0.1 --user root --ask-pass --tab</code></pre><p>检测开启后进行死锁测试</p><pre><code class="hljs sql"><span class="hljs-comment"># 插入两条测试数据</span><span class="hljs-keyword">INSERT</span> <span class="hljs-keyword">INTO</span> <span class="hljs-keyword">test</span>(<span class="hljs-keyword">value</span>) <span class="hljs-keyword">VALUES</span>(<span class="hljs-string">'test1'</span>);<span class="hljs-keyword">INSERT</span> <span class="hljs-keyword">INTO</span> <span class="hljs-keyword">test</span>(<span class="hljs-keyword">value</span>) <span class="hljs-keyword">VALUES</span>(<span class="hljs-string">'test2'</span>);<span class="hljs-comment"># 在两个终端下进行交叉事务</span><span class="hljs-comment"># 统一关闭自动提交</span>terminal_1 <span class="hljs-comment"># SET AUTOCOMMIT = 0;</span>terminal_2 <span class="hljs-comment"># SET AUTOCOMMIT = 0;</span><span class="hljs-comment"># 交叉事务，终端 1 先更新第一条数据，终端 2 先更新第二条数据</span>terminal_1 <span class="hljs-comment"># BEGIN;</span>terminal_1 <span class="hljs-comment"># UPDATE test set value='x1' where id=1;</span>terminal_2 <span class="hljs-comment"># BEGIN;</span>terminal_2 <span class="hljs-comment"># UPDATE test set value='x2' where id=2;</span><span class="hljs-comment"># 此后终端 1 再尝试更新第二条数据，终端 2 再尝试更新第一条数据；造成等待互向释放锁的死锁</span>terminal_1 <span class="hljs-comment"># UPDATE test set value='lock2' where id=2;</span>terminal_2 <span class="hljs-comment"># UPDATE test set value='lock1' where id=1;</span><span class="hljs-comment"># 此时由于开启了 mysql innodb 的死锁自动检测机制，会导致终端 2 弹出错误</span>ERROR 1213 (40001): Deadlock found when trying to get <span class="hljs-keyword">lock</span>; try restarting transaction<span class="hljs-comment"># 同时 pt-deadlock-logger 有日志输出</span>server  ts      thread  txn_id  txn_time        user    hostname    ip      db      tbl     idx     lock_type       lock_mode       wait_hold       victim  query127.0.0.1       2019-12-24T14:57:10     87      0       52      root            127.0.0.1       dbatest test    PRIMARY RECORD  X       w       0       <span class="hljs-keyword">UPDATE</span> <span class="hljs-keyword">test</span> <span class="hljs-keyword">set</span> <span class="hljs-keyword">value</span>=<span class="hljs-string">'lock2'</span> <span class="hljs-keyword">where</span> <span class="hljs-keyword">id</span>=<span class="hljs-number">2</span><span class="hljs-number">127.0</span><span class="hljs-number">.0</span><span class="hljs-number">.1</span>       <span class="hljs-number">2019</span><span class="hljs-number">-12</span><span class="hljs-number">-24</span>T14:<span class="hljs-number">57</span>:<span class="hljs-number">10</span>     <span class="hljs-number">89</span>      <span class="hljs-number">0</span>       <span class="hljs-number">41</span>      root            <span class="hljs-number">127.0</span><span class="hljs-number">.0</span><span class="hljs-number">.1</span>       dbatest <span class="hljs-keyword">test</span>    PRIMARY <span class="hljs-built_in">RECORD</span>  X       w       <span class="hljs-number">1</span>       <span class="hljs-keyword">UPDATE</span> <span class="hljs-keyword">test</span> <span class="hljs-keyword">set</span> <span class="hljs-keyword">value</span>=<span class="hljs-string">'lock1'</span> <span class="hljs-keyword">where</span> <span class="hljs-keyword">id</span>=<span class="hljs-number">1</span></code></pre><h3 id="8-4、查看-IO-详情"><a href="#8-4、查看-IO-详情" class="headerlink" title="8.4、查看 IO 详情"></a>8.4、查看 IO 详情</h3><p>不同于 <code>iostat</code>，<code>pt-diskstats</code> 提供了更加详细的 IO 详情统计，并且据有交互式处理，执行一下命令将会实时检测 IO 状态</p><pre><code class="hljs sh">pt-diskstats --show-timestamps</code></pre><p>其中几个关键值含义如下(更详细的请参考官方文档 <a href="https://www.percona.com/doc/percona-toolkit/LATEST/pt-diskstats.html#output" target="_blank" rel="noopener">https://www.percona.com/doc/percona-toolkit/LATEST/pt-diskstats.html#output</a>)</p><ul><li>rd_s: 每秒平均读取次数。这是发送到基础设备的 IO 请求数。通常，此数量少于应用程序发出的逻辑IO请求的数量。更多请求可能已排队到块设备，但是其中一些请求通常在发送到磁盘之前先进行合并。</li><li>rd_avkb: 读取的平均大小，以千字节为单位。</li><li>rd_mb_s: 每秒读取的平均兆字节数。</li><li>rd_mrg: 在发送到物理设备之前在队列调度程序中合并在一起的读取请求的百分比。</li><li>rd_rt: 读取操作的平均响应时间(以毫秒为单位)；这是端到端响应时间，包括在队列中花费的时间。这是发出 IO 请求的应用程序看到的响应时间，而不是块设备下的物理磁盘的响应时间。</li><li>busy: 设备至少有一个请求 wall-clock 时间的比例；等同于 <code>iostat</code> 的 <code>％util</code>。</li><li>in_prg: 正在进行的请求数。与读写并发是从可靠数字中生成的平均值不同，该数字是一个时样本，您可以看到它可能表示请求峰值，而不是真正的长期平均值。如果此数字很大，则从本质上讲意味着设备高负载运行。</li><li>ios_s: 物理设备的平均吞吐量，以每秒 IO 操作(IOPS)为单位。此列显示基础设备正在处理的总 IOPS；它是 rd_s 和 wr_s 的总和。</li><li>qtime: 平均排队时间；也就是说，请求在发送到物理设备之前在设备调度程序队列中花费的时间。</li><li>stime: 平均服务时间；也就是说，请求完成在队列中的等待之后，物理设备处理请求的时间。</li></ul><h3 id="8-5、重复索引优化"><a href="#8-5、重复索引优化" class="headerlink" title="8.5、重复索引优化"></a>8.5、重复索引优化</h3><p>pt-duplicate-key-checker 工具提供了对数据库重复索引和外键的自动查找功能，工具使用如下</p><pre><code class="hljs sh">pt-duplicate-key-checker 127.0.0.1 --user root --ask-passEnter password:<span class="hljs-comment"># A software update is available:</span><span class="hljs-comment"># ########################################################################</span><span class="hljs-comment"># aaaaaa.aaaaaa_audit</span><span class="hljs-comment"># ########################################################################</span><span class="hljs-comment"># index_linkId is a duplicate of unique_linkId</span><span class="hljs-comment"># Key definitions:</span><span class="hljs-comment">#   KEY `index_linkId` (`link_id`)</span><span class="hljs-comment">#   UNIQUE KEY `unique_linkId` (`link_id`),</span><span class="hljs-comment"># Column types:</span><span class="hljs-comment">#         `link_id` bigint(20) not null comment 'bdid'</span><span class="hljs-comment"># To remove this duplicate index, execute:</span>ALTER TABLE `aaaaaa.aaaaaa_audit` DROP INDEX `index_linkId`;<span class="hljs-comment"># ########################################################################</span><span class="hljs-comment"># Summary of indexes</span><span class="hljs-comment"># ########################################################################</span><span class="hljs-comment"># Size Duplicate Indexes   927420</span><span class="hljs-comment"># Total Duplicate Indexes  3</span><span class="hljs-comment"># Total Indexes            847</span></code></pre><h3 id="8-6、表统计"><a href="#8-6、表统计" class="headerlink" title="8.6、表统计"></a>8.6、表统计</h3><p>pt-find 是一个很方便的表查找统计工具，默认的一些选项可以实现批量查找符合条件的表，甚至执行一些 SQL 处理命令</p><pre><code class="hljs sh"><span class="hljs-comment"># 批量查找大于 5G 的表，并排序</span>pt-find --host 127.0.0.1 --user root --ask-pass --tablesize +5G | sort -rnEnter password: `rss_service`.`test_feed_news``db_log_history`.`test_mobile_click_201912``db_log_history`.`test_mobile_click_201911``db_log_history`.`test_mobile_click_201910``test_dix`.`test_user_messages``test_dix`.`test_user_link_history``test_dix`.`test_mobile_click``test_dix`.`test_message``test_dix`.`test_link_votes``test_dix`.`test_links_mobile_content``test_dix`.`test_links``test_dix`.`test_comment_votes``test_dix`.`test_comments`</code></pre><p>如果想要定制输出可以采用 <code>--printf</code> 选项</p><pre><code class="hljs sh">pt-find --host 127.0.0.1 --user root --ask-pass --tablesize +5G --<span class="hljs-built_in">printf</span> <span class="hljs-string">"%T\t%D.%N\n"</span> | sort -rnEnter password: 13918404608     `test_dix`.`test_links_mobile_content`13735231488     `test_dix`.`test_comment_votes`12633227264     `test_dix`.`test_user_messages`12610174976     `test_dix`.`test_user_link_history`10506305536     `test_dix`.`test_links`9686745088      `test_dix`.`test_message`9603907584      `rss_service`.`test_feed_news`9004122112      `db_log_history`.`test_mobile_click_201910`8919007232      `test_dix`.`test_comments`8045707264      `db_log_history`.`test_mobile_click_201912`7855915008      `db_log_history`.`test_mobile_click_201911`6099566592      `test_dix`.`test_mobile_click`5892898816      `test_dix`.`test_link_votes`</code></pre><p><strong>遗憾的是目前 <code>printf</code> 格式来源与 Perl 的 <code>sprintf</code> 函数，所以支持格式有限，不过简单的格式定制已经基本实现，复杂的建议通过 awk 处理</strong>；其他的可选参数具体参考官方文档 <a href="https://www.percona.com/doc/percona-toolkit/LATEST/pt-find.html" target="_blank" rel="noopener">https://www.percona.com/doc/percona-toolkit/LATEST/pt-find.html</a></p><h3 id="8-7、其他命令"><a href="#8-7、其他命令" class="headerlink" title="8.7、其他命令"></a>8.7、其他命令</h3><p>迫于篇幅，其他更多的高级命令请自行查阅官方文档 <a href="https://www.percona.com/doc/percona-toolkit/LATEST/index.html" target="_blank" rel="noopener">https://www.percona.com/doc/percona-toolkit/LATEST/index.html</a></p>]]></content>
    
    
    <categories>
      
      <category>Linux</category>
      
      <category>Database</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Linux</tag>
      
      <tag>MySQL</tag>
      
      <tag>Percona</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Writing Plugin for Coredns</title>
    <link href="/2019/11/05/writing-plugin-for-coredns/"/>
    <url>/2019/11/05/writing-plugin-for-coredns/</url>
    
    <content type="html"><![CDATA[<blockquote><p>目前测试环境中有很多个 DNS 服务器，不同项目组使用的 DNS 服务器不同，但是不可避免的他们会访问一些公共域名；老的 DNS 服务器都是 dnsmasq，改起来很麻烦，最近研究了一下 CoreDNS，通过编写插件的方式可以实现让多个 CoreDNS 实例实现分布式的统一控制，以下记录了插件编写过程</p></blockquote><h2 id="一、CoreDNS-简介"><a href="#一、CoreDNS-简介" class="headerlink" title="一、CoreDNS 简介"></a>一、CoreDNS 简介</h2><p>CoreDNS 目前是 CNCF 旗下的项目(已毕业)，为 Kubernetes 等云原生环境提供可靠的 DNS 服务发现等功能；官网的描述只有一句话: <strong>CoreDNS: DNS and Service Discovery</strong>，而实际上分析源码以后发现 CoreDNS 实际上是基于 Caddy (一个现代化的负载均衡器)而开发的，通过插件式注入，并监听 TCP/UDP 端口提供 DNS 服务；<strong>得益于 Caddy 的插件机制，CoreDNS 支持自行编写插件，拦截 DNS 请求然后处理，</strong>通过这个插件机制你可以在 CoreDNS 上实现各种功能，比如构建分布式一致性的 DNS 集群、动态的 DNS 负载均衡等等</p><h2 id="二、CoreDNS-插件规范"><a href="#二、CoreDNS-插件规范" class="headerlink" title="二、CoreDNS 插件规范"></a>二、CoreDNS 插件规范</h2><h3 id="2-1、插件模式"><a href="#2-1、插件模式" class="headerlink" title="2.1、插件模式"></a>2.1、插件模式</h3><p>CoreDNS 插件编写目前有两种方式:</p><ul><li>深度耦合 CoreDNS，使用 Go 编写插件，直接编译进 CoreDNS 二进制文件</li><li>通过 GRPC 解耦，任意语言编写 GRPC 接口实现，CoreDNS 通过 GRPC 与插件交互</li></ul><p>由于 GRPC 链接实际上借助于 CoreDNS 的 GRPC 插件，同时 GRPC 会有网络开销，TCP 链接不稳定可能造成 DNS 响应过慢等问题，所以本文只介绍如何使用 Go 编写 CoreDNS 的插件，这种插件将直接编译进 CoreDNS 二进制文件中</p><h3 id="2-2、插件注册"><a href="#2-2、插件注册" class="headerlink" title="2.2、插件注册"></a>2.2、插件注册</h3><p>在通常情况下，插件中应当包含一个 <code>setup.go</code> 文件，这个文件的 <code>init</code> 方法调用插件注册，类似这样</p><pre><code class="hljs go"><span class="hljs-function"><span class="hljs-keyword">func</span> <span class="hljs-title">init</span><span class="hljs-params">()</span></span> &#123;     plugin.Register(<span class="hljs-string">"gdns"</span>, setup) &#125;</code></pre><p>注册方法的第一个参数是插件名称，第二个是一个 func，func 签名如下</p><pre><code class="hljs go"><span class="hljs-comment">// SetupFunc is used to set up a plugin, or in other words,</span><span class="hljs-comment">// execute a directive. It will be called once per key for</span><span class="hljs-comment">// each server block it appears in.</span><span class="hljs-keyword">type</span> SetupFunc <span class="hljs-function"><span class="hljs-keyword">func</span><span class="hljs-params">(c *Controller)</span> <span class="hljs-title">error</span></span></code></pre><p><strong>在这个 SetupFunc 中，插件编写者应当通过 <code>*Controller</code> 拿到 CoreDNS 的配置并解析它，从而完成自己插件的初始化配置；</strong>比如你的插件需要连接 Etcd，那么在这个方法里你要通过 <code>*Controller</code> 遍历配置，拿到 Etcd 的地址、证书、用户名密码配置等信息；</p><p>如果配置信息没有问题，该插件应当初始化完成；如果有问题就报错退出，然后整个 CoreDNS 启动失败；如果插件初始化完成，最后不要忘记将自己的插件加入到整个插件链路中(CoreDNS 根据情况逐个调用)</p><pre><code class="hljs go"><span class="hljs-function"><span class="hljs-keyword">func</span> <span class="hljs-title">setup</span><span class="hljs-params">(c *caddy.Controller)</span> <span class="hljs-title">error</span></span> &#123;e, err := etcdParse(c)<span class="hljs-keyword">if</span> err != <span class="hljs-literal">nil</span> &#123;<span class="hljs-keyword">return</span> plugin.Error(<span class="hljs-string">"gdns"</span>, err)&#125;dnsserver.GetConfig(c).AddPlugin(<span class="hljs-function"><span class="hljs-keyword">func</span><span class="hljs-params">(next plugin.Handler)</span> <span class="hljs-title">plugin</span>.<span class="hljs-title">Handler</span></span> &#123;e.Next = next<span class="hljs-keyword">return</span> e&#125;)<span class="hljs-keyword">return</span> <span class="hljs-literal">nil</span>&#125;</code></pre><h3 id="2-3、插件结构体"><a href="#2-3、插件结构体" class="headerlink" title="2.3、插件结构体"></a>2.3、插件结构体</h3><p>一般来说，每一个插件都会定义一个结构体，<strong>结构体中包含必要的 CoreDNS 内置属性，以及当前插件特性的相关配置；</strong>一个样例的插件结构体如下所示</p><pre><code class="hljs go"><span class="hljs-keyword">type</span> GDNS <span class="hljs-keyword">struct</span> &#123;  <span class="hljs-comment">// Next 属性在 Setup 之后会被设置到下一个插件的引用，以便在本插件解析失败后可以交由下面的插件继续解析</span>Next       plugin.Handler<span class="hljs-comment">// Fall 列表用来控制哪些域名的请求解析失败后可以继续穿透到下一个插件重新处理</span>Fall       fall.F<span class="hljs-comment">// Zones 表示当前插件应该 case 哪些域名的 DNS 请求</span>Zones      []<span class="hljs-keyword">string</span><span class="hljs-comment">// PathPrefix 和 Client 就是插件本身的业务属性了，由于插件要连 Etcd</span><span class="hljs-comment">// PathPrefix 就是 Etcd 目录前缀，Client 是一个 Etcd 的 client</span><span class="hljs-comment">// endpoints 是 Etcd api 端点的地址</span>PathPrefix <span class="hljs-keyword">string</span>Client     *etcdcv3.Clientendpoints []<span class="hljs-keyword">string</span> <span class="hljs-comment">// Stored here as well, to aid in testing.</span>&#125;</code></pre><h3 id="2-4、插件接口"><a href="#2-4、插件接口" class="headerlink" title="2.4、插件接口"></a>2.4、插件接口</h3><p>一个 Go 编写的 CoreDNS 插件实际上只需要实现一个 <code>Handler</code> 接口既可，接口定义如下</p><pre><code class="hljs go"><span class="hljs-comment">// Handler is like dns.Handler except ServeDNS may return an rcode</span><span class="hljs-comment">// and/or error.</span><span class="hljs-comment">//</span><span class="hljs-comment">// If ServeDNS writes to the response body, it should return a status</span><span class="hljs-comment">// code. CoreDNS assumes *no* reply has yet been written if the status</span><span class="hljs-comment">// code is one of the following:</span><span class="hljs-comment">//</span><span class="hljs-comment">// * SERVFAIL (dns.RcodeServerFailure)</span><span class="hljs-comment">//</span><span class="hljs-comment">// * REFUSED (dns.RecodeRefused)</span><span class="hljs-comment">//</span><span class="hljs-comment">// * FORMERR (dns.RcodeFormatError)</span><span class="hljs-comment">//</span><span class="hljs-comment">// * NOTIMP (dns.RcodeNotImplemented)</span><span class="hljs-comment">//</span><span class="hljs-comment">// All other response codes signal other handlers above it that the</span><span class="hljs-comment">// response message is already written, and that they should not write</span><span class="hljs-comment">// to it also.</span><span class="hljs-comment">//</span><span class="hljs-comment">// If ServeDNS encounters an error, it should return the error value</span><span class="hljs-comment">// so it can be logged by designated error-handling plugin.</span><span class="hljs-comment">//</span><span class="hljs-comment">// If writing a response after calling another ServeDNS method, the</span><span class="hljs-comment">// returned rcode SHOULD be used when writing the response.</span><span class="hljs-comment">//</span><span class="hljs-comment">// If handling errors after calling another ServeDNS method, the</span><span class="hljs-comment">// returned error value SHOULD be logged or handled accordingly.</span><span class="hljs-comment">//</span><span class="hljs-comment">// Otherwise, return values should be propagated down the plugin</span><span class="hljs-comment">// chain by returning them unchanged.</span>Handler <span class="hljs-keyword">interface</span> &#123;ServeDNS(context.Context, dns.ResponseWriter, *dns.Msg) (<span class="hljs-keyword">int</span>, error)Name() <span class="hljs-keyword">string</span>&#125;</code></pre><ul><li><code>ServeDNS</code> 方法是插件需要实现的主要逻辑方法，DNS 请求接受后会从这个方法传入，插件编写者需要实现查询并返回结果</li><li><code>Name</code> 方法只返回一个插件名称标识，具体作用记不太清楚，好像是为了判断插件命名唯一性然后做链式顺序调用的，原则只要你不跟系统插件重名就行</li></ul><p><strong>基本逻辑就是在 setup 阶段通过配置文件创建你的插件结构体对象；然后插件结构体实现这个 <code>Handler</code> 接口，运行期 CoreDNS 会调用接口的 <code>ServeDNS</code> 方法来向插件查询 DNS 请求</strong></p><h3 id="2-5、ServeDNS-方法"><a href="#2-5、ServeDNS-方法" class="headerlink" title="2.5、ServeDNS 方法"></a>2.5、ServeDNS 方法</h3><p>ServeDNS 方法入参有 3 个:</p><ul><li><code>context.Context</code> 用来控制超时等情况的 context</li><li><code>dns.ResponseWriter</code> 插件通过这个对象写入对 Client DNS 请求的响应结果</li><li><code>*dns.Msg</code> 这个是 Client 发起的 DNS 请求，插件负责处理它，比如当你发现请求类型是 <code>AAAA</code> 而你的插件又不想去支持时要如何返回结果</li></ul><p>对于返回结果，插件编写者应当通过 <code>dns.ResponseWriter.WriteMsg</code> 方法写入返回结果，基本代码如下</p><pre><code class="hljs go"><span class="hljs-comment">// ServeDNS implements the plugin.Handler interface.</span><span class="hljs-function"><span class="hljs-keyword">func</span> <span class="hljs-params">(gDNS *GDNS)</span> <span class="hljs-title">ServeDNS</span><span class="hljs-params">(ctx context.Context, w dns.ResponseWriter, r *dns.Msg)</span> <span class="hljs-params">(<span class="hljs-keyword">int</span>, error)</span></span> &#123;<span class="hljs-comment">// ...... 这里应当实现你的业务逻辑，查找相应的 DNS 记录</span><span class="hljs-comment">// 最后通过 new 一个 dns.Msg 作为返回结果</span>resp := <span class="hljs-built_in">new</span>(dns.Msg)resp.SetReply(r)resp.Authoritative = <span class="hljs-literal">true</span><span class="hljs-comment">// records 是真正的记录结果，应当在业务逻辑区准备好</span>resp.Answer = <span class="hljs-built_in">append</span>(resp.Answer, records...)<span class="hljs-comment">// 返回结果</span>err = w.WriteMsg(resp)<span class="hljs-keyword">if</span> err != <span class="hljs-literal">nil</span> &#123;log.Error(err)&#125;   <span class="hljs-comment">// 告诉 CoreDNS 是否处理成功</span><span class="hljs-keyword">return</span> dns.RcodeSuccess, <span class="hljs-literal">nil</span>&#125;</code></pre><p><strong>需要注意的是，无论根据业务逻辑是否查询到 DNS 记录，都要返回响应结果(没有就返回空)，错误或者未返回将会导致 Client 端查询 DNS 超时，然后不断重试，最终可能导致 Client 端服务故障</strong></p><h3 id="2-6、Name-方法"><a href="#2-6、Name-方法" class="headerlink" title="2.6、Name 方法"></a>2.6、Name 方法</h3><p><code>Name</code> 方法非常简单，只需要返回当前插件名称既可；该方法的作用是为了其他插件判断本插件是否加载等情况</p><pre><code class="hljs go"><span class="hljs-comment">// Name implements the Handler interface.</span><span class="hljs-function"><span class="hljs-keyword">func</span> <span class="hljs-params">(gDNS *GDNS)</span> <span class="hljs-title">Name</span><span class="hljs-params">()</span> <span class="hljs-title">string</span></span> &#123; <span class="hljs-keyword">return</span> <span class="hljs-string">"gdns"</span> &#125;</code></pre><h2 id="三、CoreDNS-插件处理"><a href="#三、CoreDNS-插件处理" class="headerlink" title="三、CoreDNS 插件处理"></a>三、CoreDNS 插件处理</h2><p>对于实际的业务处理，可以通过 <code>case</code> 请求 <code>QType</code> 来做具体的业务实现</p><pre><code class="hljs go"><span class="hljs-comment">// ServeDNS implements the plugin.Handler interface.</span><span class="hljs-function"><span class="hljs-keyword">func</span> <span class="hljs-params">(gDNS *GDNS)</span> <span class="hljs-title">ServeDNS</span><span class="hljs-params">(ctx context.Context, w dns.ResponseWriter, r *dns.Msg)</span> <span class="hljs-params">(<span class="hljs-keyword">int</span>, error)</span></span> &#123;state := request.Request&#123;W: w, Req: r&#125;zone := plugin.Zones(gDNS.Zones).Matches(state.Name())<span class="hljs-keyword">if</span> zone == <span class="hljs-string">""</span> &#123;<span class="hljs-keyword">return</span> plugin.NextOrFailure(gDNS.Name(), gDNS.Next, ctx, w, r)&#125;<span class="hljs-comment">// ...业务处理</span><span class="hljs-keyword">switch</span> state.QType() &#123;<span class="hljs-keyword">case</span> dns.TypeA:<span class="hljs-comment">// A 记录查询业务逻辑</span><span class="hljs-keyword">case</span> dns.TypeAAAA:<span class="hljs-comment">// AAAA 记录查询业务逻辑</span><span class="hljs-keyword">default</span>:<span class="hljs-keyword">return</span> <span class="hljs-literal">false</span>resp := <span class="hljs-built_in">new</span>(dns.Msg)resp.SetReply(r)resp.Authoritative = <span class="hljs-literal">true</span>resp.Answer = <span class="hljs-built_in">append</span>(resp.Answer, records...)err = w.WriteMsg(resp)<span class="hljs-keyword">if</span> err != <span class="hljs-literal">nil</span> &#123;log.Error(err)&#125;<span class="hljs-keyword">return</span> dns.RcodeSuccess, <span class="hljs-literal">nil</span>&#125;</code></pre><h2 id="四、插件编译及测试"><a href="#四、插件编译及测试" class="headerlink" title="四、插件编译及测试"></a>四、插件编译及测试</h2><h3 id="4-1、官方标准操作"><a href="#4-1、官方标准操作" class="headerlink" title="4.1、官方标准操作"></a>4.1、官方标准操作</h3><p>根据官方文档的描述，当你编写好插件以后，<strong>你的插件应当提交到一个 Git 仓库中，可以使 Github 等(保证可以 <code>go get</code> 拉取就行)，然后修改 <code>plugin.cfg</code>，最后执行 <code>make</code> 既可</strong>；具体修改如下所示</p><p><img src="https://cdn.oss.link/markdown/vey4u.png" srcset="/img/loading.gif" alt="plugin.cfg"></p><p><strong>值得注意的是: 插件配置在 <code>plugin.cfg</code> 内的顺序决定了插件的执行顺序；通俗的讲，如果 Client 的一个 DNS 请求进来，CoreDNS 根据你在 <code>plugin.cfg</code> 内书写的顺序依次调用，而并非 <code>Corefile</code> 内的配置顺序</strong></p><p>配置好以后直接执行 <code>make</code> 既可编译成功一个包含自定义插件的 CoreDNS 二进制文件(编译过程的 <code>go mod</code> 下载加速问题不在本文讨论范围内)；你可以直接通过这个二进制测试插件的处理情况，当然这种测试不够直观，而且频繁修改由于 <code>go mod</code> 缓存等原因并不一定能保证每次编译的都包含最新插件代码，所以另一种方式请看下一章节</p><h3 id="4-2、经验性的操作"><a href="#4-2、经验性的操作" class="headerlink" title="4.2、经验性的操作"></a>4.2、经验性的操作</h3><p>根据个人测试以及对源码的分析，在修改 <code>plugin.cfg</code> 然后执行 <code>make</code> 命令后，实际上是进行了代码生成；当你通过 git 命令查看相关修改文件时，整个插件加载体系便没什么秘密可言了；<strong>在整个插件体系中，插件加载是通过 <code>init</code> 方法注册的，那么既然用 go 写插件，那么应该清楚 <code>init</code> 方法只有在包引用之后才会执行，所以整个插件体系实际上是这样事儿的:</strong></p><p>首先 <code>make</code> 以后会修改 <code>core/plugin/zplugin.go</code> 文件，这个文件啥也不干，就是 <code>import</code> 来实现调用对应包的 <code>init</code> 方法</p><p><img src="https://cdn.oss.link/markdown/ny1rz.png" srcset="/img/loading.gif" alt="zplugin.go"></p><p>当 <code>init</code> 执行后你去追源码，实际上就是 Caddy 维护了一个 <code>map[string]Plugin</code>，<code>init</code> 会把你的插件 func 塞进去然后后面再调用，实现一个懒加载或者说延迟初始化</p><p><img src="https://cdn.oss.link/markdown/idno4.png" srcset="/img/loading.gif" alt="caddy_plugin"></p><p>接着修改了一下 <code>core/dnsserver/zdirectives.go</code>，这个里面也没啥，就是一个 <code>[]string</code>，<strong>但是 <code>[]string</code> 这玩意有顺序啊，这就是为什么你在 <code>plugin.cfg</code> 里写的顺序决定了插件处理顺序的原因(因为生成的这个切片有顺序)</strong></p><p><img src="https://cdn.oss.link/markdown/bixos.png" srcset="/img/loading.gif" alt="zdirectives.go"></p><p>综上所述，实际上 <code>make</code> 命令一共修改了两个文件，如果想在 IDE 内直接 debug CoreDNS + Plugin 源码，那么只需要这样做:</p><p>复制自己编写的插件目录到 <code>plugin</code> 目录，类似这样</p><p><img src="https://cdn.oss.link/markdown/whwuy.png" srcset="/img/loading.gif" alt="gdns"></p><p>手动修改 <code>core/plugin/zplugin.go</code>，加入自己插件的 <code>import</code>(此时你直接复制系统其他插件，改一下目录名既可)</p><p><img src="https://cdn.oss.link/markdown/g7wp0.png" srcset="/img/loading.gif" alt="update_zplugin"></p><p>手动修改 <code>core/dnsserver/zdirectives.go</code> 把自己插件名称写进去(自己控制顺序)，然后 debug 启动 <code>coredns.go</code> 里面的 main 方法测试既可</p><p><img src="https://cdn.oss.link/markdown/4ucqg.png" srcset="/img/loading.gif" alt="coredns.go"></p><h2 id="五、本文参考"><a href="#五、本文参考" class="headerlink" title="五、本文参考"></a>五、本文参考</h2><ul><li>Writing Plugins for CoreDNS: <a href="https://coredns.io/2016/12/19/writing-plugins-for-coredns" target="_blank" rel="noopener">https://coredns.io/2016/12/19/writing-plugins-for-coredns</a></li><li>how-to-add-plugins.md: <a href="https://github.com/coredns/coredns.io/blob/master/content/blog/how-to-add-plugins.md" target="_blank" rel="noopener">https://github.com/coredns/coredns.io/blob/master/content/blog/how-to-add-plugins.md</a></li><li>example plugin: <a href="https://github.com/coredns/example" target="_blank" rel="noopener">https://github.com/coredns/example</a></li></ul>]]></content>
    
    
    <categories>
      
      <category>Golang</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Kubernetes</tag>
      
      <tag>Golang</tag>
      
      <tag>CoreDNS</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Golang Etcd Client Example</title>
    <link href="/2019/10/15/golang-etcd-client-example/"/>
    <url>/2019/10/15/golang-etcd-client-example/</url>
    
    <content type="html"><![CDATA[<blockquote><p>准备开发点东西，需要用到 Etcd，由于生产 Etcd 全部开启了 TLS 加密，所以客户端需要相应修改，以下为 Golang 链接 Etcd 并且使用客户端证书验证的样例代码</p></blockquote><h2 id="API-V2"><a href="#API-V2" class="headerlink" title="API V2"></a>API V2</h2><pre><code class="hljs go"><span class="hljs-keyword">package</span> main<span class="hljs-keyword">import</span> (<span class="hljs-string">"context"</span><span class="hljs-string">"crypto/tls"</span><span class="hljs-string">"crypto/x509"</span><span class="hljs-string">"io/ioutil"</span><span class="hljs-string">"log"</span><span class="hljs-string">"net"</span><span class="hljs-string">"net/http"</span><span class="hljs-string">"time"</span><span class="hljs-string">"go.etcd.io/etcd/client"</span>)<span class="hljs-function"><span class="hljs-keyword">func</span> <span class="hljs-title">main</span><span class="hljs-params">()</span></span> &#123;<span class="hljs-comment">// 为了保证 HTTPS 链接可信，需要预先加载目标证书签发机构的 CA 根证书</span>etcdCA, err := ioutil.ReadFile(<span class="hljs-string">"/Users/mritd/tmp/etcd_ssl/etcd-root-ca.pem"</span>)<span class="hljs-keyword">if</span> err != <span class="hljs-literal">nil</span> &#123;log.Fatal(err)&#125;<span class="hljs-comment">// etcd 启用了双向 TLS 认证，所以客户端证书同样需要加载</span>etcdClientCert, err := tls.LoadX509KeyPair(<span class="hljs-string">"/Users/mritd/tmp/etcd_ssl/etcd.pem"</span>, <span class="hljs-string">"/Users/mritd/tmp/etcd_ssl/etcd-key.pem"</span>)<span class="hljs-keyword">if</span> err != <span class="hljs-literal">nil</span> &#123;log.Fatal(err)&#125;<span class="hljs-comment">// 创建一个空的 CA Pool</span><span class="hljs-comment">// 因为后续只会链接 Etcd 的 api 端点，所以此处选择使用空的 CA Pool，然后只加入 Etcd CA 既可</span><span class="hljs-comment">// 如果期望链接其他 TLS 端点，那么最好使用 x509.SystemCertPool() 方法先 copy 一份系统根 CA</span><span class="hljs-comment">// 然后再向这个 Pool 中添加自定义 CA</span>rootCertPool := x509.NewCertPool()rootCertPool.AppendCertsFromPEM(etcdCA)cfg := client.Config&#123;<span class="hljs-comment">// Etcd HTTPS api 端点</span>Endpoints: []<span class="hljs-keyword">string</span>&#123;<span class="hljs-string">"https://172.16.14.114:2379"</span>&#125;,<span class="hljs-comment">// 自定义 Transport 实现自签 CA 加载以及 Client Cert 加载</span><span class="hljs-comment">// 其他参数最好从 client.DefaultTranspor copy，以保证与默认 client 相同的行为</span>Transport: &amp;http.Transport&#123;Proxy: http.ProxyFromEnvironment,<span class="hljs-comment">// Dial 方法已被启用，采用新的 DialContext 设置超时</span>DialContext: (&amp;net.Dialer&#123;KeepAlive: <span class="hljs-number">30</span> * time.Second,Timeout:   <span class="hljs-number">30</span> * time.Second,&#125;).DialContext,<span class="hljs-comment">// 自定义 CA 及 Client Cert 配置</span>TLSClientConfig: &amp;tls.Config&#123;RootCAs:      rootCertPool,Certificates: []tls.Certificate&#123;etcdClientCert&#125;,&#125;,TLSHandshakeTimeout: <span class="hljs-number">10</span> * time.Second,&#125;,<span class="hljs-comment">// set timeout per request to fail fast when the target endpoint is unavailable</span>HeaderTimeoutPerRequest: time.Second,&#125;c, err := client.New(cfg)<span class="hljs-keyword">if</span> err != <span class="hljs-literal">nil</span> &#123;log.Fatal(err)&#125;kapi := client.NewKeysAPI(c)<span class="hljs-comment">// set "/foo" key with "bar" value</span>log.Print(<span class="hljs-string">"Setting '/foo' key with 'bar' value"</span>)resp, err := kapi.Set(context.Background(), <span class="hljs-string">"/foo"</span>, <span class="hljs-string">"bar"</span>, <span class="hljs-literal">nil</span>)<span class="hljs-keyword">if</span> err != <span class="hljs-literal">nil</span> &#123;log.Fatal(err)&#125; <span class="hljs-keyword">else</span> &#123;<span class="hljs-comment">// print common key info</span>log.Printf(<span class="hljs-string">"Set is done. Metadata is %q\n"</span>, resp)&#125;<span class="hljs-comment">// get "/foo" key's value</span>log.Print(<span class="hljs-string">"Getting '/foo' key value"</span>)resp, err = kapi.Get(context.Background(), <span class="hljs-string">"/foo"</span>, <span class="hljs-literal">nil</span>)<span class="hljs-keyword">if</span> err != <span class="hljs-literal">nil</span> &#123;log.Fatal(err)&#125; <span class="hljs-keyword">else</span> &#123;<span class="hljs-comment">// print common key info</span>log.Printf(<span class="hljs-string">"Get is done. Metadata is %q\n"</span>, resp)<span class="hljs-comment">// print value</span>log.Printf(<span class="hljs-string">"%q key has %q value\n"</span>, resp.Node.Key, resp.Node.Value)&#125;&#125;</code></pre><h2 id="API-V3"><a href="#API-V3" class="headerlink" title="API V3"></a>API V3</h2><pre><code class="hljs go"><span class="hljs-keyword">package</span> main<span class="hljs-keyword">import</span> (<span class="hljs-string">"context"</span><span class="hljs-string">"crypto/tls"</span><span class="hljs-string">"crypto/x509"</span><span class="hljs-string">"io/ioutil"</span><span class="hljs-string">"log"</span><span class="hljs-string">"time"</span><span class="hljs-string">"go.etcd.io/etcd/clientv3"</span>)<span class="hljs-function"><span class="hljs-keyword">func</span> <span class="hljs-title">main</span><span class="hljs-params">()</span></span> &#123;<span class="hljs-comment">// 为了保证 HTTPS 链接可信，需要预先加载目标证书签发机构的 CA 根证书</span>etcdCA, err := ioutil.ReadFile(<span class="hljs-string">"/Users/mritd/tmp/etcd_ssl/etcd-root-ca.pem"</span>)<span class="hljs-keyword">if</span> err != <span class="hljs-literal">nil</span> &#123;log.Fatal(err)&#125;<span class="hljs-comment">// etcd 启用了双向 TLS 认证，所以客户端证书同样需要加载</span>etcdClientCert, err := tls.LoadX509KeyPair(<span class="hljs-string">"/Users/mritd/tmp/etcd_ssl/etcd.pem"</span>, <span class="hljs-string">"/Users/mritd/tmp/etcd_ssl/etcd-key.pem"</span>)<span class="hljs-keyword">if</span> err != <span class="hljs-literal">nil</span> &#123;log.Fatal(err)&#125;<span class="hljs-comment">// 创建一个空的 CA Pool</span><span class="hljs-comment">// 因为后续只会链接 Etcd 的 api 端点，所以此处选择使用空的 CA Pool，然后只加入 Etcd CA 既可</span><span class="hljs-comment">// 如果期望链接其他 TLS 端点，那么最好使用 x509.SystemCertPool() 方法先 copy 一份系统根 CA</span><span class="hljs-comment">// 然后再向这个 Pool 中添加自定义 CA</span>rootCertPool := x509.NewCertPool()rootCertPool.AppendCertsFromPEM(etcdCA)<span class="hljs-comment">// 创建 api v3 的 client</span>cli, err := clientv3.New(clientv3.Config&#123;<span class="hljs-comment">// etcd https api 端点</span>Endpoints:   []<span class="hljs-keyword">string</span>&#123;<span class="hljs-string">"https://172.16.14.114:2379"</span>&#125;,DialTimeout: <span class="hljs-number">5</span> * time.Second,<span class="hljs-comment">// 自定义 CA 及 Client Cert 配置</span>TLS: &amp;tls.Config&#123;RootCAs:      rootCertPool,Certificates: []tls.Certificate&#123;etcdClientCert&#125;,&#125;,&#125;)<span class="hljs-keyword">if</span> err != <span class="hljs-literal">nil</span> &#123;log.Fatal(err)&#125;<span class="hljs-keyword">defer</span> <span class="hljs-function"><span class="hljs-keyword">func</span><span class="hljs-params">()</span></span> &#123; _ = cli.Close() &#125;()ctx, cancel := context.WithTimeout(context.Background(), <span class="hljs-number">3</span>*time.Second)putResp, err := cli.Put(ctx, <span class="hljs-string">"sample_key"</span>, <span class="hljs-string">"sample_value"</span>)<span class="hljs-keyword">if</span> err != <span class="hljs-literal">nil</span> &#123;log.Fatal(err)&#125; <span class="hljs-keyword">else</span> &#123;log.Println(putResp)&#125;cancel()ctx, cancel = context.WithTimeout(context.Background(), <span class="hljs-number">3</span>*time.Second)delResp, err := cli.Delete(ctx, <span class="hljs-string">"sample_key"</span>)<span class="hljs-keyword">if</span> err != <span class="hljs-literal">nil</span> &#123;log.Fatal(err)&#125; <span class="hljs-keyword">else</span> &#123;log.Println(delResp)&#125;cancel()&#125;</code></pre>]]></content>
    
    
    <categories>
      
      <category>Golang</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Golang</tag>
      
      <tag>etcd</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Podman 初试 - 容器发展史</title>
    <link href="/2019/06/26/podman-history-of-container/"/>
    <url>/2019/06/26/podman-history-of-container/</url>
    
    <content type="html"><![CDATA[<blockquote><p>这是一篇纯介绍性文章，本文不包含任何技术层面的操作，本文仅作为后续 Podman 文章铺垫；本文细节部份并未阐述，很多地方并不详实(一家只谈，不可轻信)。</p></blockquote><h2 id="一、缘起"><a href="#一、缘起" class="headerlink" title="一、缘起"></a>一、缘起</h2><h3 id="1-1、鸿蒙"><a href="#1-1、鸿蒙" class="headerlink" title="1.1、鸿蒙"></a>1.1、鸿蒙</h3><p>在上古时期，天地初开，一群称之为 “运维” 的人们每天在一种叫作 “服务器” 的神秘盒子中创造属于他们的世界；他们在这个世界中每日劳作，一遍又一遍的写入他们的历史，比如搭建一个 nginx、布署一个 java web 应用…</p><p>大多数人其实并没有那么聪明，他们所 “创造” 的事实上可能是有人已经创造过的东西，他们可能每天都在做着重复的劳动；久而久之，一些人厌倦了、疲惫了…又过了一段时间，一些功力深厚的老前辈创造了一些批量布署工具来帮助人们做一些重复性的劳动，这些工具被起名为 “Asible”、”Chef”、”Puppet” 等等…</p><p>而随着时代的发展，”世界” 变得越来越复杂，运维们需要处理的事情越来越多，比如各种网络、磁盘环境的隔离，各种应用服务的高可用…在时代的洪流下，运维们急需要一种简单高效的布署工具，既能有一定的隔离性，又能方便使用，并且最大程度降低重复劳动来提升效率。</p><h3 id="1-2、创世"><a href="#1-2、创世" class="headerlink" title="1.2、创世"></a>1.2、创世</h3><p>在时代洪流的冲击下，一位名为 “Solomon Hykes” 的人异军突起，他创造了一个称之为 Docker 的工具，Docker 被创造以后就以灭世之威向运维们展示了它的强大；一个战斗力只有 5 的运维只需要学习 Docker 很短时间就可以完成资深运维们才能完成的事情，在某些情况下以前需要 1 天才能完成的工作使用 Docker 后几分钟就可以完成；此时运维们已经意识到 “新的时代” 开启了，接下来 Docker 开源并被整个运维界人们使用，Docker 也不断地完善增加各种各样的功能，此后世界正式进入 “容器纪元”。</p><h2 id="二、纷争"><a href="#二、纷争" class="headerlink" title="二、纷争"></a>二、纷争</h2><h3 id="2-1、发展"><a href="#2-1、发展" class="headerlink" title="2.1、发展"></a>2.1、发展</h3><p>随着 Docker 的日益成熟，一些人开始在 Docker 之上创造更加强大的工具，一些人开始在 Docker 之下为其提供更稳定的运行环境…</p><p>其中一个叫作 Google 的公司在 Docker 之上创建了名为 “Kubernetes” 的工具，Kubernetes 操纵 Docker 完成更加复杂的任务；Kubernetes 的出现更加印证了 Docker 的强大，以及 “容器纪元” 的发展正确性。</p><h3 id="2-2、野心"><a href="#2-2、野心" class="headerlink" title="2.2、野心"></a>2.2、野心</h3><p>当然这是一个充满利益的世界，Google 公司创造 Kubernetes 是可以为他们带来利益的，比如他们可以让 Kubernetes 深度适配他们的云平台，以此来增加云平台的销量等；此时 Docker 创始人也成立了一个公司，提供 Docker 的付费服务以及深度定制等；不过值得一提的是 Docker 公司提供的付费服务始终没有 Kubernetes 为 Google 公司带来的利益高，所以在利益的驱使下，Docker 公司开始动起了歪心思: <strong>创造一个 Kubernetes 的替代品，利用用户粘度复制 Kubernetes 的成功，从 Google 嘴里抢下这块蛋糕！</strong>此时 Docker 公司只想把蛋糕抢过来，但是他们根本没有在意到暗中一群人创造了一个叫 “rkt” 的东西也在妄图夺走他们嘴里的蛋糕。</p><h3 id="2-3、冲突"><a href="#2-3、冲突" class="headerlink" title="2.3、冲突"></a>2.3、冲突</h3><p>在一段时间的沉默后，Docker 公司又创造了 “Swarm” 这个工具，妄图夺走 Google 公司利用 Kubernetes 赢来的蛋糕；当然，Google 这个公司极其庞大，人数众多，而且在这个社会有很大的影响地位…</p><p>终于，巨人苏醒了，Google 联合了 Redhat、Microsoft、IBM、Intel、Cisco 等公司决定对这个爱动歪脑筋的 Docker 公司进行制裁；当然制裁的手段不能过于暴力，那样会让别人落下把柄，成为别人的笑料，被人所不耻；<strong>最总他们决定制订规范，成立组织，明确规定 Docker 的角色，以及它应当拥有的能力，这些规范包括但不限于 <code>CRI</code>、<code>CNI</code> 等；自此之后各大公司宣布他们容器相关的工具只兼容 CRI 等相关标准，无论是 Docker 还是 rkt 等工具，只要实现了这些标准，就可以配合这些容器工具进行使用</strong>。</p><h2 id="三、成败"><a href="#三、成败" class="headerlink" title="三、成败"></a>三、成败</h2><p>自此之后，Docker 跌下神坛，各路大神纷纷创造满足 CRI 等规范的工具用来取代 Docker，Docker 丢失了往日一家独大的场面，最终为了顺应时代发展，拆分自己成为模块化组件；这些模块化组件被放置在 <a href="https://mobyproject.org/" target="_blank" rel="noopener">mobyproject</a> 中方便其他人重复利用。</p><p>时至今日，虽然 Docker 已经不负以前，但是仍然是容器化首选工具，因为 Docker 是一个完整的产品，它可以提供除了满足 CRI 等标准以外更加方便的功能；但是制裁并非没有结果，Google 公司借此创造了 cri-o 用来满足 CRI 标准，其他公司也相应创建了对应的 CRI 实现；<strong>为了进一步分化 Docker 势力，一个叫作 Podman 的工具被创建，它以 cri-o 为基础，兼容大部份 Docker 命令的方式开始抢夺 Dcoker 用户</strong>；到目前为止 Podman 已经可以在大部份功能上替代 Docker。</p>]]></content>
    
    
    <categories>
      
      <category>Docker</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Docker</tag>
      
      <tag>Podman</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Calico 3.6 转发外部流量到集群 Pod</title>
    <link href="/2019/06/18/calico-3.6-forward-network-traffic/"/>
    <url>/2019/06/18/calico-3.6-forward-network-traffic/</url>
    
    <content type="html"><![CDATA[<blockquote><p>由于开发有部份服务使用 GRPC 进行通讯，同时采用 Consul 进行服务发现；在微服务架构下可能会导致一些访问问题，目前解决方案就是打通开发环境网络与测试环境 Kubernetes 内部 Pod 网络；翻了好多资料发现都是 2.x 的，而目前测试集群 Calico 版本为 3.6.3，很多文档都不适用只能自己折腾，目前折腾完了这里记录一下</p></blockquote><p><strong>本文默认为读者已经存在一个运行正常的 Kubernetes 集群，并且采用 Calico 作为 CNI 组件，且 Calico 工作正常；同时应当在某个节点完成了 calicoctl 命令行工具的配置</strong></p><h2 id="一、问题描述"><a href="#一、问题描述" class="headerlink" title="一、问题描述"></a>一、问题描述</h2><p>在微服务架构下，由于服务组件很多，开发在本地机器想测试应用需要启动整套服务，这对开发机器的性能确实是个考验；但如果直接连接测试环境的服务，由于服务发现问题最终得到的具体服务 IP 是 Kubernetes Pod IP，此 IP 由集群内部 Calico 维护与分配，外部不可访问；最终目标为打通开发环境与集群内部网络，实现开发网络下直连 Pod IP，这或许在以后对生产服务暴露负载均衡有一定帮助意义；目前网络环境如下:</p><p>开发网段: <code>10.10.0.0/24</code><br>测试网段: <code>172.16.0.0/24</code><br>Kubernetes Pod 网段: <code>10.20.0.0/16</code></p><h2 id="二、打通网络"><a href="#二、打通网络" class="headerlink" title="二、打通网络"></a>二、打通网络</h2><p>首先面临的第一个问题是 Calico 处理，因为<strong>如果想要让数据包能从开发网络到达 Pod 网络，那么必然需要测试环境宿主机上的 Calico Node 帮忙转发</strong>；因为 Pod 网络由 Calico 维护，只要 Calico Node 帮忙转发那么数据一定可以到达 Pod IP 上；</p><p>一开始我很天真的认为这就是个 <code>ip route add 10.20.0.0/16 via 172.16.0.13</code> 的问题… 后来发现</p><p><img src="https://cdn.oss.link/markdown/hwp9s.jpg" srcset="/img/loading.gif" alt="没那么简单"></p><p>经过翻文档、issue、blog 等最终发现需要进行以下步骤</p><h3 id="2-1、关闭全互联模式"><a href="#2-1、关闭全互联模式" class="headerlink" title="2.1、关闭全互联模式"></a>2.1、关闭全互联模式</h3><p><strong>注意: 关闭全互联时可能导致网络暂时中断，请在夜深人静时操作</strong></p><p>首先执行以下命令查看是否存在默认的 BGP 配置</p><pre><code class="hljs sh">calicoctl get bgpconfig default</code></pre><p>如果存在则将其保存为配置文件</p><pre><code class="hljs sh">calicoctl get bgpconfig default -o yaml &gt; bgp.yaml</code></pre><p>修改其中的 <code>spec.nodeToNodeMeshEnabled</code> 为 <code>false</code>，然后进行替换</p><pre><code class="hljs sh">calicoctl apply -f bgp.yaml</code></pre><p>如果不存在则手动创建一个配置，然后应用</p><pre><code class="hljs sh"> cat &lt;&lt; EOF | calicoctl create -f - apiVersion: projectcalico.org/v3 kind: BGPConfiguration metadata:   name: default spec:   logSeverityScreen: Info   nodeToNodeMeshEnabled: <span class="hljs-literal">false</span>   asNumber: 63400EOF</code></pre><p>本部分参考: </p><ul><li><a href="https://docs.projectcalico.org/v3.6/networking/bgp" target="_blank" rel="noopener">Disabling the full node-to-node BGP mesh</a></li></ul><h3 id="2-2、开启集群内-RR-模式"><a href="#2-2、开启集群内-RR-模式" class="headerlink" title="2.2、开启集群内 RR 模式"></a>2.2、开启集群内 RR 模式</h3><p>在 Calico 3.3 后支持了集群内节点的 RR 模式，即将某个集群内的 Calico Node 转变为 RR 节点；将某个节点设置为 RR 节点只需要增加 <code>routeReflectorClusterID</code> 既可，为了后面方便配置同时增加了一个 lable 字段 <code>route-reflector: &quot;true&quot;</code></p><pre><code class="hljs sh">calicoctl get node CALICO_NODE_NAME -o yaml &gt; node.yaml</code></pre><p>然后增加 <code>routeReflectorClusterID</code> 字段，样例如下</p><pre><code class="hljs yaml"><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">projectcalico.org/v3</span><span class="hljs-attr">kind:</span> <span class="hljs-string">Node</span><span class="hljs-attr">metadata:</span>  <span class="hljs-attr">annotations:</span>    <span class="hljs-attr">projectcalico.org/kube-labels:</span> <span class="hljs-string">'&#123;"beta.kubernetes.io/arch":"amd64","beta.kubernetes.io/os":"linux","kubernetes.io/hostname":"d13.node","node-role.kubernetes.io/k8s-master":"true"&#125;'</span>  <span class="hljs-attr">creationTimestamp:</span> <span class="hljs-number">2019</span><span class="hljs-number">-06</span><span class="hljs-string">-17T13:55:44Z</span>  <span class="hljs-attr">labels:</span>    <span class="hljs-attr">beta.kubernetes.io/arch:</span> <span class="hljs-string">amd64</span>    <span class="hljs-attr">beta.kubernetes.io/os:</span> <span class="hljs-string">linux</span>    <span class="hljs-attr">kubernetes.io/hostname:</span> <span class="hljs-string">d13.node</span>    <span class="hljs-attr">node-role.kubernetes.io/k8s-master:</span> <span class="hljs-string">"true"</span>    <span class="hljs-attr">route-reflector:</span> <span class="hljs-string">"true"</span>  <span class="hljs-comment"># 增加 lable</span>  <span class="hljs-attr">name:</span> <span class="hljs-string">d13.node</span>  <span class="hljs-attr">resourceVersion:</span> <span class="hljs-string">"61822269"</span>  <span class="hljs-attr">uid:</span> <span class="hljs-string">9a1897e0-9107-11e9-bc1c-90b11c53d1e3</span><span class="hljs-attr">spec:</span>  <span class="hljs-attr">bgp:</span>    <span class="hljs-attr">ipv4Address:</span> <span class="hljs-number">172.16</span><span class="hljs-number">.0</span><span class="hljs-number">.13</span><span class="hljs-string">/19</span>    <span class="hljs-attr">ipv4IPIPTunnelAddr:</span> <span class="hljs-number">10.20</span><span class="hljs-number">.73</span><span class="hljs-number">.82</span>    <span class="hljs-attr">routeReflectorClusterID:</span> <span class="hljs-number">172.16</span><span class="hljs-number">.20</span><span class="hljs-number">.1</span> <span class="hljs-comment"># 添加集群 ID</span>  <span class="hljs-attr">orchRefs:</span>  <span class="hljs-bullet">-</span> <span class="hljs-attr">nodeName:</span> <span class="hljs-string">d13.node</span>    <span class="hljs-attr">orchestrator:</span> <span class="hljs-string">k8s</span></code></pre><p><strong>事实上我们应当导出多个 Calico Node 的配置，并将其配置为 RR 节点以进行冗余；对于 <code>routeReflectorClusterID</code> 目前测试只是作为一个 ID(至少在本文是这样的)，所以理论上可以是任何 IP，个人猜测最好在同一集群网络下采用相同的 IP，由于这是真正的测试环境我没有对 ID 做过多的测试(怕玩挂)</strong></p><p>修改完成后只需要应用一下就行</p><pre><code class="hljs sh">calicoctl apply -f node.yaml</code></pre><p>接下来需要创建对等规则，规则文件如下</p><pre><code class="hljs yaml"><span class="hljs-attr">kind:</span> <span class="hljs-string">BGPPeer</span><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">projectcalico.org/v3</span><span class="hljs-attr">metadata:</span>  <span class="hljs-attr">name:</span> <span class="hljs-string">peer-to-rrs</span><span class="hljs-attr">spec:</span>  <span class="hljs-attr">nodeSelector:</span> <span class="hljs-string">"!has(route-reflector)"</span>  <span class="hljs-attr">peerSelector:</span> <span class="hljs-string">has(route-reflector)</span><span class="hljs-meta">---</span><span class="hljs-attr">kind:</span> <span class="hljs-string">BGPPeer</span><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">projectcalico.org/v3</span><span class="hljs-attr">metadata:</span>  <span class="hljs-attr">name:</span> <span class="hljs-string">rr-mesh</span><span class="hljs-attr">spec:</span>  <span class="hljs-attr">nodeSelector:</span> <span class="hljs-string">has(route-reflector)</span>  <span class="hljs-attr">peerSelector:</span> <span class="hljs-string">has(route-reflector)</span></code></pre><p>假定规则文件名称为 <code>rr.yaml</code>，则创建命令为 <code>calicoctl create -f rr.yaml</code>；此时在 RR 节点上使用 <code>calicoctl node status</code> 应该能看到类似如下输出</p><pre><code class="hljs sh">Calico process is running.IPv4 BGP status+--------------+---------------+-------+----------+-------------+| PEER ADDRESS |   PEER TYPE   | STATE |  SINCE   |    INFO     |+--------------+---------------+-------+----------+-------------+| 172.16.0.19  | node specific | up    | 05:43:51 | Established || 172.16.0.16  | node specific | up    | 05:43:51 | Established || 172.16.0.17  | node specific | up    | 05:43:51 | Established || 172.16.0.13  | node specific | up    | 13:01:17 | Established |+--------------+---------------+-------+----------+-------------+IPv6 BGP statusNo IPv6 peers found.</code></pre><p><strong><code>PEER ADDRESS</code> 应当包含所有非 RR 节点 IP(由于真实测试环境，以上输出已人为修改)</strong></p><p>同时在非 RR 节点上使用 <code>calicoctl node status</code> 应该能看到以下输出</p><pre><code class="hljs sh">Calico process is running.IPv4 BGP status+--------------+---------------+-------+----------+-------------+| PEER ADDRESS |   PEER TYPE   | STATE |  SINCE   |    INFO     |+--------------+---------------+-------+----------+-------------+| 172.16.0.10  | node specific | up    | 05:43:51 | Established || 172.16.0.13  | node specific | up    | 13:01:20 | Established |+--------------+---------------+-------+----------+-------------+IPv6 BGP statusNo IPv6 peers found.</code></pre><p><strong><code>PEER ADDRESS</code> 应当包含所有 RR 节点 IP，此时原本的 Pod 网络连接应当已经恢复</strong></p><p>本部分参考:</p><ul><li><a href="https://www.projectcalico.org/how-does-in-cluster-route-reflection-work/" target="_blank" rel="noopener">In-cluster Route Reflection</a></li><li><a href="https://docs.projectcalico.org/v3.6/networking/bgp" target="_blank" rel="noopener">Configuring in-cluster route reflectors</a></li></ul><h3 id="2-3、调整-IPIP-规则"><a href="#2-3、调整-IPIP-规则" class="headerlink" title="2.3、调整 IPIP 规则"></a>2.3、调整 IPIP 规则</h3><p>先说一下 Calico IPIP 模式的三个可选项:</p><ul><li><code>Always</code>: 永远进行 IPIP 封装(默认)</li><li><code>CrossSubnet</code>: 只在跨网段时才进行 IPIP 封装，适合有 Kubernetes 节点在其他网段的情况，属于中肯友好方案</li><li><code>Never</code>: 从不进行 IPIP 封装，适合确认所有 Kubernetes 节点都在同一个网段下的情况</li></ul><p>在默认情况下，默认的 ipPool 启用了 IPIP 封装(至少通过官方安装文档安装的 Calico 是这样)，并且封装模式为 <code>Always</code>；这也就意味着任何时候都会在原报文上封装新 IP 地址，<strong>在这种情况下将外部流量路由到 RR 节点，RR 节点再转发进行 IPIP 封装时，可能出现网络无法联通的情况(没仔细追查，网络渣，猜测是 Pod 那边得到的源 IP 不对导致的)；</strong>此时我们应当调整 IPIP 封装策略为 <code>CrossSubnet</code></p><p>导出 ipPool 配置</p><pre><code class="hljs sh">calicoctl get ippool default-ipv4-ippool -o yaml &gt; ippool.yaml</code></pre><p>修改 <code>ipipMode</code> 值为 <code>CrossSubnet</code></p><pre><code class="hljs yaml"><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">projectcalico.org/v3</span><span class="hljs-attr">kind:</span> <span class="hljs-string">IPPool</span><span class="hljs-attr">metadata:</span>  <span class="hljs-attr">creationTimestamp:</span> <span class="hljs-number">2019</span><span class="hljs-number">-06</span><span class="hljs-string">-17T13:55:44Z</span>  <span class="hljs-attr">name:</span> <span class="hljs-string">default-ipv4-ippool</span>  <span class="hljs-attr">resourceVersion:</span> <span class="hljs-string">"61858741"</span>  <span class="hljs-attr">uid:</span> <span class="hljs-string">99a82055-9107-11e9-815b-b82a72dffa9f</span><span class="hljs-attr">spec:</span>  <span class="hljs-attr">blockSize:</span> <span class="hljs-number">26</span>  <span class="hljs-attr">cidr:</span> <span class="hljs-number">10.20</span><span class="hljs-number">.0</span><span class="hljs-number">.0</span><span class="hljs-string">/16</span>  <span class="hljs-attr">ipipMode:</span> <span class="hljs-string">CrossSubnet</span>  <span class="hljs-attr">natOutgoing:</span> <span class="hljs-literal">true</span>  <span class="hljs-attr">nodeSelector:</span> <span class="hljs-string">all()</span></code></pre><p>重新使用 <code>calicoctl apply -f ippool.yaml</code> 应用既可</p><p>本部分参考:</p><ul><li><a href="https://docs.projectcalico.org/v3.6/networking/ip-in-ip" target="_blank" rel="noopener">Configuring IP-in-IP</a></li><li><a href="https://docs.projectcalico.org/v3.6/reference/calicoctl/resources/ippool" target="_blank" rel="noopener">IP pool resource</a></li></ul><h3 id="2-4、增加路由联通网络"><a href="#2-4、增加路由联通网络" class="headerlink" title="2.4、增加路由联通网络"></a>2.4、增加路由联通网络</h3><p>万事俱备只欠东风，最后只需要在开发机器添加路由既可</p><p>将 Pod IP <code>10.20.0.0/16</code> 和 Service IP <code>10.254.0.0/16</code> 路由到 RR 节点 <code>172.16.0.13</code></p><pre><code class="hljs sh"><span class="hljs-comment"># Pod IP</span>ip route add 10.20.0.0/16 via 172.16.0.13<span class="hljs-comment"># Service IP</span>ip route add 10.254.0.0/16 via 172.16.0.13</code></pre><p>当然最方便的肯定是将这一步在开发网络的路由上做，设置完成后开发网络就可以直连集群内的 Pod IP 和 Service IP 了；至于想直接访问 Service Name 只需要调整上游 DNS 解析既可</p>]]></content>
    
    
    <categories>
      
      <category>Kubernetes</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Kubernetes</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Dockerfile 目前可扩展的语法</title>
    <link href="/2019/05/13/dockerfile-extended-syntax/"/>
    <url>/2019/05/13/dockerfile-extended-syntax/</url>
    
    <content type="html"><![CDATA[<blockquote><p>最近在调整公司项目的 CI，目前主要使用 GitLab CI，在尝试多阶段构建中踩了点坑，然后发现了一些有意思的玩意</p></blockquote><p>本文参考:</p><ul><li><a href="https://github.com/moby/buildkit/blob/master/frontend/dockerfile/docs/experimental.md" target="_blank" rel="noopener">Dockerfile frontend experimental syntaxes</a></li><li><a href="https://medium.com/@tonistiigi/advanced-multi-stage-build-patterns-6f741b852fae" target="_blank" rel="noopener">Advanced multi-stage build patterns</a></li><li><a href="https://docs.docker.com/engine/reference/commandline/build/" target="_blank" rel="noopener">docker build Document</a></li></ul><h2 id="一、起因"><a href="#一、起因" class="headerlink" title="一、起因"></a>一、起因</h2><p>公司目前主要使用 GitLab CI 作为主力 CI 构建工具，而且由于机器有限，我们对一些包管理器的本地 cache 直接持久化到了本机；比如 maven 的 <code>.m2</code> 目录，nodejs 的 <code>.npm</code> 目录等；虽然我们创建了对应的私服，但是在 build 时毕竟会下载，所以当时索性调整 GitLab Runner 在每个由 GitLab Runner 启动的容器中挂载这些缓存目录(GitLab CI 在 build 时会新启动容器运行 build 任务)；今天调整 nodejs 项目浪了一下，直接采用 Dockerfile 的 multi-stage build 功能进行 “Build =&gt; Package(docker image)” 的实现，基本 Dockerfile 如下</p><pre><code class="hljs sh">FROM gozap/build as builderCOPY . /xxxxWORKDIR /xxxxRUN <span class="hljs-built_in">source</span> ~/.bashrc \    &amp;&amp; cnpm install \    &amp;&amp; cnpm run buildFROM gozap/nginx-react:v1.0.0LABEL maintainer=<span class="hljs-string">"mritd &lt;mritd@linux.com&gt;"</span>COPY --from=builder /xxxx/public /usr/share/nginx/htmlEXPOSE 80STOPSIGNAL SIGTERMCMD [<span class="hljs-string">"nginx"</span>, <span class="hljs-string">"-g"</span>, <span class="hljs-string">"daemon off;"</span>]</code></pre><p>本来这个 <code>cnpm</code> 命令是带有 cache 的(<a href="https://github.com/Gozap/dockerfile/blob/master/build/cnpm" target="_blank" rel="noopener">见这里</a>)，不过运行完 build 以后发现很慢，检查宿主机 cache 目录发现根本没有 cache…然后突然感觉</p><p><img src="https://cdn.oss.link/markdown/6ieh4.jpg" srcset="/img/loading.gif" alt="事情并没有这么简单"></p><p>仔细想想，情况应该是这样事儿的…</p><pre><code class="hljs sh">+------------+                +-------------+            +----------------+|            |                |             |            |                ||            |                |    build    |            |   Multi-stage  ||   Runner   +---------------&gt;+  conatiner  +-----------&gt;+     Build      ||            |                |             |            |                ||            |                |             |            |                |+------------+                +------+------+            +----------------+                                     ^                                     |                                     |                                     |                                     |                              +------+------+                              |             |                              |    Cache    |                              |             |                              +-------------+</code></pre><p><img src="https://cdn.oss.link/markdown/9ov8m.jpg" srcset="/img/loading.gif" alt="挂载不管用"></p><p>后来经过查阅文档，发现 Dockerfile 是有扩展语法的(当然最终我还是没用)，具体请见<del>下篇文章</del>(我怕被打死)下面，<strong>先说好，下面的内容无法完美的解决上面的问题，目前只是支持了一部分功能，当然未来很可能支持类似 <code>IF ELSE</code> 语法、直接挂载宿主机目录等功能</strong></p><h2 id="二、开启-Dockerfile-扩展语法"><a href="#二、开启-Dockerfile-扩展语法" class="headerlink" title="二、开启 Dockerfile 扩展语法"></a>二、开启 Dockerfile 扩展语法</h2><h3 id="2-1、开启实验性功能"><a href="#2-1、开启实验性功能" class="headerlink" title="2.1、开启实验性功能"></a>2.1、开启实验性功能</h3><p>目前这个扩展语法还处于实验性功能，所以需要配置 dockerd 守护进程，修改如下</p><pre><code class="hljs sh">ExecStart=/usr/bin/dockerd  -H unix:// \                            --init \                            --live-restore \                            --data-root=/data/docker \                            --experimental \                            --<span class="hljs-built_in">log</span>-driver json-file \                            --<span class="hljs-built_in">log</span>-opt max-size=30m \                            --<span class="hljs-built_in">log</span>-opt max-file=3</code></pre><p>主要是 <code>--experimental</code> 参数，参考<a href="https://docs.docker.com/engine/reference/commandline/dockerd/#description" target="_blank" rel="noopener">官方文档</a>；<strong>同时在 build 前声明 <code>export DOCKER_BUILDKIT=1</code> 变量</strong></p><h3 id="2-2、修改-Dockerfile"><a href="#2-2、修改-Dockerfile" class="headerlink" title="2.2、修改 Dockerfile"></a>2.2、修改 Dockerfile</h3><p>开启实验性功能后，只需要在 Dockerfile 头部增加 <code># syntax=docker/dockerfile:experimental</code> 既可；为了保证稳定性，你也可以指定具体的版本号，类似这样</p><pre><code class="hljs sh"><span class="hljs-comment"># syntax=docker/dockerfile:1.1.1-experimental</span>FROM tomcat</code></pre><h3 id="2-3、可用的扩展语法"><a href="#2-3、可用的扩展语法" class="headerlink" title="2.3、可用的扩展语法"></a>2.3、可用的扩展语法</h3><ul><li><code>RUN --mount=type=bind</code></li></ul><p>这个是默认的挂载模式，这个允许将上下文或者镜像以可都可写/只读模式挂载到 build 容器中，可选参数如下(不翻译了)</p><table><thead><tr><th>Option</th><th>Description</th></tr></thead><tbody><tr><td><code>target</code> (required)</td><td>Mount path.</td></tr><tr><td><code>source</code></td><td>Source path in the <code>from</code>. Defaults to the root of the <code>from</code>.</td></tr><tr><td><code>from</code></td><td>Build stage or image name for the root of the source. Defaults to the build context.</td></tr><tr><td><code>rw</code>,<code>readwrite</code></td><td>Allow writes on the mount. Written data will be discarded.</td></tr></tbody></table><ul><li><code>RUN --mount=type=cache</code></li></ul><p>专用于作为 cache 的挂载位置，一般用于 cache 包管理器的下载等</p><table><thead><tr><th>Option</th><th>Description</th></tr></thead><tbody><tr><td><code>id</code></td><td>Optional ID to identify separate/different caches</td></tr><tr><td><code>target</code> (required)</td><td>Mount path.</td></tr><tr><td><code>ro</code>,<code>readonly</code></td><td>Read-only if set.</td></tr><tr><td><code>sharing</code></td><td>One of <code>shared</code>, <code>private</code>, or <code>locked</code>. Defaults to <code>shared</code>. A <code>shared</code> cache mount can be used concurrently by multiple writers. <code>private</code> creates a new mount if there are multiple writers. <code>locked</code> pauses the second writer until the first one releases the mount.</td></tr><tr><td><code>from</code></td><td>Build stage to use as a base of the cache mount. Defaults to empty directory.</td></tr><tr><td><code>source</code></td><td>Subpath in the <code>from</code> to mount. Defaults to the root of the <code>from</code>.</td></tr></tbody></table><p><strong>Example: cache Go packages</strong></p><pre><code class="hljs sh"><span class="hljs-comment"># syntax = docker/dockerfile:experimental</span>FROM golang...RUN --mount=<span class="hljs-built_in">type</span>=cache,target=/root/.cache/go-build go build ...</code></pre><p><strong>Example: cache apt packages</strong></p><pre><code class="hljs sh"><span class="hljs-comment"># syntax = docker/dockerfile:experimental</span>FROM ubuntuRUN rm -f /etc/apt/apt.conf.d/docker-clean; <span class="hljs-built_in">echo</span> <span class="hljs-string">'Binary::apt::APT::Keep-Downloaded-Packages "true";'</span> &gt; /etc/apt/apt.conf.d/keep-cacheRUN --mount=<span class="hljs-built_in">type</span>=cache,target=/var/cache/apt --mount=<span class="hljs-built_in">type</span>=cache,target=/var/lib/apt \  apt update &amp;&amp; apt install -y gcc</code></pre><ul><li><code>RUN --mount=type=tmpfs</code></li></ul><p>专用于挂载 tmpfs 的选项</p><table><thead><tr><th>Option</th><th>Description</th></tr></thead><tbody><tr><td><code>target</code> (required)</td><td>Mount path.</td></tr></tbody></table><ul><li><code>RUN --mount=type=secret</code></li></ul><p>这个类似 k8s 的 secret，用来挂载一些不想打入镜像，但是构建时想使用的密钥等，例如 docker 的 <code>config.json</code>，S3 的 <code>credentials</code></p><table><thead><tr><th>Option</th><th>Description</th></tr></thead><tbody><tr><td><code>id</code></td><td>ID of the secret. Defaults to basename of the target path.</td></tr><tr><td><code>target</code></td><td>Mount path. Defaults to <code>/run/secrets/</code> + <code>id</code>.</td></tr><tr><td><code>required</code></td><td>If set to <code>true</code>, the instruction errors out when the secret is unavailable. Defaults to <code>false</code>.</td></tr><tr><td><code>mode</code></td><td>File mode for secret file in octal. Default 0400.</td></tr><tr><td><code>uid</code></td><td>User ID for secret file. Default 0.</td></tr><tr><td><code>gid</code></td><td>Group ID for secret file. Default 0.</td></tr></tbody></table><p><strong>Example: access to S3</strong></p><pre><code class="hljs sh"><span class="hljs-comment"># syntax = docker/dockerfile:experimental</span>FROM python:3RUN pip install awscliRUN --mount=<span class="hljs-built_in">type</span>=secret,id=aws,target=/root/.aws/credentials aws s3 cp s3://... ...</code></pre><p><strong>注意: <code>buildctl</code> 是 BuildKit 的命令，你要测试的话自己换成 <code>docker build</code> 相关参数</strong></p><pre><code class="hljs console"><span class="hljs-meta">$</span><span class="bash"> buildctl build --frontend=dockerfile.v0 --<span class="hljs-built_in">local</span> context=. --<span class="hljs-built_in">local</span> dockerfile=. \</span>  --secret id=aws,src=$HOME/.aws/credentials</code></pre><ul><li><code>RUN --mount=type=ssh</code></li></ul><p>允许 build 容器通过 SSH agent 访问 SSH key，并且支持 <code>passphrases</code></p><table><thead><tr><th>Option</th><th>Description</th></tr></thead><tbody><tr><td><code>id</code></td><td>ID of SSH agent socket or key. Defaults to “default”.</td></tr><tr><td><code>target</code></td><td>SSH agent socket path. Defaults to <code>/run/buildkit/ssh_agent.${N}</code>.</td></tr><tr><td><code>required</code></td><td>If set to <code>true</code>, the instruction errors out when the key is unavailable. Defaults to <code>false</code>.</td></tr><tr><td><code>mode</code></td><td>File mode for socket in octal. Default 0600.</td></tr><tr><td><code>uid</code></td><td>User ID for socket. Default 0.</td></tr><tr><td><code>gid</code></td><td>Group ID for socket. Default 0.</td></tr></tbody></table><p><strong>Example: access to Gitlab</strong></p><pre><code class="hljs sh"><span class="hljs-comment"># syntax = docker/dockerfile:experimental</span>FROM alpineRUN apk add --no-cache openssh-clientRUN mkdir -p -m 0700 ~/.ssh &amp;&amp; ssh-keyscan gitlab.com &gt;&gt; ~/.ssh/known_hostsRUN --mount=<span class="hljs-built_in">type</span>=ssh ssh -q -T git@gitlab.com 2&gt;&amp;1 | tee /hello<span class="hljs-comment"># "Welcome to GitLab, @GITLAB_USERNAME_ASSOCIATED_WITH_SSHKEY" should be printed here</span><span class="hljs-comment"># with the type of build progress is defined as `plain`.</span></code></pre><pre><code class="hljs sh">$ <span class="hljs-built_in">eval</span> $(ssh-agent)$ ssh-add ~/.ssh/id_rsa(Input your passphrase here)$ buildctl build --frontend=dockerfile.v0 --<span class="hljs-built_in">local</span> context=. --<span class="hljs-built_in">local</span> dockerfile=. \  --ssh default=<span class="hljs-variable">$SSH_AUTH_SOCK</span></code></pre><p>你也可以直接使用宿主机目录的 pem 文件，但是带有密码的 pem 目前不支持</p><p><strong>目前根据文档测试，当前的挂载类型比如 <code>cache</code> 类型，仅用于 multi-stage 内的挂载，比如你有 2+ 个构建步骤，<code>cache</code> 挂载类型能帮你在各个阶段内共享文件；但是它目前无法解决直接将宿主机目录挂载到 multi-stage 的问题(可以采取些曲线救国方案，但是很不优雅)；但是未来还是很有展望的，可以关注一下</strong></p>]]></content>
    
    
    <categories>
      
      <category>Docker</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Docker</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Mac 下调校 Rime</title>
    <link href="/2019/03/23/oh-my-rime/"/>
    <url>/2019/03/23/oh-my-rime/</url>
    
    <content type="html"><![CDATA[<blockquote><p>由于对国内输入法隐私问题的担忧，决定放弃搜狗等输入法；为了更加 Geek 一些，最终决定了折腾 Rime(鼠须管) 输入法，以下为一些折腾的过程</p></blockquote><p><strong>国际惯例先放点图压压惊</strong></p><p><img src="https://cdn.oss.link/markdown/t3otb.jpg" srcset="/img/loading.gif" alt="example1"><br><img src="https://cdn.oss.link/markdown/ep8sl.jpg" srcset="/img/loading.gif" alt="example2"><br><img src="https://cdn.oss.link/markdown/wth6n.jpg" srcset="/img/loading.gif" alt="example3"><br><img src="https://cdn.oss.link/markdown/5b85o.jpg" srcset="/img/loading.gif" alt="example4"></p><h2 id="一、安装"><a href="#一、安装" class="headerlink" title="一、安装"></a>一、安装</h2><p>安装 Rime 没啥好说的，直接从<a href="https://rime.im" target="_blank" rel="noopener">官网</a>下载最新版本的安装包既可；安装完成后配置文件位于 <code>~/Library/Rime</code> 位置；在进行后续折腾之前我建议还是先 <code>cp -r ~/Library/Rime ~/Library/Rime.bak</code> 备份一下配置文件，以防制后续折腾挂了还可以还原；安装完成以后按 <code>⌘ + 反引号(~)</code> 切换到 <code>朙月拼音-简化字</code> 既可开启简体中文输入</p><h2 id="二、乱码解决"><a href="#二、乱码解决" class="headerlink" title="二、乱码解决"></a>二、乱码解决</h2><p>安装完成后在打字时可能出现乱码情况(俗称豆腐块)，这是由于 Rime 默认 UTF-8 字符集比较大，预选词内会出现生僻字，而 mac 字体内又不包含这些字体，从而导致乱码；解决方案很简单，下载 <a href="https://github.com/mritd/rime/tree/master/fonts" target="_blank" rel="noopener">花园明朝</a> A、B 两款字体安装既可，安装后重启一下就不会出现乱码了</p><p><img src="https://cdn.oss.link/markdown/yfbis.png" srcset="/img/loading.gif" alt="fonts"></p><h2 id="三、配置文件"><a href="#三、配置文件" class="headerlink" title="三、配置文件"></a>三、配置文件</h2><p>官方并不建议直接修改原始的配置文件，因为输入法更新时会重新覆盖默认配置，可能导致某些自定义配置丢失；推荐作法是创建一系列的 patch 配置，通过类似打补丁替换这种方式来实现无感的增加自定义配置；</p><p>由于使用的是 <code>朙月拼音-简化字</code> 输入方案，所以需要创建 <code>luna_pinyin_simp.custom.yaml</code> 等配置文件，后面就是查文档 + 各种 Google 一顿魔改了；目前我将我自己用的配置放在了 <a href="https://github.com/mritd/rime" target="_blank" rel="noopener">Github</a> 上，有需要的可以直接 clone 下来，用里面的配置文件直接覆盖 <code>~/Library/Rime</code> 下的文件，然后重新部署既可，关于具体配置细节在下面写</p><h2 id="四、自定义配色"><a href="#四、自定义配色" class="headerlink" title="四、自定义配色"></a>四、自定义配色</h2><p>皮肤配色配置方案位于 <code>squirrel.custom.yaml</code> 配置文件中，我的配置目前是参考搜狗输入法皮肤自己调试的；官方也提供了一些皮肤外观配置，详见 <a href="https://gist.github.com/lotem/2290714" target="_blank" rel="noopener">Gist</a>；想要切换皮肤配色只需要修改 <code>style/color_scheme</code> 为相应的皮肤配色名称既可</p><pre><code class="hljs yaml"><span class="hljs-attr">patch:</span>  <span class="hljs-attr">show_notifications_when:</span> <span class="hljs-string">appropriate</span>          <span class="hljs-comment"># 状态通知，适当，也可设为全开（always）全关（never）</span>  <span class="hljs-attr">style/color_scheme:</span> <span class="hljs-string">mritd_dark</span>                <span class="hljs-comment"># 方案命名，不能有空格</span>  <span class="hljs-attr">preset_color_schemes:</span>    <span class="hljs-attr">mritd_dark:</span>      <span class="hljs-attr">name:</span> <span class="hljs-string">漠然／mritd</span> <span class="hljs-string">dark</span>      <span class="hljs-attr">author:</span> <span class="hljs-string">mritd</span> <span class="hljs-string">&lt;mritd1234@gmail.com&gt;</span>      <span class="hljs-attr">horizontal:</span> <span class="hljs-literal">true</span>                          <span class="hljs-comment"># 水平排列</span>      <span class="hljs-attr">inline_preedit:</span> <span class="hljs-literal">true</span>                      <span class="hljs-comment"># 单行显示，false双行显示</span>      <span class="hljs-attr">candidate_format:</span> <span class="hljs-string">"%c\u2005%@"</span>            <span class="hljs-comment"># 用 1/6 em 空格 U+2005 来控制编号 %c 和候选词 %@ 前后的空间。</span>      <span class="hljs-attr">corner_radius:</span> <span class="hljs-number">5</span>                          <span class="hljs-comment"># 候选条圆角</span>      <span class="hljs-attr">hilited_corner_radius:</span> <span class="hljs-number">3</span>                  <span class="hljs-comment"># 高亮圆角</span>      <span class="hljs-attr">border_height:</span> <span class="hljs-number">6</span>                          <span class="hljs-comment"># 窗口边界高度，大于圆角半径才生效</span>      <span class="hljs-attr">border_width:</span> <span class="hljs-number">6</span>                           <span class="hljs-comment"># 窗口边界宽度，大于圆角半径才生效</span>      <span class="hljs-attr">border_color_width:</span> <span class="hljs-number">0</span>      <span class="hljs-comment">#font_face: "PingFangSC"                   # 候选词字体</span>      <span class="hljs-attr">font_point:</span> <span class="hljs-number">16</span>                            <span class="hljs-comment"># 候选字词大小</span>      <span class="hljs-attr">label_font_point:</span> <span class="hljs-number">14</span>                      <span class="hljs-comment"># 候选编号大小</span>      <span class="hljs-attr">text_color:</span> <span class="hljs-number">0xdedddd</span>                      <span class="hljs-comment"># 拼音行文字颜色，24位色值，16进制，BGR顺序</span>      <span class="hljs-attr">back_color:</span> <span class="hljs-number">0x4b4b4b</span>                      <span class="hljs-comment"># 候选条背景色</span>      <span class="hljs-attr">label_color:</span> <span class="hljs-number">0x888785</span>                     <span class="hljs-comment"># 预选栏编号颜色</span>      <span class="hljs-attr">border_color:</span> <span class="hljs-number">0x4b4b4b</span>                    <span class="hljs-comment"># 边框色</span>      <span class="hljs-attr">candidate_text_color:</span> <span class="hljs-number">0xffffff</span>            <span class="hljs-comment"># 预选项文字颜色</span>      <span class="hljs-attr">hilited_text_color:</span> <span class="hljs-number">0xdedddd</span>              <span class="hljs-comment"># 高亮拼音 (需要开启内嵌编码)</span>      <span class="hljs-attr">hilited_back_color:</span> <span class="hljs-number">0x252320</span>              <span class="hljs-comment"># 高亮拼音 (需要开启内嵌编码)</span>      <span class="hljs-attr">hilited_candidate_text_color:</span> <span class="hljs-number">0xFFE696</span>    <span class="hljs-comment"># 第一候选项文字颜色</span>      <span class="hljs-attr">hilited_candidate_back_color:</span> <span class="hljs-number">0x4b4b4b</span>    <span class="hljs-comment"># 第一候选项背景背景色</span>      <span class="hljs-attr">hilited_candidate_label_color:</span> <span class="hljs-number">0xffffff</span>   <span class="hljs-comment"># 第一候选项编号颜色</span>      <span class="hljs-attr">comment_text_color:</span> <span class="hljs-number">0xdedddd</span>              <span class="hljs-comment"># 拼音等提示文字颜色</span></code></pre><h2 id="五、增加自定义快捷字符"><a href="#五、增加自定义快捷字符" class="headerlink" title="五、增加自定义快捷字符"></a>五、增加自定义快捷字符</h2><p>快捷字符例如在中文输入法状态下可以直接输入 <code>/dn</code> 来调出特殊符号输入；这些配置位于 <code>luna_pinyin_simp.custom.yaml</code> 的 <code>punctuator</code> 配置中，我目前自行定义了一些，有需要的可以依葫芦画瓢直接修改</p><pre><code class="hljs yaml"><span class="hljs-attr">punctuator:</span>    <span class="hljs-attr">import_preset:</span> <span class="hljs-string">symbols</span>    <span class="hljs-attr">symbols:</span>      <span class="hljs-string">"/fs"</span><span class="hljs-string">:</span> <span class="hljs-string">[½,‰,¼,⅓,⅔,¾,⅒]</span>      <span class="hljs-string">"/dq"</span><span class="hljs-string">:</span> <span class="hljs-string">[🌍,🌎,🌏,🌐,🌑,🌒,🌓,🌔,🌕,🌖,🌗,🌘,🌙,🌚,🌛,🌜,🌝,🌞,⭐,🌟,🌠,⛅,⚡,❄,🔥,💧,🌊]</span>      <span class="hljs-string">"/jt"</span><span class="hljs-string">:</span> <span class="hljs-string">[⬆,↗,➡,↘,⬇,↙,⬅,↖,↕,↔,↩,↪,⤴,⤵,🔃,🔄,🔙,🔚,🔛,🔜,🔝]</span>      <span class="hljs-string">"/sg"</span><span class="hljs-string">:</span> <span class="hljs-string">[🍇,🍈,🍉,🍊,🍋,🍌,🍍,🍎,🍏,🍐,🍑,🍒,🍓,🍅,🍆,🌽,🍄,🌰,🍞,🍖,🍗,🍔,🍟,🍕,🍳,🍲,🍱,🍘,🍙,🍚,🍛,🍜,🍝,🍠,🍢,🍣,🍤,🍥,🍡,🍦,🍧,🍨,🍩,🍪,🎂,🍰,🍫,🍬,🍭,🍮,🍯,🍼,🍵,🍶,🍷,🍸,🍹,🍺,🍻,🍴]</span>      <span class="hljs-string">"/dw"</span><span class="hljs-string">:</span> <span class="hljs-string">[🙈,🙉,🙊,🐵,🐒,🐶,🐕,🐩,🐺,🐱,😺,😸,😹,😻,😼,😽,🙀,😿,😾,🐈,🐯,🐅,🐆,🐴,🐎,🐮,🐂,🐃,🐄,🐷,🐖,🐗,🐽,🐏,🐑,🐐,🐪,🐫,🐘,🐭,🐁,🐀,🐹,🐰,🐇,🐻,🐨,🐼,🐾,🐔,🐓,🐣,🐤,🐥,🐦,🐧,🐸,🐊,🐢,🐍,🐲,🐉,🐳,🐋,🐬,🐟,🐠,🐡,🐙,🐚,🐌,🐛,🐜,🐝,🐞,🦋]</span>      <span class="hljs-string">"/bq"</span><span class="hljs-string">:</span> <span class="hljs-string">[😀,😁,😂,😃,😄,😅,😆,😉,😊,😋,😎,😍,😘,😗,😙,😚,😇,😐,😑,😶,😏,😣,😥,😮,😯,😪,😫,😴,😌,😛,😜,😝,😒,😓,😔,😕,😲,😷,😖,😞,😟,😤,😢,😭,😦,😧,😨,😬,😰,😱,😳,😵,😡,😠]</span>      <span class="hljs-string">"/ss"</span><span class="hljs-string">:</span> <span class="hljs-string">[💪,👈,👉,👆,👇,✋,👌,👍,👎,✊,👊,👋,👏,👐]</span>      <span class="hljs-string">"/dn"</span><span class="hljs-string">:</span> <span class="hljs-string">[⌘,</span> <span class="hljs-string">⌥,</span> <span class="hljs-string">⇧,</span> <span class="hljs-string">⌃,</span> <span class="hljs-string">⎋,</span> <span class="hljs-string">⇪,</span> <span class="hljs-string">,</span> <span class="hljs-string">⌫,</span> <span class="hljs-string">⌦,</span> <span class="hljs-string">↩︎,</span> <span class="hljs-string">⏎,</span> <span class="hljs-string">↑,</span> <span class="hljs-string">↓,</span> <span class="hljs-string">←,</span> <span class="hljs-string">→,</span> <span class="hljs-string">↖,</span> <span class="hljs-string">↘,</span> <span class="hljs-string">⇟,</span> <span class="hljs-string">⇞]</span>      <span class="hljs-string">"/fh"</span><span class="hljs-string">:</span> <span class="hljs-string">[©,®,℗,ⓘ,℠,™,℡,␡,♂,♀,☉,☊,☋,☌,☍,☑︎,☒,☜,☝,☞,☟,✎,✄,♻,⚐,⚑,⚠]</span>      <span class="hljs-string">"/xh"</span><span class="hljs-string">:</span> <span class="hljs-string">[＊,×,✱,★,☆,✩,✧,❋,❊,❉,❈,❅,✿,✲]</span></code></pre><h2 id="六、设置输入方案"><a href="#六、设置输入方案" class="headerlink" title="六、设置输入方案"></a>六、设置输入方案</h2><p>在第一次按 <code>⌘ + 反引号(~)</code> 设置输入法时实际上我们可以看到很多的输入方案，而事实上很多方案我们根本用不上；想要删除和修改方案可以调整 <code>default.custom.yaml</code> 中的 <code>schema_list</code> 字段</p><pre><code class="hljs yaml"><span class="hljs-attr">patch:</span>  <span class="hljs-attr">menu:</span>    <span class="hljs-attr">page_size:</span> <span class="hljs-number">8</span>  <span class="hljs-attr">schema_list:</span>  <span class="hljs-bullet">-</span> <span class="hljs-attr">schema:</span> <span class="hljs-string">luna_pinyin_simp</span>      <span class="hljs-comment"># 朙月拼音 简化字</span>  <span class="hljs-bullet">-</span> <span class="hljs-attr">schema:</span> <span class="hljs-string">luna_pinyin</span>           <span class="hljs-comment"># 朙月拼音</span>  <span class="hljs-bullet">-</span> <span class="hljs-attr">schema:</span> <span class="hljs-string">luna_pinyin_fluency</span>   <span class="hljs-comment"># 语句流</span><span class="hljs-comment">#  - schema: double_pinyin         # 自然碼雙拼</span><span class="hljs-comment">#  - schema: double_pinyin_flypy   # 小鹤雙拼</span><span class="hljs-comment">#  - schema: double_pinyin_pyjj    # 拼音加加双拼</span><span class="hljs-comment">#  - schema: wubi_pinyin           # 五笔拼音混合輸入</span></code></pre><p><strong>实际上我只能用上第一个…毕竟写了好几年代码还得看键盘的人也只能这样了…</strong></p><h2 id="七、调整特殊键行为"><a href="#七、调整特殊键行为" class="headerlink" title="七、调整特殊键行为"></a>七、调整特殊键行为</h2><p>在刚安装完以后发现在中文输入法状态下输入英文，按 <code>shift</code> 键后字符上屏，然后还得回车一下，这就很让我难受…最后找到了这篇 <a href="https://gist.github.com/lotem/2981316" target="_blank" rel="noopener">Gist</a>，目前将大写锁定、<code>shift</code> 键调整为了跟搜狗一致的配置，有需要调整的可以自行编辑 <code>default.custom.yaml</code> 中的 <code>ascii_composer/switch_key</code> 部分</p><pre><code class="hljs yaml"><span class="hljs-comment"># capslock 键切换英文并输出大写</span><span class="hljs-attr">ascii_composer/good_old_caps_lock:</span> <span class="hljs-literal">true</span><span class="hljs-comment"># 输入法中英文状态快捷键</span><span class="hljs-attr">ascii_composer/switch_key:</span>  <span class="hljs-attr">Caps_Lock:</span> <span class="hljs-string">commit_code</span>  <span class="hljs-attr">Control_L:</span> <span class="hljs-string">noop</span>  <span class="hljs-attr">Control_R:</span> <span class="hljs-string">noop</span>  <span class="hljs-comment"># 按下左 shift 英文字符直接上屏，不需要再次回车，输入法保持英文状态</span>  <span class="hljs-attr">Shift_L:</span> <span class="hljs-string">commit_code</span>  <span class="hljs-attr">Shift_R:</span> <span class="hljs-string">noop</span></code></pre><h2 id="八、自定义词库"><a href="#八、自定义词库" class="headerlink" title="八、自定义词库"></a>八、自定义词库</h2><p>Rime 默认的词库稍为有点弱，我们可以下载一些搜狗词库来进行扩展；不过搜狗词库格式默认是无法解析的，好在有人开发了工具可以方便的将搜狗细胞词库转化为 Rime 的格式(工具<a href="https://github.com/studyzy/imewlconverter/releases" target="_blank" rel="noopener">点击这里</a>下载)；目前该工具只支持 Windows(也有些别人写的 py 脚本啥的，但是我没用)，所以词库转换这种操作还得需要一个 Windows 虚拟机；</p><p>转换过程很简单，先从<a href="https://pinyin.sogou.com/dict/" target="_blank" rel="noopener">搜狗词库</a>下载一系列的 <code>scel</code> 文件，然后批量选中，接着调整一下输入和输出格式点击转换，最后保存成一个 <code>txt</code> 文本</p><p><img src="https://cdn.oss.link/markdown/jtv97.png" srcset="/img/loading.gif" alt="input-setting"></p><p><img src="https://cdn.oss.link/markdown/p7qha.png" srcset="/img/loading.gif" alt="convert"></p><p>光有这个文本还不够，我们要将它塞到词库的 <code>yaml</code> 配置里，所以新建一个词库配置文件 <code>luna_pinyin.sougou.dict.yaml</code>，然后写上头部说明(<strong>注意最后三个点后面加一个换行</strong>)</p><pre><code class="hljs yaml"><span class="hljs-comment"># Rime dictionary</span><span class="hljs-comment"># encoding: utf-8</span><span class="hljs-comment"># 搜狗词库 目前包含如下:</span><span class="hljs-comment"># IT计算机 实用IT词汇 亲戚称呼 化学品名 数字时间 数学词汇 淘宝词库 编程语言 软件专业 颜色名称 程序猿词库 开发专用词库 搜狗标准词库</span><span class="hljs-comment"># 摄影专业名词 计算机专业词库 计算机词汇大全 保险词汇 最详细的全国地名大全 饮食大全 常见花卉名称 房地产词汇大全 中国传统节日大全 财经金融词汇大全</span><span class="hljs-meta">---</span><span class="hljs-attr">name:</span> <span class="hljs-string">luna_pinyin.sougou</span><span class="hljs-attr">version:</span> <span class="hljs-string">"1.0"</span><span class="hljs-attr">sort:</span> <span class="hljs-string">by_weight</span><span class="hljs-attr">use_preset_vocabulary:</span> <span class="hljs-literal">true</span><span class="hljs-string">...</span></code></pre><p>接着只需要把生成好的词库 <code>txt</code> 文件内容粘贴到三个点下面既可；但是词库太多的话你会发现这个文本有好几十 M，一般编辑器打开都会卡死，解决这种情况只需要用命令行 <code>cat</code> 一下就行</p><pre><code class="hljs sh">cat sougou.txt &gt;&gt; luna_pinyin.sougou.dict.yaml</code></pre><p>最后修改 <code>luna_pinyin.extended.dict.yaml</code> 中的 <code>import_tables</code> 字段，加入刚刚新建的词库既可</p><pre><code class="hljs yaml"><span class="hljs-meta">---</span><span class="hljs-attr">name:</span> <span class="hljs-string">luna_pinyin.extended</span><span class="hljs-attr">version:</span> <span class="hljs-string">"2016.06.26"</span><span class="hljs-attr">sort:</span> <span class="hljs-string">by_weight</span>  <span class="hljs-comment">#字典初始排序，可選original或by_weight</span><span class="hljs-attr">use_preset_vocabulary:</span> <span class="hljs-literal">true</span><span class="hljs-comment">#此處爲明月拼音擴充詞庫（基本）默認鏈接載入的詞庫，有朙月拼音官方詞庫、明月拼音擴充詞庫（漢語大詞典）、明月拼音擴充詞庫（詩詞）、明月拼音擴充詞庫（含西文的詞彙）。如果不需要加載某个詞庫請將其用「#」註釋掉。</span><span class="hljs-comment">#雙拼不支持 luna_pinyin.cn_en 詞庫，請用戶手動禁用。</span><span class="hljs-attr">import_tables:</span>  <span class="hljs-bullet">-</span> <span class="hljs-string">luna_pinyin</span>  <span class="hljs-comment"># 加入搜狗词库</span>  <span class="hljs-bullet">-</span> <span class="hljs-string">luna_pinyin.sougou</span>  <span class="hljs-bullet">-</span> <span class="hljs-string">luna_pinyin.poetry</span>  <span class="hljs-bullet">-</span> <span class="hljs-string">luna_pinyin.cn_en</span>  <span class="hljs-bullet">-</span> <span class="hljs-string">luna_pinyin.kaomoji</span></code></pre><h2 id="九、定制特殊单词"><a href="#九、定制特殊单词" class="headerlink" title="九、定制特殊单词"></a>九、定制特殊单词</h2><p>由于长期撸码，24 小时离不开命令行，偶尔在中文输入法下输入了一些命令导致汉字直接出现在 terminal 上就很尴尬…这时候我们可以在 <code>luna_pinyin.cn_en.dict.yaml</code> 加入一些我们自己的专属词库，比如这样</p><pre><code class="hljs yaml"><span class="hljs-meta">---</span><span class="hljs-attr">name:</span> <span class="hljs-string">luna_pinyin.cn_en</span><span class="hljs-attr">version:</span> <span class="hljs-string">"2017.9.13"</span><span class="hljs-attr">sort:</span> <span class="hljs-string">by_weight</span><span class="hljs-attr">use_preset_vocabulary:</span> <span class="hljs-literal">true</span><span class="hljs-string">...</span><span class="hljs-string">git</span><span class="hljs-string">git</span><span class="hljs-string">ls</span><span class="hljs-string">ls</span><span class="hljs-string">cd</span><span class="hljs-string">cd</span><span class="hljs-string">pwd</span><span class="hljs-string">pwd</span><span class="hljs-string">git</span> <span class="hljs-string">ps</span><span class="hljs-string">gitps</span><span class="hljs-string">kubernetes</span><span class="hljs-string">kubernetes</span><span class="hljs-string">kubernetes</span><span class="hljs-string">kuber</span><span class="hljs-string">kubectl</span><span class="hljs-string">kubectl</span><span class="hljs-string">kubectl</span><span class="hljs-string">kubec</span><span class="hljs-string">docker</span><span class="hljs-string">docker</span><span class="hljs-string">docker</span><span class="hljs-string">dock</span><span class="hljs-string">ipvs</span><span class="hljs-string">ipvs</span><span class="hljs-string">ps</span><span class="hljs-string">ps</span><span class="hljs-string">bash</span><span class="hljs-string">bash</span><span class="hljs-string">source</span><span class="hljs-string">source</span><span class="hljs-string">source</span><span class="hljs-string">sou</span><span class="hljs-string">rm</span><span class="hljs-string">rm</span></code></pre><p>配置后如果我在中文输入法下输入 git 则会自动匹配 git 这个单词，避免错误的键入中文字符；<strong>需要注意的是第一列代表上屏的字符，第二列代表输入的单词，即 “当输入第二列时候选词为第一列”；两列之间要用 tag 制表符隔开，记住不是空格</strong></p>]]></content>
    
    
    <categories>
      
      <category>Mac</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Mac</tag>
      
      <tag>Rime</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Ubuntu 设置多个源</title>
    <link href="/2019/03/19/how-to-set-multiple-apt-mirrors-for-ubuntu/"/>
    <url>/2019/03/19/how-to-set-multiple-apt-mirrors-for-ubuntu/</url>
    
    <content type="html"><![CDATA[<h2 id="一、源起"><a href="#一、源起" class="headerlink" title="一、源起"></a>一、源起</h2><p>使用 Ubuntu 作为生产容器系统好久了，但是 apt 源问题一致有点困扰: <strong>由于众所周知的原因，官方源执行 <code>apt update</code> 等命令会非常慢；而国内有很多镜像服务，但是某些偶尔也会抽风(比如清华大源)，最后的结果就是日常修改 apt 源…</strong>Google 查了了好久发现事实上 apt 源是支持 <code>mirror</code> 协议的，从而自动选择可用的一个</p><h2 id="二、使用-mirror-协议"><a href="#二、使用-mirror-协议" class="headerlink" title="二、使用 mirror 协议"></a>二、使用 mirror 协议</h2><p>废话不说多直接上代码，编辑 <code>/etc/apt/sources.list</code>，替换为如下内容</p><pre><code class="hljs sh"><span class="hljs-comment">#------------------------------------------------------------------------------#</span><span class="hljs-comment">#                            OFFICIAL UBUNTU REPOS                             #</span><span class="hljs-comment">#------------------------------------------------------------------------------#</span><span class="hljs-comment">###### Ubuntu Main Repos</span>deb mirror://mirrors.ubuntu.com/mirrors.txt bionic main restricted universe multiversedeb-src mirror://mirrors.ubuntu.com/mirrors.txt bionic main restricted universe multiverse<span class="hljs-comment">###### Ubuntu Update Repos</span>deb mirror://mirrors.ubuntu.com/mirrors.txt bionic-security main restricted universe multiversedeb mirror://mirrors.ubuntu.com/mirrors.txt bionic-updates main restricted universe multiversedeb mirror://mirrors.ubuntu.com/mirrors.txt bionic-backports main restricted universe multiversedeb-src mirror://mirrors.ubuntu.com/mirrors.txt bionic-security main restricted universe multiversedeb-src mirror://mirrors.ubuntu.com/mirrors.txt bionic-updates main restricted universe multiversedeb-src mirror://mirrors.ubuntu.com/mirrors.txt bionic-backports main restricted universe multiverse</code></pre><p>当使用 <code>mirror</code> 协议后，执行 <code>apt update</code> 时会首先<strong>通过 http 访问</strong> <code>mirrors.ubuntu.com/mirrors.txt</code> 文本；文本内容实际上就是当前可用的镜像源列表，如下所示</p><pre><code class="hljs sh">http://ftp.sjtu.edu.cn/ubuntu/http://mirrors.nju.edu.cn/ubuntu/http://mirrors.nwafu.edu.cn/ubuntu/http://mirrors.sohu.com/ubuntu/http://mirrors.aliyun.com/ubuntu/http://mirrors.shu.edu.cn/ubuntu/http://mirrors.cqu.edu.cn/ubuntu/http://mirrors.huaweicloud.com/repository/ubuntu/http://mirrors.cn99.com/ubuntu/http://mirrors.yun-idc.com/ubuntu/http://mirrors.tuna.tsinghua.edu.cn/ubuntu/http://mirrors.ustc.edu.cn/ubuntu/http://mirrors.njupt.edu.cn/ubuntu/http://mirror.lzu.edu.cn/ubuntu/http://archive.ubuntu.com/ubuntu/</code></pre><p>得到列表后 apt 会自动选择一个(选择规则暂不清楚，国外有文章说是选择最快的，但是不清楚这个最快是延迟还是网速)进行下载；<strong>同时根据地区不通，官方也提供指定国家的 <code>mirror.txt</code></strong>，比如中国的实际上可以设置为 <code>mirrors.ubuntu.com/CN.txt</code>(我测试跟官方一样，推测可能是使用了类似 DNS 选优的策略)</p><h2 id="三、自定义-mirror-地址"><a href="#三、自定义-mirror-地址" class="headerlink" title="三、自定义 mirror 地址"></a>三、自定义 mirror 地址</h2><p>现在已经解决了能同时使用多个源的问题，但是有些时候你会发现源的可用性检测并不是很精准，比如某个源只有 40k 的下载速度…不巧你某个下载还命中了，这就很尴尬；<strong>所以有时候我们可能需要自定义 <code>mirror.txt</code> 这个源列表</strong>，经过测试证明<strong>只需要开启一个标准的 <code>http server</code> 能返回一个文本即可，不过需要注意只能是 <code>http</code>，而不是 <code>https</code></strong>；所以我们首先下载一下这个文本，把不想要的删掉；然后弄个 nginx，甚至 <code>python -m http.server</code> 把文本文件暴露出去就可以；我比较懒…扔 CDN 上了: <a href="http://oss.link/config/apt-mirrors.txt" target="_blank" rel="noopener">http://oss.link/config/apt-mirrors.txt</a></p><p>关于源的精简，我建议将一些 <code>edu</code> 的删掉，因为敏感时期他们很不稳定；优选阿里云、网易、华为这种大公司的，比较有名的清华大的什么的可以留着，其他的可以考虑都删掉</p>]]></content>
    
    
    <categories>
      
      <category>Linux</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Linux</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Kubernetes 1.13.4 搭建</title>
    <link href="/2019/03/16/set-up-kubernetes-1.13.4-cluster/"/>
    <url>/2019/03/16/set-up-kubernetes-1.13.4-cluster/</url>
    
    <content type="html"><![CDATA[<blockquote><p>年后回来有点懒，也有点忙；1.13 出来好久了，周末还是决定折腾一下吧</p></blockquote><h2 id="一、环境准备"><a href="#一、环境准备" class="headerlink" title="一、环境准备"></a>一、环境准备</h2><p>老样子，安装环境为 5 台 Ubuntu 18.04.2 LTS 虚拟机，其他详细信息如下</p><table><thead><tr><th>System OS</th><th>IP Address</th><th>Docker</th><th>Kernel</th><th>Application</th></tr></thead><tbody><tr><td>Ubuntu 18.04.2 LTS</td><td>192.168.1.51</td><td>18.09.2</td><td>4.15.0-46-generic</td><td>k8s-master、etcd</td></tr><tr><td>Ubuntu 18.04.2 LTS</td><td>192.168.1.52</td><td>18.09.2</td><td>4.15.0-46-generic</td><td>k8s-master、etcd</td></tr><tr><td>Ubuntu 18.04.2 LTS</td><td>192.168.1.53</td><td>18.09.2</td><td>4.15.0-46-generic</td><td>k8s-master、etcd</td></tr><tr><td>Ubuntu 18.04.2 LTS</td><td>192.168.1.54</td><td>18.09.2</td><td>4.15.0-46-generic</td><td>k8s-node</td></tr><tr><td>Ubuntu 18.04.2 LTS</td><td>192.168.1.55</td><td>18.09.2</td><td>4.15.0-46-generic</td><td>k8s-node</td></tr></tbody></table><p><strong>所有配置生成将在第一个节点上完成，第一个节点与其他节点 root 用户免密码登录，用于分发文件；为了方便搭建弄了一点小脚本，仓库地址 <a href="https://github.com/mritd/ktool" target="_blank" rel="noopener">ktool</a>，本文后续所有脚本、配置都可以在此仓库找到；关于 <a href="https://github.com/cloudflare/cfssl" target="_blank" rel="noopener">cfssl</a> 等基本工具使用，本文不再阐述</strong></p><h2 id="二、安装-Etcd"><a href="#二、安装-Etcd" class="headerlink" title="二、安装 Etcd"></a>二、安装 Etcd</h2><h3 id="2-1、生成证书"><a href="#2-1、生成证书" class="headerlink" title="2.1、生成证书"></a>2.1、生成证书</h3><p>Etcd 仍然开启 TLS 认证，所以先使用 cfssl 生成相关证书</p><ul><li>etcd-root-ca-csr.json</li></ul><pre><code class="hljs json">&#123;    <span class="hljs-attr">"CN"</span>: <span class="hljs-string">"etcd-root-ca"</span>,    <span class="hljs-attr">"key"</span>: &#123;        <span class="hljs-attr">"algo"</span>: <span class="hljs-string">"rsa"</span>,        <span class="hljs-attr">"size"</span>: <span class="hljs-number">4096</span>    &#125;,    <span class="hljs-attr">"names"</span>: [        &#123;            <span class="hljs-attr">"O"</span>: <span class="hljs-string">"etcd"</span>,            <span class="hljs-attr">"OU"</span>: <span class="hljs-string">"etcd Security"</span>,            <span class="hljs-attr">"L"</span>: <span class="hljs-string">"Beijing"</span>,            <span class="hljs-attr">"ST"</span>: <span class="hljs-string">"Beijing"</span>,            <span class="hljs-attr">"C"</span>: <span class="hljs-string">"CN"</span>        &#125;    ],    <span class="hljs-attr">"ca"</span>: &#123;        <span class="hljs-attr">"expiry"</span>: <span class="hljs-string">"87600h"</span>    &#125;&#125;</code></pre><ul><li>etcd-gencert.json</li></ul><pre><code class="hljs json">&#123;  <span class="hljs-attr">"signing"</span>: &#123;    <span class="hljs-attr">"default"</span>: &#123;        <span class="hljs-attr">"usages"</span>: [          <span class="hljs-string">"signing"</span>,          <span class="hljs-string">"key encipherment"</span>,          <span class="hljs-string">"server auth"</span>,          <span class="hljs-string">"client auth"</span>        ],        <span class="hljs-attr">"expiry"</span>: <span class="hljs-string">"87600h"</span>    &#125;  &#125;&#125;</code></pre><ul><li>etcd-csr.json</li></ul><pre><code class="hljs json">&#123;    <span class="hljs-attr">"key"</span>: &#123;        <span class="hljs-attr">"algo"</span>: <span class="hljs-string">"rsa"</span>,        <span class="hljs-attr">"size"</span>: <span class="hljs-number">2048</span>    &#125;,    <span class="hljs-attr">"names"</span>: [        &#123;            <span class="hljs-attr">"O"</span>: <span class="hljs-string">"etcd"</span>,            <span class="hljs-attr">"OU"</span>: <span class="hljs-string">"etcd Security"</span>,            <span class="hljs-attr">"L"</span>: <span class="hljs-string">"Beijing"</span>,            <span class="hljs-attr">"ST"</span>: <span class="hljs-string">"Beijing"</span>,            <span class="hljs-attr">"C"</span>: <span class="hljs-string">"CN"</span>        &#125;    ],    <span class="hljs-attr">"CN"</span>: <span class="hljs-string">"etcd"</span>,    <span class="hljs-attr">"hosts"</span>: [        <span class="hljs-string">"127.0.0.1"</span>,        <span class="hljs-string">"localhost"</span>,        <span class="hljs-string">"192.168.1.51"</span>,        <span class="hljs-string">"192.168.1.52"</span>,        <span class="hljs-string">"192.168.1.53"</span>    ]&#125;</code></pre><p>接下来执行生成即可；<strong>我建议在生产环境在证书内预留几个 IP，已防止意外故障迁移时还需要重新生成证书；证书默认期限为 10 年(包括 CA 证书)，有需要加强安全性的可以适当减小</strong></p><pre><code class="hljs sh">cfssl gencert --initca=<span class="hljs-literal">true</span> etcd-root-ca-csr.json | cfssljson --bare etcd-root-cacfssl gencert --ca etcd-root-ca.pem --ca-key etcd-root-ca-key.pem --config etcd-gencert.json etcd-csr.json | cfssljson --bare etcd</code></pre><h3 id="2-2、安装-Etcd"><a href="#2-2、安装-Etcd" class="headerlink" title="2.2、安装 Etcd"></a>2.2、安装 Etcd</h3><h4 id="2-2-1、安装脚本"><a href="#2-2-1、安装脚本" class="headerlink" title="2.2.1、安装脚本"></a>2.2.1、安装脚本</h4><p>安装 Etcd 只需要将二进制文件放在可执行目录下，然后修改配置增加 systemd service 配置文件即可；为了安全性起见最好使用单独的用户启动 Etcd</p><pre><code class="hljs sh"><span class="hljs-meta">#!/bin/bash</span><span class="hljs-built_in">set</span> -eETCD_DEFAULT_VERSION=<span class="hljs-string">"3.3.12"</span><span class="hljs-keyword">if</span> [ <span class="hljs-string">"<span class="hljs-variable">$1</span>"</span> != <span class="hljs-string">""</span> ]; <span class="hljs-keyword">then</span>  ETCD_VERSION=<span class="hljs-variable">$1</span><span class="hljs-keyword">else</span>  <span class="hljs-built_in">echo</span> -e <span class="hljs-string">"\033[33mWARNING: ETCD_VERSION is blank,use default version: <span class="hljs-variable">$&#123;ETCD_DEFAULT_VERSION&#125;</span>\033[0m"</span>  ETCD_VERSION=<span class="hljs-variable">$&#123;ETCD_DEFAULT_VERSION&#125;</span><span class="hljs-keyword">fi</span><span class="hljs-comment"># 下载 Etcd 二进制文件</span><span class="hljs-keyword">function</span> <span class="hljs-function"><span class="hljs-title">download</span></span>()&#123;    <span class="hljs-keyword">if</span> [ ! -f <span class="hljs-string">"etcd-v<span class="hljs-variable">$&#123;ETCD_VERSION&#125;</span>-linux-amd64.tar.gz"</span> ]; <span class="hljs-keyword">then</span>        wget https://github.com/coreos/etcd/releases/download/v<span class="hljs-variable">$&#123;ETCD_VERSION&#125;</span>/etcd-v<span class="hljs-variable">$&#123;ETCD_VERSION&#125;</span>-linux-amd64.tar.gz        tar -zxvf etcd-v<span class="hljs-variable">$&#123;ETCD_VERSION&#125;</span>-linux-amd64.tar.gz    <span class="hljs-keyword">fi</span>&#125;<span class="hljs-comment"># 为 Etcd 创建单独的用户</span><span class="hljs-keyword">function</span> <span class="hljs-function"><span class="hljs-title">preinstall</span></span>()&#123;getent group etcd &gt;/dev/null || groupadd -r etcdgetent passwd etcd &gt;/dev/null || useradd -r -g etcd -d /var/lib/etcd -s /sbin/nologin -c <span class="hljs-string">"etcd user"</span> etcd&#125;<span class="hljs-comment"># 安装(复制文件)</span><span class="hljs-keyword">function</span> <span class="hljs-function"><span class="hljs-title">install</span></span>()&#123;    <span class="hljs-comment"># 释放 Etcd 二进制文件</span>    <span class="hljs-built_in">echo</span> -e <span class="hljs-string">"\033[32mINFO: Copy etcd...\033[0m"</span>    tar -zxvf etcd-v<span class="hljs-variable">$&#123;ETCD_VERSION&#125;</span>-linux-amd64.tar.gz    cp etcd-v<span class="hljs-variable">$&#123;ETCD_VERSION&#125;</span>-linux-amd64/etcd* /usr/<span class="hljs-built_in">local</span>/bin    rm -rf etcd-v<span class="hljs-variable">$&#123;ETCD_VERSION&#125;</span>-linux-amd64    <span class="hljs-comment"># 复制 配置文件 到 /etc/etcd(目录内文件结构在下面)</span>    <span class="hljs-built_in">echo</span> -e <span class="hljs-string">"\033[32mINFO: Copy etcd config...\033[0m"</span>    cp -r conf /etc/etcd    chown -R etcd:etcd /etc/etcd    chmod -R 755 /etc/etcd/ssl    <span class="hljs-comment"># 复制 systemd service 配置</span>    <span class="hljs-built_in">echo</span> -e <span class="hljs-string">"\033[32mINFO: Copy etcd systemd config...\033[0m"</span>    cp systemd/*.service /lib/systemd/system    systemctl daemon-reload&#125;<span class="hljs-comment"># 创建 Etcd 存储目录(如需要更改，请求改 /etc/etcd/etcd.conf 配置文件)</span><span class="hljs-keyword">function</span> <span class="hljs-function"><span class="hljs-title">postinstall</span></span>()&#123;    <span class="hljs-keyword">if</span> [ ! -d <span class="hljs-string">"/var/lib/etcd"</span> ]; <span class="hljs-keyword">then</span>        mkdir /var/lib/etcd        chown -R etcd:etcd /var/lib/etcd    <span class="hljs-keyword">fi</span>&#125;<span class="hljs-comment"># 依次执行</span>downloadpreinstallinstallpostinstall</code></pre><h4 id="2-2-2、配置文件"><a href="#2-2-2、配置文件" class="headerlink" title="2.2.2、配置文件"></a>2.2.2、配置文件</h4><p><strong>关于配置文件目录结构如下(请自行复制证书)</strong></p><pre><code class="hljs sh">conf├── etcd.conf├── etcd.conf.cluster.example├── etcd.conf.single.example└── ssl    ├── etcd-key.pem    ├── etcd.pem    ├── etcd-root-ca-key.pem    └── etcd-root-ca.pem1 directory, 7 files</code></pre><ul><li>etcd.conf</li></ul><pre><code class="hljs sh"><span class="hljs-comment"># [member]</span>ETCD_NAME=etcd1ETCD_DATA_DIR=<span class="hljs-string">"/var/lib/etcd/data"</span>ETCD_WAL_DIR=<span class="hljs-string">"/var/lib/etcd/wal"</span>ETCD_SNAPSHOT_COUNT=<span class="hljs-string">"100"</span>ETCD_HEARTBEAT_INTERVAL=<span class="hljs-string">"100"</span>ETCD_ELECTION_TIMEOUT=<span class="hljs-string">"1000"</span>ETCD_LISTEN_PEER_URLS=<span class="hljs-string">"https://192.168.1.51:2380"</span>ETCD_LISTEN_CLIENT_URLS=<span class="hljs-string">"https://192.168.1.51:2379,http://127.0.0.1:2379"</span>ETCD_MAX_SNAPSHOTS=<span class="hljs-string">"5"</span>ETCD_MAX_WALS=<span class="hljs-string">"5"</span><span class="hljs-comment">#ETCD_CORS=""</span><span class="hljs-comment"># [cluster]</span>ETCD_INITIAL_ADVERTISE_PEER_URLS=<span class="hljs-string">"https://192.168.1.51:2380"</span><span class="hljs-comment"># if you use different ETCD_NAME (e.g. test), set ETCD_INITIAL_CLUSTER value for this name, i.e. "test=http://..."</span>ETCD_INITIAL_CLUSTER=<span class="hljs-string">"etcd1=https://192.168.1.51:2380,etcd2=https://192.168.1.52:2380,etcd3=https://192.168.1.53:2380"</span>ETCD_INITIAL_CLUSTER_STATE=<span class="hljs-string">"new"</span>ETCD_INITIAL_CLUSTER_TOKEN=<span class="hljs-string">"etcd-cluster"</span>ETCD_ADVERTISE_CLIENT_URLS=<span class="hljs-string">"https://192.168.1.51:2379"</span><span class="hljs-comment">#ETCD_DISCOVERY=""</span><span class="hljs-comment">#ETCD_DISCOVERY_SRV=""</span><span class="hljs-comment">#ETCD_DISCOVERY_FALLBACK="proxy"</span><span class="hljs-comment">#ETCD_DISCOVERY_PROXY=""</span><span class="hljs-comment">#ETCD_STRICT_RECONFIG_CHECK="false"</span><span class="hljs-comment">#ETCD_AUTO_COMPACTION_RETENTION="0"</span><span class="hljs-comment"># [proxy]</span><span class="hljs-comment">#ETCD_PROXY="off"</span><span class="hljs-comment">#ETCD_PROXY_FAILURE_WAIT="5000"</span><span class="hljs-comment">#ETCD_PROXY_REFRESH_INTERVAL="30000"</span><span class="hljs-comment">#ETCD_PROXY_DIAL_TIMEOUT="1000"</span><span class="hljs-comment">#ETCD_PROXY_WRITE_TIMEOUT="5000"</span><span class="hljs-comment">#ETCD_PROXY_READ_TIMEOUT="0"</span><span class="hljs-comment"># [security]</span>ETCD_CERT_FILE=<span class="hljs-string">"/etc/etcd/ssl/etcd.pem"</span>ETCD_KEY_FILE=<span class="hljs-string">"/etc/etcd/ssl/etcd-key.pem"</span>ETCD_CLIENT_CERT_AUTH=<span class="hljs-string">"true"</span>ETCD_TRUSTED_CA_FILE=<span class="hljs-string">"/etc/etcd/ssl/etcd-root-ca.pem"</span>ETCD_AUTO_TLS=<span class="hljs-string">"true"</span>ETCD_PEER_CERT_FILE=<span class="hljs-string">"/etc/etcd/ssl/etcd.pem"</span>ETCD_PEER_KEY_FILE=<span class="hljs-string">"/etc/etcd/ssl/etcd-key.pem"</span>ETCD_PEER_CLIENT_CERT_AUTH=<span class="hljs-string">"true"</span>ETCD_PEER_TRUSTED_CA_FILE=<span class="hljs-string">"/etc/etcd/ssl/etcd-root-ca.pem"</span>ETCD_PEER_AUTO_TLS=<span class="hljs-string">"true"</span><span class="hljs-comment"># [logging]</span><span class="hljs-comment">#ETCD_DEBUG="false"</span><span class="hljs-comment"># examples for -log-package-levels etcdserver=WARNING,security=DEBUG</span><span class="hljs-comment">#ETCD_LOG_PACKAGE_LEVELS=""</span></code></pre><ul><li>etcd.service</li></ul><pre><code class="hljs sh">[Unit]Description=Etcd ServerAfter=network.targetAfter=network-online.targetWants=network-online.target[Service]Type=notifyWorkingDirectory=/var/lib/etcd/EnvironmentFile=-/etc/etcd/etcd.confUser=etcd<span class="hljs-comment"># set GOMAXPROCS to number of processors</span>ExecStart=/bin/bash -c <span class="hljs-string">"GOMAXPROCS=<span class="hljs-variable">$(nproc)</span> /usr/local/bin/etcd --name=\"<span class="hljs-variable">$&#123;ETCD_NAME&#125;</span>\" --data-dir=\"<span class="hljs-variable">$&#123;ETCD_DATA_DIR&#125;</span>\" --listen-client-urls=\"<span class="hljs-variable">$&#123;ETCD_LISTEN_CLIENT_URLS&#125;</span>\""</span>Restart=on-failureLimitNOFILE=65536[Install]WantedBy=multi-user.target</code></pre><p>最后三台机器依次修改 <code>IP</code>、<code>ETCD_NAME</code> 然后启动即可，<strong>生产环境请不要忘记修改集群 Token 为真实随机字符串 (<code>ETCD_INITIAL_CLUSTER_TOKEN</code> 变量)</strong>启动后可以通过以下命令测试集群联通性</p><pre><code class="hljs sh">docker1.node ➜  ~ <span class="hljs-built_in">export</span> ETCDCTL_API=3docker1.node ➜  ~ etcdctl member list238b72cdd26e304f, started, etcd2, https://192.168.1.52:2380, https://192.168.1.52:23798034142cf01c5d1c, started, etcd3, https://192.168.1.53:2380, https://192.168.1.53:23798da171dbef9ded69, started, etcd1, https://192.168.1.51:2380, https://192.168.1.51:2379</code></pre><h2 id="三、安装-Kubernetes"><a href="#三、安装-Kubernetes" class="headerlink" title="三、安装 Kubernetes"></a>三、安装 Kubernetes</h2><h3 id="3-1、生成证书及配置"><a href="#3-1、生成证书及配置" class="headerlink" title="3.1、生成证书及配置"></a>3.1、生成证书及配置</h3><h4 id="3-1-1、生成证书"><a href="#3-1-1、生成证书" class="headerlink" title="3.1.1、生成证书"></a>3.1.1、生成证书</h4><p>新版本已经越来越趋近全面 TLS + RBAC 配置，<strong>所以本次安装将会启动大部分 TLS + RBAC 配置，包括 <code>kube-controler-manager</code>、<code>kube-scheduler</code> 组件不再连接本地 <code>kube-apiserver</code> 的 8080 非认证端口，<code>kubelet</code> 等组件 API 端点关闭匿名访问，启动 RBAC 认证等</strong>；为了满足这些认证，需要签署以下证书</p><ul><li>k8s-root-ca-csr.json 集群 CA 根证书</li></ul><pre><code class="hljs json">&#123;    <span class="hljs-attr">"CN"</span>: <span class="hljs-string">"kubernetes"</span>,    <span class="hljs-attr">"key"</span>: &#123;        <span class="hljs-attr">"algo"</span>: <span class="hljs-string">"rsa"</span>,        <span class="hljs-attr">"size"</span>: <span class="hljs-number">4096</span>    &#125;,    <span class="hljs-attr">"names"</span>: [        &#123;            <span class="hljs-attr">"C"</span>: <span class="hljs-string">"CN"</span>,            <span class="hljs-attr">"ST"</span>: <span class="hljs-string">"BeiJing"</span>,            <span class="hljs-attr">"L"</span>: <span class="hljs-string">"BeiJing"</span>,            <span class="hljs-attr">"O"</span>: <span class="hljs-string">"kubernetes"</span>,            <span class="hljs-attr">"OU"</span>: <span class="hljs-string">"System"</span>        &#125;    ],    <span class="hljs-attr">"ca"</span>: &#123;        <span class="hljs-attr">"expiry"</span>: <span class="hljs-string">"87600h"</span>    &#125;&#125;</code></pre><ul><li>k8s-gencert.json 用于生成其他证书的标准配置</li></ul><pre><code class="hljs json">&#123;    <span class="hljs-attr">"signing"</span>: &#123;        <span class="hljs-attr">"default"</span>: &#123;            <span class="hljs-attr">"expiry"</span>: <span class="hljs-string">"87600h"</span>        &#125;,        <span class="hljs-attr">"profiles"</span>: &#123;            <span class="hljs-attr">"kubernetes"</span>: &#123;                <span class="hljs-attr">"usages"</span>: [                    <span class="hljs-string">"signing"</span>,                    <span class="hljs-string">"key encipherment"</span>,                    <span class="hljs-string">"server auth"</span>,                    <span class="hljs-string">"client auth"</span>                ],                <span class="hljs-attr">"expiry"</span>: <span class="hljs-string">"87600h"</span>            &#125;        &#125;    &#125;&#125;</code></pre><ul><li>kube-apiserver-csr.json apiserver TLS 认证端口需要的证书</li></ul><pre><code class="hljs json">&#123;    <span class="hljs-attr">"CN"</span>: <span class="hljs-string">"kubernetes"</span>,    <span class="hljs-attr">"hosts"</span>: [        <span class="hljs-string">"127.0.0.1"</span>,        <span class="hljs-string">"10.254.0.1"</span>,        <span class="hljs-string">"localhost"</span>,        <span class="hljs-string">"*.master.kubernetes.node"</span>,        <span class="hljs-string">"kubernetes"</span>,        <span class="hljs-string">"kubernetes.default"</span>,        <span class="hljs-string">"kubernetes.default.svc"</span>,        <span class="hljs-string">"kubernetes.default.svc.cluster"</span>,        <span class="hljs-string">"kubernetes.default.svc.cluster.local"</span>    ],    <span class="hljs-attr">"key"</span>: &#123;        <span class="hljs-attr">"algo"</span>: <span class="hljs-string">"rsa"</span>,        <span class="hljs-attr">"size"</span>: <span class="hljs-number">2048</span>    &#125;,    <span class="hljs-attr">"names"</span>: [        &#123;            <span class="hljs-attr">"C"</span>: <span class="hljs-string">"CN"</span>,            <span class="hljs-attr">"ST"</span>: <span class="hljs-string">"BeiJing"</span>,            <span class="hljs-attr">"L"</span>: <span class="hljs-string">"BeiJing"</span>,            <span class="hljs-attr">"O"</span>: <span class="hljs-string">"kubernetes"</span>,            <span class="hljs-attr">"OU"</span>: <span class="hljs-string">"System"</span>        &#125;    ]&#125;</code></pre><ul><li>kube-controller-manager-csr.json controller manager 连接 apiserver 需要使用的证书，同时本身 <code>10257</code> 端口也会使用此证书</li></ul><pre><code class="hljs json">&#123;  <span class="hljs-attr">"CN"</span>: <span class="hljs-string">"system:kube-controller-manager"</span>,  <span class="hljs-attr">"hosts"</span>: [    <span class="hljs-string">"127.0.0.1"</span>,    <span class="hljs-string">"localhost"</span>,    <span class="hljs-string">"*.master.kubernetes.node"</span>  ],  <span class="hljs-attr">"key"</span>: &#123;    <span class="hljs-attr">"algo"</span>: <span class="hljs-string">"rsa"</span>,    <span class="hljs-attr">"size"</span>: <span class="hljs-number">2048</span>  &#125;,  <span class="hljs-attr">"names"</span>: [    &#123;      <span class="hljs-attr">"C"</span>: <span class="hljs-string">"CN"</span>,      <span class="hljs-attr">"ST"</span>: <span class="hljs-string">"BeiJing"</span>,      <span class="hljs-attr">"L"</span>: <span class="hljs-string">"BeiJing"</span>,      <span class="hljs-attr">"O"</span>: <span class="hljs-string">"system:kube-controller-manager"</span>,      <span class="hljs-attr">"OU"</span>: <span class="hljs-string">"System"</span>    &#125;  ]&#125;</code></pre><ul><li>kube-scheduler-csr.json scheduler 连接 apiserver 需要使用的证书，同时本身 <code>10259</code> 端口也会使用此证书</li></ul><pre><code class="hljs json">&#123;  <span class="hljs-attr">"CN"</span>: <span class="hljs-string">"system:kube-scheduler"</span>,  <span class="hljs-attr">"hosts"</span>: [    <span class="hljs-string">"127.0.0.1"</span>,    <span class="hljs-string">"localhost"</span>,    <span class="hljs-string">"*.master.kubernetes.node"</span>  ],  <span class="hljs-attr">"key"</span>: &#123;    <span class="hljs-attr">"algo"</span>: <span class="hljs-string">"rsa"</span>,    <span class="hljs-attr">"size"</span>: <span class="hljs-number">2048</span>  &#125;,  <span class="hljs-attr">"names"</span>: [    &#123;      <span class="hljs-attr">"C"</span>: <span class="hljs-string">"CN"</span>,      <span class="hljs-attr">"ST"</span>: <span class="hljs-string">"BeiJing"</span>,      <span class="hljs-attr">"L"</span>: <span class="hljs-string">"BeiJing"</span>,      <span class="hljs-attr">"O"</span>: <span class="hljs-string">"system:kube-scheduler"</span>,      <span class="hljs-attr">"OU"</span>: <span class="hljs-string">"System"</span>    &#125;  ]&#125;</code></pre><ul><li>kube-proxy-csr.json proxy 组件连接 apiserver 需要使用的证书</li></ul><pre><code class="hljs json">&#123;    <span class="hljs-attr">"CN"</span>: <span class="hljs-string">"system:kube-proxy"</span>,    <span class="hljs-attr">"hosts"</span>: [],    <span class="hljs-attr">"key"</span>: &#123;        <span class="hljs-attr">"algo"</span>: <span class="hljs-string">"rsa"</span>,        <span class="hljs-attr">"size"</span>: <span class="hljs-number">2048</span>    &#125;,    <span class="hljs-attr">"names"</span>: [        &#123;            <span class="hljs-attr">"C"</span>: <span class="hljs-string">"CN"</span>,            <span class="hljs-attr">"ST"</span>: <span class="hljs-string">"BeiJing"</span>,            <span class="hljs-attr">"L"</span>: <span class="hljs-string">"BeiJing"</span>,            <span class="hljs-attr">"O"</span>: <span class="hljs-string">"system:kube-proxy"</span>,            <span class="hljs-attr">"OU"</span>: <span class="hljs-string">"System"</span>        &#125;    ]&#125;</code></pre><ul><li>kubelet-api-admin-csr.json apiserver 反向连接 kubelet 组件 <code>10250</code> 端口需要使用的证书(例如执行 <code>kubectl logs</code>)</li></ul><pre><code class="hljs json">&#123;    <span class="hljs-attr">"CN"</span>: <span class="hljs-string">"system:kubelet-api-admin"</span>,    <span class="hljs-attr">"hosts"</span>: [],    <span class="hljs-attr">"key"</span>: &#123;        <span class="hljs-attr">"algo"</span>: <span class="hljs-string">"rsa"</span>,        <span class="hljs-attr">"size"</span>: <span class="hljs-number">2048</span>    &#125;,    <span class="hljs-attr">"names"</span>: [        &#123;            <span class="hljs-attr">"C"</span>: <span class="hljs-string">"CN"</span>,            <span class="hljs-attr">"ST"</span>: <span class="hljs-string">"BeiJing"</span>,            <span class="hljs-attr">"L"</span>: <span class="hljs-string">"BeiJing"</span>,            <span class="hljs-attr">"O"</span>: <span class="hljs-string">"system:kubelet-api-admin"</span>,            <span class="hljs-attr">"OU"</span>: <span class="hljs-string">"System"</span>        &#125;    ]&#125;</code></pre><ul><li>admin-csr.json 集群管理员(kubectl)连接 apiserver 需要使用的证书</li></ul><pre><code class="hljs json">&#123;    <span class="hljs-attr">"CN"</span>: <span class="hljs-string">"system:masters"</span>,    <span class="hljs-attr">"hosts"</span>: [],    <span class="hljs-attr">"key"</span>: &#123;        <span class="hljs-attr">"algo"</span>: <span class="hljs-string">"rsa"</span>,        <span class="hljs-attr">"size"</span>: <span class="hljs-number">2048</span>    &#125;,    <span class="hljs-attr">"names"</span>: [        &#123;            <span class="hljs-attr">"C"</span>: <span class="hljs-string">"CN"</span>,            <span class="hljs-attr">"ST"</span>: <span class="hljs-string">"BeiJing"</span>,            <span class="hljs-attr">"L"</span>: <span class="hljs-string">"BeiJing"</span>,            <span class="hljs-attr">"O"</span>: <span class="hljs-string">"system:masters"</span>,            <span class="hljs-attr">"OU"</span>: <span class="hljs-string">"System"</span>        &#125;    ]&#125;</code></pre><p><strong>注意: 请不要修改证书配置的 <code>CN</code>、<code>O</code> 字段，这两个字段名称比较特殊，大多数为 <code>system:</code> 开头，实际上是为了匹配 RBAC 规则，具体请参考 <a href="https://kubernetes.io/docs/reference/access-authn-authz/rbac/#default-roles-and-role-bindings" target="_blank" rel="noopener">Default Roles and Role Bindings</a></strong></p><p>最后使用如下命令生成即可:</p><pre><code class="hljs sh">cfssl gencert --initca=<span class="hljs-literal">true</span> k8s-root-ca-csr.json | cfssljson --bare k8s-root-ca<span class="hljs-keyword">for</span> targetName <span class="hljs-keyword">in</span> kube-apiserver kube-controller-manager kube-scheduler kube-proxy kubelet-api-admin admin; <span class="hljs-keyword">do</span>    cfssl gencert --ca k8s-root-ca.pem --ca-key k8s-root-ca-key.pem --config k8s-gencert.json --profile kubernetes <span class="hljs-variable">$targetName</span>-csr.json | cfssljson --bare <span class="hljs-variable">$targetName</span><span class="hljs-keyword">done</span></code></pre><h4 id="3-1-2、生成配置文件"><a href="#3-1-2、生成配置文件" class="headerlink" title="3.1.2、生成配置文件"></a>3.1.2、生成配置文件</h4><p>集群搭建需要预先生成一系列配置文件，生成配置需要预先安装 <code>kubectl</code> 命令，请自行根据文档安装 <a href="https://kubernetes.io/docs/tasks/tools/install-kubectl/#install-kubectl-binary-using-curl" target="_blank" rel="noopener">Install kubectl binary using curl</a>；其中配置文件及其作用如下:</p><ul><li><code>bootstrap.kubeconfig</code> kubelet TLS Bootstarp 引导阶段需要使用的配置文件</li><li><code>kube-controller-manager.kubeconfig</code> controller manager 组件开启安全端口及 RBAC 认证所需配置</li><li><code>kube-scheduler.kubeconfig</code> scheduler 组件开启安全端口及 RBAC 认证所需配置</li><li><code>kube-proxy.kubeconfig</code> proxy 组件连接 apiserver 所需配置文件</li><li><code>audit-policy.yaml</code> apiserver RBAC 审计日志配置文件</li><li><code>bootstrap.secret.yaml</code> kubelet TLS Bootstarp 引导阶段使用 Bootstrap Token 方式引导，需要预先创建此 Token</li></ul><p>生成这些配置文件的脚本如下</p><pre><code class="hljs sh"><span class="hljs-comment"># 指定 apiserver 地址</span>KUBE_APISERVER=<span class="hljs-string">"https://127.0.0.1:6443"</span><span class="hljs-comment"># 生成 Bootstrap Token</span>BOOTSTRAP_TOKEN_ID=$(head -c 6 /dev/urandom | md5sum | head -c 6)BOOTSTRAP_TOKEN_SECRET=$(head -c 16 /dev/urandom | md5sum | head -c 16)BOOTSTRAP_TOKEN=<span class="hljs-string">"<span class="hljs-variable">$&#123;BOOTSTRAP_TOKEN_ID&#125;</span>.<span class="hljs-variable">$&#123;BOOTSTRAP_TOKEN_SECRET&#125;</span>"</span><span class="hljs-built_in">echo</span> <span class="hljs-string">"Bootstrap Tokne: <span class="hljs-variable">$&#123;BOOTSTRAP_TOKEN&#125;</span>"</span><span class="hljs-comment"># 生成 kubelet tls bootstrap 配置</span><span class="hljs-built_in">echo</span> <span class="hljs-string">"Create kubelet bootstrapping kubeconfig..."</span>kubectl config <span class="hljs-built_in">set</span>-cluster kubernetes \  --certificate-authority=k8s-root-ca.pem \  --embed-certs=<span class="hljs-literal">true</span> \  --server=<span class="hljs-variable">$&#123;KUBE_APISERVER&#125;</span> \  --kubeconfig=bootstrap.kubeconfigkubectl config <span class="hljs-built_in">set</span>-credentials <span class="hljs-string">"system:bootstrap:<span class="hljs-variable">$&#123;BOOTSTRAP_TOKEN_ID&#125;</span>"</span> \  --token=<span class="hljs-variable">$&#123;BOOTSTRAP_TOKEN&#125;</span> \  --kubeconfig=bootstrap.kubeconfigkubectl config <span class="hljs-built_in">set</span>-context default \  --cluster=kubernetes \  --user=<span class="hljs-string">"system:bootstrap:<span class="hljs-variable">$&#123;BOOTSTRAP_TOKEN_ID&#125;</span>"</span> \  --kubeconfig=bootstrap.kubeconfigkubectl config use-context default --kubeconfig=bootstrap.kubeconfig<span class="hljs-comment"># 生成 kube-controller-manager 配置文件</span><span class="hljs-built_in">echo</span> <span class="hljs-string">"Create kube-controller-manager kubeconfig..."</span>kubectl config <span class="hljs-built_in">set</span>-cluster kubernetes \  --certificate-authority=k8s-root-ca.pem \  --embed-certs=<span class="hljs-literal">true</span> \  --server=<span class="hljs-variable">$&#123;KUBE_APISERVER&#125;</span> \  --kubeconfig=kube-controller-manager.kubeconfigkubectl config <span class="hljs-built_in">set</span>-credentials <span class="hljs-string">"system:kube-controller-manager"</span> \  --client-certificate=kube-controller-manager.pem \  --client-key=kube-controller-manager-key.pem \  --embed-certs=<span class="hljs-literal">true</span> \  --kubeconfig=kube-controller-manager.kubeconfigkubectl config <span class="hljs-built_in">set</span>-context default \  --cluster=kubernetes \  --user=system:kube-controller-manager \  --kubeconfig=kube-controller-manager.kubeconfigkubectl config use-context default --kubeconfig=kube-controller-manager.kubeconfig <span class="hljs-comment"># 生成 kube-scheduler 配置文件</span><span class="hljs-built_in">echo</span> <span class="hljs-string">"Create kube-scheduler kubeconfig..."</span>kubectl config <span class="hljs-built_in">set</span>-cluster kubernetes \  --certificate-authority=k8s-root-ca.pem \  --embed-certs=<span class="hljs-literal">true</span> \  --server=<span class="hljs-variable">$&#123;KUBE_APISERVER&#125;</span> \  --kubeconfig=kube-scheduler.kubeconfigkubectl config <span class="hljs-built_in">set</span>-credentials <span class="hljs-string">"system:kube-scheduler"</span> \  --client-certificate=kube-scheduler.pem \  --client-key=kube-scheduler-key.pem \  --embed-certs=<span class="hljs-literal">true</span> \  --kubeconfig=kube-scheduler.kubeconfigkubectl config <span class="hljs-built_in">set</span>-context default \  --cluster=kubernetes \  --user=system:kube-scheduler \  --kubeconfig=kube-scheduler.kubeconfigkubectl config use-context default --kubeconfig=kube-scheduler.kubeconfig <span class="hljs-comment"># 生成 kube-proxy 配置文件</span><span class="hljs-built_in">echo</span> <span class="hljs-string">"Create kube-proxy kubeconfig..."</span>kubectl config <span class="hljs-built_in">set</span>-cluster kubernetes \  --certificate-authority=k8s-root-ca.pem \  --embed-certs=<span class="hljs-literal">true</span> \  --server=<span class="hljs-variable">$&#123;KUBE_APISERVER&#125;</span> \  --kubeconfig=kube-proxy.kubeconfigkubectl config <span class="hljs-built_in">set</span>-credentials <span class="hljs-string">"system:kube-proxy"</span> \  --client-certificate=kube-proxy.pem \  --client-key=kube-proxy-key.pem \  --embed-certs=<span class="hljs-literal">true</span> \  --kubeconfig=kube-proxy.kubeconfigkubectl config <span class="hljs-built_in">set</span>-context default \  --cluster=kubernetes \  --user=system:kube-proxy \  --kubeconfig=kube-proxy.kubeconfigkubectl config use-context default --kubeconfig=kube-proxy.kubeconfig <span class="hljs-comment"># 生成 apiserver RBAC 审计配置文件 </span>cat &gt;&gt; audit-policy.yaml &lt;&lt;EOF<span class="hljs-comment"># Log all requests at the Metadata level.</span>apiVersion: audit.k8s.io/v1kind: Policyrules:- level: MetadataEOF<span class="hljs-comment"># 生成 tls bootstrap token secret 配置文件</span>cat &gt;&gt; bootstrap.secret.yaml &lt;&lt;EOFapiVersion: v1kind: Secretmetadata:  <span class="hljs-comment"># Name MUST be of form "bootstrap-token-&lt;token id&gt;"</span>  name: bootstrap-token-<span class="hljs-variable">$&#123;BOOTSTRAP_TOKEN_ID&#125;</span>  namespace: kube-system<span class="hljs-comment"># Type MUST be 'bootstrap.kubernetes.io/token'</span><span class="hljs-built_in">type</span>: bootstrap.kubernetes.io/tokenstringData:  <span class="hljs-comment"># Human readable description. Optional.</span>  description: <span class="hljs-string">"The default bootstrap token."</span>  <span class="hljs-comment"># Token ID and secret. Required.</span>  token-id: <span class="hljs-variable">$&#123;BOOTSTRAP_TOKEN_ID&#125;</span>  token-secret: <span class="hljs-variable">$&#123;BOOTSTRAP_TOKEN_SECRET&#125;</span>  <span class="hljs-comment"># Expiration. Optional.</span>  expiration: $(date -d<span class="hljs-string">'+2 day'</span> -u +<span class="hljs-string">"%Y-%m-%dT%H:%M:%SZ"</span>)  <span class="hljs-comment"># Allowed usages.</span>  usage-bootstrap-authentication: <span class="hljs-string">"true"</span>  usage-bootstrap-signing: <span class="hljs-string">"true"</span>  <span class="hljs-comment"># Extra groups to authenticate the token as. Must start with "system:bootstrappers:"</span><span class="hljs-comment">#  auth-extra-groups: system:bootstrappers:worker,system:bootstrappers:ingress</span>EOF</code></pre><h3 id="3-2、处理-ipvs-及依赖"><a href="#3-2、处理-ipvs-及依赖" class="headerlink" title="3.2、处理 ipvs 及依赖"></a>3.2、处理 ipvs 及依赖</h3><p>新版本目前 <code>kube-proxy</code> 组件全部采用 ipvs 方式负载，所以为了 <code>kube-proxy</code> 能正常工作需要预先处理一下 ipvs 配置以及相关依赖(每台 node 都要处理)</p><pre><code class="hljs sh">cat &gt;&gt; /etc/sysctl.conf &lt;&lt;EOFnet.ipv4.ip_forward=1net.bridge.bridge-nf-call-iptables=1net.bridge.bridge-nf-call-ip6tables=1EOFsysctl -pcat &gt;&gt; /etc/modules &lt;&lt;EOFip_vsip_vs_lcip_vs_wlcip_vs_rrip_vs_wrrip_vs_lblcip_vs_lblcrip_vs_dhip_vs_ship_vs_foip_vs_nqip_vs_sedip_vs_ftpEOFapt install -y conntrack ipvsadm</code></pre><h3 id="3-3、部署-Master"><a href="#3-3、部署-Master" class="headerlink" title="3.3、部署 Master"></a>3.3、部署 Master</h3><h4 id="3-3-1、安装脚本"><a href="#3-3-1、安装脚本" class="headerlink" title="3.3.1、安装脚本"></a>3.3.1、安装脚本</h4><p>master 节点上需要三个组件: <code>kube-apiserver</code>、<code>kube-controller-manager</code>、<code>kube-scheduler</code></p><p><strong>安装流程整体为以下几步</strong></p><ul><li><strong>创建单独的 <code>kube</code> 用户</strong></li><li><strong>复制相关二进制文件到 <code>/usr/bin</code>，可以采用 <code>all in one</code> 的 <code>hyperkube</code></strong></li><li><strong>复制配置文件到 <code>/etc/kubernetes</code></strong></li><li><strong>复制证书文件到 <code>/etc/kubernetes/ssl</code></strong></li><li><strong>修改配置并启动</strong></li></ul><p>安装脚本如下所示:</p><pre><code class="hljs sh">KUBE_DEFAULT_VERSION=<span class="hljs-string">"1.13.4"</span><span class="hljs-keyword">if</span> [ <span class="hljs-string">"<span class="hljs-variable">$1</span>"</span> != <span class="hljs-string">""</span> ]; <span class="hljs-keyword">then</span>  KUBE_VERSION=<span class="hljs-variable">$1</span><span class="hljs-keyword">else</span>  <span class="hljs-built_in">echo</span> -e <span class="hljs-string">"\033[33mWARNING: KUBE_VERSION is blank,use default version: <span class="hljs-variable">$&#123;KUBE_DEFAULT_VERSION&#125;</span>\033[0m"</span>  KUBE_VERSION=<span class="hljs-variable">$&#123;KUBE_DEFAULT_VERSION&#125;</span><span class="hljs-keyword">fi</span><span class="hljs-comment"># 下载 hyperkube</span><span class="hljs-keyword">function</span> <span class="hljs-function"><span class="hljs-title">download_k8s</span></span>()&#123;    <span class="hljs-keyword">if</span> [ ! -f <span class="hljs-string">"hyperkube_v<span class="hljs-variable">$&#123;KUBE_VERSION&#125;</span>"</span> ]; <span class="hljs-keyword">then</span>        wget https://storage.googleapis.com/kubernetes-release/release/v<span class="hljs-variable">$&#123;KUBE_VERSION&#125;</span>/bin/linux/amd64/hyperkube -O hyperkube_v<span class="hljs-variable">$&#123;KUBE_VERSION&#125;</span>        chmod +x hyperkube_v<span class="hljs-variable">$&#123;KUBE_VERSION&#125;</span>    <span class="hljs-keyword">fi</span>&#125;<span class="hljs-comment"># 创建专用用户 kube</span><span class="hljs-keyword">function</span> <span class="hljs-function"><span class="hljs-title">preinstall</span></span>()&#123;    getent group kube &gt;/dev/null || groupadd -r kube    getent passwd kube &gt;/dev/null || useradd -r -g kube -d / -s /sbin/nologin -c <span class="hljs-string">"Kubernetes user"</span> kube&#125;<span class="hljs-comment"># 复制可执行文件和配置以及证书</span><span class="hljs-keyword">function</span> <span class="hljs-function"><span class="hljs-title">install_k8s</span></span>()&#123;    <span class="hljs-built_in">echo</span> -e <span class="hljs-string">"\033[32mINFO: Copy hyperkube...\033[0m"</span>    cp hyperkube_v<span class="hljs-variable">$&#123;KUBE_VERSION&#125;</span> /usr/bin/hyperkube    <span class="hljs-built_in">echo</span> -e <span class="hljs-string">"\033[32mINFO: Create symbolic link...\033[0m"</span>    (<span class="hljs-built_in">cd</span> /usr/bin &amp;&amp; hyperkube --make-symlinks)    <span class="hljs-built_in">echo</span> -e <span class="hljs-string">"\033[32mINFO: Copy kubernetes config...\033[0m"</span>    cp -r conf /etc/kubernetes    <span class="hljs-keyword">if</span> [ -d <span class="hljs-string">"/etc/kubernetes/ssl"</span> ]; <span class="hljs-keyword">then</span>        chown -R kube:kube /etc/kubernetes/ssl    <span class="hljs-keyword">fi</span>    <span class="hljs-built_in">echo</span> -e <span class="hljs-string">"\033[32mINFO: Copy kubernetes systemd config...\033[0m"</span>    cp systemd/*.service /lib/systemd/system    systemctl daemon-reload&#125;<span class="hljs-comment"># 创建必要的目录并修改权限</span><span class="hljs-keyword">function</span> <span class="hljs-function"><span class="hljs-title">postinstall</span></span>()&#123;    <span class="hljs-keyword">if</span> [ ! -d <span class="hljs-string">"/var/log/kube-audit"</span> ]; <span class="hljs-keyword">then</span>        mkdir /var/<span class="hljs-built_in">log</span>/kube-audit    <span class="hljs-keyword">fi</span>        <span class="hljs-keyword">if</span> [ ! -d <span class="hljs-string">"/var/lib/kubelet"</span> ]; <span class="hljs-keyword">then</span>        mkdir /var/lib/kubelet    <span class="hljs-keyword">fi</span>    <span class="hljs-keyword">if</span> [ ! -d <span class="hljs-string">"/usr/libexec"</span> ]; <span class="hljs-keyword">then</span>        mkdir /usr/libexec    <span class="hljs-keyword">fi</span>    chown -R kube:kube /etc/kubernetes /var/<span class="hljs-built_in">log</span>/kube-audit /var/lib/kubelet /usr/libexec&#125;<span class="hljs-comment"># 执行</span>download_k8spreinstallinstall_k8spostinstall</code></pre><p><strong>hyperkube 是一个多合一的可执行文件，通过 <code>--make-symlinks</code> 会在当前目录生成 kubernetes 各个组件的软连接</strong></p><p>被复制的 conf 目录结构如下(最终被复制到 <code>/etc/kubernetes</code>)</p><pre><code class="hljs sh">.├── apiserver├── audit-policy.yaml├── bootstrap.kubeconfig├── bootstrap.secret.yaml├── controller-manager├── kube-controller-manager.kubeconfig├── kubelet├── kube-proxy.kubeconfig├── kube-scheduler.kubeconfig├── proxy├── scheduler└── ssl    ├── admin-key.pem    ├── admin.pem    ├── k8s-root-ca-key.pem    ├── k8s-root-ca.pem    ├── kube-apiserver-key.pem    ├── kube-apiserver.pem    ├── kube-controller-manager-key.pem    ├── kube-controller-manager.pem    ├── kubelet-api-admin-key.pem    ├── kubelet-api-admin.pem    ├── kube-proxy-key.pem    ├── kube-proxy.pem    ├── kube-scheduler-key.pem    └── kube-scheduler.pem1 directory, 25 files</code></pre><h4 id="3-3-2、配置文件"><a href="#3-3-2、配置文件" class="headerlink" title="3.3.2、配置文件"></a>3.3.2、配置文件</h4><p>以下为相关配置文件内容</p><p><strong>systemd 配置如下</strong></p><ul><li>kube-apiserver.service</li></ul><pre><code class="hljs sh">[Unit]Description=Kubernetes API ServerDocumentation=https://github.com/GoogleCloudPlatform/kubernetesAfter=network.targetAfter=etcd.service[Service]EnvironmentFile=-/etc/kubernetes/apiserverUser=kubeExecStart=/usr/bin/kube-apiserver \    <span class="hljs-variable">$KUBE_LOGTOSTDERR</span> \    <span class="hljs-variable">$KUBE_LOG_LEVEL</span> \    <span class="hljs-variable">$KUBE_ETCD_SERVERS</span> \    <span class="hljs-variable">$KUBE_API_ADDRESS</span> \    <span class="hljs-variable">$KUBE_API_PORT</span> \    <span class="hljs-variable">$KUBELET_PORT</span> \    <span class="hljs-variable">$KUBE_ALLOW_PRIV</span> \    <span class="hljs-variable">$KUBE_SERVICE_ADDRESSES</span> \    <span class="hljs-variable">$KUBE_ADMISSION_CONTROL</span> \    <span class="hljs-variable">$KUBE_API_ARGS</span>Restart=on-failureType=notifyLimitNOFILE=65536[Install]WantedBy=multi-user.target</code></pre><ul><li>kube-controller-manager.service</li></ul><pre><code class="hljs sh">[Unit]Description=Kubernetes Controller ManagerDocumentation=https://github.com/GoogleCloudPlatform/kubernetes[Service]EnvironmentFile=-/etc/kubernetes/controller-managerUser=kubeExecStart=/usr/bin/kube-controller-manager \    <span class="hljs-variable">$KUBE_LOGTOSTDERR</span> \    <span class="hljs-variable">$KUBE_LOG_LEVEL</span> \    <span class="hljs-variable">$KUBE_MASTER</span> \    <span class="hljs-variable">$KUBE_CONTROLLER_MANAGER_ARGS</span>Restart=on-failureLimitNOFILE=65536[Install]WantedBy=multi-user.target</code></pre><ul><li>kube-scheduler.service</li></ul><pre><code class="hljs sh">[Unit]Description=Kubernetes Scheduler PluginDocumentation=https://github.com/GoogleCloudPlatform/kubernetes[Service]EnvironmentFile=-/etc/kubernetes/schedulerUser=kubeExecStart=/usr/bin/kube-scheduler \    <span class="hljs-variable">$KUBE_LOGTOSTDERR</span> \    <span class="hljs-variable">$KUBE_LOG_LEVEL</span> \    <span class="hljs-variable">$KUBE_MASTER</span> \    <span class="hljs-variable">$KUBE_SCHEDULER_ARGS</span>Restart=on-failureLimitNOFILE=65536[Install]WantedBy=multi-user.target</code></pre><p><strong>核心配置文件</strong></p><ul><li>apiserver</li></ul><pre><code class="hljs sh"><span class="hljs-comment">###</span><span class="hljs-comment"># kubernetes system config</span><span class="hljs-comment">#</span><span class="hljs-comment"># The following values are used to configure the kube-apiserver</span><span class="hljs-comment">#</span><span class="hljs-comment"># The address on the local server to listen to.</span>KUBE_API_ADDRESS=<span class="hljs-string">"--advertise-address=192.168.1.51 --bind-address=0.0.0.0"</span><span class="hljs-comment"># The port on the local server to listen on.</span>KUBE_API_PORT=<span class="hljs-string">"--secure-port=6443"</span><span class="hljs-comment"># Port minions listen on</span><span class="hljs-comment"># KUBELET_PORT="--kubelet-port=10250"</span><span class="hljs-comment"># Comma separated list of nodes in the etcd cluster</span>KUBE_ETCD_SERVERS=<span class="hljs-string">"--etcd-servers=https://192.168.1.51:2379,https://192.168.1.52:2379,https://192.168.1.53:2379"</span><span class="hljs-comment"># Address range to use for services</span>KUBE_SERVICE_ADDRESSES=<span class="hljs-string">"--service-cluster-ip-range=10.254.0.0/16"</span><span class="hljs-comment"># default admission control policies</span>KUBE_ADMISSION_CONTROL=<span class="hljs-string">"--enable-admission-plugins=NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,Priority,ResourceQuota"</span><span class="hljs-comment"># Add your own!</span>KUBE_API_ARGS=<span class="hljs-string">" --allow-privileged=true \</span><span class="hljs-string">                --anonymous-auth=false \</span><span class="hljs-string">                --alsologtostderr \</span><span class="hljs-string">                --apiserver-count=3 \</span><span class="hljs-string">                --audit-log-maxage=30 \</span><span class="hljs-string">                --audit-log-maxbackup=3 \</span><span class="hljs-string">                --audit-log-maxsize=100 \</span><span class="hljs-string">                --audit-log-path=/var/log/kube-audit/audit.log \</span><span class="hljs-string">                --audit-policy-file=/etc/kubernetes/audit-policy.yaml \</span><span class="hljs-string">                --authorization-mode=Node,RBAC \</span><span class="hljs-string">                --client-ca-file=/etc/kubernetes/ssl/k8s-root-ca.pem \</span><span class="hljs-string">                --enable-bootstrap-token-auth \</span><span class="hljs-string">                --enable-garbage-collector \</span><span class="hljs-string">                --enable-logs-handler \</span><span class="hljs-string">                --endpoint-reconciler-type=lease \</span><span class="hljs-string">                --etcd-cafile=/etc/etcd/ssl/etcd-root-ca.pem \</span><span class="hljs-string">                --etcd-certfile=/etc/etcd/ssl/etcd.pem \</span><span class="hljs-string">                --etcd-keyfile=/etc/etcd/ssl/etcd-key.pem \</span><span class="hljs-string">                --etcd-compaction-interval=0s \</span><span class="hljs-string">                --event-ttl=168h0m0s \</span><span class="hljs-string">                --kubelet-https=true \</span><span class="hljs-string">                --kubelet-certificate-authority=/etc/kubernetes/ssl/k8s-root-ca.pem \</span><span class="hljs-string">                --kubelet-client-certificate=/etc/kubernetes/ssl/kubelet-api-admin.pem \</span><span class="hljs-string">                --kubelet-client-key=/etc/kubernetes/ssl/kubelet-api-admin-key.pem \</span><span class="hljs-string">                --kubelet-timeout=3s \</span><span class="hljs-string">                --runtime-config=api/all=true \</span><span class="hljs-string">                --service-node-port-range=30000-50000 \</span><span class="hljs-string">                --service-account-key-file=/etc/kubernetes/ssl/k8s-root-ca.pem \</span><span class="hljs-string">                --tls-cert-file=/etc/kubernetes/ssl/kube-apiserver.pem \</span><span class="hljs-string">                --tls-private-key-file=/etc/kubernetes/ssl/kube-apiserver-key.pem \</span><span class="hljs-string">                --v=2"</span></code></pre><p>配置解释:</p><table><thead><tr><th>选项</th><th>作用</th></tr></thead><tbody><tr><td><code>--client-ca-file</code></td><td>定义客户端 CA</td></tr><tr><td><code>--endpoint-reconciler-type</code></td><td>master endpoint 策略</td></tr><tr><td><code>--kubelet-client-certificate</code>、<code>--kubelet-client-key</code></td><td>master 反向连接 kubelet 使用的证书</td></tr><tr><td><code>--service-account-key-file</code></td><td>service account 签名 key(用于有效性验证)</td></tr><tr><td><code>--tls-cert-file</code>、<code>--tls-private-key-file</code></td><td>master apiserver <code>6443</code> 端口证书</td></tr></tbody></table><ul><li>controller-manager</li></ul><pre><code class="hljs sh"><span class="hljs-comment">###</span><span class="hljs-comment"># The following values are used to configure the kubernetes controller-manager</span><span class="hljs-comment"># defaults from config and apiserver should be adequate</span><span class="hljs-comment"># Add your own!</span>KUBE_CONTROLLER_MANAGER_ARGS=<span class="hljs-string">"  --address=127.0.0.1 \</span><span class="hljs-string">                                --authentication-kubeconfig=/etc/kubernetes/kube-controller-manager.kubeconfig \</span><span class="hljs-string">                                --authorization-kubeconfig=/etc/kubernetes/kube-controller-manager.kubeconfig \</span><span class="hljs-string">                                --bind-address=0.0.0.0 \</span><span class="hljs-string">                                --cluster-name=kubernetes \</span><span class="hljs-string">                                --cluster-signing-cert-file=/etc/kubernetes/ssl/k8s-root-ca.pem \</span><span class="hljs-string">                                --cluster-signing-key-file=/etc/kubernetes/ssl/k8s-root-ca-key.pem \</span><span class="hljs-string">                                --client-ca-file=/etc/kubernetes/ssl/k8s-root-ca.pem \</span><span class="hljs-string">                                --controllers=*,bootstrapsigner,tokencleaner \</span><span class="hljs-string">                                --deployment-controller-sync-period=10s \</span><span class="hljs-string">                                --experimental-cluster-signing-duration=87600h0m0s \</span><span class="hljs-string">                                --enable-garbage-collector=true \</span><span class="hljs-string">                                --kubeconfig=/etc/kubernetes/kube-controller-manager.kubeconfig \</span><span class="hljs-string">                                --leader-elect=true \</span><span class="hljs-string">                                --node-monitor-grace-period=20s \</span><span class="hljs-string">                                --node-monitor-period=5s \</span><span class="hljs-string">                                --port=10252 \</span><span class="hljs-string">                                --pod-eviction-timeout=2m0s \</span><span class="hljs-string">                                --requestheader-client-ca-file=/etc/kubernetes/ssl/k8s-root-ca.pem \</span><span class="hljs-string">                                --terminated-pod-gc-threshold=50 \</span><span class="hljs-string">                                --tls-cert-file=/etc/kubernetes/ssl/kube-controller-manager.pem \</span><span class="hljs-string">                                --tls-private-key-file=/etc/kubernetes/ssl/kube-controller-manager-key.pem \</span><span class="hljs-string">                                --root-ca-file=/etc/kubernetes/ssl/k8s-root-ca.pem \</span><span class="hljs-string">                                --secure-port=10257 \</span><span class="hljs-string">                                --service-cluster-ip-range=10.254.0.0/16 \</span><span class="hljs-string">                                --service-account-private-key-file=/etc/kubernetes/ssl/k8s-root-ca-key.pem \</span><span class="hljs-string">                                --use-service-account-credentials=true \</span><span class="hljs-string">                                --v=2"</span></code></pre><p>controller manager 将不安全端口 <code>10252</code> 绑定到 127.0.0.1 确保 <code>kuebctl get cs</code> 有正确返回；将安全端口 <code>10257</code> 绑定到 0.0.0.0 公开，提供服务调用；<strong>由于 controller manager 开始连接 apiserver 的 <code>6443</code> 认证端口，所以需要 <code>--use-service-account-credentials</code> 选项来让 controller manager 创建单独的 service account(默认 <code>system:kube-controller-manager</code> 用户没有那么高权限)</strong></p><ul><li>scheduler</li></ul><pre><code class="hljs sh"><span class="hljs-comment">###</span><span class="hljs-comment"># kubernetes scheduler config</span><span class="hljs-comment"># default config should be adequate</span><span class="hljs-comment"># Add your own!</span>KUBE_SCHEDULER_ARGS=<span class="hljs-string">"   --address=127.0.0.1 \</span><span class="hljs-string">                        --authentication-kubeconfig=/etc/kubernetes/kube-scheduler.kubeconfig \</span><span class="hljs-string">                        --authorization-kubeconfig=/etc/kubernetes/kube-scheduler.kubeconfig \</span><span class="hljs-string">                        --bind-address=0.0.0.0 \</span><span class="hljs-string">                        --client-ca-file=/etc/kubernetes/ssl/k8s-root-ca.pem \</span><span class="hljs-string">                        --kubeconfig=/etc/kubernetes/kube-scheduler.kubeconfig \</span><span class="hljs-string">                        --requestheader-client-ca-file=/etc/kubernetes/ssl/k8s-root-ca.pem \</span><span class="hljs-string">                        --secure-port=10259 \</span><span class="hljs-string">                        --leader-elect=true \</span><span class="hljs-string">                        --port=10251 \</span><span class="hljs-string">                        --tls-cert-file=/etc/kubernetes/ssl/kube-scheduler.pem \</span><span class="hljs-string">                        --tls-private-key-file=/etc/kubernetes/ssl/kube-scheduler-key.pem \</span><span class="hljs-string">                        --v=2"</span></code></pre><p>shceduler 同  controller manager 一样将不安全端口绑定在本地，安全端口对外公开</p><p><strong>最后在三台节点上调整一下 IP 配置，启动即可</strong></p><h3 id="3-4、部署-Node"><a href="#3-4、部署-Node" class="headerlink" title="3.4、部署 Node"></a>3.4、部署 Node</h3><h4 id="3-4-1、安装脚本"><a href="#3-4-1、安装脚本" class="headerlink" title="3.4.1、安装脚本"></a>3.4.1、安装脚本</h4><p>node 安装与 master 安装过程一致，这里不再阐述</p><h4 id="3-4-2、配置文件"><a href="#3-4-2、配置文件" class="headerlink" title="3.4.2、配置文件"></a>3.4.2、配置文件</h4><p><strong>systemd 配置文件</strong></p><ul><li>kubelet.service</li></ul><pre><code class="hljs sh">[Unit]Description=Kubernetes Kubelet ServerDocumentation=https://github.com/GoogleCloudPlatform/kubernetesAfter=docker.serviceRequires=docker.service[Service]WorkingDirectory=/var/lib/kubeletEnvironmentFile=-/etc/kubernetes/kubeletExecStart=/usr/bin/kubelet \    <span class="hljs-variable">$KUBE_LOGTOSTDERR</span> \    <span class="hljs-variable">$KUBE_LOG_LEVEL</span> \    <span class="hljs-variable">$KUBELET_API_SERVER</span> \    <span class="hljs-variable">$KUBELET_ADDRESS</span> \    <span class="hljs-variable">$KUBELET_PORT</span> \    <span class="hljs-variable">$KUBELET_HOSTNAME</span> \    <span class="hljs-variable">$KUBE_ALLOW_PRIV</span> \    <span class="hljs-variable">$KUBELET_ARGS</span>Restart=on-failureKillMode=process[Install]WantedBy=multi-user.target</code></pre><ul><li>kube-proxy.service</li></ul><pre><code class="hljs sh">[Unit]Description=Kubernetes Kube-Proxy ServerDocumentation=https://github.com/GoogleCloudPlatform/kubernetesAfter=network.target[Service]EnvironmentFile=-/etc/kubernetes/proxyExecStart=/usr/bin/kube-proxy \    <span class="hljs-variable">$KUBE_LOGTOSTDERR</span> \    <span class="hljs-variable">$KUBE_LOG_LEVEL</span> \    <span class="hljs-variable">$KUBE_MASTER</span> \    <span class="hljs-variable">$KUBE_PROXY_ARGS</span>Restart=on-failureLimitNOFILE=65536[Install]WantedBy=multi-user.target</code></pre><p><strong>核心配置文件</strong></p><ul><li>kubelet</li></ul><pre><code class="hljs sh"><span class="hljs-comment">###</span><span class="hljs-comment"># kubernetes kubelet (minion) config</span><span class="hljs-comment"># The address for the info server to serve on (set to 0.0.0.0 or "" for all interfaces)</span>KUBELET_ADDRESS=<span class="hljs-string">"--node-ip=192.168.1.54"</span><span class="hljs-comment"># The port for the info server to serve on</span><span class="hljs-comment"># KUBELET_PORT="--port=10250"</span><span class="hljs-comment"># You may leave this blank to use the actual hostname</span>KUBELET_HOSTNAME=<span class="hljs-string">"--hostname-override=docker4.node"</span><span class="hljs-comment"># location of the api-server</span><span class="hljs-comment"># KUBELET_API_SERVER=""</span><span class="hljs-comment"># Add your own!</span>KUBELET_ARGS=<span class="hljs-string">"  --address=0.0.0.0 \</span><span class="hljs-string">                --allow-privileged \</span><span class="hljs-string">                --anonymous-auth=false \</span><span class="hljs-string">                --authorization-mode=Webhook \</span><span class="hljs-string">                --bootstrap-kubeconfig=/etc/kubernetes/bootstrap.kubeconfig \</span><span class="hljs-string">                --client-ca-file=/etc/kubernetes/ssl/k8s-root-ca.pem \</span><span class="hljs-string">                --network-plugin=cni \</span><span class="hljs-string">                --cgroup-driver=cgroupfs \</span><span class="hljs-string">                --cert-dir=/etc/kubernetes/ssl \</span><span class="hljs-string">                --cluster-dns=10.254.0.2 \</span><span class="hljs-string">                --cluster-domain=cluster.local \</span><span class="hljs-string">                --cni-conf-dir=/etc/cni/net.d \</span><span class="hljs-string">                --eviction-soft=imagefs.available&lt;15%,memory.available&lt;512Mi,nodefs.available&lt;15%,nodefs.inodesFree&lt;10% \</span><span class="hljs-string">                --eviction-soft-grace-period=imagefs.available=3m,memory.available=1m,nodefs.available=3m,nodefs.inodesFree=1m \</span><span class="hljs-string">                --eviction-hard=imagefs.available&lt;10%,memory.available&lt;256Mi,nodefs.available&lt;10%,nodefs.inodesFree&lt;5% \</span><span class="hljs-string">                --eviction-max-pod-grace-period=30 \</span><span class="hljs-string">                --image-gc-high-threshold=80 \</span><span class="hljs-string">                --image-gc-low-threshold=70 \</span><span class="hljs-string">                --image-pull-progress-deadline=30s \</span><span class="hljs-string">                --kube-reserved=cpu=500m,memory=512Mi,ephemeral-storage=1Gi \</span><span class="hljs-string">                --kubeconfig=/etc/kubernetes/kubelet.kubeconfig \</span><span class="hljs-string">                --max-pods=100 \</span><span class="hljs-string">                --minimum-image-ttl-duration=720h0m0s \</span><span class="hljs-string">                --node-labels=node.kubernetes.io/k8s-node=true \</span><span class="hljs-string">                --pod-infra-container-image=gcr.azk8s.cn/google_containers/pause-amd64:3.1 \</span><span class="hljs-string">                --port=10250 \</span><span class="hljs-string">                --read-only-port=0 \</span><span class="hljs-string">                --rotate-certificates \</span><span class="hljs-string">                --rotate-server-certificates \</span><span class="hljs-string">                --resolv-conf=/run/systemd/resolve/resolv.conf \</span><span class="hljs-string">                --system-reserved=cpu=500m,memory=512Mi,ephemeral-storage=1Gi \</span><span class="hljs-string">                --fail-swap-on=false \</span><span class="hljs-string">                --v=2"</span></code></pre><p><strong>当 kubelet 组件设置了 <code>--rotate-certificates</code>，<code>--rotate-server-certificates</code> 后会自动更新其使用的相关证书，同时指定 <code>--authorization-mode=Webhook</code> 后 <code>10250</code> 端口 RBAC 授权将会委托给 apiserver</strong></p><ul><li>proxy</li></ul><pre><code class="hljs sh"><span class="hljs-comment">###</span><span class="hljs-comment"># kubernetes proxy config</span><span class="hljs-comment"># default config should be adequate</span><span class="hljs-comment"># Add your own!</span>KUBE_PROXY_ARGS=<span class="hljs-string">"   --bind-address=0.0.0.0 \</span><span class="hljs-string">                    --cleanup-ipvs=true \</span><span class="hljs-string">                    --cluster-cidr=10.254.0.0/16 \</span><span class="hljs-string">                    --hostname-override=docker4.node \</span><span class="hljs-string">                    --healthz-bind-address=0.0.0.0 \</span><span class="hljs-string">                    --healthz-port=10256 \</span><span class="hljs-string">                    --masquerade-all=true \</span><span class="hljs-string">                    --proxy-mode=ipvs \</span><span class="hljs-string">                    --ipvs-min-sync-period=5s \</span><span class="hljs-string">                    --ipvs-sync-period=5s \</span><span class="hljs-string">                    --ipvs-scheduler=wrr \</span><span class="hljs-string">                    --kubeconfig=/etc/kubernetes/kube-proxy.kubeconfig \</span><span class="hljs-string">                    --logtostderr=true \</span><span class="hljs-string">                    --v=2"</span></code></pre><p>由于 <code>kubelet</code> 组件是采用 TLS Bootstrap 启动，所以需要预先创建相关配置</p><pre><code class="hljs sh"><span class="hljs-comment"># 创建用于 tls bootstrap 的 token secret</span>kubectl create -f bootstrap.secret.yaml<span class="hljs-comment"># 为了能让 kubelet 实现自动更新证书，需要配置相关 clusterrolebinding</span><span class="hljs-comment"># 允许 kubelet tls bootstrap 创建 csr 请求</span>kubectl create clusterrolebinding create-csrs-for-bootstrapping \    --clusterrole=system:node-bootstrapper \    --group=system:bootstrappers<span class="hljs-comment"># 自动批准 system:bootstrappers 组用户 TLS bootstrapping 首次申请证书的 CSR 请求</span>kubectl create clusterrolebinding auto-approve-csrs-for-group \    --clusterrole=system:certificates.k8s.io:certificatesigningrequests:nodeclient \    --group=system:bootstrappers<span class="hljs-comment"># 自动批准 system:nodes 组用户更新 kubelet 自身与 apiserver 通讯证书的 CSR 请求</span>kubectl create clusterrolebinding auto-approve-renewals-for-nodes \    --clusterrole=system:certificates.k8s.io:certificatesigningrequests:selfnodeclient \    --group=system:nodes<span class="hljs-comment"># 在 kubelet server 开启 api 认证的情况下，apiserver 反向访问 kubelet 10250 需要此授权(eg: kubectl logs)</span>kubectl create clusterrolebinding system:kubelet-api-admin \    --clusterrole=system:kubelet-api-admin \    --user=system:kubelet-api-admin</code></pre><h4 id="3-4-3、Nginx-代理"><a href="#3-4-3、Nginx-代理" class="headerlink" title="3.4.3、Nginx 代理"></a>3.4.3、Nginx 代理</h4><p>为了保证 apiserver 的 HA，需要在每个 node 上部署 nginx 来反向代理(tcp)所有 apiserver；然后 kubelet、kube-proxy 组件连接本地 <code>127.0.0.1:6443</code> 访问 apiserver，以确保任何 master 挂掉以后 node 都不会受到影响</p><ul><li>nginx.conf</li></ul><pre><code class="hljs sh">error_log stderr notice;worker_processes auto;events &#123;  multi_accept on;  use epoll;  worker_connections 1024;&#125;stream &#123;    upstream kube_apiserver &#123;        least_conn;        server 192.168.1.51:6443;        server 192.168.1.52:6443;        server 192.168.1.53:6443;    &#125;    server &#123;        listen        0.0.0.0:6443;        proxy_pass    kube_apiserver;        proxy_timeout 10m;        proxy_connect_timeout 1s;    &#125;&#125;</code></pre><ul><li>nginx-proxy.service</li></ul><pre><code class="hljs sh">[Unit]Description=kubernetes apiserver docker wrapperWants=docker.socketAfter=docker.service[Service]User=rootPermissionsStartOnly=<span class="hljs-literal">true</span>ExecStart=/usr/bin/docker run -p 127.0.0.1:6443:6443 \                              -v /etc/nginx:/etc/nginx \                              --name nginx-proxy \                              --net=host \                              --restart=on-failure:5 \                              --memory=512M \                              nginx:1.14.2-alpineExecStartPre=-/usr/bin/docker rm -f nginx-proxyExecStop=/usr/bin/docker stop nginx-proxyRestart=alwaysRestartSec=15sTimeoutStartSec=30s[Install]WantedBy=multi-user.target</code></pre><p>然后在每个 node 上先启动 nginx-proxy，接着启动 kubelet 与 kube-proxy 即可(master 上的 kubelet、kube-proxy 只需要修改 ip 和 node name)</p><h4 id="3-4-4、kubelet-server-证书"><a href="#3-4-4、kubelet-server-证书" class="headerlink" title="3.4.4、kubelet server 证书"></a>3.4.4、kubelet server 证书</h4><p><strong>注意: 新版本 kubelet server 的证书自动签发已经被关闭(看 issue 好像是由于安全原因)，所以对于 kubelet server 的证书仍需要手动签署</strong></p><pre><code class="hljs sh">docker1.node ➜  ~ kubectl get csrNAME                                                   AGE   REQUESTOR                  CONDITIONcsr-99l77                                              10s   system:node:docker4.node   Pendingnode-csr-aGwaNKorMc0MZBYOuJsJGCB8Bg8ds97rmE3oKBTV-_E   11s   system:bootstrap:5d820b    Approved,Issueddocker1.node ➜  ~ kubectl certificate approve csr-99l77certificatesigningrequest.certificates.k8s.io/csr-99l77 approved</code></pre><h3 id="3-5、部署-Calico"><a href="#3-5、部署-Calico" class="headerlink" title="3.5、部署 Calico"></a>3.5、部署 Calico</h3><p>当 node 全部启动后，由于网络组件(CNI)未安装会显示为 NotReady 状态；下面将部署 Calico 作为网络组件，完成跨节点网络通讯；具体安装文档可以参考 <a href="https://docs.projectcalico.org/v3.6/getting-started/kubernetes/installation/calico#installing-with-the-etcd-datastore" target="_blank" rel="noopener">Installing with the etcd datastore</a></p><p>以下为 calico 的配置文件</p><ul><li>calico.yaml</li></ul><pre><code class="hljs yaml"><span class="hljs-meta">---</span><span class="hljs-comment"># Source: calico/templates/calico-etcd-secrets.yaml</span><span class="hljs-comment"># The following contains k8s Secrets for use with a TLS enabled etcd cluster.</span><span class="hljs-comment"># For information on populating Secrets, see http://kubernetes.io/docs/user-guide/secrets/</span><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">v1</span><span class="hljs-attr">kind:</span> <span class="hljs-string">Secret</span><span class="hljs-attr">type:</span> <span class="hljs-string">Opaque</span><span class="hljs-attr">metadata:</span>  <span class="hljs-attr">name:</span> <span class="hljs-string">calico-etcd-secrets</span>  <span class="hljs-attr">namespace:</span> <span class="hljs-string">kube-system</span><span class="hljs-attr">data:</span>  <span class="hljs-comment"># Populate the following with etcd TLS configuration if desired, but leave blank if</span>  <span class="hljs-comment"># not using TLS for etcd.</span>  <span class="hljs-comment"># The keys below should be uncommented and the values populated with the base64</span>  <span class="hljs-comment"># encoded contents of each file that would be associated with the TLS data.</span>  <span class="hljs-comment"># Example command for encoding a file contents: cat &lt;file&gt; | base64 -w 0</span>  <span class="hljs-attr">etcd-key:</span> <span class="hljs-string">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</span>  <span class="hljs-attr">etcd-cert:</span> <span class="hljs-string">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</span>  <span class="hljs-attr">etcd-ca:</span> <span class="hljs-string">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</span><span class="hljs-meta">---</span><span class="hljs-comment"># Source: calico/templates/calico-config.yaml</span><span class="hljs-comment"># This ConfigMap is used to configure a self-hosted Calico installation.</span><span class="hljs-attr">kind:</span> <span class="hljs-string">ConfigMap</span><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">v1</span><span class="hljs-attr">metadata:</span>  <span class="hljs-attr">name:</span> <span class="hljs-string">calico-config</span>  <span class="hljs-attr">namespace:</span> <span class="hljs-string">kube-system</span><span class="hljs-attr">data:</span>  <span class="hljs-comment"># Configure this with the location of your etcd cluster.</span>  <span class="hljs-attr">etcd_endpoints:</span> <span class="hljs-string">"https://192.168.1.51:2379,https://192.168.1.52:2379,https://192.168.1.53:2379"</span>  <span class="hljs-comment"># If you're using TLS enabled etcd uncomment the following.</span>  <span class="hljs-comment"># You must also populate the Secret below with these files.</span>  <span class="hljs-attr">etcd_ca:</span> <span class="hljs-string">"/calico-secrets/etcd-ca"</span>  <span class="hljs-attr">etcd_cert:</span> <span class="hljs-string">"/calico-secrets/etcd-cert"</span>  <span class="hljs-attr">etcd_key:</span> <span class="hljs-string">"/calico-secrets/etcd-key"</span>  <span class="hljs-comment"># Typha is disabled.</span>  <span class="hljs-attr">typha_service_name:</span> <span class="hljs-string">"none"</span>  <span class="hljs-comment"># Configure the Calico backend to use.</span>  <span class="hljs-attr">calico_backend:</span> <span class="hljs-string">"bird"</span>  <span class="hljs-comment"># Configure the MTU to use</span>  <span class="hljs-attr">veth_mtu:</span> <span class="hljs-string">"1440"</span>  <span class="hljs-comment"># The CNI network configuration to install on each node.  The special</span>  <span class="hljs-comment"># values in this config will be automatically populated.</span>  <span class="hljs-attr">cni_network_config:</span> <span class="hljs-string">|-</span>    <span class="hljs-string">&#123;</span>      <span class="hljs-attr">"name":</span> <span class="hljs-string">"k8s-pod-network"</span><span class="hljs-string">,</span>      <span class="hljs-attr">"cniVersion":</span> <span class="hljs-string">"0.3.0"</span><span class="hljs-string">,</span>      <span class="hljs-attr">"plugins":</span> <span class="hljs-string">[</span>        <span class="hljs-string">&#123;</span>          <span class="hljs-attr">"type":</span> <span class="hljs-string">"calico"</span><span class="hljs-string">,</span>          <span class="hljs-attr">"log_level":</span> <span class="hljs-string">"info"</span><span class="hljs-string">,</span>          <span class="hljs-attr">"etcd_endpoints":</span> <span class="hljs-string">"__ETCD_ENDPOINTS__"</span><span class="hljs-string">,</span>          <span class="hljs-attr">"etcd_key_file":</span> <span class="hljs-string">"__ETCD_KEY_FILE__"</span><span class="hljs-string">,</span>          <span class="hljs-attr">"etcd_cert_file":</span> <span class="hljs-string">"__ETCD_CERT_FILE__"</span><span class="hljs-string">,</span>          <span class="hljs-attr">"etcd_ca_cert_file":</span> <span class="hljs-string">"__ETCD_CA_CERT_FILE__"</span><span class="hljs-string">,</span>          <span class="hljs-attr">"mtu":</span> <span class="hljs-string">__CNI_MTU__,</span>          <span class="hljs-attr">"ipam":</span> <span class="hljs-string">&#123;</span>              <span class="hljs-attr">"type":</span> <span class="hljs-string">"calico-ipam"</span>          <span class="hljs-string">&#125;,</span>          <span class="hljs-attr">"policy":</span> <span class="hljs-string">&#123;</span>              <span class="hljs-attr">"type":</span> <span class="hljs-string">"k8s"</span>          <span class="hljs-string">&#125;,</span>          <span class="hljs-attr">"kubernetes":</span> <span class="hljs-string">&#123;</span>              <span class="hljs-attr">"kubeconfig":</span> <span class="hljs-string">"__KUBECONFIG_FILEPATH__"</span>          <span class="hljs-string">&#125;</span>        <span class="hljs-string">&#125;,</span>        <span class="hljs-string">&#123;</span>          <span class="hljs-attr">"type":</span> <span class="hljs-string">"portmap"</span><span class="hljs-string">,</span>          <span class="hljs-attr">"snat":</span> <span class="hljs-literal">true</span><span class="hljs-string">,</span>          <span class="hljs-attr">"capabilities":</span> <span class="hljs-string">&#123;"portMappings":</span> <span class="hljs-literal">true</span><span class="hljs-string">&#125;</span>        <span class="hljs-string">&#125;</span>      <span class="hljs-string">]</span>    <span class="hljs-string">&#125;</span><span class="hljs-meta">---</span><span class="hljs-comment"># Source: calico/templates/rbac.yaml</span><span class="hljs-comment"># Include a clusterrole for the kube-controllers component,</span><span class="hljs-comment"># and bind it to the calico-kube-controllers serviceaccount.</span><span class="hljs-attr">kind:</span> <span class="hljs-string">ClusterRole</span><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">rbac.authorization.k8s.io/v1beta1</span><span class="hljs-attr">metadata:</span>  <span class="hljs-attr">name:</span> <span class="hljs-string">calico-kube-controllers</span><span class="hljs-attr">rules:</span>  <span class="hljs-comment"># Pods are monitored for changing labels.</span>  <span class="hljs-comment"># The node controller monitors Kubernetes nodes.</span>  <span class="hljs-comment"># Namespace and serviceaccount labels are used for policy.</span>  <span class="hljs-bullet">-</span> <span class="hljs-attr">apiGroups:</span> <span class="hljs-string">[""]</span>    <span class="hljs-attr">resources:</span>      <span class="hljs-bullet">-</span> <span class="hljs-string">pods</span>      <span class="hljs-bullet">-</span> <span class="hljs-string">nodes</span>      <span class="hljs-bullet">-</span> <span class="hljs-string">namespaces</span>      <span class="hljs-bullet">-</span> <span class="hljs-string">serviceaccounts</span>    <span class="hljs-attr">verbs:</span>      <span class="hljs-bullet">-</span> <span class="hljs-string">watch</span>      <span class="hljs-bullet">-</span> <span class="hljs-string">list</span>  <span class="hljs-comment"># Watch for changes to Kubernetes NetworkPolicies.</span>  <span class="hljs-bullet">-</span> <span class="hljs-attr">apiGroups:</span> <span class="hljs-string">["networking.k8s.io"]</span>    <span class="hljs-attr">resources:</span>      <span class="hljs-bullet">-</span> <span class="hljs-string">networkpolicies</span>    <span class="hljs-attr">verbs:</span>      <span class="hljs-bullet">-</span> <span class="hljs-string">watch</span>      <span class="hljs-bullet">-</span> <span class="hljs-string">list</span><span class="hljs-meta">---</span><span class="hljs-attr">kind:</span> <span class="hljs-string">ClusterRoleBinding</span><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">rbac.authorization.k8s.io/v1beta1</span><span class="hljs-attr">metadata:</span>  <span class="hljs-attr">name:</span> <span class="hljs-string">calico-kube-controllers</span><span class="hljs-attr">roleRef:</span>  <span class="hljs-attr">apiGroup:</span> <span class="hljs-string">rbac.authorization.k8s.io</span>  <span class="hljs-attr">kind:</span> <span class="hljs-string">ClusterRole</span>  <span class="hljs-attr">name:</span> <span class="hljs-string">calico-kube-controllers</span><span class="hljs-attr">subjects:</span><span class="hljs-bullet">-</span> <span class="hljs-attr">kind:</span> <span class="hljs-string">ServiceAccount</span>  <span class="hljs-attr">name:</span> <span class="hljs-string">calico-kube-controllers</span>  <span class="hljs-attr">namespace:</span> <span class="hljs-string">kube-system</span><span class="hljs-meta">---</span><span class="hljs-comment"># Include a clusterrole for the calico-node DaemonSet,</span><span class="hljs-comment"># and bind it to the calico-node serviceaccount.</span><span class="hljs-attr">kind:</span> <span class="hljs-string">ClusterRole</span><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">rbac.authorization.k8s.io/v1beta1</span><span class="hljs-attr">metadata:</span>  <span class="hljs-attr">name:</span> <span class="hljs-string">calico-node</span><span class="hljs-attr">rules:</span>  <span class="hljs-comment"># The CNI plugin needs to get pods, nodes, and namespaces.</span>  <span class="hljs-bullet">-</span> <span class="hljs-attr">apiGroups:</span> <span class="hljs-string">[""]</span>    <span class="hljs-attr">resources:</span>      <span class="hljs-bullet">-</span> <span class="hljs-string">pods</span>      <span class="hljs-bullet">-</span> <span class="hljs-string">nodes</span>      <span class="hljs-bullet">-</span> <span class="hljs-string">namespaces</span>    <span class="hljs-attr">verbs:</span>      <span class="hljs-bullet">-</span> <span class="hljs-string">get</span>  <span class="hljs-bullet">-</span> <span class="hljs-attr">apiGroups:</span> <span class="hljs-string">[""]</span>    <span class="hljs-attr">resources:</span>      <span class="hljs-bullet">-</span> <span class="hljs-string">endpoints</span>      <span class="hljs-bullet">-</span> <span class="hljs-string">services</span>    <span class="hljs-attr">verbs:</span>      <span class="hljs-comment"># Used to discover service IPs for advertisement.</span>      <span class="hljs-bullet">-</span> <span class="hljs-string">watch</span>      <span class="hljs-bullet">-</span> <span class="hljs-string">list</span>  <span class="hljs-bullet">-</span> <span class="hljs-attr">apiGroups:</span> <span class="hljs-string">[""]</span>    <span class="hljs-attr">resources:</span>      <span class="hljs-bullet">-</span> <span class="hljs-string">nodes/status</span>    <span class="hljs-attr">verbs:</span>      <span class="hljs-comment"># Needed for clearing NodeNetworkUnavailable flag.</span>      <span class="hljs-bullet">-</span> <span class="hljs-string">patch</span><span class="hljs-meta">---</span><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">rbac.authorization.k8s.io/v1beta1</span><span class="hljs-attr">kind:</span> <span class="hljs-string">ClusterRoleBinding</span><span class="hljs-attr">metadata:</span>  <span class="hljs-attr">name:</span> <span class="hljs-string">calico-node</span><span class="hljs-attr">roleRef:</span>  <span class="hljs-attr">apiGroup:</span> <span class="hljs-string">rbac.authorization.k8s.io</span>  <span class="hljs-attr">kind:</span> <span class="hljs-string">ClusterRole</span>  <span class="hljs-attr">name:</span> <span class="hljs-string">calico-node</span><span class="hljs-attr">subjects:</span><span class="hljs-bullet">-</span> <span class="hljs-attr">kind:</span> <span class="hljs-string">ServiceAccount</span>  <span class="hljs-attr">name:</span> <span class="hljs-string">calico-node</span>  <span class="hljs-attr">namespace:</span> <span class="hljs-string">kube-system</span><span class="hljs-meta">---</span><span class="hljs-meta">---</span><span class="hljs-comment"># Source: calico/templates/calico-node.yaml</span><span class="hljs-comment"># This manifest installs the calico/node container, as well</span><span class="hljs-comment"># as the Calico CNI plugins and network config on</span><span class="hljs-comment"># each master and worker node in a Kubernetes cluster.</span><span class="hljs-attr">kind:</span> <span class="hljs-string">DaemonSet</span><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">extensions/v1beta1</span><span class="hljs-attr">metadata:</span>  <span class="hljs-attr">name:</span> <span class="hljs-string">calico-node</span>  <span class="hljs-attr">namespace:</span> <span class="hljs-string">kube-system</span>  <span class="hljs-attr">labels:</span>    <span class="hljs-attr">k8s-app:</span> <span class="hljs-string">calico-node</span><span class="hljs-attr">spec:</span>  <span class="hljs-attr">selector:</span>    <span class="hljs-attr">matchLabels:</span>      <span class="hljs-attr">k8s-app:</span> <span class="hljs-string">calico-node</span>  <span class="hljs-attr">updateStrategy:</span>    <span class="hljs-attr">type:</span> <span class="hljs-string">RollingUpdate</span>    <span class="hljs-attr">rollingUpdate:</span>      <span class="hljs-attr">maxUnavailable:</span> <span class="hljs-number">1</span>  <span class="hljs-attr">template:</span>    <span class="hljs-attr">metadata:</span>      <span class="hljs-attr">labels:</span>        <span class="hljs-attr">k8s-app:</span> <span class="hljs-string">calico-node</span>      <span class="hljs-attr">annotations:</span>        <span class="hljs-comment"># This, along with the CriticalAddonsOnly toleration below,</span>        <span class="hljs-comment"># marks the pod as a critical add-on, ensuring it gets</span>        <span class="hljs-comment"># priority scheduling and that its resources are reserved</span>        <span class="hljs-comment"># if it ever gets evicted.</span>        <span class="hljs-attr">scheduler.alpha.kubernetes.io/critical-pod:</span> <span class="hljs-string">''</span>    <span class="hljs-attr">spec:</span>      <span class="hljs-attr">nodeSelector:</span>        <span class="hljs-attr">beta.kubernetes.io/os:</span> <span class="hljs-string">linux</span>      <span class="hljs-attr">hostNetwork:</span> <span class="hljs-literal">true</span>      <span class="hljs-attr">tolerations:</span>        <span class="hljs-comment"># Make sure calico-node gets scheduled on all nodes.</span>        <span class="hljs-bullet">-</span> <span class="hljs-attr">effect:</span> <span class="hljs-string">NoSchedule</span>          <span class="hljs-attr">operator:</span> <span class="hljs-string">Exists</span>        <span class="hljs-comment"># Mark the pod as a critical add-on for rescheduling.</span>        <span class="hljs-bullet">-</span> <span class="hljs-attr">key:</span> <span class="hljs-string">CriticalAddonsOnly</span>          <span class="hljs-attr">operator:</span> <span class="hljs-string">Exists</span>        <span class="hljs-bullet">-</span> <span class="hljs-attr">effect:</span> <span class="hljs-string">NoExecute</span>          <span class="hljs-attr">operator:</span> <span class="hljs-string">Exists</span>      <span class="hljs-attr">serviceAccountName:</span> <span class="hljs-string">calico-node</span>      <span class="hljs-comment"># Minimize downtime during a rolling upgrade or deletion; tell Kubernetes to do a "force</span>      <span class="hljs-comment"># deletion": https://kubernetes.io/docs/concepts/workloads/pods/pod/#termination-of-pods.</span>      <span class="hljs-attr">terminationGracePeriodSeconds:</span> <span class="hljs-number">0</span>      <span class="hljs-attr">initContainers:</span>        <span class="hljs-comment"># This container installs the Calico CNI binaries</span>        <span class="hljs-comment"># and CNI network config file on each node.</span>        <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">install-cni</span>          <span class="hljs-attr">image:</span> <span class="hljs-string">calico/cni:v3.6.0</span>          <span class="hljs-attr">command:</span> <span class="hljs-string">["/install-cni.sh"]</span>          <span class="hljs-attr">env:</span>            <span class="hljs-comment"># Name of the CNI config file to create.</span>            <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">CNI_CONF_NAME</span>              <span class="hljs-attr">value:</span> <span class="hljs-string">"10-calico.conflist"</span>            <span class="hljs-comment"># The CNI network config to install on each node.</span>            <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">CNI_NETWORK_CONFIG</span>              <span class="hljs-attr">valueFrom:</span>                <span class="hljs-attr">configMapKeyRef:</span>                  <span class="hljs-attr">name:</span> <span class="hljs-string">calico-config</span>                  <span class="hljs-attr">key:</span> <span class="hljs-string">cni_network_config</span>            <span class="hljs-comment"># The location of the Calico etcd cluster.</span>            <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">ETCD_ENDPOINTS</span>              <span class="hljs-attr">valueFrom:</span>                <span class="hljs-attr">configMapKeyRef:</span>                  <span class="hljs-attr">name:</span> <span class="hljs-string">calico-config</span>                  <span class="hljs-attr">key:</span> <span class="hljs-string">etcd_endpoints</span>            <span class="hljs-comment"># CNI MTU Config variable</span>            <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">CNI_MTU</span>              <span class="hljs-attr">valueFrom:</span>                <span class="hljs-attr">configMapKeyRef:</span>                  <span class="hljs-attr">name:</span> <span class="hljs-string">calico-config</span>                  <span class="hljs-attr">key:</span> <span class="hljs-string">veth_mtu</span>            <span class="hljs-comment"># Prevents the container from sleeping forever.</span>            <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">SLEEP</span>              <span class="hljs-attr">value:</span> <span class="hljs-string">"false"</span>          <span class="hljs-attr">volumeMounts:</span>            <span class="hljs-bullet">-</span> <span class="hljs-attr">mountPath:</span> <span class="hljs-string">/host/opt/cni/bin</span>              <span class="hljs-attr">name:</span> <span class="hljs-string">cni-bin-dir</span>            <span class="hljs-bullet">-</span> <span class="hljs-attr">mountPath:</span> <span class="hljs-string">/host/etc/cni/net.d</span>              <span class="hljs-attr">name:</span> <span class="hljs-string">cni-net-dir</span>            <span class="hljs-bullet">-</span> <span class="hljs-attr">mountPath:</span> <span class="hljs-string">/calico-secrets</span>              <span class="hljs-attr">name:</span> <span class="hljs-string">etcd-certs</span>      <span class="hljs-attr">containers:</span>        <span class="hljs-comment"># Runs calico/node container on each Kubernetes node.  This</span>        <span class="hljs-comment"># container programs network policy and routes on each</span>        <span class="hljs-comment"># host.</span>        <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">calico-node</span>          <span class="hljs-attr">image:</span> <span class="hljs-string">calico/node:v3.6.0</span>          <span class="hljs-attr">env:</span>            <span class="hljs-comment"># The location of the Calico etcd cluster.</span>            <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">ETCD_ENDPOINTS</span>              <span class="hljs-attr">valueFrom:</span>                <span class="hljs-attr">configMapKeyRef:</span>                  <span class="hljs-attr">name:</span> <span class="hljs-string">calico-config</span>                  <span class="hljs-attr">key:</span> <span class="hljs-string">etcd_endpoints</span>            <span class="hljs-comment"># Location of the CA certificate for etcd.</span>            <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">ETCD_CA_CERT_FILE</span>              <span class="hljs-attr">valueFrom:</span>                <span class="hljs-attr">configMapKeyRef:</span>                  <span class="hljs-attr">name:</span> <span class="hljs-string">calico-config</span>                  <span class="hljs-attr">key:</span> <span class="hljs-string">etcd_ca</span>            <span class="hljs-comment"># Location of the client key for etcd.</span>            <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">ETCD_KEY_FILE</span>              <span class="hljs-attr">valueFrom:</span>                <span class="hljs-attr">configMapKeyRef:</span>                  <span class="hljs-attr">name:</span> <span class="hljs-string">calico-config</span>                  <span class="hljs-attr">key:</span> <span class="hljs-string">etcd_key</span>            <span class="hljs-comment"># Location of the client certificate for etcd.</span>            <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">ETCD_CERT_FILE</span>              <span class="hljs-attr">valueFrom:</span>                <span class="hljs-attr">configMapKeyRef:</span>                  <span class="hljs-attr">name:</span> <span class="hljs-string">calico-config</span>                  <span class="hljs-attr">key:</span> <span class="hljs-string">etcd_cert</span>            <span class="hljs-comment"># Set noderef for node controller.</span>            <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">CALICO_K8S_NODE_REF</span>              <span class="hljs-attr">valueFrom:</span>                <span class="hljs-attr">fieldRef:</span>                  <span class="hljs-attr">fieldPath:</span> <span class="hljs-string">spec.nodeName</span>            <span class="hljs-comment"># Choose the backend to use.</span>            <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">CALICO_NETWORKING_BACKEND</span>              <span class="hljs-attr">valueFrom:</span>                <span class="hljs-attr">configMapKeyRef:</span>                  <span class="hljs-attr">name:</span> <span class="hljs-string">calico-config</span>                  <span class="hljs-attr">key:</span> <span class="hljs-string">calico_backend</span>            <span class="hljs-comment"># Cluster type to identify the deployment type</span>            <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">CLUSTER_TYPE</span>              <span class="hljs-attr">value:</span> <span class="hljs-string">"k8s,bgp"</span>            <span class="hljs-comment"># Auto-detect the BGP IP address.</span>            <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">IP</span>              <span class="hljs-attr">value:</span> <span class="hljs-string">"autodetect"</span>            <span class="hljs-comment"># Enable IPIP</span>            <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">CALICO_IPV4POOL_IPIP</span>              <span class="hljs-attr">value:</span> <span class="hljs-string">"Always"</span>            <span class="hljs-comment"># Set MTU for tunnel device used if ipip is enabled</span>            <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">FELIX_IPINIPMTU</span>              <span class="hljs-attr">valueFrom:</span>                <span class="hljs-attr">configMapKeyRef:</span>                  <span class="hljs-attr">name:</span> <span class="hljs-string">calico-config</span>                  <span class="hljs-attr">key:</span> <span class="hljs-string">veth_mtu</span>            <span class="hljs-comment"># The default IPv4 pool to create on startup if none exists. Pod IPs will be</span>            <span class="hljs-comment"># chosen from this range. Changing this value after installation will have</span>            <span class="hljs-comment"># no effect. This should fall within `--cluster-cidr`.</span>            <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">CALICO_IPV4POOL_CIDR</span>              <span class="hljs-attr">value:</span> <span class="hljs-string">"10.20.0.0/16"</span>            <span class="hljs-comment"># Disable file logging so `kubectl logs` works.</span>            <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">CALICO_DISABLE_FILE_LOGGING</span>              <span class="hljs-attr">value:</span> <span class="hljs-string">"true"</span>            <span class="hljs-comment"># Set Felix endpoint to host default action to ACCEPT.</span>            <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">FELIX_DEFAULTENDPOINTTOHOSTACTION</span>              <span class="hljs-attr">value:</span> <span class="hljs-string">"ACCEPT"</span>            <span class="hljs-comment"># Disable IPv6 on Kubernetes.</span>            <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">FELIX_IPV6SUPPORT</span>              <span class="hljs-attr">value:</span> <span class="hljs-string">"false"</span>            <span class="hljs-comment"># Set Felix logging to "info"</span>            <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">FELIX_LOGSEVERITYSCREEN</span>              <span class="hljs-attr">value:</span> <span class="hljs-string">"info"</span>            <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">FELIX_HEALTHENABLED</span>              <span class="hljs-attr">value:</span> <span class="hljs-string">"true"</span>            <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">IP_AUTODETECTION_METHOD</span>              <span class="hljs-attr">value:</span> <span class="hljs-string">can-reach=192.168.1.51</span>          <span class="hljs-attr">securityContext:</span>            <span class="hljs-attr">privileged:</span> <span class="hljs-literal">true</span>          <span class="hljs-attr">resources:</span>            <span class="hljs-attr">requests:</span>              <span class="hljs-attr">cpu:</span> <span class="hljs-string">250m</span>          <span class="hljs-attr">livenessProbe:</span>            <span class="hljs-attr">httpGet:</span>              <span class="hljs-attr">path:</span> <span class="hljs-string">/liveness</span>              <span class="hljs-attr">port:</span> <span class="hljs-number">9099</span>              <span class="hljs-attr">host:</span> <span class="hljs-string">localhost</span>            <span class="hljs-attr">periodSeconds:</span> <span class="hljs-number">10</span>            <span class="hljs-attr">initialDelaySeconds:</span> <span class="hljs-number">10</span>            <span class="hljs-attr">failureThreshold:</span> <span class="hljs-number">6</span>          <span class="hljs-attr">readinessProbe:</span>            <span class="hljs-attr">exec:</span>              <span class="hljs-attr">command:</span>              <span class="hljs-bullet">-</span> <span class="hljs-string">/bin/calico-node</span>              <span class="hljs-bullet">-</span> <span class="hljs-string">-bird-ready</span>              <span class="hljs-bullet">-</span> <span class="hljs-string">-felix-ready</span>            <span class="hljs-attr">periodSeconds:</span> <span class="hljs-number">10</span>          <span class="hljs-attr">volumeMounts:</span>            <span class="hljs-bullet">-</span> <span class="hljs-attr">mountPath:</span> <span class="hljs-string">/lib/modules</span>              <span class="hljs-attr">name:</span> <span class="hljs-string">lib-modules</span>              <span class="hljs-attr">readOnly:</span> <span class="hljs-literal">true</span>            <span class="hljs-bullet">-</span> <span class="hljs-attr">mountPath:</span> <span class="hljs-string">/run/xtables.lock</span>              <span class="hljs-attr">name:</span> <span class="hljs-string">xtables-lock</span>              <span class="hljs-attr">readOnly:</span> <span class="hljs-literal">false</span>            <span class="hljs-bullet">-</span> <span class="hljs-attr">mountPath:</span> <span class="hljs-string">/var/run/calico</span>              <span class="hljs-attr">name:</span> <span class="hljs-string">var-run-calico</span>              <span class="hljs-attr">readOnly:</span> <span class="hljs-literal">false</span>            <span class="hljs-bullet">-</span> <span class="hljs-attr">mountPath:</span> <span class="hljs-string">/var/lib/calico</span>              <span class="hljs-attr">name:</span> <span class="hljs-string">var-lib-calico</span>              <span class="hljs-attr">readOnly:</span> <span class="hljs-literal">false</span>            <span class="hljs-bullet">-</span> <span class="hljs-attr">mountPath:</span> <span class="hljs-string">/calico-secrets</span>              <span class="hljs-attr">name:</span> <span class="hljs-string">etcd-certs</span>      <span class="hljs-attr">volumes:</span>        <span class="hljs-comment"># Used by calico/node.</span>        <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">lib-modules</span>          <span class="hljs-attr">hostPath:</span>            <span class="hljs-attr">path:</span> <span class="hljs-string">/lib/modules</span>        <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">var-run-calico</span>          <span class="hljs-attr">hostPath:</span>            <span class="hljs-attr">path:</span> <span class="hljs-string">/var/run/calico</span>        <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">var-lib-calico</span>          <span class="hljs-attr">hostPath:</span>            <span class="hljs-attr">path:</span> <span class="hljs-string">/var/lib/calico</span>        <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">xtables-lock</span>          <span class="hljs-attr">hostPath:</span>            <span class="hljs-attr">path:</span> <span class="hljs-string">/run/xtables.lock</span>            <span class="hljs-attr">type:</span> <span class="hljs-string">FileOrCreate</span>        <span class="hljs-comment"># Used to install CNI.</span>        <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">cni-bin-dir</span>          <span class="hljs-attr">hostPath:</span>            <span class="hljs-attr">path:</span> <span class="hljs-string">/opt/cni/bin</span>        <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">cni-net-dir</span>          <span class="hljs-attr">hostPath:</span>            <span class="hljs-attr">path:</span> <span class="hljs-string">/etc/cni/net.d</span>        <span class="hljs-comment"># Mount in the etcd TLS secrets with mode 400.</span>        <span class="hljs-comment"># See https://kubernetes.io/docs/concepts/configuration/secret/</span>        <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">etcd-certs</span>          <span class="hljs-attr">secret:</span>            <span class="hljs-attr">secretName:</span> <span class="hljs-string">calico-etcd-secrets</span>            <span class="hljs-attr">defaultMode:</span> <span class="hljs-number">0400</span><span class="hljs-meta">---</span><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">v1</span><span class="hljs-attr">kind:</span> <span class="hljs-string">ServiceAccount</span><span class="hljs-attr">metadata:</span>  <span class="hljs-attr">name:</span> <span class="hljs-string">calico-node</span>  <span class="hljs-attr">namespace:</span> <span class="hljs-string">kube-system</span><span class="hljs-meta">---</span><span class="hljs-comment"># Source: calico/templates/calico-kube-controllers.yaml</span><span class="hljs-comment"># This manifest deploys the Calico Kubernetes controllers.</span><span class="hljs-comment"># See https://github.com/projectcalico/kube-controllers</span><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">extensions/v1beta1</span><span class="hljs-attr">kind:</span> <span class="hljs-string">Deployment</span><span class="hljs-attr">metadata:</span>  <span class="hljs-attr">name:</span> <span class="hljs-string">calico-kube-controllers</span>  <span class="hljs-attr">namespace:</span> <span class="hljs-string">kube-system</span>  <span class="hljs-attr">labels:</span>    <span class="hljs-attr">k8s-app:</span> <span class="hljs-string">calico-kube-controllers</span>  <span class="hljs-attr">annotations:</span>    <span class="hljs-attr">scheduler.alpha.kubernetes.io/critical-pod:</span> <span class="hljs-string">''</span><span class="hljs-attr">spec:</span>  <span class="hljs-comment"># The controllers can only have a single active instance.</span>  <span class="hljs-attr">replicas:</span> <span class="hljs-number">1</span>  <span class="hljs-attr">strategy:</span>    <span class="hljs-attr">type:</span> <span class="hljs-string">Recreate</span>  <span class="hljs-attr">template:</span>    <span class="hljs-attr">metadata:</span>      <span class="hljs-attr">name:</span> <span class="hljs-string">calico-kube-controllers</span>      <span class="hljs-attr">namespace:</span> <span class="hljs-string">kube-system</span>      <span class="hljs-attr">labels:</span>        <span class="hljs-attr">k8s-app:</span> <span class="hljs-string">calico-kube-controllers</span>    <span class="hljs-attr">spec:</span>      <span class="hljs-attr">nodeSelector:</span>        <span class="hljs-attr">beta.kubernetes.io/os:</span> <span class="hljs-string">linux</span>      <span class="hljs-comment"># The controllers must run in the host network namespace so that</span>      <span class="hljs-comment"># it isn't governed by policy that would prevent it from working.</span>      <span class="hljs-attr">hostNetwork:</span> <span class="hljs-literal">true</span>      <span class="hljs-attr">tolerations:</span>        <span class="hljs-comment"># Mark the pod as a critical add-on for rescheduling.</span>        <span class="hljs-bullet">-</span> <span class="hljs-attr">key:</span> <span class="hljs-string">CriticalAddonsOnly</span>          <span class="hljs-attr">operator:</span> <span class="hljs-string">Exists</span>        <span class="hljs-bullet">-</span> <span class="hljs-attr">key:</span> <span class="hljs-string">node-role.kubernetes.io/master</span>          <span class="hljs-attr">effect:</span> <span class="hljs-string">NoSchedule</span>      <span class="hljs-attr">serviceAccountName:</span> <span class="hljs-string">calico-kube-controllers</span>      <span class="hljs-attr">containers:</span>        <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">calico-kube-controllers</span>          <span class="hljs-attr">image:</span> <span class="hljs-string">calico/kube-controllers:v3.6.0</span>          <span class="hljs-attr">env:</span>            <span class="hljs-comment"># The location of the Calico etcd cluster.</span>            <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">ETCD_ENDPOINTS</span>              <span class="hljs-attr">valueFrom:</span>                <span class="hljs-attr">configMapKeyRef:</span>                  <span class="hljs-attr">name:</span> <span class="hljs-string">calico-config</span>                  <span class="hljs-attr">key:</span> <span class="hljs-string">etcd_endpoints</span>            <span class="hljs-comment"># Location of the CA certificate for etcd.</span>            <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">ETCD_CA_CERT_FILE</span>              <span class="hljs-attr">valueFrom:</span>                <span class="hljs-attr">configMapKeyRef:</span>                  <span class="hljs-attr">name:</span> <span class="hljs-string">calico-config</span>                  <span class="hljs-attr">key:</span> <span class="hljs-string">etcd_ca</span>            <span class="hljs-comment"># Location of the client key for etcd.</span>            <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">ETCD_KEY_FILE</span>              <span class="hljs-attr">valueFrom:</span>                <span class="hljs-attr">configMapKeyRef:</span>                  <span class="hljs-attr">name:</span> <span class="hljs-string">calico-config</span>                  <span class="hljs-attr">key:</span> <span class="hljs-string">etcd_key</span>            <span class="hljs-comment"># Location of the client certificate for etcd.</span>            <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">ETCD_CERT_FILE</span>              <span class="hljs-attr">valueFrom:</span>                <span class="hljs-attr">configMapKeyRef:</span>                  <span class="hljs-attr">name:</span> <span class="hljs-string">calico-config</span>                  <span class="hljs-attr">key:</span> <span class="hljs-string">etcd_cert</span>            <span class="hljs-comment"># Choose which controllers to run.</span>            <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">ENABLED_CONTROLLERS</span>              <span class="hljs-attr">value:</span> <span class="hljs-string">policy,namespace,serviceaccount,workloadendpoint,node</span>          <span class="hljs-attr">volumeMounts:</span>            <span class="hljs-comment"># Mount in the etcd TLS secrets.</span>            <span class="hljs-bullet">-</span> <span class="hljs-attr">mountPath:</span> <span class="hljs-string">/calico-secrets</span>              <span class="hljs-attr">name:</span> <span class="hljs-string">etcd-certs</span>          <span class="hljs-attr">readinessProbe:</span>            <span class="hljs-attr">exec:</span>              <span class="hljs-attr">command:</span>              <span class="hljs-bullet">-</span> <span class="hljs-string">/usr/bin/check-status</span>              <span class="hljs-bullet">-</span> <span class="hljs-string">-r</span>      <span class="hljs-attr">volumes:</span>        <span class="hljs-comment"># Mount in the etcd TLS secrets with mode 400.</span>        <span class="hljs-comment"># See https://kubernetes.io/docs/concepts/configuration/secret/</span>        <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">etcd-certs</span>          <span class="hljs-attr">secret:</span>            <span class="hljs-attr">secretName:</span> <span class="hljs-string">calico-etcd-secrets</span>            <span class="hljs-attr">defaultMode:</span> <span class="hljs-number">0400</span><span class="hljs-meta">---</span><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">v1</span><span class="hljs-attr">kind:</span> <span class="hljs-string">ServiceAccount</span><span class="hljs-attr">metadata:</span>  <span class="hljs-attr">name:</span> <span class="hljs-string">calico-kube-controllers</span>  <span class="hljs-attr">namespace:</span> <span class="hljs-string">kube-system</span></code></pre><p><strong>需要注意的是我们添加了 <code>IP_AUTODETECTION_METHOD</code> 变量，这个变量会设置 calcio 获取 node ip 的方式；默认情况下采用 <a href="https://docs.projectcalico.org/v3.6/reference/node/configuration#ip-autodetection-methods" target="_blank" rel="noopener">first-found</a> 方式获取，即获取第一个有效网卡的 IP 作为 node ip；在某些多网卡机器上可能会出现问题；这里将值设置为 <code>can-reach=192.168.1.51</code>，即使用第一个能够访问 master <code>192.168.1.51</code> 的网卡地址作为 node ip</strong></p><p>最后执行创建即可，创建成功后如下所示</p><pre><code class="hljs sh">docker1.node ➜  ~ kubectl get pod -o wide -n kube-systemNAME                                      READY   STATUS    RESTARTS   AGE   IP             NODE           NOMINATED NODE   READINESS GATEScalico-kube-controllers-65bc6b9f9-cn27f   1/1     Running   0          85s   192.168.1.53   docker3.node   &lt;none&gt;           &lt;none&gt;calico-node-c5nl8                         1/1     Running   0          85s   192.168.1.53   docker3.node   &lt;none&gt;           &lt;none&gt;calico-node-fqknv                         1/1     Running   0          85s   192.168.1.51   docker1.node   &lt;none&gt;           &lt;none&gt;calico-node-ldfzs                         1/1     Running   0          85s   192.168.1.55   docker5.node   &lt;none&gt;           &lt;none&gt;calico-node-ngjxc                         1/1     Running   0          85s   192.168.1.52   docker2.node   &lt;none&gt;           &lt;none&gt;calico-node-vj8np                         1/1     Running   0          85s   192.168.1.54   docker4.node   &lt;none&gt;           &lt;none&gt;</code></pre><p>此时所有 node 应当变为 Ready 状态</p><h3 id="3-5、部署-DNS"><a href="#3-5、部署-DNS" class="headerlink" title="3.5、部署 DNS"></a>3.5、部署 DNS</h3><p>其他组件全部完成后我们应当部署集群 DNS 使 service 等能够正常解析；集群 DNS 这里采用 coredns，具体安装文档参考 <a href="https://github.com/coredns/deployment/tree/master/kubernetes" target="_blank" rel="noopener">coredns/deployment</a>；coredns 完整配置如下</p><ul><li>coredns.yaml</li></ul><pre><code class="hljs yaml"><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">v1</span><span class="hljs-attr">kind:</span> <span class="hljs-string">ServiceAccount</span><span class="hljs-attr">metadata:</span>  <span class="hljs-attr">name:</span> <span class="hljs-string">coredns</span>  <span class="hljs-attr">namespace:</span> <span class="hljs-string">kube-system</span><span class="hljs-meta">---</span><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">rbac.authorization.k8s.io/v1</span><span class="hljs-attr">kind:</span> <span class="hljs-string">ClusterRole</span><span class="hljs-attr">metadata:</span>  <span class="hljs-attr">labels:</span>    <span class="hljs-attr">kubernetes.io/bootstrapping:</span> <span class="hljs-string">rbac-defaults</span>  <span class="hljs-attr">name:</span> <span class="hljs-string">system:coredns</span><span class="hljs-attr">rules:</span><span class="hljs-bullet">-</span> <span class="hljs-attr">apiGroups:</span>  <span class="hljs-bullet">-</span> <span class="hljs-string">""</span>  <span class="hljs-attr">resources:</span>  <span class="hljs-bullet">-</span> <span class="hljs-string">endpoints</span>  <span class="hljs-bullet">-</span> <span class="hljs-string">services</span>  <span class="hljs-bullet">-</span> <span class="hljs-string">pods</span>  <span class="hljs-bullet">-</span> <span class="hljs-string">namespaces</span>  <span class="hljs-attr">verbs:</span>  <span class="hljs-bullet">-</span> <span class="hljs-string">list</span>  <span class="hljs-bullet">-</span> <span class="hljs-string">watch</span><span class="hljs-bullet">-</span> <span class="hljs-attr">apiGroups:</span>  <span class="hljs-bullet">-</span> <span class="hljs-string">""</span>  <span class="hljs-attr">resources:</span>  <span class="hljs-bullet">-</span> <span class="hljs-string">nodes</span>  <span class="hljs-attr">verbs:</span>  <span class="hljs-bullet">-</span> <span class="hljs-string">get</span><span class="hljs-meta">---</span><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">rbac.authorization.k8s.io/v1</span><span class="hljs-attr">kind:</span> <span class="hljs-string">ClusterRoleBinding</span><span class="hljs-attr">metadata:</span>  <span class="hljs-attr">annotations:</span>    <span class="hljs-attr">rbac.authorization.kubernetes.io/autoupdate:</span> <span class="hljs-string">"true"</span>  <span class="hljs-attr">labels:</span>    <span class="hljs-attr">kubernetes.io/bootstrapping:</span> <span class="hljs-string">rbac-defaults</span>  <span class="hljs-attr">name:</span> <span class="hljs-string">system:coredns</span><span class="hljs-attr">roleRef:</span>  <span class="hljs-attr">apiGroup:</span> <span class="hljs-string">rbac.authorization.k8s.io</span>  <span class="hljs-attr">kind:</span> <span class="hljs-string">ClusterRole</span>  <span class="hljs-attr">name:</span> <span class="hljs-string">system:coredns</span><span class="hljs-attr">subjects:</span><span class="hljs-bullet">-</span> <span class="hljs-attr">kind:</span> <span class="hljs-string">ServiceAccount</span>  <span class="hljs-attr">name:</span> <span class="hljs-string">coredns</span>  <span class="hljs-attr">namespace:</span> <span class="hljs-string">kube-system</span><span class="hljs-meta">---</span><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">v1</span><span class="hljs-attr">kind:</span> <span class="hljs-string">ConfigMap</span><span class="hljs-attr">metadata:</span>  <span class="hljs-attr">name:</span> <span class="hljs-string">coredns</span>  <span class="hljs-attr">namespace:</span> <span class="hljs-string">kube-system</span><span class="hljs-attr">data:</span>  <span class="hljs-attr">Corefile:</span> <span class="hljs-string">|</span>    <span class="hljs-string">.:53</span> <span class="hljs-string">&#123;</span>        <span class="hljs-string">errors</span>        <span class="hljs-string">health</span>        <span class="hljs-string">kubernetes</span> <span class="hljs-string">cluster.local</span> <span class="hljs-string">in-addr.arpa</span> <span class="hljs-string">ip6.arpa</span> <span class="hljs-string">&#123;</span>          <span class="hljs-string">pods</span> <span class="hljs-string">insecure</span>          <span class="hljs-string">upstream</span>          <span class="hljs-string">fallthrough</span> <span class="hljs-string">in-addr.arpa</span> <span class="hljs-string">ip6.arpa</span>        <span class="hljs-string">&#125;</span>        <span class="hljs-string">prometheus</span> <span class="hljs-string">:9153</span>        <span class="hljs-string">forward</span> <span class="hljs-string">.</span> <span class="hljs-string">/etc/resolv.conf</span>        <span class="hljs-string">cache</span> <span class="hljs-number">30</span>        <span class="hljs-string">loop</span>        <span class="hljs-string">reload</span>        <span class="hljs-string">loadbalance</span>    <span class="hljs-string">&#125;</span><span class="hljs-meta">---</span><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">apps/v1</span><span class="hljs-attr">kind:</span> <span class="hljs-string">Deployment</span><span class="hljs-attr">metadata:</span>  <span class="hljs-attr">name:</span> <span class="hljs-string">coredns</span>  <span class="hljs-attr">namespace:</span> <span class="hljs-string">kube-system</span>  <span class="hljs-attr">labels:</span>    <span class="hljs-attr">k8s-app:</span> <span class="hljs-string">kube-dns</span>    <span class="hljs-attr">kubernetes.io/name:</span> <span class="hljs-string">"CoreDNS"</span><span class="hljs-attr">spec:</span>  <span class="hljs-attr">replicas:</span> <span class="hljs-number">2</span>  <span class="hljs-attr">strategy:</span>    <span class="hljs-attr">type:</span> <span class="hljs-string">RollingUpdate</span>    <span class="hljs-attr">rollingUpdate:</span>      <span class="hljs-attr">maxUnavailable:</span> <span class="hljs-number">1</span>  <span class="hljs-attr">selector:</span>    <span class="hljs-attr">matchLabels:</span>      <span class="hljs-attr">k8s-app:</span> <span class="hljs-string">kube-dns</span>  <span class="hljs-attr">template:</span>    <span class="hljs-attr">metadata:</span>      <span class="hljs-attr">labels:</span>        <span class="hljs-attr">k8s-app:</span> <span class="hljs-string">kube-dns</span>    <span class="hljs-attr">spec:</span>      <span class="hljs-attr">priorityClassName:</span> <span class="hljs-string">system-cluster-critical</span>      <span class="hljs-attr">serviceAccountName:</span> <span class="hljs-string">coredns</span>      <span class="hljs-attr">tolerations:</span>        <span class="hljs-bullet">-</span> <span class="hljs-attr">key:</span> <span class="hljs-string">"CriticalAddonsOnly"</span>          <span class="hljs-attr">operator:</span> <span class="hljs-string">"Exists"</span>      <span class="hljs-attr">nodeSelector:</span>        <span class="hljs-attr">beta.kubernetes.io/os:</span> <span class="hljs-string">linux</span>      <span class="hljs-attr">containers:</span>      <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">coredns</span>        <span class="hljs-attr">image:</span> <span class="hljs-string">coredns/coredns:1.3.1</span>        <span class="hljs-attr">imagePullPolicy:</span> <span class="hljs-string">IfNotPresent</span>        <span class="hljs-attr">resources:</span>          <span class="hljs-attr">limits:</span>            <span class="hljs-attr">memory:</span> <span class="hljs-string">170Mi</span>          <span class="hljs-attr">requests:</span>            <span class="hljs-attr">cpu:</span> <span class="hljs-string">100m</span>            <span class="hljs-attr">memory:</span> <span class="hljs-string">70Mi</span>        <span class="hljs-attr">args:</span> <span class="hljs-string">[</span> <span class="hljs-string">"-conf"</span><span class="hljs-string">,</span> <span class="hljs-string">"/etc/coredns/Corefile"</span> <span class="hljs-string">]</span>        <span class="hljs-attr">volumeMounts:</span>        <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">config-volume</span>          <span class="hljs-attr">mountPath:</span> <span class="hljs-string">/etc/coredns</span>          <span class="hljs-attr">readOnly:</span> <span class="hljs-literal">true</span>        <span class="hljs-attr">ports:</span>        <span class="hljs-bullet">-</span> <span class="hljs-attr">containerPort:</span> <span class="hljs-number">53</span>          <span class="hljs-attr">name:</span> <span class="hljs-string">dns</span>          <span class="hljs-attr">protocol:</span> <span class="hljs-string">UDP</span>        <span class="hljs-bullet">-</span> <span class="hljs-attr">containerPort:</span> <span class="hljs-number">53</span>          <span class="hljs-attr">name:</span> <span class="hljs-string">dns-tcp</span>          <span class="hljs-attr">protocol:</span> <span class="hljs-string">TCP</span>        <span class="hljs-bullet">-</span> <span class="hljs-attr">containerPort:</span> <span class="hljs-number">9153</span>          <span class="hljs-attr">name:</span> <span class="hljs-string">metrics</span>          <span class="hljs-attr">protocol:</span> <span class="hljs-string">TCP</span>        <span class="hljs-attr">securityContext:</span>          <span class="hljs-attr">allowPrivilegeEscalation:</span> <span class="hljs-literal">false</span>          <span class="hljs-attr">capabilities:</span>            <span class="hljs-attr">add:</span>            <span class="hljs-bullet">-</span> <span class="hljs-string">NET_BIND_SERVICE</span>            <span class="hljs-attr">drop:</span>            <span class="hljs-bullet">-</span> <span class="hljs-string">all</span>          <span class="hljs-attr">readOnlyRootFilesystem:</span> <span class="hljs-literal">true</span>        <span class="hljs-attr">livenessProbe:</span>          <span class="hljs-attr">httpGet:</span>            <span class="hljs-attr">path:</span> <span class="hljs-string">/health</span>            <span class="hljs-attr">port:</span> <span class="hljs-number">8080</span>            <span class="hljs-attr">scheme:</span> <span class="hljs-string">HTTP</span>          <span class="hljs-attr">initialDelaySeconds:</span> <span class="hljs-number">60</span>          <span class="hljs-attr">timeoutSeconds:</span> <span class="hljs-number">5</span>          <span class="hljs-attr">successThreshold:</span> <span class="hljs-number">1</span>          <span class="hljs-attr">failureThreshold:</span> <span class="hljs-number">5</span>        <span class="hljs-attr">readinessProbe:</span>          <span class="hljs-attr">httpGet:</span>            <span class="hljs-attr">path:</span> <span class="hljs-string">/health</span>            <span class="hljs-attr">port:</span> <span class="hljs-number">8080</span>            <span class="hljs-attr">scheme:</span> <span class="hljs-string">HTTP</span>      <span class="hljs-attr">dnsPolicy:</span> <span class="hljs-string">Default</span>      <span class="hljs-attr">volumes:</span>        <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">config-volume</span>          <span class="hljs-attr">configMap:</span>            <span class="hljs-attr">name:</span> <span class="hljs-string">coredns</span>            <span class="hljs-attr">items:</span>            <span class="hljs-bullet">-</span> <span class="hljs-attr">key:</span> <span class="hljs-string">Corefile</span>              <span class="hljs-attr">path:</span> <span class="hljs-string">Corefile</span><span class="hljs-meta">---</span><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">v1</span><span class="hljs-attr">kind:</span> <span class="hljs-string">Service</span><span class="hljs-attr">metadata:</span>  <span class="hljs-attr">name:</span> <span class="hljs-string">kube-dns</span>  <span class="hljs-attr">namespace:</span> <span class="hljs-string">kube-system</span>  <span class="hljs-attr">annotations:</span>    <span class="hljs-attr">prometheus.io/port:</span> <span class="hljs-string">"9153"</span>    <span class="hljs-attr">prometheus.io/scrape:</span> <span class="hljs-string">"true"</span>  <span class="hljs-attr">labels:</span>    <span class="hljs-attr">k8s-app:</span> <span class="hljs-string">kube-dns</span>    <span class="hljs-attr">kubernetes.io/cluster-service:</span> <span class="hljs-string">"true"</span>    <span class="hljs-attr">kubernetes.io/name:</span> <span class="hljs-string">"CoreDNS"</span><span class="hljs-attr">spec:</span>  <span class="hljs-attr">selector:</span>    <span class="hljs-attr">k8s-app:</span> <span class="hljs-string">kube-dns</span>  <span class="hljs-attr">clusterIP:</span> <span class="hljs-number">10.254</span><span class="hljs-number">.0</span><span class="hljs-number">.2</span>  <span class="hljs-attr">ports:</span>  <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">dns</span>    <span class="hljs-attr">port:</span> <span class="hljs-number">53</span>    <span class="hljs-attr">protocol:</span> <span class="hljs-string">UDP</span>  <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">dns-tcp</span>    <span class="hljs-attr">port:</span> <span class="hljs-number">53</span>    <span class="hljs-attr">protocol:</span> <span class="hljs-string">TCP</span>  <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">metrics</span>    <span class="hljs-attr">port:</span> <span class="hljs-number">9153</span>    <span class="hljs-attr">protocol:</span> <span class="hljs-string">TCP</span></code></pre><h3 id="3-5、部署-DNS-自动扩容"><a href="#3-5、部署-DNS-自动扩容" class="headerlink" title="3.5、部署 DNS 自动扩容"></a>3.5、部署 DNS 自动扩容</h3><p>在大规模集群的情况下，可能需要集群 DNS 自动扩容，具体文档请参考 <a href="https://github.com/kubernetes/kubernetes/tree/master/cluster/addons/dns-horizontal-autoscaler" target="_blank" rel="noopener">DNS Horizontal Autoscaler</a>，DNS 扩容算法可参考 <a href="https://github.com/kubernetes-incubator/cluster-proportional-autoscaler/" target="_blank" rel="noopener">Github</a>，如有需要请自行修改；以下为具体配置</p><ul><li>dns-horizontal-autoscaler.yaml</li></ul><pre><code class="hljs yaml"><span class="hljs-comment"># Copyright 2016 The Kubernetes Authors.</span><span class="hljs-comment">#</span><span class="hljs-comment"># Licensed under the Apache License, Version 2.0 (the "License");</span><span class="hljs-comment"># you may not use this file except in compliance with the License.</span><span class="hljs-comment"># You may obtain a copy of the License at</span><span class="hljs-comment">#</span><span class="hljs-comment">#     http://www.apache.org/licenses/LICENSE-2.0</span><span class="hljs-comment">#</span><span class="hljs-comment"># Unless required by applicable law or agreed to in writing, software</span><span class="hljs-comment"># distributed under the License is distributed on an "AS IS" BASIS,</span><span class="hljs-comment"># WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.</span><span class="hljs-comment"># See the License for the specific language governing permissions and</span><span class="hljs-comment"># limitations under the License.</span><span class="hljs-attr">kind:</span> <span class="hljs-string">ServiceAccount</span><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">v1</span><span class="hljs-attr">metadata:</span>  <span class="hljs-attr">name:</span> <span class="hljs-string">kube-dns-autoscaler</span>  <span class="hljs-attr">namespace:</span> <span class="hljs-string">kube-system</span>  <span class="hljs-attr">labels:</span>    <span class="hljs-attr">addonmanager.kubernetes.io/mode:</span> <span class="hljs-string">Reconcile</span><span class="hljs-meta">---</span><span class="hljs-attr">kind:</span> <span class="hljs-string">ClusterRole</span><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">rbac.authorization.k8s.io/v1</span><span class="hljs-attr">metadata:</span>  <span class="hljs-attr">name:</span> <span class="hljs-string">system:kube-dns-autoscaler</span>  <span class="hljs-attr">labels:</span>    <span class="hljs-attr">addonmanager.kubernetes.io/mode:</span> <span class="hljs-string">Reconcile</span><span class="hljs-attr">rules:</span>  <span class="hljs-bullet">-</span> <span class="hljs-attr">apiGroups:</span> <span class="hljs-string">[""]</span>    <span class="hljs-attr">resources:</span> <span class="hljs-string">["nodes"]</span>    <span class="hljs-attr">verbs:</span> <span class="hljs-string">["list"]</span>  <span class="hljs-bullet">-</span> <span class="hljs-attr">apiGroups:</span> <span class="hljs-string">[""]</span>    <span class="hljs-attr">resources:</span> <span class="hljs-string">["replicationcontrollers/scale"]</span>    <span class="hljs-attr">verbs:</span> <span class="hljs-string">["get",</span> <span class="hljs-string">"update"</span><span class="hljs-string">]</span>  <span class="hljs-bullet">-</span> <span class="hljs-attr">apiGroups:</span> <span class="hljs-string">["extensions"]</span>    <span class="hljs-attr">resources:</span> <span class="hljs-string">["deployments/scale",</span> <span class="hljs-string">"replicasets/scale"</span><span class="hljs-string">]</span>    <span class="hljs-attr">verbs:</span> <span class="hljs-string">["get",</span> <span class="hljs-string">"update"</span><span class="hljs-string">]</span><span class="hljs-comment"># Remove the configmaps rule once below issue is fixed:</span><span class="hljs-comment"># kubernetes-incubator/cluster-proportional-autoscaler#16</span>  <span class="hljs-bullet">-</span> <span class="hljs-attr">apiGroups:</span> <span class="hljs-string">[""]</span>    <span class="hljs-attr">resources:</span> <span class="hljs-string">["configmaps"]</span>    <span class="hljs-attr">verbs:</span> <span class="hljs-string">["get",</span> <span class="hljs-string">"create"</span><span class="hljs-string">]</span><span class="hljs-meta">---</span><span class="hljs-attr">kind:</span> <span class="hljs-string">ClusterRoleBinding</span><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">rbac.authorization.k8s.io/v1</span><span class="hljs-attr">metadata:</span>  <span class="hljs-attr">name:</span> <span class="hljs-string">system:kube-dns-autoscaler</span>  <span class="hljs-attr">labels:</span>    <span class="hljs-attr">addonmanager.kubernetes.io/mode:</span> <span class="hljs-string">Reconcile</span><span class="hljs-attr">subjects:</span>  <span class="hljs-bullet">-</span> <span class="hljs-attr">kind:</span> <span class="hljs-string">ServiceAccount</span>    <span class="hljs-attr">name:</span> <span class="hljs-string">kube-dns-autoscaler</span>    <span class="hljs-attr">namespace:</span> <span class="hljs-string">kube-system</span><span class="hljs-attr">roleRef:</span>  <span class="hljs-attr">kind:</span> <span class="hljs-string">ClusterRole</span>  <span class="hljs-attr">name:</span> <span class="hljs-string">system:kube-dns-autoscaler</span>  <span class="hljs-attr">apiGroup:</span> <span class="hljs-string">rbac.authorization.k8s.io</span><span class="hljs-meta">---</span><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">apps/v1</span><span class="hljs-attr">kind:</span> <span class="hljs-string">Deployment</span><span class="hljs-attr">metadata:</span>  <span class="hljs-attr">name:</span> <span class="hljs-string">kube-dns-autoscaler</span>  <span class="hljs-attr">namespace:</span> <span class="hljs-string">kube-system</span>  <span class="hljs-attr">labels:</span>    <span class="hljs-attr">k8s-app:</span> <span class="hljs-string">kube-dns-autoscaler</span>    <span class="hljs-attr">kubernetes.io/cluster-service:</span> <span class="hljs-string">"true"</span>    <span class="hljs-attr">addonmanager.kubernetes.io/mode:</span> <span class="hljs-string">Reconcile</span><span class="hljs-attr">spec:</span>  <span class="hljs-attr">selector:</span>    <span class="hljs-attr">matchLabels:</span>      <span class="hljs-attr">k8s-app:</span> <span class="hljs-string">kube-dns-autoscaler</span>  <span class="hljs-attr">template:</span>    <span class="hljs-attr">metadata:</span>      <span class="hljs-attr">labels:</span>        <span class="hljs-attr">k8s-app:</span> <span class="hljs-string">kube-dns-autoscaler</span>      <span class="hljs-attr">annotations:</span>        <span class="hljs-attr">scheduler.alpha.kubernetes.io/critical-pod:</span> <span class="hljs-string">''</span>    <span class="hljs-attr">spec:</span>      <span class="hljs-attr">priorityClassName:</span> <span class="hljs-string">system-cluster-critical</span>      <span class="hljs-attr">containers:</span>      <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">autoscaler</span>        <span class="hljs-attr">image:</span> <span class="hljs-string">gcr.azk8s.cn/google_containers/cluster-proportional-autoscaler-amd64:1.1.2-r2</span>        <span class="hljs-attr">resources:</span>            <span class="hljs-attr">requests:</span>                <span class="hljs-attr">cpu:</span> <span class="hljs-string">"20m"</span>                <span class="hljs-attr">memory:</span> <span class="hljs-string">"10Mi"</span>        <span class="hljs-attr">command:</span>          <span class="hljs-bullet">-</span> <span class="hljs-string">/cluster-proportional-autoscaler</span>          <span class="hljs-bullet">-</span> <span class="hljs-string">--namespace=kube-system</span>          <span class="hljs-bullet">-</span> <span class="hljs-string">--configmap=kube-dns-autoscaler</span>          <span class="hljs-comment"># Should keep target in sync with cluster/addons/dns/kube-dns.yaml.base</span>          <span class="hljs-bullet">-</span> <span class="hljs-string">--target=Deployment/coredns</span>          <span class="hljs-comment"># When cluster is using large nodes(with more cores), "coresPerReplica" should dominate.</span>          <span class="hljs-comment"># If using small nodes, "nodesPerReplica" should dominate.</span>          <span class="hljs-bullet">-</span> <span class="hljs-string">--default-params=&#123;"linear":&#123;"coresPerReplica":256,"nodesPerReplica":16,"preventSinglePointFailure":true&#125;&#125;</span>          <span class="hljs-bullet">-</span> <span class="hljs-string">--logtostderr=true</span>          <span class="hljs-bullet">-</span> <span class="hljs-string">--v=2</span>      <span class="hljs-attr">tolerations:</span>      <span class="hljs-bullet">-</span> <span class="hljs-attr">key:</span> <span class="hljs-string">"CriticalAddonsOnly"</span>        <span class="hljs-attr">operator:</span> <span class="hljs-string">"Exists"</span>      <span class="hljs-attr">serviceAccountName:</span> <span class="hljs-string">kube-dns-autoscaler</span></code></pre><h2 id="四、其他"><a href="#四、其他" class="headerlink" title="四、其他"></a>四、其他</h2><h3 id="4-1、集群测试"><a href="#4-1、集群测试" class="headerlink" title="4.1、集群测试"></a>4.1、集群测试</h3><p>为测试集群工作正常，我们创建一个 deployment 和一个 service，用于测试联通性和 DNS 工作是否正常；测试配置如下</p><ul><li>test.yaml</li></ul><pre><code class="hljs yaml"><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">apps/v1</span><span class="hljs-attr">kind:</span> <span class="hljs-string">Deployment</span><span class="hljs-attr">metadata:</span>  <span class="hljs-attr">name:</span> <span class="hljs-string">test</span>  <span class="hljs-attr">labels:</span>    <span class="hljs-attr">app:</span> <span class="hljs-string">test</span><span class="hljs-attr">spec:</span>  <span class="hljs-attr">replicas:</span> <span class="hljs-number">5</span>  <span class="hljs-attr">selector:</span>    <span class="hljs-attr">matchLabels:</span>      <span class="hljs-attr">app:</span> <span class="hljs-string">test</span>  <span class="hljs-attr">template:</span>    <span class="hljs-attr">metadata:</span>      <span class="hljs-attr">labels:</span>        <span class="hljs-attr">app:</span> <span class="hljs-string">test</span>    <span class="hljs-attr">spec:</span>      <span class="hljs-attr">containers:</span>      <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">test</span>        <span class="hljs-attr">image:</span> <span class="hljs-string">nginx:1.14.2-alpine</span>        <span class="hljs-attr">ports:</span>        <span class="hljs-bullet">-</span> <span class="hljs-attr">containerPort:</span> <span class="hljs-number">80</span><span class="hljs-meta">---</span><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">v1</span><span class="hljs-attr">kind:</span> <span class="hljs-string">Service</span><span class="hljs-attr">metadata:</span>  <span class="hljs-attr">name:</span> <span class="hljs-string">test-service</span><span class="hljs-attr">spec:</span>  <span class="hljs-attr">selector:</span>    <span class="hljs-attr">app:</span> <span class="hljs-string">test</span>  <span class="hljs-attr">ports:</span>  <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">nginx</span>    <span class="hljs-attr">port:</span> <span class="hljs-number">80</span>    <span class="hljs-attr">nodePort:</span> <span class="hljs-number">30001</span>    <span class="hljs-attr">targetPort:</span> <span class="hljs-number">80</span>    <span class="hljs-attr">protocol:</span> <span class="hljs-string">TCP</span>  <span class="hljs-attr">type:</span> <span class="hljs-string">NodePort</span></code></pre><p>测试方式很简单，进入某一个 pod ping 其他 pod ip 确认网络是否正常，直接访问 service 名称测试 DNS 是否工作正常，这里不再演示</p><h3 id="4-2、其他说明"><a href="#4-2、其他说明" class="headerlink" title="4.2、其他说明"></a>4.2、其他说明</h3><p>此次搭建开启了大部分认证，限于篇幅原因没有将每个选项作用做完整解释，推荐搭建完成后仔细阅读以下 <code>--help</code> 中的描述(官方文档页面有时候更新不完整)；目前 apiserver 仍然保留了 8080 端口(因为直接使用 kubectl 方便)，但是在高安全性环境请关闭 8080 端口，因为即使绑定在 127.0.0.1 上，对于任何能够登录 master 机器的用户仍然能够不经验证操作整个集群</p>]]></content>
    
    
    <categories>
      
      <category>Kubernetes</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Kubernetes</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Kubernetes sample-cli-plugin 源码分析</title>
    <link href="/2019/01/16/understand-kubernetes-sample-cli-plugin-source-code/"/>
    <url>/2019/01/16/understand-kubernetes-sample-cli-plugin-source-code/</url>
    
    <content type="html"><![CDATA[<blockquote><p>写这篇文章的目的是为了继续上篇 <a href="https://mritd.me/2018/11/30/kubectl-plugin-new-solution-on-kubernetes-1.12/" target="_blank" rel="noopener">Kubernetes 1.12 新的插件机制</a> 中最后部分对 <code>Golang 的插件辅助库</code> 说明；以及为后续使用 Golang 编写自己的 Kubernetes 插件做一个基础铺垫；顺边说一下 <strong>sample-cli-plugin 这个项目是官方为 Golang 开发者编写的一个用于快速切换配置文件中 Namespace 的一个插件样例</strong></p></blockquote><h2 id="一、基础准备"><a href="#一、基础准备" class="headerlink" title="一、基础准备"></a>一、基础准备</h2><p>在开始分析源码之前，<strong>我们假设读者已经熟悉 Golang 语言，至少对基本语法、指针、依赖管理工具有一定认知</strong>；下面介绍一下 <a href="https://github.com/kubernetes/sample-cli-plugin" target="_blank" rel="noopener">sample-cli-plugin</a> 这个项目一些基础核心的依赖:</p><h3 id="1-1、Cobra-终端库"><a href="#1-1、Cobra-终端库" class="headerlink" title="1.1、Cobra 终端库"></a>1.1、Cobra 终端库</h3><p>这是一个强大的 Golang 的 command line interface 库，其支持用非常简单的代码创建出符合 Unix 风格的 cli 程序；甚至官方提供了用于创建 cli 工程脚手架的 cli 命令工具；Cobra 官方 Github 地址 <a href="https://github.com/spf13/cobra" target="_blank" rel="noopener">点击这里</a>，具体用法请自行 Google，以下只做一个简单的命令定义介绍(docker、kubernetes 终端 cli 都基于这个库)</p><pre><code class="hljs golang"># 每一个命令(不论是子命令还是主命令)都会是一个 cobra.Command 对象<span class="hljs-keyword">var</span> lsCmd = &amp;cobra.Command&#123;    <span class="hljs-comment">// 一些命令帮助文档有关的描述信息</span>    Use:   <span class="hljs-string">"ls"</span>,    Short: <span class="hljs-string">"A brief description of your command"</span>,    Long: <span class="hljs-string">`A longer description that spans multiple lines and likely contains examples</span><span class="hljs-string">and usage of using your command. For example:</span><span class="hljs-string"></span><span class="hljs-string">Cobra is a CLI library for Go that empowers applications.</span><span class="hljs-string">This application is a tool to generate the needed files</span><span class="hljs-string">to quickly create a Cobra application.`</span>,    <span class="hljs-comment">// 命令运行时真正执行逻辑，如果需要返回 Error 信息，我们一般设置 RunE</span>    Run: <span class="hljs-function"><span class="hljs-keyword">func</span><span class="hljs-params">(cmd *cobra.Command, args []<span class="hljs-keyword">string</span>)</span></span> &#123;        fmt.Println(<span class="hljs-string">"ls called"</span>)    &#125;,&#125;<span class="hljs-comment">// 为这个命令添加 flag，比如 `--help`、`-p`</span><span class="hljs-comment">// PersistentFlags() 方法添加的 flag 在所有子 command 也会生效</span><span class="hljs-comment">// Cobra 的 command 可以无限级联，比如 `kubectl get pod` 就是在 `kubectl` command 下增加了子 `get` command</span>lsCmd.PersistentFlags().String(<span class="hljs-string">"foo"</span>, <span class="hljs-string">""</span>, <span class="hljs-string">"A help for foo"</span>)<span class="hljs-comment">// Flags() 方法添加的 flag 仅在直接调用此子命令时生效</span>lsCmd.Flags().BoolP(<span class="hljs-string">"toggle"</span>, <span class="hljs-string">"t"</span>, <span class="hljs-literal">false</span>, <span class="hljs-string">"Help message for toggle"</span>)</code></pre><h3 id="1-2、vendor-依赖"><a href="#1-2、vendor-依赖" class="headerlink" title="1.2、vendor 依赖"></a>1.2、vendor 依赖</h3><p>vendor 目录用于存放 Golang 的依赖库，sample-cli-plugin 这个项目采用 <a href="https://github.com/tools/godep" target="_blank" rel="noopener">godep</a> 工具管理依赖；依赖配置信息被保存在 <code>Godeps/Godeps.json</code> 中，<strong>一般项目不会上传 vendor 目录，因为它的依赖信息已经在 Godeps.json 中存在，只需要在项目下使用 <code>godep restore</code> 命令恢复就可自动重新下载</strong>；这里上传了 vendor 目录的原因应该是为了方便开发者直接使用 <code>go get</code> 命令安装；顺边说一下在 Golang 新版本已经开始转换到 <code>go mod</code> 依赖管理工具，标志就是项目下会有 <code>go.mod</code> 文件</p><h2 id="二、源码分析"><a href="#二、源码分析" class="headerlink" title="二、源码分析"></a>二、源码分析</h2><h3 id="2-1、环境搭建"><a href="#2-1、环境搭建" class="headerlink" title="2.1、环境搭建"></a>2.1、环境搭建</h3><p>这里准备一笔带过了，基本就是 clone 源码到 <code>$GOPATH/src/k8s.io/sample-cli-plugin</code> 目录，然后在 GoLand 中打开；目前我使用的 Go 版本为最新的 1.11.4；以下时导入源码后的截图</p><p><img src="https://cdn.oss.link/markdown/sn8o8.png" srcset="/img/loading.gif" alt="GoLand"></p><h3 id="2-2、定位核心运行方法"><a href="#2-2、定位核心运行方法" class="headerlink" title="2.2、定位核心运行方法"></a>2.2、定位核心运行方法</h3><p>熟悉过 Cobra 库以后，再从整个项目包名上分析，首先想到的启动入口应该在 <code>cmd</code> 包下(一般 <code>cmd</code> 包下的文件都会编译成最终可执行文件名，Kubernetes 也是一样)</p><p><img src="https://cdn.oss.link/markdown/rafeq.png" srcset="/img/loading.gif" alt="main"></p><p>从以上截图中可以看出，首先通过 <code>cmd.NewCmdNamespace</code> 方法创建了一个 Command 对象 <code>root</code>，然后调用了 <code>root.Execute</code> 就结束了；那么也就说明 <code>root</code> 这个 Command 是唯一的核心命令对象，整个插件实现都在这个 <code>root</code> 里；所以我们需要查看一下这个 <code>cmd.NewCmdNamespace</code> 是如何对它初始化的，找到 Cobra 中的 <code>Run</code> 或者 <code>RunE</code> 设置</p><p><img src="https://cdn.oss.link/markdown/77krg.png" srcset="/img/loading.gif" alt="NewCmdNamespace"></p><p>定位到 <code>NewCmdNamespace</code> 方法以后，基本上就是标准的 Cobra 库的使用方式了；<strong>从截图上可以看到，<code>RunE</code> 设置的函数总共运行了 3 个动作: <code>o.Complete</code>、<code>o.Validate</code>、<code>o.Run</code></strong>；所以接下来我们主要分析这三个方法就行了</p><h3 id="2-3、NamespaceOptions-结构体"><a href="#2-3、NamespaceOptions-结构体" class="headerlink" title="2.3、NamespaceOptions 结构体"></a>2.3、NamespaceOptions 结构体</h3><p>在分析上面说的这三个方法之前，我们还应当了解一下这个 <code>o</code> 是什么玩意</p><p><img src="https://cdn.oss.link/markdown/4b3cc.png" srcset="/img/loading.gif" alt="NamespaceOptions"></p><p>从源码中可以看到，<code>o</code> 这个对象由 <code>NewNamespaceOptions</code> 创建，而 <code>NewNamespaceOptions</code> 方法返回的实际上是一个 <code>NamespaceOptions</code> 结构体；接下来我们需要研究一下这个结构体都是由什么组成的，换句话说要基本大致上整明白结构体的基本结构，比如里面的属性都是干啥的</p><h4 id="2-3-1、-genericclioptions-ConfigFlags"><a href="#2-3-1、-genericclioptions-ConfigFlags" class="headerlink" title="2.3.1、*genericclioptions.ConfigFlags"></a>2.3.1、*genericclioptions.ConfigFlags</h4><p>首先看下第一个属性 <code>configFlags</code>，它的实际类型是 <code>*genericclioptions.ConfigFlags</code>，点击查看以后如下</p><p><img src="https://cdn.oss.link/markdown/li6s4.png" srcset="/img/loading.gif" alt="genericclioptions.ConfigFlags"></p><p>从这些字段上来看，我们可以暂且模糊的推测出这应该是个基础配置型的字段，负责存储一些全局基本设置，比如 API Server 认证信息等</p><h4 id="2-3-2、-api-Context"><a href="#2-3-2、-api-Context" class="headerlink" title="2.3.2、*api.Context"></a>2.3.2、*api.Context</h4><p>下面这两个 <code>resultingContext</code>、<code>resultingContextName</code> 就很好理解了，从名字上看就可以知道它们应该是用来存储结果集的 Context 信息的；当然这个 <code>*api.Context</code> 就是 Kubernetes 配置文件中 Context 的 Go 结构体</p><h4 id="2-3-3、userSpecified"><a href="#2-3-3、userSpecified" class="headerlink" title="2.3.3、userSpecified*"></a>2.3.3、userSpecified*</h4><p>这几个字段从名字上就可以区分出，他们应该用于存储用户设置的或者说是通过命令行选项输入的一些指定配置信息，比如 Cluster、Context 等</p><h4 id="2-3-4、rawConfig"><a href="#2-3-4、rawConfig" class="headerlink" title="2.3.4、rawConfig"></a>2.3.4、rawConfig</h4><p>rawConfig 这个变量名字有点子奇怪，不过它实际上是个 <code>api.Config</code>；里面保存了与 API Server 通讯的配置信息；<strong>至于为什么要有这玩意，是因为配置信息输入源有两个: cli 命令行选项(eg: <code>--namespace</code>)和用户配置文件(eg: <code>~/.kube/config</code>)；最终这两个地方的配置合并后会存储在这个 rawConfig 里</strong></p><h4 id="2-3-5、listNamespaces"><a href="#2-3-5、listNamespaces" class="headerlink" title="2.3.5、listNamespaces"></a>2.3.5、listNamespaces</h4><p>这个变量实际上相当于一个 flag，用于存储插件是否使用了 <code>--list</code> 选项；在分析结构体这里没法看出来；不过只要稍稍的多看一眼代码就能看在 <code>NewCmdNamespace</code> 方法中有这么一行代码</p><p><img src="https://cdn.oss.link/markdown/f07l3.png" srcset="/img/loading.gif" alt="listNamespaces"></p><h3 id="2-4、核心处理逻辑"><a href="#2-4、核心处理逻辑" class="headerlink" title="2.4、核心处理逻辑"></a>2.4、核心处理逻辑</h3><p>介绍完了结构体的基本属性，最后我们只需要弄明白在核心 Command 方法内运行的这三个核心方法就行了</p><p><img src="https://cdn.oss.link/markdown/8lm4b.png" srcset="/img/loading.gif" alt="core func"></p><h4 id="2-4-1、-NamespaceOptions-Complete"><a href="#2-4-1、-NamespaceOptions-Complete" class="headerlink" title="2.4.1、*NamespaceOptions.Complete"></a>2.4.1、*NamespaceOptions.Complete</h4><p>这个方法代码稍微有点多，这里不会对每一行代码都做解释，只要大体明白都在干什么就行了；我们的目的是理解它，后续模仿它创造自己的插件；下面是代码截图</p><p><img src="https://cdn.oss.link/markdown/qqf0f.png" srcset="/img/loading.gif" alt="NamespaceOptions.Complete"></p><p>从截图上可以看到，首先弄出了 <code>rawConfig</code> 这个玩意，<code>rawConfig</code> 上面也提到了，它就是终端选项和用户配置文件的最终合并，至于为什么可以查看 <code>ToRawKubeConfigLoader().RawConfig()</code> 这两个方法的注释和实现即可；</p><p>接下来就是各种获取插件执行所需要的变量信息，比如获取用户指定的 <code>Namespace</code>、<code>Cluster</code>、<code>Context</code> 等，其中还包含了一些必要的校验；比如不允许使用 <code>kubectl ns NS_NAME1 --namespace NS_NAME2</code> 这种操作(因为这么干很让人难以理解 “你到底是要切换到 <code>NS_NAME1</code> 还是 <code>NS_NAME2</code>“)</p><p>最后从 <code>153</code> 行 <code>o.resultingContext = api.NewContext()</code> 开始就是创建最终的 <code>resultingContext</code> 对象，把获取到的用户指定的 <code>Namespace</code> 等各种信息赋值好，为下一步将其持久化到配置文件中做准备</p><h4 id="2-4-2、-NamespaceOptions-Validate"><a href="#2-4-2、-NamespaceOptions-Validate" class="headerlink" title="2.4.2、*NamespaceOptions.Validate"></a>2.4.2、*NamespaceOptions.Validate</h4><p>这个方法看名字就知道，里面全是对最终结果的校验；比如检查一下 <code>rawConfig</code> 中的 <code>CurrentContext</code> 是否获取到了，看看命令行参数是否正确，确保你不会瞎鸡儿输入 <code>kubectl ns NS_NAME1 NS_NAME2</code> 这种命令</p><p><img src="https://cdn.oss.link/markdown/frqpb.png" srcset="/img/loading.gif" alt="NamespaceOptions.Validate"></p><h4 id="2-4-3、-NamespaceOptions-Run"><a href="#2-4-3、-NamespaceOptions-Run" class="headerlink" title="2.4.3、*NamespaceOptions.Run"></a>2.4.3、*NamespaceOptions.Run</h4><p>第一步合并配置信息并获取到用户设置(输入)的配置，第二部做参数校验；可以说前面的两步操作都是为这一步做准备，<code>Run</code> 方法真正的做了配置文件写入、终端返回结果打印操作</p><p><img src="https://cdn.oss.link/markdown/6tkjz.png" srcset="/img/loading.gif" alt="NamespaceOptions.Run"></p><p>可以看到，<code>Run</code> 方法第一步就是更加谨慎的检查了一下参数是否正常，然后调用了 <code>o.setNamespace</code>；这个方法截图如下</p><p><img src="https://cdn.oss.link/markdown/1jc3k.png" srcset="/img/loading.gif" alt="NamespaceOptions.setNamespace"></p><p>这个 <code>setNamespace</code>是真正的做了配置文件写入动作的，实际写入方法就是 <code>clientcmd.ModifyConfig</code>；这个是 <code>Kubernetes</code> <code>client-go</code> 提供的方法，这些库的作用就是提供给我们非常方便的 API 操作；比如修改配置文件，你不需要关心配置文件在哪，你更不需要关系文件句柄是否被释放</p><p>从 <code>o.setNamespace</code> 方法以后其实就没什么看头了，毕竟插件的核心功能就是快速修改 <code>Namespace</code>；下面的各种 <code>for</code> 循环遍历其实就是在做打印输出；比如当你没有设置 <code>Namespace</code> 而使用了 <code>--list</code> 选项，插件就通过这里帮你打印设置过那些 <code>Namespace</code></p><h2 id="三、插件总结"><a href="#三、插件总结" class="headerlink" title="三、插件总结"></a>三、插件总结</h2><p>分析完了这个官方的插件，然后想一下自己以后写插件可能的需求，最后对比一下，可以为以后写插件做个总结:</p><ul><li>我们最好也弄个 <code>xxxOptions</code> 这种结构体存存一些配置</li><li>结构体内至少我们应当存储 <code>configFlags</code>、<code>rawConfig</code> 这两个基础配置信息</li><li>结构体内其它参数都应当是跟自己实际业务有关的</li><li>最后在在结构体上增加适当的方法完成自己的业务逻辑并保持好适当的校验</li></ul><p>转载请注明出n，本文采用 [CC4.0](<a href="http://c" target="_blank" rel="noopener">http://c</a> 1.12 新的插件机制](<a href="https://mritd.me/2018/11/30/kubectl-plugin-new-solution-on-kubernetes-1.12/" target="_blank" rel="noopener">https://mritd.me/2018/11/30/kubectl-plugin-new-solution-on-kubernetes-1.12/</a>) 中最后部分对 <code>Golang 的插件辅助库</code> 说明；以及为后续使用 Golang 编写自己的 Kubernetes 插件做一个基础铺垫；顺边说一下 <strong>sample-cli-plugin 这个项目是官方为 Golang 开发者编写的一个用于快速切换配置文件中 Namespace 的一个插件样例</strong></p>]]></content>
    
    
    <categories>
      
      <category>Kubernetes</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Kubernetes</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Kubernetes 1.12 新的插件机制</title>
    <link href="/2018/11/30/kubectl-plugin-new-solution-on-kubernetes-1.12/"/>
    <url>/2018/11/30/kubectl-plugin-new-solution-on-kubernetes-1.12/</url>
    
    <content type="html"><![CDATA[<blockquote><p>在很久以前的版本研究过 kubernetes 的插件机制，当时弄了一个快速切换 <code>namespace</code> 的小插件；最近把自己本机的 kubectl 升级到了 1.12，突然发现插件不能用了；撸了一下文档发现插件机制彻底改了…</p></blockquote><h2 id="一、插件编写语言"><a href="#一、插件编写语言" class="headerlink" title="一、插件编写语言"></a>一、插件编写语言</h2><p>kubernetes 1.12 新的插件机制在编写语言上同以前一样，<strong>可以以任意语言编写，只要能弄一个可执行的文件出来就行</strong>，插件可以是一个 <code>bash</code>、<code>python</code> 脚本，也可以是 <code>Go</code> 等编译语言最终编译的二进制；以下是一个 Copy 自官方文档的 <code>bash</code> 编写的插件样例</p><pre><code class="hljs sh"><span class="hljs-meta">#!/bin/bash</span><span class="hljs-comment"># optional argument handling</span><span class="hljs-keyword">if</span> [[ <span class="hljs-string">"<span class="hljs-variable">$1</span>"</span> == <span class="hljs-string">"version"</span> ]]<span class="hljs-keyword">then</span>    <span class="hljs-built_in">echo</span> <span class="hljs-string">"1.0.0"</span>    <span class="hljs-built_in">exit</span> 0<span class="hljs-keyword">fi</span><span class="hljs-comment"># optional argument handling</span><span class="hljs-keyword">if</span> [[ <span class="hljs-string">"<span class="hljs-variable">$1</span>"</span> == <span class="hljs-string">"config"</span> ]]<span class="hljs-keyword">then</span>    <span class="hljs-built_in">echo</span> <span class="hljs-variable">$KUBECONFIG</span>    <span class="hljs-built_in">exit</span> 0<span class="hljs-keyword">fi</span><span class="hljs-built_in">echo</span> <span class="hljs-string">"I am a plugin named kubectl-foo"</span></code></pre><h2 id="二、插件加载方式"><a href="#二、插件加载方式" class="headerlink" title="二、插件加载方式"></a>二、插件加载方式</h2><h3 id="2-1、插件位置"><a href="#2-1、插件位置" class="headerlink" title="2.1、插件位置"></a>2.1、插件位置</h3><p>1.12 kubectl 插件最大的变化就是加载方式变了，由原来的放置在指定位置，还要为其编写 yaml 配置变成了现在的类似 git 扩展命令的方式: <strong>只要放置在 PATH 下，并以 <code>kubectl-</code> 开头的可执行文件都被认为是 <code>kubectl</code> 的插件</strong>；所以你可以随便弄个小脚本(比如上面的代码)，然后改好名字赋予可执行权限，扔到 PATH 下即可</p><p><img src="https://cdn.oss.link/markdown/s64v6.png" srcset="/img/loading.gif" alt="test-plugin"></p><h3 id="2-2、插件变量"><a href="#2-2、插件变量" class="headerlink" title="2.2、插件变量"></a>2.2、插件变量</h3><p>同以前不通，<strong>以前版本的执行插件时，<code>kubectl</code> 会向插件传递一些特定的与 <code>kubectl</code> 相关的变量，现在则只会传递标准变量；即 <code>kubectl</code> 能读到什么变量，插件就能读到，其他的私有化变量(比如 <code>KUBECTL_PLUGINS_CURRENT_NAMESPACE</code>)不会再提供</strong></p><p><img src="https://cdn.oss.link/markdown/vs1c3.png" srcset="/img/loading.gif" alt="plugin env"></p><p><strong>并且新版本的插件体系，所有选项(<code>flag</code>) 将全部交由插件本身处理，kubectl 不会再解析</strong>，比如下面的 <code>--help</code> 交给了自定义插件处理，由于脚本内没有处理这个选项，所以相当于选项无效了</p><p><img src="https://cdn.oss.link/markdown/8ch88.png" srcset="/img/loading.gif" alt="plugin flag"></p><p>还有就是 <strong>传递给插件的第一个参数永远是插件自己的绝对位置，比如这个 <code>test</code> 插件在执行时的 <code>$0</code> 是 <code>/usr/local/bin/kubectl-test</code></strong></p><h3 id="2-3、插件命名及查找"><a href="#2-3、插件命名及查找" class="headerlink" title="2.3、插件命名及查找"></a>2.3、插件命名及查找</h3><p>目前在插件命名及查找顺序上官方文档写的非常详尽，不给过对于普通使用者来说，实际上命名规则和查找与常规的 Linux 下的命令查找机制相同，只不过还做了增强；增强后的基本规则如下</p><ul><li><code>PATH</code> 优先匹配原则</li><li>短横线 <code>-</code> 自动分割匹配以及智能转义</li><li>以最精确匹配为首要目标</li><li>查找失败自动转换参数</li></ul><p><code>PATH</code> 优先匹配原则跟传统的命令查找一致，即当多个路径下存在同名的插件时，则采用最先查找到的插件</p><p><img src="https://cdn.oss.link/markdown/ljyp5.png" srcset="/img/loading.gif" alt="plugin path"></p><p>当你的插件文件名中包含 <code>-</code> ，并且 <code>kubectl</code> 在无法精确找到插件时会尝试自动拼接命令来尝试匹配；如下所示，在没有找到 <code>kubectl-test</code> 这个命令时会尝试拼接参数查找</p><p><img src="https://cdn.oss.link/markdown/l85bp.png" srcset="/img/loading.gif" alt="auto merge"></p><p>由于以上这种查找机制，<strong>当命令中确实包含 <code>-</code> 时，必须进行转义以 <code>_</code> 替换，否则 <code>kubectl</code> 会提示命令未找到错误</strong>；替换后可直接使用 <code>kubectl 插件命令(包含-)</code> 执行，同时也支持以原始插件名称执行(使用 <code>_</code>)</p><p><img src="https://cdn.oss.link/markdown/7vm0l.png" srcset="/img/loading.gif" alt="name contains dash"></p><p>在复杂插件体系下，多个插件可能包含同样的前缀，此时将遵序最精确查找原则；即当两个插件 <code>kubectl-test-aaa</code>、<code>kubectl-test-aaa-bbb</code> 同时存在，并且执行 <code>kubectl test aaa bbb</code> 命令时，优先匹配最精确的插件 <code>kubectl-test-aaa-bbb</code>，<strong>而不是将 <code>bbb</code> 作为参数传递给 <code>kubectl-test-aaa</code> 插件</strong></p><p><img src="https://cdn.oss.link/markdown/god8q.png" srcset="/img/loading.gif" alt="precise search"></p><h3 id="2-4、总结"><a href="#2-4、总结" class="headerlink" title="2.4、总结"></a>2.4、总结</h3><p>插件查找机制在一般情况下与传统 PATH 查找方式相同，同时 <code>kubectl</code> 实现了智能的 <code>-</code> 自动匹配查找、更精确的命令命中功能；这两种机制的实现主要为了方便编写插件的命令树(插件命令的子命令…)，类似下面这种</p><pre><code class="hljs sh">$ ls ./plugin_command_treekubectl-parentkubectl-parent-subcommandkubectl-parent-subcommand-subsubcommand</code></pre><p>当出现多个位置有同名插件时，执行 <code>kubectl plugin list</code> 能够检测出哪些插件由于 PATH 查找顺序原因导致永远不会被执行问题</p><pre><code class="hljs sh">$ kubectl plugin listThe following kubectl-compatible plugins are available:<span class="hljs-built_in">test</span>/fixtures/pkg/kubectl/plugins/kubectl-foo/usr/<span class="hljs-built_in">local</span>/bin/kubectl-foo  - warning: /usr/<span class="hljs-built_in">local</span>/bin/kubectl-foo is overshadowed by a similarly named plugin: <span class="hljs-built_in">test</span>/fixtures/pkg/kubectl/plugins/kubectl-fooplugins/kubectl-invalid  - warning: plugins/kubectl-invalid identified as a kubectl plugin, but it is not executableerror: 2 plugin warnings were found</code></pre><h3 id="三、Golang-的插件辅助库"><a href="#三、Golang-的插件辅助库" class="headerlink" title="三、Golang 的插件辅助库"></a>三、Golang 的插件辅助库</h3><p>由于插件机制的变更，导致其他语言编写的插件在实时获取某些配置信息、动态修改 <code>kubectl</code> 配置方面可能造成一定的阻碍；为此 kubernetes 提供了一个 <a href="https://github.com/kubernetes/cli-runtime" target="_blank" rel="noopener">command line runtime package</a>，使用 Go 编写插件，配合这个库可以更加方便的解析和调整 <code>kubectl</code> 的配置信息</p><p>官方为了演示如何使用这个 <a href="https://github.com/kubernetes/cli-runtime" target="_blank" rel="noopener">cli-runtime</a> 库编写了一个 <code>namespace</code> 切换的插件(自己白写了…)，仓库地址在 <a href="https://github.com/kubernetes/sample-cli-plugin" target="_blank" rel="noopener">Github</a> 上，基本编译使用如下(直接 <code>go get</code> 后编译文件默认为目录名 <code>cmd</code>)</p><pre><code class="hljs sh">➜  ~ go get k8s.io/sample-cli-plugin/cmd➜  ~ sudo mv gopath/bin/cmd /usr/<span class="hljs-built_in">local</span>/bin/kubectl-ns➜  ~ kubectl nsdefault➜  ~ kubectl ns --<span class="hljs-built_in">help</span>View or <span class="hljs-built_in">set</span> the current namespaceUsage:  ns [new-namespace] [flags]Examples:        <span class="hljs-comment"># view the current namespace in your KUBECONFIG</span>        kubectl ns        <span class="hljs-comment"># view all of the namespaces in use by contexts in your KUBECONFIG</span>        kubectl ns --list        <span class="hljs-comment"># switch your current-context to one that contains the desired namespace</span>        kubectl ns fooFlags:      --as string                      Username to impersonate <span class="hljs-keyword">for</span> the operation      --as-group stringArray           Group to impersonate <span class="hljs-keyword">for</span> the operation, this flag can be repeated to specify multiple groups.      --cache-dir string               Default HTTP cache directory (default <span class="hljs-string">"/Users/mritd/.kube/http-cache"</span>)      --certificate-authority string   Path to a cert file <span class="hljs-keyword">for</span> the certificate authority      --client-certificate string      Path to a client certificate file <span class="hljs-keyword">for</span> TLS      --client-key string              Path to a client key file <span class="hljs-keyword">for</span> TLS      --cluster string                 The name of the kubeconfig cluster to use      --context string                 The name of the kubeconfig context to use  -h, --<span class="hljs-built_in">help</span>                           <span class="hljs-built_in">help</span> <span class="hljs-keyword">for</span> ns      --insecure-skip-tls-verify       If <span class="hljs-literal">true</span>, the server<span class="hljs-string">'s certificate will not be checked for validity. This will make your HTTPS connections insecure</span><span class="hljs-string">      --kubeconfig string              Path to the kubeconfig file to use for CLI requests.</span><span class="hljs-string">      --list                           if true, print the list of all namespaces in the current KUBECONFIG</span><span class="hljs-string">  -n, --namespace string               If present, the namespace scope for this CLI request</span><span class="hljs-string">      --request-timeout string         The length of time to wait before giving up on a single server request. Non-zero values should contain a corresponding time unit (e.g. 1s, 2m, 3h). A value of zero means don'</span>t timeout requests. (default <span class="hljs-string">"0"</span>)  -s, --server string                  The address and port of the Kubernetes API server      --token string                   Bearer token <span class="hljs-keyword">for</span> authentication to the API server      --user string                    The name of the kubeconfig user to use</code></pre><p>限于篇幅原因，具体这个 <code>cli-runtime</code> 包怎么用请自行参考官方写的这个 <code>sample-cli-plugin</code> (其实并不怎么 “simple”…)</p><p>本文参考文档:</p><ul><li><a href="https://kubernetes.io/docs/tasks/extend-kubectl/kubectl-plugins/" target="_blank" rel="noopener">Extend kubectl with plugins</a></li><li><a href="https://github.com/kubernetes/cli-runtime" target="_blank" rel="noopener">cli-runtime</a></li><li><a href="https://github.com/kubernetes/sample-cli-plugin" target="_blank" rel="noopener">sample-cli-plugin</a></li></ul>]]></content>
    
    
    <categories>
      
      <category>Kubernetes</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Kubernetes</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Go 编写的一些常用小工具</title>
    <link href="/2018/11/27/simple-tool-written-in-golang/"/>
    <url>/2018/11/27/simple-tool-written-in-golang/</url>
    
    <content type="html"><![CDATA[<blockquote><p>迫于 Github 上 Star 的项目有点多，今天整理一下一些有意思的 Go 编写的小工具；大多数为终端下的实用工具，装逼的比如天气预报啥的就不写了</p></blockquote><h3 id="syncthing"><a href="#syncthing" class="headerlink" title="syncthing"></a>syncthing</h3><p>强大的文件同步工具，构建私人同步盘 👉 <a href="https://github.com/syncthing、syncthing" target="_blank" rel="noopener">Github</a></p><p><img src="https://cdn.oss.link/markdown/er3tj.jpg" srcset="/img/loading.gif" alt="syncthing"></p><h3 id="fzf"><a href="#fzf" class="headerlink" title="fzf"></a>fzf</h3><p>一个强大的终端文件浏览器 👉 <a href="https://github.com/junegunn/fzf" target="_blank" rel="noopener">Github</a></p><p><img src="https://cdn.oss.link/markdown/ihhqy.jpg" srcset="/img/loading.gif" alt="fzf"></p><h3 id="hey"><a href="#hey" class="headerlink" title="hey"></a>hey</h3><p>http 负载测试工具，简单好用 👉 <a href="https://github.com/rakyll/hey" target="_blank" rel="noopener">Github</a></p><pre><code class="hljs sh">Usage: hey [options...] &lt;url&gt;Options:  -n  Number of requests to run. Default is 200.  -c  Number of requests to run concurrently. Total number of requests cannot      be smaller than the concurrency level. Default is 50.  -q  Rate <span class="hljs-built_in">limit</span>, <span class="hljs-keyword">in</span> queries per second (QPS). Default is no rate <span class="hljs-built_in">limit</span>.  -z  Duration of application to send requests. When duration is reached,      application stops and exits. If duration is specified, n is ignored.      Examples: -z 10s -z 3m.  -o  Output <span class="hljs-built_in">type</span>. If none provided, a summary is printed.      <span class="hljs-string">"csv"</span> is the only supported alternative. Dumps the response      metrics <span class="hljs-keyword">in</span> comma-separated values format.  -m  HTTP method, one of GET, POST, PUT, DELETE, HEAD, OPTIONS.  -H  Custom HTTP header. You can specify as many as needed by repeating the flag.      For example, -H <span class="hljs-string">"Accept: text/html"</span> -H <span class="hljs-string">"Content-Type: application/xml"</span> .  -t  Timeout <span class="hljs-keyword">for</span> each request <span class="hljs-keyword">in</span> seconds. Default is 20, use 0 <span class="hljs-keyword">for</span> infinite.  -A  HTTP Accept header.  -d  HTTP request body.  -D  HTTP request body from file. For example, /home/user/file.txt or ./file.txt.  -T  Content-type, defaults to <span class="hljs-string">"text/html"</span>.  -a  Basic authentication, username:password.  -x  HTTP Proxy address as host:port.  -h2 Enable HTTP/2.  -host    HTTP Host header.  -<span class="hljs-built_in">disable</span>-compression  Disable compression.  -<span class="hljs-built_in">disable</span>-keepalive    Disable keep-alive, prevents re-use of TCP                        connections between different HTTP requests.  -<span class="hljs-built_in">disable</span>-redirects    Disable following of HTTP redirects  -cpus                 Number of used cpu cores.                        (default <span class="hljs-keyword">for</span> current machine is 8 cores)</code></pre><h3 id="vegeta"><a href="#vegeta" class="headerlink" title="vegeta"></a>vegeta</h3><p>http 负载测试工具，功能强大 👉 <a href="https://github.com/tsenart/vegeta" target="_blank" rel="noopener">Github</a></p><pre><code class="hljs sh">Usage: vegeta [global flags] &lt;<span class="hljs-built_in">command</span>&gt; [<span class="hljs-built_in">command</span> flags]global flags:  -cpus int        Number of CPUs to use (default 8)  -profile string        Enable profiling of [cpu, heap]  -version        Print version and <span class="hljs-built_in">exit</span>attack <span class="hljs-built_in">command</span>:  -body string        Requests body file  -cert string        TLS client PEM encoded certificate file  -connections int        Max open idle connections per target host (default 10000)  -duration duration        Duration of the <span class="hljs-built_in">test</span> [0 = forever]  -format string        Targets format [http, json] (default <span class="hljs-string">"http"</span>)  -h2c        Send HTTP/2 requests without TLS encryption  -header value        Request header  -http2        Send HTTP/2 requests when supported by the server (default <span class="hljs-literal">true</span>)  -insecure        Ignore invalid server TLS certificates  -keepalive        Use persistent connections (default <span class="hljs-literal">true</span>)  -key string        TLS client PEM encoded private key file  -laddr value        Local IP address (default 0.0.0.0)  -lazy        Read targets lazily  -max-body value        Maximum number of bytes to capture from response bodies. [-1 = no <span class="hljs-built_in">limit</span>] (default -1)  -name string        Attack name  -output string        Output file (default <span class="hljs-string">"stdout"</span>)  -rate value        Number of requests per time unit (default 50/1s)  -redirects int        Number of redirects to follow. -1 will not follow but marks as success (default 10)  -resolvers value        List of addresses (ip:port) to use <span class="hljs-keyword">for</span> DNS resolution. Disables use of <span class="hljs-built_in">local</span> system DNS. (comma separated list)  -root-certs value        TLS root certificate files (comma separated list)  -targets string        Targets file (default <span class="hljs-string">"stdin"</span>)  -timeout duration        Requests timeout (default 30s)  -workers uint        Initial number of workers (default 10)encode <span class="hljs-built_in">command</span>:  -output string        Output file (default <span class="hljs-string">"stdout"</span>)  -to string        Output encoding [csv, gob, json] (default <span class="hljs-string">"json"</span>)plot <span class="hljs-built_in">command</span>:  -output string        Output file (default <span class="hljs-string">"stdout"</span>)  -threshold int        Threshold of data points above <span class="hljs-built_in">which</span> series are downsampled. (default 4000)  -title string        Title and header of the resulting HTML page (default <span class="hljs-string">"Vegeta Plot"</span>)report <span class="hljs-built_in">command</span>:  -every duration        Report interval  -output string        Output file (default <span class="hljs-string">"stdout"</span>)  -<span class="hljs-built_in">type</span> string        Report <span class="hljs-built_in">type</span> to generate [text, json, hist[buckets]] (default <span class="hljs-string">"text"</span>)examples:  <span class="hljs-built_in">echo</span> <span class="hljs-string">"GET http://localhost/"</span> | vegeta attack -duration=5s | tee results.bin | vegeta report  vegeta report -<span class="hljs-built_in">type</span>=json results.bin &gt; metrics.json  cat results.bin | vegeta plot &gt; plot.html  cat results.bin | vegeta report -<span class="hljs-built_in">type</span>=<span class="hljs-string">"hist[0,100ms,200ms,300ms]"</span></code></pre><h3 id="dive"><a href="#dive" class="headerlink" title="dive"></a>dive</h3><p>功能强大的 Docker 镜像分析工具，可以查看每层镜像的具体差异等 👉 <a href="https://github.com/wagoodman/dive" target="_blank" rel="noopener">Github</a></p><p><img src="https://cdn.oss.link/markdown/ik3ng.gif" srcset="/img/loading.gif" alt="dive"></p><h3 id="ctop"><a href="#ctop" class="headerlink" title="ctop"></a>ctop</h3><p>容器运行时资源分析，如 CPU、内存消耗等 👉 <a href="https://github.com/bcicen/ctop" target="_blank" rel="noopener">Github</a></p><p><img src="https://cdn.oss.link/markdown/mr3x3.gif" srcset="/img/loading.gif" alt="ctop"></p><h3 id="container-diff"><a href="#container-diff" class="headerlink" title="container-diff"></a>container-diff</h3><p>Google 推出的工具，功能就顾名思义了 👉 <a href="https://github.com/GoogleContainerTools/container-diff" target="_blank" rel="noopener">Github</a></p><p><img src="https://cdn.oss.link/markdown/dtapx.png" srcset="/img/loading.gif" alt="container-diff"></p><h3 id="transfer-sh"><a href="#transfer-sh" class="headerlink" title="transfer.sh"></a>transfer.sh</h3><p>快捷的终端文件分享工具 👉 <a href="https://github.com/dutchcoders/transfer.sh" target="_blank" rel="noopener">Github</a></p><p><img src="https://cdn.oss.link/markdown/76vh0.png" srcset="/img/loading.gif" alt="transfer.sh"></p><h3 id="vuls"><a href="#vuls" class="headerlink" title="vuls"></a>vuls</h3><p> Linux/FreeBSD 漏洞扫描工具 👉 <a href="https://github.com/future-architect/vuls" target="_blank" rel="noopener">Github</a></p><p> <img src="https://cdn.oss.link/markdown/bpsps.jpg" srcset="/img/loading.gif" alt="vuls"></p><h3 id="restic"><a href="#restic" class="headerlink" title="restic"></a>restic</h3><p>高性能安全的文件备份工具 👉 <a href="https://github.com/restic/restic" target="_blank" rel="noopener">Github</a></p><p><img src="https://cdn.oss.link/markdown/g51z4.png" srcset="/img/loading.gif" alt="restic"></p><h3 id="gitql"><a href="#gitql" class="headerlink" title="gitql"></a>gitql</h3><p>使用 sql 的方式查询 git 提交 👉 <a href="https://github.com/cloudson/gitql" target="_blank" rel="noopener">Github</a></p><p><img src="https://cdn.oss.link/markdown/4h095.gif" srcset="/img/loading.gif" alt="gitql"></p><h3 id="gitflow-toolkit"><a href="#gitflow-toolkit" class="headerlink" title="gitflow-toolkit"></a>gitflow-toolkit</h3><p>帮助生成满足 Gitflow 格式 commit message 的小工具(自己写的) 👉 <a href="https://github.com/mritd/gitflow-toolkit" target="_blank" rel="noopener">Github</a></p><p><img src="https://cdn.oss.link/markdown/1e2v1.gif" srcset="/img/loading.gif" alt="gitflow-toolkit"></p><h3 id="git-chglog"><a href="#git-chglog" class="headerlink" title="git-chglog"></a>git-chglog</h3><p>对主流的 Gitflow 格式的 commit message 生成 CHANGELOG 👉 <a href="https://github.com/git-chglog/git-chglog" target="_blank" rel="noopener">Github</a></p><p><img src="https://cdn.oss.link/markdown/zphxd.gif" srcset="/img/loading.gif" alt="git-chglog"></p><h3 id="grv"><a href="#grv" class="headerlink" title="grv"></a>grv</h3><p>一个 git 终端图形化浏览工具 👉 <a href="https://github.com/rgburke/grv" target="_blank" rel="noopener">Github</a></p><p><img src="https://cdn.oss.link/markdown/k1vh2.jpg" srcset="/img/loading.gif" alt="grv"></p><h3 id="jid"><a href="#jid" class="headerlink" title="jid"></a>jid</h3><p>命令行 json 格式化处理工具，类似 jq，不过感觉更加强大 👉 <a href="https://github.com/simeji/jid" target="_blank" rel="noopener">Github</a></p><p><img src="https://cdn.oss.link/markdown/3k4ue.gif" srcset="/img/loading.gif" alt="jid"></p><h3 id="annie"><a href="#annie" class="headerlink" title="annie"></a>annie</h3><p>类似 youget 的一个视频下载工具，可以解析大部分视频网站直接下载 👉 <a href="https://github.com/iawia002/annie" target="_blank" rel="noopener">Github</a></p><pre><code class="hljs sh">$ annie -i https://www.youtube.com/watch?v=dQw4w9WgXcQ Site:      YouTube youtube.com Title:     Rick Astley - Never Gonna Give You Up (Video) Type:      video Streams:   <span class="hljs-comment"># All available quality</span>     [248]  -------------------     Quality:         1080p video/webm; codecs=<span class="hljs-string">"vp9"</span>     Size:            49.29 MiB (51687554 Bytes)     <span class="hljs-comment"># download with: annie -f 248 ...</span>     [137]  -------------------     Quality:         1080p video/mp4; codecs=<span class="hljs-string">"avc1.640028"</span>     Size:            43.45 MiB (45564306 Bytes)     <span class="hljs-comment"># download with: annie -f 137 ...</span>     [398]  -------------------     Quality:         720p video/mp4; codecs=<span class="hljs-string">"av01.0.05M.08"</span>     Size:            37.12 MiB (38926432 Bytes)     <span class="hljs-comment"># download with: annie -f 398 ...</span>     [136]  -------------------     Quality:         720p video/mp4; codecs=<span class="hljs-string">"avc1.4d401f"</span>     Size:            31.34 MiB (32867324 Bytes)     <span class="hljs-comment"># download with: annie -f 136 ...</span>     [247]  -------------------     Quality:         720p video/webm; codecs=<span class="hljs-string">"vp9"</span>     Size:            31.03 MiB (32536181 Bytes)     <span class="hljs-comment"># download with: annie -f 247 ...</span></code></pre><h3 id="up"><a href="#up" class="headerlink" title="up"></a>up</h3><p>Linux 下管道式终端搜索工具 👉 <a href="https://github.com/akavel/up" target="_blank" rel="noopener">Github</a></p><p><img src="https://cdn.oss.link/markdown/n8zdj.gif" srcset="/img/loading.gif" alt="up"></p><h3 id="lego"><a href="#lego" class="headerlink" title="lego"></a>lego</h3><p>Let’s Encrypt 证书申请工具 👉 <a href="https://github.com/xenolf/lego" target="_blank" rel="noopener">Github</a></p><pre><code class="hljs sh">NAME:   lego - Let<span class="hljs-string">'s Encrypt client written in Go</span><span class="hljs-string"></span><span class="hljs-string">USAGE:</span><span class="hljs-string">   lego [global options] command [command options] [arguments...]</span><span class="hljs-string"></span><span class="hljs-string">COMMANDS:</span><span class="hljs-string">     run      Register an account, then create and install a certificate</span><span class="hljs-string">     revoke   Revoke a certificate</span><span class="hljs-string">     renew    Renew a certificate</span><span class="hljs-string">     dnshelp  Shows additional help for the --dns global option</span><span class="hljs-string">     help, h  Shows a list of commands or help for one command</span><span class="hljs-string"></span><span class="hljs-string">GLOBAL OPTIONS:</span><span class="hljs-string">   --domains value, -d value   Add a domain to the process. Can be specified multiple times.</span><span class="hljs-string">   --csr value, -c value       Certificate signing request filename, if an external CSR is to be used</span><span class="hljs-string">   --server value, -s value    CA hostname (and optionally :port). The server certificate must be trusted in order to avoid further modifications to the client. (default: "https://acme-v02.api.letsencrypt.org/directory")</span><span class="hljs-string">   --email value, -m value     Email used for registration and recovery contact.</span><span class="hljs-string">   --filename value            Filename of the generated certificate</span><span class="hljs-string">   --accept-tos, -a            By setting this flag to true you indicate that you accept the current Let'</span>s Encrypt terms of service.   --eab                       Use External Account Binding <span class="hljs-keyword">for</span> account registration. Requires --kid and --hmac.   --kid value                 Key identifier from External CA. Used <span class="hljs-keyword">for</span> External Account Binding.   --hmac value                MAC key from External CA. Should be <span class="hljs-keyword">in</span> Base64 URL Encoding without padding format. Used <span class="hljs-keyword">for</span> External Account Binding.   --key-type value, -k value  Key <span class="hljs-built_in">type</span> to use <span class="hljs-keyword">for</span> private keys. Supported: rsa2048, rsa4096, rsa8192, ec256, ec384 (default: <span class="hljs-string">"rsa2048"</span>)   --path value                Directory to use <span class="hljs-keyword">for</span> storing the data (default: <span class="hljs-string">"./.lego"</span>)   --exclude value, -x value   Explicitly disallow solvers by name from being used. Solvers: <span class="hljs-string">"http-01"</span>, <span class="hljs-string">"dns-01"</span>, <span class="hljs-string">"tls-alpn-01"</span>.   --webroot value             Set the webroot folder to use <span class="hljs-keyword">for</span> HTTP based challenges to write directly <span class="hljs-keyword">in</span> a file <span class="hljs-keyword">in</span> .well-known/acme-challenge   --memcached-host value      Set the memcached host(s) to use <span class="hljs-keyword">for</span> HTTP based challenges. Challenges will be written to all specified hosts.   --http value                Set the port and interface to use <span class="hljs-keyword">for</span> HTTP based challenges to listen on. Supported: interface:port or :port   --tls value                 Set the port and interface to use <span class="hljs-keyword">for</span> TLS based challenges to listen on. Supported: interface:port or :port   --dns value                 Solve a DNS challenge using the specified provider. Disables all other challenges. Run <span class="hljs-string">'lego dnshelp'</span> <span class="hljs-keyword">for</span> <span class="hljs-built_in">help</span> on usage.   --http-timeout value        Set the HTTP timeout value to a specific value <span class="hljs-keyword">in</span> seconds. The default is 10 seconds. (default: 0)   --dns-timeout value         Set the DNS timeout value to a specific value <span class="hljs-keyword">in</span> seconds. The default is 10 seconds. (default: 0)   --dns-resolvers value       Set the resolvers to use <span class="hljs-keyword">for</span> performing recursive DNS queries. Supported: host:port. The default is to use the system resolvers, or Google<span class="hljs-string">'s DNS resolvers if the system'</span>s cannot be determined.   --pem                       Generate a .pem file by concatenating the .key and .crt files together.   --<span class="hljs-built_in">help</span>, -h                  show <span class="hljs-built_in">help</span>   --version, -v               <span class="hljs-built_in">print</span> the version</code></pre><h3 id="noti"><a href="#noti" class="headerlink" title="noti"></a>noti</h3><p>贼好用的终端命令异步执行通知工具 👉 <a href="https://github.com/variadico/noti" target="_blank" rel="noopener">Github</a></p><p><img src="https://cdn.oss.link/markdown/m2r1e.jpg" srcset="/img/loading.gif" alt="noti"></p><h3 id="gosu"><a href="#gosu" class="headerlink" title="gosu"></a>gosu</h3><p>临时切换到指定用户运行特定命令，方便测试权限问题 👉 <a href="https://github.com/tianon/gosu" target="_blank" rel="noopener">Github</a></p><pre><code class="hljs sh">$ gosuUsage: ./gosu user-spec <span class="hljs-built_in">command</span> [args]   eg: ./gosu tianon bash       ./gosu nobody:root bash -c <span class="hljs-string">'whoami &amp;&amp; id'</span>       ./gosu 1000:1 id</code></pre><h3 id="sup"><a href="#sup" class="headerlink" title="sup"></a>sup</h3><p>类似 Ansible 的一个批量执行工具，暂且称之为低配版 Ansible 👉 <a href="https://github.com/pressly/sup" target="_blank" rel="noopener">Github</a></p><p><img src="https://cdn.oss.link/markdown/x0eaz.gif" srcset="/img/loading.gif" alt="sup"></p><h3 id="aptly"><a href="#aptly" class="headerlink" title="aptly"></a>aptly</h3><p>Debian 仓库管理工具 👉 <a href="https://github.com/aptly-dev/aptly" target="_blank" rel="noopener">Github</a></p><p><img src="https://cdn.oss.link/markdown/8e0ml.jpg" srcset="/img/loading.gif" alt="aptly"></p><h3 id="mmh"><a href="#mmh" class="headerlink" title="mmh"></a>mmh</h3><p>支持无限跳板机登录的 ssh 小工具(自己写的) 👉 <a href="https://github.com/mritd/mmh" target="_blank" rel="noopener">Github</a></p><p><img src="https://cdn.oss.link/markdown/37638.gif" srcset="/img/loading.gif" alt="mmh"></p>]]></content>
    
    
    <categories>
      
      <category>Golang</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Golang</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>远程 Debug kubeadm</title>
    <link href="/2018/11/25/kubeadm-remote-debug/"/>
    <url>/2018/11/25/kubeadm-remote-debug/</url>
    
    <content type="html"><![CDATA[<blockquote><p>最近在看 kubeadm 的源码，不过有些东西光看代码还是没法太清楚，还是需要实际运行才能看到具体代码怎么跑的，还得打断点 debug；无奈的是本机是 mac，debug 得在 Linux 下，so 研究了一下 remote debug</p></blockquote><h2 id="一、环境准备"><a href="#一、环境准备" class="headerlink" title="一、环境准备"></a>一、环境准备</h2><ul><li>GoLand 2018.2.4</li><li>Golang 1.11.2</li><li>delve v1.1.0</li><li>Kubernetest master</li><li>Ubuntu 18.04</li><li>能够高速访问外网(自行理解)</li></ul><p><strong>这里不会详细写如何安装 Go 开发环境以及 GoLand 安装，本文默认读者已经至少已经对 Go 开发环境以及代码有一定了解；顺便提一下 GoLand，这玩意属于 jetbrains 系列 IDE，在大约 2018.1 版本后在线激活服务器已经全部失效，不过网上还有其他本地离线激活工具，具体请自行 Google，如果后续工资能支撑得起，请补票支持正版(感恩节全家桶半价真香😂)</strong></p><h3 id="1-1、获取源码"><a href="#1-1、获取源码" class="headerlink" title="1.1、获取源码"></a>1.1、获取源码</h3><p>需要注意的是 Kubernetes 源码虽然托管在 Github，但是在使用 <code>go get</code> 的时候要使用 <code>k8s.io</code> 域名</p><pre><code class="hljs sh">go get -d k8s.io/kubernetes</code></pre><p><code>go get</code> 命令是接受标准的 http 代理的，这个源码下载会非常慢，源码大约 1G 左右，所以最好使用加速工具下载</p><pre><code class="hljs sh">➜  ~ <span class="hljs-built_in">which</span> proxy/usr/<span class="hljs-built_in">local</span>/bin/proxy➜  ~ cat /usr/<span class="hljs-built_in">local</span>/bin/proxy<span class="hljs-meta">#!/bin/bash</span>http_proxy=http://127.0.0.1:8123 https_proxy=http://127.0.0.1:8123 $*➜  ~ proxy go get -d k8s.io/kubernetes</code></pre><h3 id="1-2、安装-delve"><a href="#1-2、安装-delve" class="headerlink" title="1.2、安装 delve"></a>1.2、安装 delve</h3><p>delve 是一个 Golang 的 debug 工具，有点类似 gdb，不过是专门针对 Golang 的，GoLand 的 debug 实际上就是使用的这个开源工具；为了进行远程 debug，运行 kubeadm 的机器必须安装 delve，从而进行远程连接</p><pre><code class="hljs sh"><span class="hljs-comment"># 同样这里省略在 Linux 安装 go 环境操作</span>go get -u github.com/derekparker/delve/cmd/dlv</code></pre><h2 id="二、远程-Debug"><a href="#二、远程-Debug" class="headerlink" title="二、远程 Debug"></a>二、远程 Debug</h2><h3 id="2-1、重新编译-kubeadm"><a href="#2-1、重新编译-kubeadm" class="headerlink" title="2.1、重新编译 kubeadm"></a>2.1、重新编译 kubeadm</h3><p>默认情况下直接编译出的 kubeadm 是无法进行 debug 的，因为 Golang 的编译器会进行编译优化，比如进行内联等；所以要关闭编译优化和内联，方便 debug</p><pre><code class="hljs sh"><span class="hljs-built_in">cd</span> <span class="hljs-variable">$&#123;GOPATH&#125;</span>/src/k8s.io/kubernetes/cmd/kubeadmGOOS=<span class="hljs-string">"linux"</span> GOARCH=<span class="hljs-string">"amd64"</span> go build -gcflags <span class="hljs-string">"all=-N -l"</span></code></pre><h3 id="2-2、远程运行-kubeadm"><a href="#2-2、远程运行-kubeadm" class="headerlink" title="2.2、远程运行 kubeadm"></a>2.2、远程运行 kubeadm</h3><p>将编译好的 kubeadm 复制到远程，并且使用 delve 启动它，此时 delve 会监听 api 端口，GoLand 就可以远程连接过来了</p><pre><code class="hljs sh">dlv --listen=192.168.1.61:2345 --headless=<span class="hljs-literal">true</span> --api-version=2 <span class="hljs-built_in">exec</span> ./kubeadm init</code></pre><p><strong>注意: 要指定需要 debug 的 kubeadm 的子命令，否则可能出现连接上以后 GoLand 无反应的情况</strong></p><h3 id="2-3、运行-GoLand"><a href="#2-3、运行-GoLand" class="headerlink" title="2.3、运行 GoLand"></a>2.3、运行 GoLand</h3><p>在 GoLand 中打开 kubernetes 源码，在需要 debug 的代码中打上断点，这里以 init 子命令为例</p><p>首先新建一个远程 debug configuration</p><p><img src="https://cdn.oss.link/markdown/i6oed.png" srcset="/img/loading.gif" alt="create configuration"></p><p>名字可以随便写，主要是地址和端口</p><p><img src="https://cdn.oss.link/markdown/rmczj.png" srcset="/img/loading.gif" alt="conifg delve"></p><p>接下来在目标源码位置打断点，以下为 init 子命令的源码位置</p><p><img src="https://cdn.oss.link/markdown/ylf97.png" srcset="/img/loading.gif" alt="create breakpoint"></p><p>最后只需要点击 debug 按钮即可</p><p><img src="https://cdn.oss.link/markdown/ns2yw.png" srcset="/img/loading.gif" alt="debug"></p><p><strong>在没有运行 GoLand debug 之前，目标机器的实际指令是不会运行的，也就是说在 GoLand 没有连接到远程 delve 启动的 <code>kubeadm init</code> 命令之前，<code>kubeadm init</code> 并不会真正运行；当点击 GoLand 的终止 debug 按钮后，远程的 delve 也会随之退出</strong></p><p><img src="https://cdn.oss.link/markdown/lmdke.png" srcset="/img/loading.gif" alt="stop"></p>]]></content>
    
    
    <categories>
      
      <category>Kubernetes</category>
      
      <category>Golang</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Kubernetes</tag>
      
      <tag>Golang</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Mac: Extract JDK to folder, without running installer</title>
    <link href="/2018/11/23/extract-jdk-to-folder-on-mac/"/>
    <url>/2018/11/23/extract-jdk-to-folder-on-mac/</url>
    
    <content type="html"><![CDATA[<blockquote><p>重装了 mac 系统，由于一些公司项目必须使用 Oracle JDK(验证码等组件用了一些 Oracle 独有的 API) 所以又得重新安装；但是 Oracle 只提供了 pkg 的安装方式，研究半天找到了一个解包 pkg 的安装方式，这里记录一下</p></blockquote><p>不使用 pkg 的原因是每次更新版本都要各种安装，最烦人的是 IDEA 选择 JDK 时候弹出的文件浏览器没法进入到这种方式安装的 JDK 的系统目录…mmp，后来从国外网站找到了一篇文章，基本套路如下</p><ul><li>下载 Oracle JDK，从 dmg 中拷贝 pkg 到任意位置</li><li>解压 pkg 到任意位置 <code>pkgutil --expand your_jdk.pkg jdkdir</code></li><li>进入到目录中，解压主文件 <code>cd jdkdir/jdk_version.pkg &amp;&amp; cpio -idv &lt; Payload</code></li><li>移动 jdk 到任意位置 <code>mv Contents/Home ~/myjdk</code></li></ul><p>原文地址: <a href="https://augustl.com/blog/2014/extracting_java_to_folder_no_installer_osx/" target="_blank" rel="noopener">OS X: Extract JDK to folder, without running installer</a></p>]]></content>
    
    
    <categories>
      
      <category>Java</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Java</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Go ssh 交互式执行命令</title>
    <link href="/2018/11/09/go-interactive-shell/"/>
    <url>/2018/11/09/go-interactive-shell/</url>
    
    <content type="html"><![CDATA[<blockquote><p>最近在写一个跳板机登录的小工具，其中涉及到了用 Go 来进行交互式执行命令，简单地说就是弄个终端出来；一开始随便 Google 了一下，copy 下来基本上就是能跑了…但是后来发现了一些各种各样的小问题，强迫症的我实在受不了，最后翻了一下 Teleport 的源码，从中学到了不少有用的知识，这里记录一下</p></blockquote><h2 id="一、原始版本"><a href="#一、原始版本" class="headerlink" title="一、原始版本"></a>一、原始版本</h2><blockquote><p>不想看太多可以直接跳转到 <a href="#三完整代码">第三部分</a> 拿代码</p></blockquote><h3 id="1-1、样例代码"><a href="#1-1、样例代码" class="headerlink" title="1.1、样例代码"></a>1.1、样例代码</h3><p>一开始随便 Google 出来的代码，copy 上就直接跑；代码基本如下:</p><pre><code class="hljs golang"><span class="hljs-function"><span class="hljs-keyword">func</span> <span class="hljs-title">main</span><span class="hljs-params">()</span></span> &#123;<span class="hljs-comment">// 创建 ssh 配置</span>sshConfig := &amp;ssh.ClientConfig&#123;User: <span class="hljs-string">"root"</span>,Auth: []ssh.AuthMethod&#123;ssh.Password(<span class="hljs-string">"password"</span>),&#125;,HostKeyCallback: ssh.InsecureIgnoreHostKey(),Timeout:         <span class="hljs-number">5</span> * time.Second,&#125;<span class="hljs-comment">// 创建 client</span>client, err := ssh.Dial(<span class="hljs-string">"tcp"</span>, <span class="hljs-string">"192.168.1.20:22"</span>, sshConfig)checkErr(err)<span class="hljs-keyword">defer</span> client.Close()<span class="hljs-comment">// 获取 session</span>session, err := client.NewSession()checkErr(err)<span class="hljs-keyword">defer</span> session.Close()<span class="hljs-comment">// 拿到当前终端文件描述符</span>fd := <span class="hljs-keyword">int</span>(os.Stdin.Fd())termWidth, termHeight, err := terminal.GetSize(fd)<span class="hljs-comment">// request pty</span>err = session.RequestPty(<span class="hljs-string">"xterm-256color"</span>, termHeight, termWidth, ssh.TerminalModes&#123;&#125;)checkErr(err)<span class="hljs-comment">// 对接 std</span>session.Stdout = os.Stdoutsession.Stderr = os.Stderrsession.Stdin = os.Stdinerr = session.Shell()checkErr(err)err = session.Wait()checkErr(err)&#125;<span class="hljs-function"><span class="hljs-keyword">func</span> <span class="hljs-title">checkErr</span><span class="hljs-params">(err error)</span></span> &#123;<span class="hljs-keyword">if</span> err != <span class="hljs-literal">nil</span> &#123;fmt.Println(err)os.Exit(<span class="hljs-number">1</span>)&#125;&#125;</code></pre><h3 id="1-2、遇到的问题"><a href="#1-2、遇到的问题" class="headerlink" title="1.2、遇到的问题"></a>1.2、遇到的问题</h3><p>以上代码跑起来后，基本上遇到了以下问题:</p><ul><li>执行命令有回显，表现为敲一个 <code>ls</code> 出现两行</li><li>本地终端大小调整，远端完全无反应，导致显示不全</li><li>Tmux 下终端连接后窗口标题显示的是原始命令，而不是目标机器 shell 环境的目录位置</li><li>首次连接一些刚装完系统的机器可能出现执行命令后回显不换行</li></ul><h2 id="二、改进代码"><a href="#二、改进代码" class="headerlink" title="二、改进代码"></a>二、改进代码</h2><h3 id="2-1、回显问题"><a href="#2-1、回显问题" class="headerlink" title="2.1、回显问题"></a>2.1、回显问题</h3><p>关于回显问题，实际上解决方案很简单，设置当前终端进入 <code>raw</code> 模式即可；代码如下:</p><pre><code class="hljs golang"><span class="hljs-comment">// 拿到当前终端文件描述符</span>fd := <span class="hljs-keyword">int</span>(os.Stdin.Fd())<span class="hljs-comment">// make raw</span>state, err := terminal.MakeRaw(fd)checkErr(err)<span class="hljs-keyword">defer</span> terminal.Restore(fd, state)</code></pre><p>代码很简单，网上一大堆，But…基本没有文章详细说这个 <code>raw</code> 模式到底是个啥玩意；好在万能的 StackOverflow 对于不熟悉 Linux 的人给出了一个很清晰的解释: <a href="https://unix.stackexchange.com/questions/21752/what-s-the-difference-between-a-raw-and-a-cooked-device-driver" target="_blank" rel="noopener">What’s the difference between a “raw” and a “cooked” device driver?</a></p><p>大致意思就是说 <strong>在终端处于 <code>Cooked</code> 模式时，当你输入一些字符后，默认是被当前终端 cache 住的，在你敲了回车之前这些文本都在 cache 中，这样允许应用程序做一些处理，比如捕获 <code>Cntl-D</code> 等按键，这时候就会出现敲回车后本地终端帮你打印了一下，导致出现类似回显的效果；当设置终端为 <code>raw</code> 模式后，所有的输入将不被 cache，而是发送到应用程序，在我们的代码中表现为通过 <code>io.Copy</code> 直接发送到了远端 shell 程序</strong></p><h3 id="2-2、终端大小问题"><a href="#2-2、终端大小问题" class="headerlink" title="2.2、终端大小问题"></a>2.2、终端大小问题</h3><p>当本地调整了终端大小后，远程终端毫无反应；后来发现在 <code>*ssh.Session</code> 上有一个 <code>WindowChange</code> 方法，用于向远端发送窗口调整事件；解决方案就是启动一个 <code>goroutine</code> 在后台不断监听窗口改变事件，然后调用 <code>WindowChange</code> 即可；代码如下:</p><pre><code class="hljs golang"><span class="hljs-keyword">go</span> <span class="hljs-function"><span class="hljs-keyword">func</span><span class="hljs-params">()</span></span> &#123;<span class="hljs-comment">// 监听窗口变更事件</span>sigwinchCh := <span class="hljs-built_in">make</span>(<span class="hljs-keyword">chan</span> os.Signal, <span class="hljs-number">1</span>)signal.Notify(sigwinchCh, syscall.SIGWINCH)fd := <span class="hljs-keyword">int</span>(os.Stdin.Fd())termWidth, termHeight, err := terminal.GetSize(fd)<span class="hljs-keyword">if</span> err != <span class="hljs-literal">nil</span> &#123;fmt.Println(err)&#125;<span class="hljs-keyword">for</span> &#123;<span class="hljs-keyword">select</span> &#123;<span class="hljs-comment">// 阻塞读取</span><span class="hljs-keyword">case</span> sigwinch := &lt;-sigwinchCh:<span class="hljs-keyword">if</span> sigwinch == <span class="hljs-literal">nil</span> &#123;<span class="hljs-keyword">return</span>&#125;currTermWidth, currTermHeight, err := terminal.GetSize(fd)<span class="hljs-comment">// 判断一下窗口尺寸是否有改变</span><span class="hljs-keyword">if</span> currTermHeight == termHeight &amp;&amp; currTermWidth == termWidth &#123;<span class="hljs-keyword">continue</span>&#125;<span class="hljs-comment">// 更新远端大小</span>session.WindowChange(currTermHeight, currTermWidth)<span class="hljs-keyword">if</span> err != <span class="hljs-literal">nil</span> &#123;fmt.Printf(<span class="hljs-string">"Unable to send window-change reqest: %s."</span>, err)<span class="hljs-keyword">continue</span>&#125;termWidth, termHeight = currTermWidth, currTermHeight&#125;&#125;&#125;()</code></pre><h3 id="2-3、Tmux-标题以及回显不换行"><a href="#2-3、Tmux-标题以及回显不换行" class="headerlink" title="2.3、Tmux 标题以及回显不换行"></a>2.3、Tmux 标题以及回显不换行</h3><p>这两个问题实际上都是由于我们直接对接了 <code>stderr</code>、<code>stdout</code> 和 <code>stdin</code> 造成的，实际上我们应当启动一个异步的管道式复制行为，并且最好带有 buf 的发送；代码如下:</p><pre><code class="hljs golang">stdin, err := session.StdinPipe()checkErr(err)stdout, err := session.StdoutPipe()checkErr(err)stderr, err := session.StderrPipe()checkErr(err)<span class="hljs-keyword">go</span> io.Copy(os.Stderr, stderr)<span class="hljs-keyword">go</span> io.Copy(os.Stdout, stdout)<span class="hljs-keyword">go</span> <span class="hljs-function"><span class="hljs-keyword">func</span><span class="hljs-params">()</span></span> &#123;buf := <span class="hljs-built_in">make</span>([]<span class="hljs-keyword">byte</span>, <span class="hljs-number">128</span>)<span class="hljs-keyword">for</span> &#123;n, err := os.Stdin.Read(buf)<span class="hljs-keyword">if</span> err != <span class="hljs-literal">nil</span> &#123;fmt.Println(err)<span class="hljs-keyword">return</span>&#125;<span class="hljs-keyword">if</span> n &gt; <span class="hljs-number">0</span> &#123;_, err = stdin.Write(buf[:n])<span class="hljs-keyword">if</span> err != <span class="hljs-literal">nil</span> &#123;checkErr(err)&#125;&#125;&#125;&#125;()</code></pre><h2 id="三、完整代码"><a href="#三、完整代码" class="headerlink" title="三、完整代码"></a>三、完整代码</h2><pre><code class="hljs golang"><span class="hljs-keyword">type</span> SSHTerminal <span class="hljs-keyword">struct</span> &#123;Session *ssh.SessionexitMsg <span class="hljs-keyword">string</span>stdout  io.Readerstdin   io.Writerstderr  io.Reader&#125;<span class="hljs-function"><span class="hljs-keyword">func</span> <span class="hljs-title">main</span><span class="hljs-params">()</span></span> &#123;sshConfig := &amp;ssh.ClientConfig&#123;User: <span class="hljs-string">"root"</span>,Auth: []ssh.AuthMethod&#123;ssh.Password(<span class="hljs-string">"password"</span>),&#125;,HostKeyCallback: ssh.InsecureIgnoreHostKey(),&#125;client, err := ssh.Dial(<span class="hljs-string">"tcp"</span>, <span class="hljs-string">"192.168.1.20:22"</span>, sshConfig)<span class="hljs-keyword">if</span> err != <span class="hljs-literal">nil</span> &#123;fmt.Println(err)&#125;<span class="hljs-keyword">defer</span> client.Close()err = New(client)<span class="hljs-keyword">if</span> err != <span class="hljs-literal">nil</span> &#123;fmt.Println(err)&#125;&#125;<span class="hljs-function"><span class="hljs-keyword">func</span> <span class="hljs-params">(t *SSHTerminal)</span> <span class="hljs-title">updateTerminalSize</span><span class="hljs-params">()</span></span> &#123;<span class="hljs-keyword">go</span> <span class="hljs-function"><span class="hljs-keyword">func</span><span class="hljs-params">()</span></span> &#123;<span class="hljs-comment">// SIGWINCH is sent to the process when the window size of the terminal has</span><span class="hljs-comment">// changed.</span>sigwinchCh := <span class="hljs-built_in">make</span>(<span class="hljs-keyword">chan</span> os.Signal, <span class="hljs-number">1</span>)signal.Notify(sigwinchCh, syscall.SIGWINCH)fd := <span class="hljs-keyword">int</span>(os.Stdin.Fd())termWidth, termHeight, err := terminal.GetSize(fd)<span class="hljs-keyword">if</span> err != <span class="hljs-literal">nil</span> &#123;fmt.Println(err)&#125;<span class="hljs-keyword">for</span> &#123;<span class="hljs-keyword">select</span> &#123;<span class="hljs-comment">// The client updated the size of the local PTY. This change needs to occur</span><span class="hljs-comment">// on the server side PTY as well.</span><span class="hljs-keyword">case</span> sigwinch := &lt;-sigwinchCh:<span class="hljs-keyword">if</span> sigwinch == <span class="hljs-literal">nil</span> &#123;<span class="hljs-keyword">return</span>&#125;currTermWidth, currTermHeight, err := terminal.GetSize(fd)<span class="hljs-comment">// Terminal size has not changed, don't do anything.</span><span class="hljs-keyword">if</span> currTermHeight == termHeight &amp;&amp; currTermWidth == termWidth &#123;<span class="hljs-keyword">continue</span>&#125;t.Session.WindowChange(currTermHeight, currTermWidth)<span class="hljs-keyword">if</span> err != <span class="hljs-literal">nil</span> &#123;fmt.Printf(<span class="hljs-string">"Unable to send window-change reqest: %s."</span>, err)<span class="hljs-keyword">continue</span>&#125;termWidth, termHeight = currTermWidth, currTermHeight&#125;&#125;&#125;()&#125;<span class="hljs-function"><span class="hljs-keyword">func</span> <span class="hljs-params">(t *SSHTerminal)</span> <span class="hljs-title">interactiveSession</span><span class="hljs-params">()</span> <span class="hljs-title">error</span></span> &#123;<span class="hljs-keyword">defer</span> <span class="hljs-function"><span class="hljs-keyword">func</span><span class="hljs-params">()</span></span> &#123;<span class="hljs-keyword">if</span> t.exitMsg == <span class="hljs-string">""</span> &#123;fmt.Fprintln(os.Stdout, <span class="hljs-string">"the connection was closed on the remote side on "</span>, time.Now().Format(time.RFC822))&#125; <span class="hljs-keyword">else</span> &#123;fmt.Fprintln(os.Stdout, t.exitMsg)&#125;&#125;()fd := <span class="hljs-keyword">int</span>(os.Stdin.Fd())state, err := terminal.MakeRaw(fd)<span class="hljs-keyword">if</span> err != <span class="hljs-literal">nil</span> &#123;<span class="hljs-keyword">return</span> err&#125;<span class="hljs-keyword">defer</span> terminal.Restore(fd, state)termWidth, termHeight, err := terminal.GetSize(fd)<span class="hljs-keyword">if</span> err != <span class="hljs-literal">nil</span> &#123;<span class="hljs-keyword">return</span> err&#125;termType := os.Getenv(<span class="hljs-string">"TERM"</span>)<span class="hljs-keyword">if</span> termType == <span class="hljs-string">""</span> &#123;termType = <span class="hljs-string">"xterm-256color"</span>&#125;err = t.Session.RequestPty(termType, termHeight, termWidth, ssh.TerminalModes&#123;&#125;)<span class="hljs-keyword">if</span> err != <span class="hljs-literal">nil</span> &#123;<span class="hljs-keyword">return</span> err&#125;t.updateTerminalSize()t.stdin, err = t.Session.StdinPipe()<span class="hljs-keyword">if</span> err != <span class="hljs-literal">nil</span> &#123;<span class="hljs-keyword">return</span> err&#125;t.stdout, err = t.Session.StdoutPipe()<span class="hljs-keyword">if</span> err != <span class="hljs-literal">nil</span> &#123;<span class="hljs-keyword">return</span> err&#125;t.stderr, err = t.Session.StderrPipe()<span class="hljs-keyword">go</span> io.Copy(os.Stderr, t.stderr)<span class="hljs-keyword">go</span> io.Copy(os.Stdout, t.stdout)<span class="hljs-keyword">go</span> <span class="hljs-function"><span class="hljs-keyword">func</span><span class="hljs-params">()</span></span> &#123;buf := <span class="hljs-built_in">make</span>([]<span class="hljs-keyword">byte</span>, <span class="hljs-number">128</span>)<span class="hljs-keyword">for</span> &#123;n, err := os.Stdin.Read(buf)<span class="hljs-keyword">if</span> err != <span class="hljs-literal">nil</span> &#123;fmt.Println(err)<span class="hljs-keyword">return</span>&#125;<span class="hljs-keyword">if</span> n &gt; <span class="hljs-number">0</span> &#123;_, err = t.stdin.Write(buf[:n])<span class="hljs-keyword">if</span> err != <span class="hljs-literal">nil</span> &#123;fmt.Println(err)t.exitMsg = err.Error()<span class="hljs-keyword">return</span>&#125;&#125;&#125;&#125;()err = t.Session.Shell()<span class="hljs-keyword">if</span> err != <span class="hljs-literal">nil</span> &#123;<span class="hljs-keyword">return</span> err&#125;err = t.Session.Wait()<span class="hljs-keyword">if</span> err != <span class="hljs-literal">nil</span> &#123;<span class="hljs-keyword">return</span> err&#125;<span class="hljs-keyword">return</span> <span class="hljs-literal">nil</span>&#125;<span class="hljs-function"><span class="hljs-keyword">func</span> <span class="hljs-title">New</span><span class="hljs-params">(client *ssh.Client)</span> <span class="hljs-title">error</span></span> &#123;session, err := client.NewSession()<span class="hljs-keyword">if</span> err != <span class="hljs-literal">nil</span> &#123;<span class="hljs-keyword">return</span> err&#125;<span class="hljs-keyword">defer</span> session.Close()s := SSHTerminal&#123;Session: session,&#125;<span class="hljs-keyword">return</span> s.interactiveSession()&#125;</code></pre>]]></content>
    
    
    <categories>
      
      <category>Golang</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Golang</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Go 代码的扩展套路</title>
    <link href="/2018/10/23/golang-code-plugin/"/>
    <url>/2018/10/23/golang-code-plugin/</url>
    
    <content type="html"><![CDATA[<blockquote><p>折腾 Go 已经有一段时间了，最近在用 Go 写点 web 的东西；在搭建脚手架的过程中总是有点不适应，尤其对可扩展性上总是感觉没有 Java 那么顺手；索性看了下 coredns 的源码，最后追踪到 caddy 源码；突然发现他们对代码内的 plugin 机制有一些骚套路，这里索性记录一下</p></blockquote><h3 id="一、问题由来"><a href="#一、问题由来" class="headerlink" title="一、问题由来"></a>一、问题由来</h3><p>纵观现在所有的 Go web 框架，在文档上可以看到使用方式很简明；非常符合我对 Go 的一贯感受: “所写即所得”；就拿 Gin 这个来说，在 README.md 上可以很轻松的看到 <code>engine</code> 或者说 <code>router</code> 这玩意的使用，比如下面这样:</p><pre><code class="hljs golang"><span class="hljs-function"><span class="hljs-keyword">func</span> <span class="hljs-title">main</span><span class="hljs-params">()</span></span> &#123;<span class="hljs-comment">// Disable Console Color</span><span class="hljs-comment">// gin.DisableConsoleColor()</span><span class="hljs-comment">// Creates a gin router with default middleware:</span><span class="hljs-comment">// logger and recovery (crash-free) middleware</span>router := gin.Default()router.GET(<span class="hljs-string">"/someGet"</span>, getting)router.POST(<span class="hljs-string">"/somePost"</span>, posting)router.PUT(<span class="hljs-string">"/somePut"</span>, putting)router.DELETE(<span class="hljs-string">"/someDelete"</span>, deleting)router.PATCH(<span class="hljs-string">"/somePatch"</span>, patching)router.HEAD(<span class="hljs-string">"/someHead"</span>, head)router.OPTIONS(<span class="hljs-string">"/someOptions"</span>, options)<span class="hljs-comment">// By default it serves on :8080 unless a</span><span class="hljs-comment">// PORT environment variable was defined.</span>router.Run()<span class="hljs-comment">// router.Run(":3000") for a hard coded port</span>&#125;</code></pre><p>乍一看简单到爆，但实际使用中，在脚手架搭建上，我们需要规划好 <strong>包结构、配置文件、命令行参数、数据库连接、cache</strong> 等等；直到目前为止，至少我没有找到一种非常规范的后端 MVC 的标准架子结构；这点目前确实不如 Java 的生态；作为最初的脚手架搭建者，站在这个角度，我想我们更应当考虑如何做好适当的抽象、隔离；以防止后面开发者对系统基础功能可能造成的破坏。</p><p>综上所述，再配合 Gin 或者说 Go 的代码风格，这就形成了一种强烈的冲突；在 Java 中，由于有注解(<code>Annotation</code>)的存在，事实上你是可以有这种操作的: <strong>新建一个 Class，创建 func，在上面加上合适的注解，最终框架会通过注解扫描的方式以适当的形式进行初始化</strong>；而 Go 中并没有 <code>Annotation</code> 这玩意，我们很难实现在 <strong>代码运行时扫描自身做出一种策略性调整</strong>；从而下面这个需求很难实现: <strong>作为脚手架搭建者，我希望我的基础代码安全的放在一个特定位置，后续开发者开发应当以一种类似可热插拔的形式注入进来</strong>，比如 Gin 的 router 路由设置，我不希望每次有修改都会有人动我的 router 核心配置文件。</p><h3 id="二、Caddy-的套路"><a href="#二、Caddy-的套路" class="headerlink" title="二、Caddy 的套路"></a>二、Caddy 的套路</h3><p>在翻了 coredns 的源码后，我发现他是依赖于 Caddy 这框架运行的，coredns 的代码内的插件机制也是直接调用的 Caddy；所以接着我就翻到了 Caddy 源码，其中的代码如下(完整代码<a href="https://github.com/mholt/caddy/blob/master/plugins.go" target="_blank" rel="noopener">点击这里</a>):</p><pre><code class="hljs golang"><span class="hljs-comment">// RegisterPlugin plugs in plugin. All plugins should register</span><span class="hljs-comment">// themselves, even if they do not perform an action associated</span><span class="hljs-comment">// with a directive. It is important for the process to know</span><span class="hljs-comment">// which plugins are available.</span><span class="hljs-comment">//</span><span class="hljs-comment">// The plugin MUST have a name: lower case and one word.</span><span class="hljs-comment">// If this plugin has an action, it must be the name of</span><span class="hljs-comment">// the directive that invokes it. A name is always required</span><span class="hljs-comment">// and must be unique for the server type.</span><span class="hljs-function"><span class="hljs-keyword">func</span> <span class="hljs-title">RegisterPlugin</span><span class="hljs-params">(name <span class="hljs-keyword">string</span>, plugin Plugin)</span></span> &#123;<span class="hljs-keyword">if</span> name == <span class="hljs-string">""</span> &#123;<span class="hljs-built_in">panic</span>(<span class="hljs-string">"plugin must have a name"</span>)&#125;<span class="hljs-keyword">if</span> _, ok := plugins[plugin.ServerType]; !ok &#123;plugins[plugin.ServerType] = <span class="hljs-built_in">make</span>(<span class="hljs-keyword">map</span>[<span class="hljs-keyword">string</span>]Plugin)&#125;<span class="hljs-keyword">if</span> _, dup := plugins[plugin.ServerType][name]; dup &#123;<span class="hljs-built_in">panic</span>(<span class="hljs-string">"plugin named "</span> + name + <span class="hljs-string">" already registered for server type "</span> + plugin.ServerType)&#125;plugins[plugin.ServerType][name] = plugin&#125;</code></pre><p>套路很清奇，为了实现我上面说的那个需求: “后面开发不需要动我核心代码，我还能允许他们动态添加”，Caddy 套路就是<strong>定义一个 map，map 里用于存放一种特定形式的 func，并且暴露出一个方法用于向 map 内添加指定 func，然后在合适的时机遍历这个 map，并执行其中的 func。</strong>这种套路利用了 Go 函数式编程的特性，将行为先存储在容器中，然后后续再去调用这些行为。</p><h3 id="三、总结"><a href="#三、总结" class="headerlink" title="三、总结"></a>三、总结</h3><p>长篇大论这么久，实际上我也是在一边折腾 Go 的过程中一边总结和对比跟 Java 的差异；在 Java 中扫描自己注解的套路 Go 中没法实现，但是 Go 利用其函数式编程的优势也可以利用一些延迟加载方式实现对应的功能；总结来说，不同语言有其自己的特性，当有对比的时候，可能更加深刻。</p>]]></content>
    
    
    <categories>
      
      <category>Golang</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Golang</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Google container registry 同步</title>
    <link href="/2018/09/17/google-container-registry-sync/"/>
    <url>/2018/09/17/google-container-registry-sync/</url>
    
    <content type="html"><![CDATA[<h2 id="一、起因"><a href="#一、起因" class="headerlink" title="一、起因"></a>一、起因</h2><p>玩 Kubenretes 的基本都很清楚，Kubernetes 很多组件的镜像全部托管在 <code>gcr.io</code> 这个域名下(现在换成了 <code>k8s.gcr.io</code>)；由于众所周知的原因，这个网站在国内是不可达的；当时由于 Docker Hub 提供了 <code>Auto Build</code> 功能，机智的想到一个解决办法；就是利用 Docker Hub 的 <code>Auto Build</code>，创建只有一行的 Dockerfile，里面就一句 <code>FROM gcr.io/xxxx</code>，然后让 Docker Hub 帮你构建完成后拉取即可</p><p>这种套路的基本方案就是利用一个第三方公共仓库，这个仓库可以访问不可达的 <code>gcr.io</code>，然后生成镜像，我们再从这个仓库 pull 即可；为此我创建了一个 Github 仓库(<a href="https://github.com/mritd/docker-library" target="_blank" rel="noopener">docker-library</a>)；时隔这么久以后，我猜想大家都已经有了这种自己的仓库…不过最近发现这个仓库仍然在有人 fork…</p><p>为了一劳永逸的解决这个问题，只能撸点代码解决这个问题了</p><h2 id="二、仓库使用"><a href="#二、仓库使用" class="headerlink" title="二、仓库使用"></a>二、仓库使用</h2><p>为了解决上述问题，我写了一个 <a href="https://github.com/mritd/gcrsync" target="_blank" rel="noopener">gcrsync</a> 工具，并且借助 <a href="https://travis-ci.org/mritd/gcrsync" target="_blank" rel="noopener">Travis CI</a> 让其每天自动运行，将所有用得到的 <code>gcr.io</code> 下的镜像同步到了 Docker Hub</p><p><strong>目前对于一个 <code>gcr.io</code> 下的镜像，可以直接替换为 <code>gcrxio</code> 用户名，然后从 Docker Hub 直接拉取</strong>，以下为一个示例:</p><pre><code class="hljs sh"><span class="hljs-comment"># 原始命令</span>docker pull k8s.gcr.io/kubernetes-dashboard-amd64:v1.10.0<span class="hljs-comment"># 使用同步仓库</span>docker pull gcrxio/kubernetes-dashboard-amd64:v1.10.0</code></pre><h2 id="三、同步细节说明"><a href="#三、同步细节说明" class="headerlink" title="三、同步细节说明"></a>三、同步细节说明</h2><p>为了保证同步镜像的安全性，同步工具已经开源在 <a href="https://github.com/mritd/gcrsync" target="_blank" rel="noopener">gcrsync</a> 仓库，同步细节如下:</p><ul><li>工具每天由 <a href="https://travis-ci.org/mritd/gcrsync" target="_blank" rel="noopener">Travis CI</a> 自动进行一次 build，然后进行推送</li><li>工具每次推送前首先 clone 元数据仓库 <a href="https://github.com/mritd/gcr" target="_blank" rel="noopener">gcr</a></li><li>工具每次推送首先获取 <code>gcr.io</code> 指定 <code>namespace</code> 下的所有镜像(<code>namesapce</code> 由 <a href="https://github.com/mritd/gcrsync/blob/master/.travis.yml" target="_blank" rel="noopener">.travis.yml</a> <code>script</code> 段定义)</li><li>获取 <code>gcr.io</code> 镜像后，再读取元数据仓库(<a href="https://github.com/mritd/gcr" target="_blank" rel="noopener">gcr</a>) 中与 <code>namesapce</code> 同名文件(实际是个 json)</li><li>接着对比双方差异，得出需要同步的镜像</li><li>最后通过 API 调用本地的 docker 进行 <code>pull</code>、<code>tag</code>、<code>push</code> 操作，完成镜像推送</li><li>所有镜像推送成功后，更新元数据仓库内 <code>namespace</code> 对应的 json 文件，最后在生成 <a href="https://github.com/mritd/gcr/blob/master/CHANGELOG.md" target="_blank" rel="noopener">CHANGELOG</a>，执行 <code>git push</code> 到远程元数据仓库</li></ul><p>综上所述，如果想得知<strong>具体 <code>gcrxio</code> 用户下都有那些镜像，可直接访问 <a href="https://github.com/mritd/gcr" target="_blank" rel="noopener">gcr</a> 元数据仓库，查看对应 <code>namesapce</code> 同名的 json 文件即可；每天增量同步的信息会追加到 <a href="https://github.com/mritd/gcr" target="_blank" rel="noopener">gcr</a> 仓库的 <code>CHANGELOG.md</code> 文件中</strong></p><h2 id="四、gcrsync"><a href="#四、gcrsync" class="headerlink" title="四、gcrsync"></a>四、gcrsync</h2><p>为方便审查镜像安全性，以下为 <a href="https://github.com/mritd/gcrsync" target="_blank" rel="noopener">gcrsync</a> 工具的代码简介，代码仓库文件如下:</p><pre><code class="hljs sh">➜  gcrsync git:(master) tree -I vendor.├── CHANGELOG.md├── Gopkg.lock├── Gopkg.toml├── LICENSE├── README.md├── cmd│   ├── compare.go│   ├── monitor.go│   ├── root.go│   ├── sync.go│   └── test.go├── dist│   ├── gcrsync_darwin_amd64│   ├── gcrsync_linux_386│   └── gcrsync_linux_amd64├── main.go└── pkg    ├── gcrsync    │   ├── docker.go    │   ├── gcr.go    │   ├── git.go    │   ├── registry.go    │   └── sync.go    └── utils        └── common.go</code></pre><p>cmd 目录下为标准的 <code>cobra</code> 框架生成的子命令文件，其中每个命令包含了对应的 flag 设置，如 <code>namesapce</code>、<code>proxy</code> 等；<code>pkg/gcrsync</code> 目录下的文件为核心代码:</p><ul><li><code>docker.go</code> 包含了对本地 docker daemon API 调用，包括 <code>pull</code>、<code>tag</code>、<code>push</code> 操作</li><li><code>gcr.go</code> 包含了对 <code>gcr.io</code> 指定 <code>namespace</code> 下镜像列表获取操作</li><li><code>registry.go</code> 包含了对 Docker Hub 下指定用户(默认 <code>gcrxio</code>)的镜像列表获取操作(其主要用于首次执行 <code>compare</code> 命令生成 json 文件)</li><li><code>sync.go</code> 为主要的程序入口，其中包含了对其他文件内方法的调用，设置并发池等</li></ul><h2 id="五、其他说明"><a href="#五、其他说明" class="headerlink" title="五、其他说明"></a>五、其他说明</h2><p>该仓库不保证镜像实时同步，默认每天同步一次(由 <a href="https://travis-ci.org/mritd/gcrsync" target="_blank" rel="noopener">Travis CI</a> 执行)，如有特殊需求，如增加 <code>namesapce</code> 等请开启 issue；最后，请不要再 fork <a href="https://github.com/mritd/docker-library" target="_blank" rel="noopener">docker-library</a> 这个仓库了</p>]]></content>
    
    
    <categories>
      
      <category>Kubernetes</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Docker</tag>
      
      <tag>Kubernetes</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>使用 Bootstrap Token 完成 TLS Bootstrapping</title>
    <link href="/2018/08/28/kubernetes-tls-bootstrapping-with-bootstrap-token/"/>
    <url>/2018/08/28/kubernetes-tls-bootstrapping-with-bootstrap-token/</url>
    
    <content type="html"><![CDATA[<blockquote><p>最近在测试 Kubernetes 1.11.2 新版本的相关东西，发现新版本的 Bootstrap Token 功能已经进入 Beta 阶段，索性便尝试了一下；虽说目前是为 kubeadm 设计的，不过手动挡用起来也不错，这里记录一下使用方式</p></blockquote><h2 id="一、环境准备"><a href="#一、环境准备" class="headerlink" title="一、环境准备"></a>一、环境准备</h2><p>首先需要有一个运行状态正常的 Master 节点，目前我测试的是版本是 1.11.2，低版本我没测试；其次本文默认 Node 节点 Docker、kubelet 二进制文件、systemd service 配置等都已经处理好，更具体的环境如下:</p><p><strong>Master 节点 IP 为 <code>192.168.1.61</code>，Node 节点 IP 为 <code>192.168.1.64</code></strong></p><pre><code class="hljs sh">docker1.node ➜  ~ kubectl versionClient Version: version.Info&#123;Major:<span class="hljs-string">"1"</span>, Minor:<span class="hljs-string">"11"</span>, GitVersion:<span class="hljs-string">"v1.11.2"</span>, GitCommit:<span class="hljs-string">"bb9ffb1654d4a729bb4cec18ff088eacc153c239"</span>, GitTreeState:<span class="hljs-string">"clean"</span>, BuildDate:<span class="hljs-string">"2018-08-07T23:08:19Z"</span>, GoVersion:<span class="hljs-string">"go1.10.3"</span>, Compiler:<span class="hljs-string">"gc"</span>, Platform:<span class="hljs-string">"linux/amd64"</span>&#125;Server Version: version.Info&#123;Major:<span class="hljs-string">"1"</span>, Minor:<span class="hljs-string">"11"</span>, GitVersion:<span class="hljs-string">"v1.11.2"</span>, GitCommit:<span class="hljs-string">"bb9ffb1654d4a729bb4cec18ff088eacc153c239"</span>, GitTreeState:<span class="hljs-string">"clean"</span>, BuildDate:<span class="hljs-string">"2018-08-07T23:08:19Z"</span>, GoVersion:<span class="hljs-string">"go1.10.3"</span>, Compiler:<span class="hljs-string">"gc"</span>, Platform:<span class="hljs-string">"linux/amd64"</span>&#125;docker1.node ➜  ~ docker infoContainers: 0 Running: 0 Paused: 0 Stopped: 0Images: 0Server Version: 18.06.1-ceStorage Driver: overlay2 Backing Filesystem: xfs Supports d_type: <span class="hljs-literal">true</span> Native Overlay Diff: <span class="hljs-literal">true</span>Logging Driver: json-fileCgroup Driver: cgroupfsPlugins: Volume: <span class="hljs-built_in">local</span> Network: bridge host macvlan null overlay Log: awslogs fluentd gcplogs gelf journald json-file logentries splunk syslogSwarm: inactiveRuntimes: runcDefault Runtime: runcInit Binary: docker-initcontainerd version: 468a545b9edcd5932818eb9de8e72413e616e86erunc version: 69663f0bd4b60df09991c08812a60108003fa340init version: fec3683Security Options: apparmor seccomp  Profile: defaultKernel Version: 4.15.0-33-genericOperating System: Ubuntu 18.04.1 LTSOSType: linuxArchitecture: x86_64CPUs: 2Total Memory: 3.847GiBName: docker1.nodeID: AJOD:RBJZ:YP3G:HCGV:KT4R:D4AF:SBDN:5B76:JM4M:OCJA:YJMJ:OCYQDocker Root Dir: /data/dockerDebug Mode (client): <span class="hljs-literal">false</span>Debug Mode (server): <span class="hljs-literal">false</span>Registry: https://index.docker.io/v1/Labels:Experimental: <span class="hljs-literal">false</span>Insecure Registries: 127.0.0.0/8Live Restore Enabled: <span class="hljs-literal">false</span></code></pre><h2 id="二、TLS-Bootstrapping-回顾"><a href="#二、TLS-Bootstrapping-回顾" class="headerlink" title="二、TLS Bootstrapping 回顾"></a>二、TLS Bootstrapping 回顾</h2><p>在正式进行 TLS Bootstrapping 操作之前，<strong>如果对 TLS Bootstrapping 完全没接触过的请先阅读 <a href="https://mritd.me/2018/01/07/kubernetes-tls-bootstrapping-note" target="_blank" rel="noopener">Kubernetes TLS bootstrapping 那点事</a></strong>；我想这里有必要简单说明下使用 Token 时整个启动引导过程:</p><ul><li>在集群内创建特定的 <code>Bootstrap Token Secret</code>，该 Secret 将替代以前的 <code>token.csv</code> 内置用户声明文件</li><li>在集群内创建首次 TLS Bootstrap 申请证书的 ClusterRole、后续 renew Kubelet client/server 的 ClusterRole，以及其相关对应的 ClusterRoleBinding；并绑定到对应的组或用户</li><li>调整 Controller Manager 配置，以使其能自动签署相关证书和自动清理过期的 TLS Bootstrapping Token</li><li>生成特定的包含 TLS Bootstrapping Token 的 <code>bootstrap.kubeconfig</code> 以供 kubelet 启动时使用</li><li>调整 Kubelet 配置，使其首次启动加载 <code>bootstrap.kubeconfig</code> 并使用其中的 TLS Bootstrapping Token 完成首次证书申请</li><li>证书被 Controller Manager 签署，成功下发，Kubelet 自动重载完成引导流程</li><li>后续 Kubelet 自动 renew 相关证书</li><li>可选的: 集群搭建成功后立即清除 <code>Bootstrap Token Secret</code>，或等待 Controller Manager 待其过期后删除，以防止被恶意利用</li></ul><h2 id="三、使用-Bootstrap-Token"><a href="#三、使用-Bootstrap-Token" class="headerlink" title="三、使用 Bootstrap Token"></a>三、使用 Bootstrap Token</h2><p>第二部分算作大纲了，这部分将会按照第二部分的总体流程来走，同时会对一些细节进行详细说明</p><h3 id="3-1、创建-Bootstrap-Token"><a href="#3-1、创建-Bootstrap-Token" class="headerlink" title="3.1、创建 Bootstrap Token"></a>3.1、创建 Bootstrap Token</h3><p>既然整个功能都时刻强调这个 Token，那么第一步肯定是生成一个 token，生成方式如下:</p><pre><code class="hljs sh">➜  ~ <span class="hljs-built_in">echo</span> <span class="hljs-string">"<span class="hljs-variable">$(head -c 6 /dev/urandom | md5sum | head -c 6)</span>"</span>.<span class="hljs-string">"<span class="hljs-variable">$(head -c 16 /dev/urandom | md5sum | head -c 16)</span>"</span>47f392.d22d04e89a65eb22</code></pre><p>这个 <code>47f392.d22d04e89a65eb22</code> 就是生成的 Bootstrap Token，保存好 token，因为后续要用；关于这个 token 解释如下:</p><p>Token 必须满足 <code>[a-z0-9]{6}\.[a-z0-9]{16}</code> 格式；以 <code>.</code> 分割，前面的部分被称作  <code>Token ID</code>，<code>Token ID</code> 并不是 “机密信息”，它可以暴露出去；相对的后面的部分称为 <code>Token Secret</code>，它应该是保密的</p><p>本部分官方文档地址 <a href="https://kubernetes.io/docs/reference/access-authn-authz/bootstrap-tokens/#token-format" target="_blank" rel="noopener">Token Format</a></p><h3 id="3-2、创建-Bootstrap-Token-Secret"><a href="#3-2、创建-Bootstrap-Token-Secret" class="headerlink" title="3.2、创建 Bootstrap Token Secret"></a>3.2、创建 Bootstrap Token Secret</h3><p>对于 Kubernetes 来说 <code>Bootstrap Token Secret</code> 也仅仅是一个特殊的 <code>Secret</code> 而已；对于这个特殊的 <code>Secret</code> 样例 yaml 配置如下:</p><pre><code class="hljs yaml"><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">v1</span><span class="hljs-attr">kind:</span> <span class="hljs-string">Secret</span><span class="hljs-attr">metadata:</span>  <span class="hljs-comment"># Name MUST be of form "bootstrap-token-&lt;token id&gt;"</span>  <span class="hljs-attr">name:</span> <span class="hljs-string">bootstrap-token-07401b</span>  <span class="hljs-attr">namespace:</span> <span class="hljs-string">kube-system</span><span class="hljs-comment"># Type MUST be 'bootstrap.kubernetes.io/token'</span><span class="hljs-attr">type:</span> <span class="hljs-string">bootstrap.kubernetes.io/token</span><span class="hljs-attr">stringData:</span>  <span class="hljs-comment"># Human readable description. Optional.</span>  <span class="hljs-attr">description:</span> <span class="hljs-string">"The default bootstrap token generated by 'kubeadm init'."</span>  <span class="hljs-comment"># Token ID and secret. Required.</span>  <span class="hljs-attr">token-id:</span> <span class="hljs-string">47f392</span>  <span class="hljs-attr">token-secret:</span> <span class="hljs-string">d22d04e89a65eb22</span>  <span class="hljs-comment"># Expiration. Optional.</span>  <span class="hljs-attr">expiration:</span> <span class="hljs-number">2018</span><span class="hljs-number">-09</span><span class="hljs-string">-10T00:00:11Z</span>  <span class="hljs-comment"># Allowed usages.</span>  <span class="hljs-attr">usage-bootstrap-authentication:</span> <span class="hljs-string">"true"</span>  <span class="hljs-attr">usage-bootstrap-signing:</span> <span class="hljs-string">"true"</span>  <span class="hljs-comment"># Extra groups to authenticate the token as. Must start with "system:bootstrappers:"</span>  <span class="hljs-attr">auth-extra-groups:</span> <span class="hljs-string">system:bootstrappers:worker,system:bootstrappers:ingress</span></code></pre><p>需要注意几点:</p><ul><li>作为 <code>Bootstrap Token Secret</code> 的 type 必须为 <code>bootstrap.kubernetes.io/token</code>，name 必须为 <code>bootstrap-token-&lt;token id&gt;</code> (Token ID 就是上一步创建的 Token 前一部分)</li><li><code>usage-bootstrap-authentication</code>、<code>usage-bootstrap-signing</code> 必须存才且设置为 <code>true</code> (我个人感觉 <code>usage-bootstrap-signing</code> 可以没有，具体见文章最后部分)</li><li><code>expiration</code> 字段是可选的，如果设置则 <code>Secret</code> 到期后将由 Controller Manager 中的 <code>tokencleaner</code> 自动清理</li><li><code>auth-extra-groups</code> 也是可选的，令牌的扩展认证组，组必须以 <code>system:bootstrappers:</code> 开头</li></ul><p>最后使用 <code>kubectl create -f bootstrap.secret.yaml</code> 创建即可</p><p>本部分官方文档地址 <a href="https://kubernetes.io/docs/reference/access-authn-authz/bootstrap-tokens/#bootstrap-token-secret-format" target="_blank" rel="noopener">Bootstrap Token Secret Format</a></p><h3 id="3-3、创建-ClusterRole-和-ClusterRoleBinding"><a href="#3-3、创建-ClusterRole-和-ClusterRoleBinding" class="headerlink" title="3.3、创建 ClusterRole 和 ClusterRoleBinding"></a>3.3、创建 ClusterRole 和 ClusterRoleBinding</h3><p>具体都有哪些 <code>ClusterRole</code> 和 <code>ClusterRoleBinding</code>，以及其作用请参考上一篇的 <a href="https://mritd.me/2018/01/07/kubernetes-tls-bootstrapping-note" target="_blank" rel="noopener">Kubernetes TLS bootstrapping 那点事</a>，不想在这里重复了</p><p>在 1.8 以后三个 <code>ClusterRole</code> 中有两个已经有了，我们只需要创建剩下的一个即可:</p><pre><code class="hljs yaml"><span class="hljs-comment"># A ClusterRole which instructs the CSR approver to approve a node requesting a</span><span class="hljs-comment"># serving cert matching its client cert.</span><span class="hljs-attr">kind:</span> <span class="hljs-string">ClusterRole</span><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">rbac.authorization.k8s.io/v1</span><span class="hljs-attr">metadata:</span>  <span class="hljs-attr">name:</span> <span class="hljs-string">system:certificates.k8s.io:certificatesigningrequests:selfnodeserver</span><span class="hljs-attr">rules:</span><span class="hljs-bullet">-</span> <span class="hljs-attr">apiGroups:</span> <span class="hljs-string">["certificates.k8s.io"]</span>  <span class="hljs-attr">resources:</span> <span class="hljs-string">["certificatesigningrequests/selfnodeserver"]</span>  <span class="hljs-attr">verbs:</span> <span class="hljs-string">["create"]</span></code></pre><p>然后是三个 <code>ClusterRole</code> 对应的 <code>ClusterRoleBinding</code>；需要注意的是 <strong>在使用 <code>Bootstrap Token</code> 进行引导时，Kubelet 组件使用 Token 发起的请求其用户名为 <code>system:bootstrap:&lt;token id&gt;</code>，用户组为 <code>system:bootstrappers</code>；so 我们在创建 <code>ClusterRoleBinding</code> 时要绑定到这个用户或者组上</strong>；当然我选择懒一点，全部绑定到组上</p><pre><code class="hljs sh"><span class="hljs-comment"># 允许 system:bootstrappers 组用户创建 CSR 请求</span>kubectl create clusterrolebinding kubelet-bootstrap --clusterrole=system:node-bootstrapper --group=system:bootstrappers<span class="hljs-comment"># 自动批准 system:bootstrappers 组用户 TLS bootstrapping 首次申请证书的 CSR 请求</span>kubectl create clusterrolebinding node-client-auto-approve-csr --clusterrole=system:certificates.k8s.io:certificatesigningrequests:nodeclient --group=system:bootstrappers<span class="hljs-comment"># 自动批准 system:nodes 组用户更新 kubelet 自身与 apiserver 通讯证书的 CSR 请求</span>kubectl create clusterrolebinding node-client-auto-renew-crt --clusterrole=system:certificates.k8s.io:certificatesigningrequests:selfnodeclient --group=system:nodes<span class="hljs-comment"># 自动批准 system:nodes 组用户更新 kubelet 10250 api 端口证书的 CSR 请求</span>kubectl create clusterrolebinding node-server-auto-renew-crt --clusterrole=system:certificates.k8s.io:certificatesigningrequests:selfnodeserver --group=system:nodes</code></pre><p>关于本部分首次请求用户名变为 <code>system:bootstrap:&lt;token id&gt;</code> 官方文档原文如下:</p><blockquote><p>Tokens authenticate as the username system:bootstrap:<token id> and are members of the group system:bootstrappers. Additional groups may be specified in the token’s Secret.</p></blockquote><h3 id="3-4、调整-Controller-Manager"><a href="#3-4、调整-Controller-Manager" class="headerlink" title="3.4、调整 Controller Manager"></a>3.4、调整 Controller Manager</h3><p>根据官方文档描述，Controller Manager 需要启用 <code>tokencleaner</code> 和 <code>bootstrapsigner</code> (目测这个 <code>bootstrapsigner</code> 实际上并不需要，顺便加着吧)，完整配置如下(为什么贴完整配置? 文章凑数啊…):</p><pre><code class="hljs sh">KUBE_CONTROLLER_MANAGER_ARGS=<span class="hljs-string">"  --address=127.0.0.1 \</span><span class="hljs-string">                                --bind-address=192.168.1.61 \</span><span class="hljs-string">                                --port=10252 \</span><span class="hljs-string">                                --secure-port=10258 \</span><span class="hljs-string">                                --cluster-name=kubernetes \</span><span class="hljs-string">                                --cluster-signing-cert-file=/etc/kubernetes/ssl/k8s-root-ca.pem \</span><span class="hljs-string">                                --cluster-signing-key-file=/etc/kubernetes/ssl/k8s-root-ca-key.pem \</span><span class="hljs-string">                                --controllers=*,bootstrapsigner,tokencleaner \</span><span class="hljs-string">                                --deployment-controller-sync-period=10s \</span><span class="hljs-string">                                --experimental-cluster-signing-duration=86700h0m0s \</span><span class="hljs-string">                                --enable-garbage-collector=true \</span><span class="hljs-string">                                --leader-elect=true \</span><span class="hljs-string">                                --master=http://127.0.0.1:8080 \</span><span class="hljs-string">                                --node-monitor-grace-period=40s \</span><span class="hljs-string">                                --node-monitor-period=5s \</span><span class="hljs-string">                                --pod-eviction-timeout=5m0s \</span><span class="hljs-string">                                --terminated-pod-gc-threshold=50 \</span><span class="hljs-string">                                --root-ca-file=/etc/kubernetes/ssl/k8s-root-ca.pem \</span><span class="hljs-string">                                --service-account-private-key-file=/etc/kubernetes/ssl/k8s-root-ca-key.pem \</span><span class="hljs-string">                                --feature-gates=RotateKubeletServerCertificate=true"</span></code></pre><h3 id="3-5、生成-bootstrap-kubeconfig"><a href="#3-5、生成-bootstrap-kubeconfig" class="headerlink" title="3.5、生成 bootstrap.kubeconfig"></a>3.5、生成 bootstrap.kubeconfig</h3><p>前面所有步骤实际上都是在处理 Api Server、Controller Manager 这一块，为的就是 “老子启动后 TLS Bootstarpping 发证书申请你两个要立马允许，不能拒绝老子”；接下来就是比较重要的 <code>bootstrap.kubeconfig</code> 配置生成，这个 <code>bootstrap.kubeconfig</code> 是最终被 Kubelet 使用的，里面包含了相关的 Token，以帮助 Kubelet 在第一次通讯时能成功沟通 Api Server；生成方式如下:</p><pre><code class="hljs sh"><span class="hljs-comment"># 设置集群参数</span>kubectl config <span class="hljs-built_in">set</span>-cluster kubernetes \  --certificate-authority=/etc/kubernetes/ssl/k8s-root-ca.pem \  --embed-certs=<span class="hljs-literal">true</span> \  --server=https://127.0.0.1:6443 \  --kubeconfig=bootstrap.kubeconfig<span class="hljs-comment"># 设置客户端认证参数</span>kubectl config <span class="hljs-built_in">set</span>-credentials system:bootstrap:47f392 \  --token=47f392.d22d04e89a65eb22 \  --kubeconfig=bootstrap.kubeconfig<span class="hljs-comment"># 设置上下文参数</span>kubectl config <span class="hljs-built_in">set</span>-context default \  --cluster=kubernetes \  --user=system:bootstrap:47f392 \  --kubeconfig=bootstrap.kubeconfig<span class="hljs-comment"># 设置默认上下文</span>kubectl config use-context default --kubeconfig=bootstrap.kubeconfig</code></pre><h3 id="3-6、调整-Kubelet"><a href="#3-6、调整-Kubelet" class="headerlink" title="3.6、调整 Kubelet"></a>3.6、调整 Kubelet</h3><p>Kubelet 启动参数需要做一些相应调整，以使其能正确的使用 <code>Bootstartp Token</code>，完整配置如下(与使用 token.csv 配置没什么变化，因为主要变更在 bootstrap.kubeconfig 中):</p><pre><code class="hljs sh">KUBELET_ARGS=<span class="hljs-string">"  --address=192.168.1.64 \</span><span class="hljs-string">                --allow-privileged=true \</span><span class="hljs-string">                --alsologtostderr \</span><span class="hljs-string">                --anonymous-auth=true \</span><span class="hljs-string">                --bootstrap-kubeconfig=/etc/kubernetes/bootstrap.kubeconfig \</span><span class="hljs-string">                --cert-dir=/etc/kubernetes/ssl \</span><span class="hljs-string">                --cgroup-driver=cgroupfs \</span><span class="hljs-string">                --cluster-dns=10.254.0.2 \</span><span class="hljs-string">                --cluster-domain=cluster.local. \</span><span class="hljs-string">                --fail-swap-on=false \</span><span class="hljs-string">                --healthz-port=10248 \</span><span class="hljs-string">                --healthz-bind-address=192.168.1.64 \</span><span class="hljs-string">                --feature-gates=RotateKubeletClientCertificate=true,RotateKubeletServerCertificate=true \</span><span class="hljs-string">                --node-labels=node-role.kubernetes.io/k8s-master=true \</span><span class="hljs-string">                --image-gc-high-threshold=70 \</span><span class="hljs-string">                --image-gc-low-threshold=50 \</span><span class="hljs-string">                --kube-reserved=cpu=500m,memory=512Mi,ephemeral-storage=1Gi \</span><span class="hljs-string">                --kubeconfig=/etc/kubernetes/kubelet.kubeconfig \</span><span class="hljs-string">                --system-reserved=cpu=1000m,memory=1024Mi,ephemeral-storage=1Gi \</span><span class="hljs-string">                --serialize-image-pulls=false \</span><span class="hljs-string">                --sync-frequency=30s \</span><span class="hljs-string">                --pod-infra-container-image=k8s.gcr.io/pause:3.1 \</span><span class="hljs-string">                --resolv-conf=/etc/resolv.conf \</span><span class="hljs-string">                --rotate-certificates"</span></code></pre><p><strong>一切准备就绪后，执行 <code>systemctl daemon-reload &amp;&amp; systemctl start kubelet</code> 启动即可</strong></p><h2 id="四、其他说明"><a href="#四、其他说明" class="headerlink" title="四、其他说明"></a>四、其他说明</h2><p>可能有人已经注意到，在官方文档中最后部分有关于 <a href="https://kubernetes.io/docs/reference/access-authn-authz/bootstrap-tokens/#configmap-signing" target="_blank" rel="noopener">ConfigMap Signing</a> 的相关描述，同时要求了启用 <code>bootstrapsigner</code> 这个 controller，而且在上文创建 <code>Bootstrap Token Secret</code> 中我也说 <code>usage-bootstrap-signing</code> 这个可以不设置；其中官方文档上的描述我们能看到的大致只说了这么两段稍微有点用的话:</p><blockquote><p>In addition to authentication, the tokens can be used to sign a ConfigMap. This is used early in a cluster bootstrap process before the client trusts the API server. The signed ConfigMap can be authenticated by the shared token.</p></blockquote><blockquote><p>The ConfigMap that is signed is cluster-info in the kube-public namespace. The typical flow is that a client reads this ConfigMap while unauthenticated and ignoring TLS errors. It then validates the payload of the ConfigMap by looking at a signature embedded in the ConfigMap.</p></blockquote><p>从这两段话中我们只能得出两个结论:</p><ul><li>Bootstrap Token 能对 ConfigMap 签名</li><li>可以签名一个 <code>kube-public</code> NameSpace 下的名字叫 <code>cluster-info</code> 的 ConfigMap，并且这个 ConfigMap 可以在没进行引导之前强行读取</li></ul><p>说实话这两段话搞得我百思不得<del>骑姐</del>其解，最终我在 kubeadm 的相关文档中找到了真正的说明及作用:</p><ul><li>在使用 <code>kubeadm init</code> 时创建 <code>cluster-info</code> 这个 ConfigMap，ConfigMap 中包含了集群基本信息</li><li>在使用 <code>kubeadm join</code> 时目标节点强行读取 ConfigMap 以得知集群基本信息，然后进行 <code>join</code></li></ul><p><strong>综上所述，我个人认为手动部署下，在仅仅使用 Bootstrap Token 进行 TLS Bootstrapping 时，<code>bootstrapsigner</code> 这个 controller 和 <code>Bootstrap Token Secret</code> 中的 <code>usage-bootstrap-signing</code> 选项是没有必要的，当然我还没测试(胡吹谁不会)…</strong></p><p>最后附上 <code>kubeadm</code> 的文档说明: <a href="https://kubernetes.io/docs/reference/setup-tools/kubeadm/implementation-details/#create-the-public-cluster-info-configmap" target="_blank" rel="noopener">Create the public cluster-info ConfigMap</a>、<a href="https://kubernetes.io/docs/reference/setup-tools/kubeadm/implementation-details/#discovery-cluster-info" target="_blank" rel="noopener">Discovery cluster-info</a></p>]]></content>
    
    
    <categories>
      
      <category>Kubernetes</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Kubernetes</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Kubernetes 证书配置</title>
    <link href="/2018/08/26/kubernetes-certificate-configuration/"/>
    <url>/2018/08/26/kubernetes-certificate-configuration/</url>
    
    <content type="html"><![CDATA[<blockquote><p>一直以来自己的 Kubernetes 集群大部分证书配置全部都在使用一个 CA，而事实上很多教程也没有具体的解释过这些证书代表的作用以及含义；今天索性仔细的翻了翻，顺便看到了一篇老外的文章，感觉写的不错，这里顺带着自己的理解总结一下。</p></blockquote><h2 id="一、Kubernetes-证书分类"><a href="#一、Kubernetes-证书分类" class="headerlink" title="一、Kubernetes 证书分类"></a>一、Kubernetes 证书分类</h2><p>这里的证书分类只是我自己定义的一种 “并不 ok” 的概念；从整体的作用上 Kubernetes 证书大致上应当分为两类:</p><ul><li>API Server 用于校验请求合法性证书</li><li>对其他敏感信息进行签名的证书(如 Service Account)</li></ul><p>对于 API Server 用于检验请求合法性的证书配置一般会在 API Server 中配置好，而对其他敏感信息签名加密的证书一般会可能放在 Controller Manager 中配置，也可能还在 API Server，具体不同版本需要撸文档</p><p>另外需要明确的是: <strong>Kubernetes 中 CA 证书并不一定只有一个，很多证书配置实际上是不相干的，只是大家为了方便普遍选择了使用一个 CA 进行签发；同时有一些证书如果不设置也会自动默认一个，就目前我所知的大约有 5 个可以完全不同的证书签发体系(或者说由不同的 CA 签发)</strong></p><h2 id="二、API-Server-中的证书配置"><a href="#二、API-Server-中的证书配置" class="headerlink" title="二、API Server 中的证书配置"></a>二、API Server 中的证书配置</h2><h3 id="2-1、API-Server-证书"><a href="#2-1、API-Server-证书" class="headerlink" title="2.1、API Server 证书"></a>2.1、API Server 证书</h3><p>API Server 证书配置中最应当明确的两个选项应该是以下两个:</p><pre><code class="hljs sh">--tls-cert-file string    File containing the default x509 Certificate <span class="hljs-keyword">for</span> HTTPS. (CA cert, <span class="hljs-keyword">if</span> any, concatenated after server cert). If HTTPS serving is enabled, and --tls-cert-file and --tls-private-key-file are not provided, a self-signed certificate and key are generated <span class="hljs-keyword">for</span> the public address and saved to the directory specified by --cert-dir.--tls-private-key-file string    File containing the default x509 private key matching --tls-cert-file.</code></pre><p>从描述上就可以看出，这两个选项配置的就是 API Server HTTPS 端点应当使用的证书</p><h3 id="2-2、Client-CA-证书"><a href="#2-2、Client-CA-证书" class="headerlink" title="2.2、Client CA 证书"></a>2.2、Client CA 证书</h3><p>接下来就是我们常见的 CA 配置:</p><pre><code class="hljs sh">--client-ca-file string    If <span class="hljs-built_in">set</span>, any request presenting a client certificate signed by one of the authorities <span class="hljs-keyword">in</span> the client-ca-file is authenticated with an identity corresponding to the CommonName of the client certificate.</code></pre><p>该配置明确了 Clent 连接 API Server 时，API Server 应当确保其证书源自哪个 CA 签发；如果其证书不是由该 CA 签发，则拒绝请求；事实上，这个 CA 不必与 HTTPS 端点所使用的证书 CA 相同；同时这里的 Client 是一个泛指的，可以是 kubectl，也可能是你自己开发的应用</p><h3 id="2-3、请求头证书"><a href="#2-3、请求头证书" class="headerlink" title="2.3、请求头证书"></a>2.3、请求头证书</h3><p>由于 API Server 是支持多种认证方式的，其中一种就是使用 HTTP 头中的指定字段来进行认证，相关配置如下:</p><pre><code class="hljs sh">--requestheader-allowed-names stringSlice    List of client certificate common names to allow to provide usernames <span class="hljs-keyword">in</span> headers specified by --requestheader-username-headers. If empty, any client certificate validated by the authorities <span class="hljs-keyword">in</span> --requestheader-client-ca-file is allowed.--requestheader-client-ca-file string    Root certificate bundle to use to verify client certificates on incoming requests before trusting usernames <span class="hljs-keyword">in</span> headers specified by --requestheader-username-headers. WARNING: generally <span class="hljs-keyword">do</span> not depend on authorization being already <span class="hljs-keyword">done</span> <span class="hljs-keyword">for</span> incoming requests.</code></pre><p>当指定这个 CA 证书后，则 API Server 使用 HTTP 头进行认证时会检测其 HTTP 头中发送的证书是否由这个 CA 签发；同样它也可独立于其他 CA(可以是个独立的 CA)；具体可以参考 <a href="https://kubernetes.io/docs/reference/access-authn-authz/authentication/#authenticating-proxy" target="_blank" rel="noopener">Authenticating Proxy</a></p><h3 id="2-4、Kubelet-证书"><a href="#2-4、Kubelet-证书" class="headerlink" title="2.4、Kubelet 证书"></a>2.4、Kubelet 证书</h3><p>对于 Kubelet 组件，API Server 单独提供了证书配置选项，同时 Kubelet 组件也提供了反向设置的相关选项:</p><pre><code class="hljs sh"><span class="hljs-comment"># API Server</span>--kubelet-certificate-authority string    Path to a cert file <span class="hljs-keyword">for</span> the certificate authority.--kubelet-client-certificate string    Path to a client cert file <span class="hljs-keyword">for</span> TLS.--kubelet-client-key string    Path to a client key file <span class="hljs-keyword">for</span> TLS.<span class="hljs-comment"># Kubelet</span>--client-ca-file string    If <span class="hljs-built_in">set</span>, any request presenting a client certificate signed by one of the authorities <span class="hljs-keyword">in</span> the client-ca-file is authenticated with an identity corresponding to the CommonName of the client certificate.--tls-cert-file string    File containing x509 Certificate used <span class="hljs-keyword">for</span> serving HTTPS (with intermediate certs, <span class="hljs-keyword">if</span> any, concatenated after server cert). If --tls-cert-file and --tls-private-key-file are not provided, a self-signed certificate and key are generated <span class="hljs-keyword">for</span> the public address and saved to the directory passed to --cert-dir.--tls-private-key-file string    File containing x509 private key matching --tls-cert-file.</code></pre><p>相信这个配置不用多说就能猜到，这个就是用于指定 API Server 与 Kubelet 通讯所使用的证书以及其签署的 CA；同样这个 CA 可以完全独立与上述其他CA</p><h2 id="三、Service-Account-证书"><a href="#三、Service-Account-证书" class="headerlink" title="三、Service Account 证书"></a>三、Service Account 证书</h2><p>在 API Server 配置中，对于 Service Account 同样有两个证书配置:</p><pre><code class="hljs sh">--service-account-key-file stringArray    File containing PEM-encoded x509 RSA or ECDSA private or public keys, used to verify ServiceAccount tokens. The specified file can contain multiple keys, and the flag can be specified multiple <span class="hljs-built_in">times</span> with different files. If unspecified, --tls-private-key-file is used. Must be specified when --service-account-signing-key is provided--service-account-signing-key-file string    Path to the file that contains the current private key of the service account token issuer. The issuer will sign issued ID tokens with this private key. (Requires the <span class="hljs-string">'TokenRequest'</span> feature gate.)</code></pre><p>这两个配置描述了对 Service Account 进行签名验证时所使用的证书；不过需要注意的是这里并没有明确要求证书 CA，所以这两个证书的 CA 理论上也是可以完全独立的；至于未要求 CA 问题，可能是由于 jwt 库并不支持 CA 验证</p><h2 id="四、总结"><a href="#四、总结" class="headerlink" title="四、总结"></a>四、总结</h2><p>Kubernetes 中大部分证书都是用于 API Server 各种鉴权使用的；在不同鉴权方案或者对象上实际证书体系可以完全不同；具体是使用多个 CA 好还是都用一个，取决于集群规模、安全性要求等等因素，至少目前来说没有明确的那个好与不好</p><p>最后，嗯…吹牛逼就吹到这，有点晚了，得睡觉了…</p>]]></content>
    
    
    <categories>
      
      <category>Kubernetes</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Kubernetes</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>编写 kubectl 插件</title>
    <link href="/2018/08/09/create-a-plugin-for-kubectl/"/>
    <url>/2018/08/09/create-a-plugin-for-kubectl/</url>
    
    <content type="html"><![CDATA[<blockquote><p>最近忙的晕头转向，博客停更了 1 个月，感觉对不起党、对不起人民、对不起 <del>CCAV</del>…不过在忙的时候操作 Kubernetes 集群要频繁的使用 <code>kubectl</code> 命令，而在多个 NameSpace 下来回切换每次都得加个 <code>-n</code> 简直让我想打人；索性翻了下 <code>kubectl</code> 的插件机制，顺便写了一个快速切换 NameSpace 的小插件，以下记录一下插件编写过程</p></blockquote><h2 id="一、插件介绍"><a href="#一、插件介绍" class="headerlink" title="一、插件介绍"></a>一、插件介绍</h2><p><code>kubectl</code> 命令从 <code>v1.8.0</code> 版本开始引入了 alpha feature 的插件机制；在此机制下我们可以对 <code>kubectl</code> 命令进行扩展，从而编写一些自己的插件集成进 <code>kubectl</code> 命令中；<strong><code>kubectl</code> 插件机制是与语言无关的，也就是说你可以用任何语言编写插件，可以是 <code>bash</code>、<code>python</code> 脚本，也可以是 <code>go</code>、<code>java</code> 等编译型语言；所以选择你熟悉的语言即可</strong>，以下是一个用 <code>go</code> 编写的用于快速切换 NameSpace 的小插件，运行截图如下:</p><p><strong>所谓: 开局一张图，功能全靠编 😂</strong><br><img src="https://cdn.oss.link/markdown/6t89g.gif" srcset="/img/loading.gif" alt="swns.gif"></p><p>当前插件代码放在 <a href="https://github.com/mritd/swns" target="_blank" rel="noopener">mritd/swns</a> 这个项目下面</p><h2 id="二、插件加载"><a href="#二、插件加载" class="headerlink" title="二、插件加载"></a>二、插件加载</h2><p><code>kubectl</code> 插件机制目前并不提供包管理器一样的功能，比如你想执行 <code>kuebctl plugin install xxx</code> 这种操作目前还没有实现(个人感觉差个规范)；所以一旦我们编写或者下载一个插件后，我们只有正确放在特定目录才会生效；</p><p><strong>目前插件根据文档描述只有两部分内容: <code>plugin.yaml</code> 和其依赖的二进制/脚本等可执行文件</strong>；根据文档说明，<code>kubectl</code> 会尝试在如下位置查找并加载插件，所以我们只需要将 <code>plugin.yaml</code> 和相关二进制放在在对应位置即可:</p><ul><li><code>${KUBECTL_PLUGINS_PATH}</code>: 如果这个环境变量定义了，那么 <code>kubectl</code> <strong>只会</strong>从这里查找；<strong>注意: 这个变量可以是多个目录，类似 PATH 变量一样，做好分割即可</strong></li><li><code>${XDG_DATA_DIRS}/kubectl/plugins</code>: 关于这个变量具体请看 <a href="https://specifications.freedesktop.org/basedir-spec/basedir-spec-latest.html" target="_blank" rel="noopener">XDG System Directory Structure</a>，我了解也不多；<strong>如果这个变量没定义则默认为 <code>/usr/local/share:/usr/share</code></strong></li><li><code>~/.kube/plugins</code>: 这个没啥可说的，我推荐还是将插件放在这个位置比较友好一点</li></ul><p>所以最终插件目录结构类似这样:</p><pre><code class="hljs sh">➜  ~ tree .kube.kube├── config└── plugins    └── swns        ├── plugin.yaml        └── swns</code></pre><h2 id="三、Plugin-yaml"><a href="#三、Plugin-yaml" class="headerlink" title="三、Plugin.yaml"></a>三、Plugin.yaml</h2><p><code>plugin.yaml</code> 这个文件实际上才是插件的核心，在这个文件里声明了插件如何使用、调用的二进制/脚本等重要配置；<strong>一个插件可以没有任何脚本/二进制可执行文件，但至少应当有一个 <code>plugin.yaml</code> 描述文件</strong>；目前 <code>plugin.yaml</code> 的结构如下:</p><pre><code class="hljs yaml"><span class="hljs-attr">name:</span> <span class="hljs-string">"targaryen"</span>                 <span class="hljs-comment"># 必填项: 用于 kuebctl 调用的插件名称</span><span class="hljs-attr">shortDesc:</span> <span class="hljs-string">"Dragonized plugin"</span>    <span class="hljs-comment"># 必填项: 用于 help 该插件时的简短描述</span><span class="hljs-attr">longDesc:</span> <span class="hljs-string">""</span>                      <span class="hljs-comment"># 非必填: 插件的长描述</span><span class="hljs-attr">example:</span> <span class="hljs-string">""</span>                       <span class="hljs-comment"># 非必填: 插件的使用样例</span><span class="hljs-attr">command:</span> <span class="hljs-string">"./dracarys"</span>             <span class="hljs-comment"># 必填项: 插件实际执行的文件位置，可以相对路径 or 绝对路径，或者在 PATH 里也行</span><span class="hljs-attr">flags:</span>                            <span class="hljs-comment"># 非必填: 插件支持的 flag</span>  <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">"heat"</span>                  <span class="hljs-comment"># 必填项: 如果你写了支持的 flag，那么此项必填</span>    <span class="hljs-attr">shorthand:</span> <span class="hljs-string">"h"</span>                <span class="hljs-comment"># 非必填: 该选项的缩短形式</span>    <span class="hljs-attr">desc:</span> <span class="hljs-string">"Fire heat"</span>             <span class="hljs-comment"># 必填项: 同样每个 flag 都必须书写描述</span>    <span class="hljs-attr">defValue:</span> <span class="hljs-string">"extreme"</span>           <span class="hljs-comment"># 非必填: 默认值</span><span class="hljs-attr">tree:</span>                             <span class="hljs-comment"># 允许定义一些子命令</span>  <span class="hljs-bullet">-</span> <span class="hljs-string">...</span>                           <span class="hljs-comment"># 子命令支持同样的设置属性(我想知道子命令的子命令的子命令支不支持...我还没去试过)</span></code></pre><h2 id="四、插件环境变量"><a href="#四、插件环境变量" class="headerlink" title="四、插件环境变量"></a>四、插件环境变量</h2><p>在编写插件时，<strong>有时插件运行时需要获取到一些参数，比如 <code>kubectl</code> 执行时的全局 flag 等，</strong>为了方便插件开发者，<code>kuebctl</code> 的插件机制提供一些预置的环境变量方便我们读取；即如果你用 <code>bash</code> 写插件，那么这些变量你只需要 <code>${xxxx}</code> 即可拿到，然后做一些你想做的事情；这些变量目前支持如下:</p><ul><li><code>KUBECTL_PLUGINS_CALLER</code>: <code>kubectl</code> 二进制文件所在位置；<strong>作为插件编写者，我们无需关系 api server 是否能联通，因为配置是否正确应当由使用者决定；在需要时我们只需要直接调用 <code>kubectl</code> 即可；</strong>比如在 <code>bash</code> 脚本中执行 <code>get pod</code> 等</li><li><code>KUBECTL_PLUGINS_CURRENT_NAMESPACE</code>: 当前 <code>kuebctl</code> 命令所对应的 NameSpace，<strong>插件机制确保了该值一定正确；即这是经过解析了 <code>--namespace</code> 选项或者 <code>kubeconfig</code> 配置后的最终结果；作为插件编写者，我们无需关心处理过程</strong>；想详细了解的的可以去看源码，以及 <code>Cobra</code> 库(Kubernetes 用这个库解析命令行参数和配置)</li><li><code>KUBECTL_PLUGINS_DESCRIPTOR_*</code>: 插件自己本身位于 <code>plugin.yaml</code> 中的描述信息，比如 <code>KUBECTL_PLUGINS_DESCRIPTOR_NAME</code> 输出 <code>plugin.yaml</code> 下的 <code>name</code> 属性；一般可以用作插件输出自己的帮助文档等</li><li><code>KUBECTL_PLUGINS_GLOBAL_FLAG_*</code>: 获取 <code>kubectl</code> 所有全局 flag 值的变量，比如 <code>KUBECTL_PLUGINS_GLOBAL_FLAG_NAMESPACE</code> 能拿到 <code>--namespace</code> 选项的值</li><li><code>KUBECTL_PLUGINS_LOCAL_FLAG_*</code>: 同上面类似，只不过这个是获取插件自己本身 flag 的值，个人认为在脚本语言中，比如 <code>bash</code> 等处理选项不怎么好用时，可以考虑直接从变量拿</li></ul><p>以上变量我并未都测试，具体以测试为准，<strong>删库跑路等情况本人概不负责</strong></p><h2 id="五、写一个切换-NameSpace-的插件"><a href="#五、写一个切换-NameSpace-的插件" class="headerlink" title="五、写一个切换 NameSpace 的插件"></a>五、写一个切换 NameSpace 的插件</h2><p>前面墨迹一大堆只是为了描述清楚 <strong>要写一个插件应该怎么干</strong> 的问题，下面开始 <strong>这么干</strong></p><h3 id="5-1、编写配置"><a href="#5-1、编写配置" class="headerlink" title="5.1、编写配置"></a>5.1、编写配置</h3><p>上面已经介绍好了 <code>plugin.yaml</code> 怎么写，那么根据我自己的需求，我写的这个切换 NameSpace 插件的名字暂且叫做 <code>swns</code>；我希望 <code>swns</code> 执行后接受一个 NameSpace 的字符串，然后调用 <code>kuebctl config</code> 去设置当前默认的 NameSpace，这样在后续命令中我就不用再一直加个 <code>-n xxx</code> 参数了；同时我希望使用更方便点，当执行 <code>swns</code> 命令时，如果不提供 NameSpace 的字符串，那我就弹出下拉列表供用户选择；综上需求自己想明白后，就写一个 <code>plugin.yaml</code>，如下:</p><pre><code class="hljs yaml"><span class="hljs-attr">name:</span> <span class="hljs-string">"swns"</span><span class="hljs-attr">shortDesc:</span> <span class="hljs-string">"Switch NameSpace"</span><span class="hljs-attr">longDesc:</span> <span class="hljs-string">"Switch Kubernetes current context namespace."</span><span class="hljs-attr">example:</span> <span class="hljs-string">"kubectl plugin swns [NAMESPACE]"</span><span class="hljs-attr">command:</span> <span class="hljs-string">"./swns"</span></code></pre><h3 id="5-2、编写插件"><a href="#5-2、编写插件" class="headerlink" title="5.2、编写插件"></a>5.2、编写插件</h3><p>上面 <code>plugin.yaml</code> 已经定义好了，那么接下来就简单了，撸代码实现了就好；代码如下:</p><pre><code class="hljs go"><span class="hljs-comment">// 注意: 下面的模板语法大括号中间没有空格，此处空格是为了防止博客渲染出错</span><span class="hljs-keyword">package</span> main<span class="hljs-keyword">import</span> (<span class="hljs-string">"fmt"</span><span class="hljs-string">"os"</span><span class="hljs-string">"os/exec"</span><span class="hljs-string">"strings"</span><span class="hljs-string">"github.com/mritd/promptx"</span>)<span class="hljs-function"><span class="hljs-keyword">func</span> <span class="hljs-title">main</span><span class="hljs-params">()</span></span> &#123;<span class="hljs-comment">// 先拿到当前的 context</span>cmd := exec.Command(<span class="hljs-string">"kubectl"</span>, <span class="hljs-string">"config"</span>, <span class="hljs-string">"current-context"</span>)cmd.Stdin = os.Stdincmd.Stderr = os.Stderrb, err := cmd.Output()checkAndExit(err)currentContext := strings.TrimSpace(<span class="hljs-keyword">string</span>(b))<span class="hljs-comment">// 如果提供了 NameSpace 字符串，我直接改就行了</span><span class="hljs-keyword">if</span> <span class="hljs-built_in">len</span>(os.Args) &gt; <span class="hljs-number">1</span> &#123;cmd = exec.Command(<span class="hljs-string">"kubectl"</span>, <span class="hljs-string">"config"</span>, <span class="hljs-string">"set-context"</span>, currentContext, <span class="hljs-string">"--namespace="</span>+os.Args[<span class="hljs-number">1</span>])cmd.Stdout = os.StdoutcheckAndExit(cmd.Run())fmt.Printf(<span class="hljs-string">"Kubernetes namespace switch to %s.\n"</span>, os.Args[<span class="hljs-number">1</span>])&#125; <span class="hljs-keyword">else</span> &#123;<span class="hljs-comment">// 没提供我就得先把所有的 NameSpace 弄出来</span>cmd = exec.Command(<span class="hljs-string">"kubectl"</span>, <span class="hljs-string">"get"</span>, <span class="hljs-string">"ns"</span>, <span class="hljs-string">"-o"</span>, <span class="hljs-string">"template"</span>, <span class="hljs-string">"--template"</span>, <span class="hljs-string">"&#123; &#123; range .items &#125; &#125;&#123; &#123; .metadata.name &#125; &#125; &#123; &#123; end &#125; &#125;"</span>)b, err = cmd.Output()checkAndExit(err)allNameSpace := strings.Fields(<span class="hljs-keyword">string</span>(b))<span class="hljs-comment">// 弄到所有的 NameSpace 后，我在弄一个下拉列表(这是我自己造的一个下拉列表库)</span>cfg := &amp;promptx.SelectConfig&#123;ActiveTpl:    <span class="hljs-string">"»  &#123; &#123; . | cyan &#125; &#125;"</span>,InactiveTpl:  <span class="hljs-string">"  &#123; &#123; . | white &#125; &#125;"</span>,SelectPrompt: <span class="hljs-string">"NameSpace"</span>,SelectedTpl:  <span class="hljs-string">"&#123; &#123; \"» \" | green &#125; &#125;&#123; &#123;\"NameSpace:\" | cyan &#125; &#125; &#123; &#123; . &#125; &#125;"</span>,DisPlaySize:  <span class="hljs-number">9</span>,DetailsTpl:   <span class="hljs-string">` `</span>,&#125;s := &amp;promptx.Select&#123;Items:  allNameSpace,Config: cfg,&#125;<span class="hljs-comment">// 用户选中一个 NameSpace 后我就拿到了想要设置的 NameSpace 字符串</span>selectNameSpace := allNameSpace[s.Run()]<span class="hljs-comment">// 跟上面套路一样，写进去就行了</span>cmd = exec.Command(<span class="hljs-string">"kubectl"</span>, <span class="hljs-string">"config"</span>, <span class="hljs-string">"set-context"</span>, currentContext, <span class="hljs-string">"--namespace="</span>+selectNameSpace)cmd.Stdout = os.StdoutcheckAndExit(cmd.Run())fmt.Printf(<span class="hljs-string">"Kubernetes namespace switch to %s.\n"</span>, selectNameSpace)&#125;&#125;<span class="hljs-function"><span class="hljs-keyword">func</span> <span class="hljs-title">checkErr</span><span class="hljs-params">(err error)</span> <span class="hljs-title">bool</span></span> &#123;<span class="hljs-keyword">if</span> err != <span class="hljs-literal">nil</span> &#123;fmt.Println(err)<span class="hljs-keyword">return</span> <span class="hljs-literal">false</span>&#125;<span class="hljs-keyword">return</span> <span class="hljs-literal">true</span>&#125;<span class="hljs-function"><span class="hljs-keyword">func</span> <span class="hljs-title">checkAndExit</span><span class="hljs-params">(err error)</span></span> &#123;<span class="hljs-keyword">if</span> !checkErr(err) &#123;os.Exit(<span class="hljs-number">1</span>)&#125;&#125;</code></pre><p>最后编译后放到上面所说的插件加载目录即可</p><p>到此，<strong>“全局一张图，功能全靠编”</strong> 图上面也有了，编的的也差不多 😂</p>]]></content>
    
    
    <categories>
      
      <category>Kubernetes</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Kubernetes</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Traefik 另类的服务暴露方式</title>
    <link href="/2018/05/24/kubernetes-traefik-service-exposure/"/>
    <url>/2018/05/24/kubernetes-traefik-service-exposure/</url>
    
    <content type="html"><![CDATA[<blockquote><p>最近准备重新折腾一下 Kubernetes 的服务暴露方式，以前的方式是彻底剥离 Kubenretes 本身的服务发现，然后改动应用实现 应用+Consul+Fabio 的服务暴露方式；总感觉这种方式不算优雅，所以折腾了一下 Traefik，试了下效果还不错，以下记录了使用 Traefik 的新的服务暴露方式(本文仅针对 HTTP 协议)；</p></blockquote><h2 id="一、Traefik-服务暴露方案"><a href="#一、Traefik-服务暴露方案" class="headerlink" title="一、Traefik 服务暴露方案"></a>一、Traefik 服务暴露方案</h2><h3 id="1-1、以前的-Consul-Fabio-方案"><a href="#1-1、以前的-Consul-Fabio-方案" class="headerlink" title="1.1、以前的 Consul+Fabio 方案"></a>1.1、以前的 Consul+Fabio 方案</h3><p>以前的服务暴露方案是修改应用代码，使其能对接 Consul，然后 Consul 负责健康监测，检测通过后 Fabio 负责读取，最终上层 Nginx 将流量打到 Fabio 上，Fabio 再将流量路由到健康的 Pod 上；总体架构如下</p><p><img src="https://cdn.oss.link/markdown/hkwp3.png" srcset="/img/loading.gif" alt="consul+fabio"></p><p>这种架构目前有几点不太好的地方，首先是必须应用能成功集成 Consul，需要动应用代码不通用；其次组件过多增加维护成本，尤其是调用链日志不好追踪；这里面需要吐槽下 Consul 和 Fabio，Consul 的集群设计模式要想做到完全的 HA 那么需要在每个 pod 中启动一个 agent，因为只要这个 agent 挂了那么集群认为其上所有注册服务都挂了，这点很恶心人；而 Fabio 的日志目前好像还是不支持合理的输出，好像只能 stdout；目前来看不论是组件复杂度还是维护成本都不怎么友好</p><h3 id="1-2、新的-Traefik-方案"><a href="#1-2、新的-Traefik-方案" class="headerlink" title="1.2、新的 Traefik 方案"></a>1.2、新的 Traefik 方案</h3><p>使用 Traefik 首先想到就是直接怼 Ingress，这个确实方便也简单；但是在集群 kube-proxy 不走 ipvs 的情况下 iptables 性能确实堪忧；虽说 Traefik 会直连 Pod，但是你 Ingress 暴露 80、443 端口在本机没有对应 Ingress Controller 的情况下还是会走一下 iptables；<strong>不论是换 kube-router、kube-proxy 走 ipvs 都不是我想要的，我们需要一种完全远离 Kubernetes Service 的新路子</strong>；在看 Traefik 文档的时候，其中说明了 Traefik 只利用 Kubernetes 的 API 来读取相关后端数据，那么我们就可以以此来使用如下的套路</p><p><img src="https://cdn.oss.link/markdown/tfo2f.png" srcset="/img/loading.gif" alt="traefik"></p><p>这个套路的方案很简单，<strong>将 Traefik 部署在物理机上，让其直连 Kubernets api 以读取 Ingress 配置和 Pod IP 等信息，然后在这几台物理机上部署好 Kubernetes 的网络组件使其能直连 Pod IP</strong>；这种方案能够让流量经过 Traefik 直接路由到后端 Pod，健康检测还是由集群来做；<strong>由于 Traefik 连接 Kubernetes api 需要获取一些数据；所以在集群内还是像往常一样创建 Ingress，只不过此时我们并没有 Ingress Controller；这样避免了经过 iptables 转发，不占用全部集群机器的 80、443 端口，同时还能做到高可控</strong></p><h2 id="二、Traefik-部署"><a href="#二、Traefik-部署" class="headerlink" title="二、Traefik 部署"></a>二、Traefik 部署</h2><p>部署之前首先需要有一个正常访问的集群，然后在另外几台机器上部署 Kubernetes 的网络组件；最终要保证另外几台机器能够直接连通 Pod 的 IP，我这里偷懒直接在 Kubernetes 的其中几个 Node 上部署 Traefik</p><h3 id="2-1、Docker-Compose"><a href="#2-1、Docker-Compose" class="headerlink" title="2.1、Docker Compose"></a>2.1、Docker Compose</h3><p>Traefik 的 Docker Compose 如下</p><pre><code class="hljs yml"><span class="hljs-attr">version:</span> <span class="hljs-string">'3.5'</span><span class="hljs-attr">services:</span>  <span class="hljs-attr">traefik:</span>    <span class="hljs-attr">image:</span> <span class="hljs-string">traefik:v1.6.1-alpine</span>    <span class="hljs-attr">container_name:</span> <span class="hljs-string">traefik</span>    <span class="hljs-attr">command:</span> <span class="hljs-string">--configFile=/etc/traefik.toml</span>    <span class="hljs-attr">ports:</span>      <span class="hljs-bullet">-</span> <span class="hljs-string">"2080:2080"</span>      <span class="hljs-bullet">-</span> <span class="hljs-string">"2180:2180"</span>    <span class="hljs-attr">volumes:</span>      <span class="hljs-bullet">-</span> <span class="hljs-string">./traefik.toml:/etc/traefik.toml</span>      <span class="hljs-bullet">-</span> <span class="hljs-string">./k8s-root-ca.pem:/etc/kubernetes/ssl/k8s-root-ca.pem</span>      <span class="hljs-bullet">-</span> <span class="hljs-string">./log:/var/log/traefik</span></code></pre><p>由于 Kubernetes 集群开启了 RBAC 认证同时采用 TLS 通讯，所以需要挂载 Kubernetes CA 证书，还需要为 Traefik 创建对应的 RBAC 账户以使其能够访问 Kubernetes API</p><h3 id="2-2、创建-RBAC-账户"><a href="#2-2、创建-RBAC-账户" class="headerlink" title="2.2、创建 RBAC 账户"></a>2.2、创建 RBAC 账户</h3><p>Traefik 连接 Kubernetes API 时需要使用 Service Account 的 Token，Service Account 以及 ClusterRole 等配置具体见 <a href="https://docs.traefik.io/user-guide/kubernetes/" target="_blank" rel="noopener">官方文档</a>，下面是我从当前版本的文档中 Copy 出来的</p><pre><code class="hljs yml"><span class="hljs-meta">---</span><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">v1</span><span class="hljs-attr">kind:</span> <span class="hljs-string">ServiceAccount</span><span class="hljs-attr">metadata:</span>  <span class="hljs-attr">name:</span> <span class="hljs-string">traefik-ingress-controller</span>  <span class="hljs-attr">namespace:</span> <span class="hljs-string">kube-system</span><span class="hljs-meta">---</span><span class="hljs-attr">kind:</span> <span class="hljs-string">ClusterRole</span><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">rbac.authorization.k8s.io/v1beta1</span><span class="hljs-attr">metadata:</span>  <span class="hljs-attr">name:</span> <span class="hljs-string">traefik-ingress-controller</span><span class="hljs-attr">rules:</span>  <span class="hljs-bullet">-</span> <span class="hljs-attr">apiGroups:</span>      <span class="hljs-bullet">-</span> <span class="hljs-string">""</span>    <span class="hljs-attr">resources:</span>      <span class="hljs-bullet">-</span> <span class="hljs-string">services</span>      <span class="hljs-bullet">-</span> <span class="hljs-string">endpoints</span>      <span class="hljs-bullet">-</span> <span class="hljs-string">secrets</span>    <span class="hljs-attr">verbs:</span>      <span class="hljs-bullet">-</span> <span class="hljs-string">get</span>      <span class="hljs-bullet">-</span> <span class="hljs-string">list</span>      <span class="hljs-bullet">-</span> <span class="hljs-string">watch</span>  <span class="hljs-bullet">-</span> <span class="hljs-attr">apiGroups:</span>      <span class="hljs-bullet">-</span> <span class="hljs-string">extensions</span>    <span class="hljs-attr">resources:</span>      <span class="hljs-bullet">-</span> <span class="hljs-string">ingresses</span>    <span class="hljs-attr">verbs:</span>      <span class="hljs-bullet">-</span> <span class="hljs-string">get</span>      <span class="hljs-bullet">-</span> <span class="hljs-string">list</span>      <span class="hljs-bullet">-</span> <span class="hljs-string">watch</span><span class="hljs-meta">---</span><span class="hljs-attr">kind:</span> <span class="hljs-string">ClusterRoleBinding</span><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">rbac.authorization.k8s.io/v1beta1</span><span class="hljs-attr">metadata:</span>  <span class="hljs-attr">name:</span> <span class="hljs-string">traefik-ingress-controller</span><span class="hljs-attr">roleRef:</span>  <span class="hljs-attr">apiGroup:</span> <span class="hljs-string">rbac.authorization.k8s.io</span>  <span class="hljs-attr">kind:</span> <span class="hljs-string">ClusterRole</span>  <span class="hljs-attr">name:</span> <span class="hljs-string">traefik-ingress-controller</span><span class="hljs-attr">subjects:</span><span class="hljs-bullet">-</span> <span class="hljs-attr">kind:</span> <span class="hljs-string">ServiceAccount</span>  <span class="hljs-attr">name:</span> <span class="hljs-string">traefik-ingress-controller</span>  <span class="hljs-attr">namespace:</span> <span class="hljs-string">kube-system</span></code></pre><p>创建好以后需要提取 Service Account 的 Token 方便下面使用，提取命令如下</p><pre><code class="hljs sh">kubectl describe secret -n kube-system $(kubectl get secrets -n kube-system | grep traefik-ingress-controller | cut -f1 -d <span class="hljs-string">' '</span>) | grep -E <span class="hljs-string">'^token'</span></code></pre><h3 id="2-3、创建-Traefik-配置"><a href="#2-3、创建-Traefik-配置" class="headerlink" title="2.3、创建 Traefik 配置"></a>2.3、创建 Traefik 配置</h3><p>Traefik 的具体配置细节请参考 <a href="https://docs.traefik.io/configuration/commons/" target="_blank" rel="noopener">官方文档</a>，以下仅给出一个样例配置</p><pre><code class="hljs toml"><span class="hljs-comment"># DEPRECATED - for general usage instruction see [lifeCycle.graceTimeOut].</span><span class="hljs-comment">#</span><span class="hljs-comment"># If both the deprecated option and the new one are given, the deprecated one</span><span class="hljs-comment"># takes precedence.</span><span class="hljs-comment"># A value of zero is equivalent to omitting the parameter, causing</span><span class="hljs-comment"># [lifeCycle.graceTimeOut] to be effective. Pass zero to the new option in</span><span class="hljs-comment"># order to disable the grace period.</span><span class="hljs-comment">#</span><span class="hljs-comment"># Optional</span><span class="hljs-comment"># Default: "0s"</span><span class="hljs-comment">#</span><span class="hljs-comment"># graceTimeOut = "10s"</span><span class="hljs-comment"># Enable debug mode.</span><span class="hljs-comment"># This will install HTTP handlers to expose Go expvars under /debug/vars and</span><span class="hljs-comment"># pprof profiling data under /debug/pprof.</span><span class="hljs-comment"># The log level will be set to DEBUG unless `logLevel` is specified.</span><span class="hljs-comment">#</span><span class="hljs-comment"># Optional</span><span class="hljs-comment"># Default: false</span><span class="hljs-comment">#</span><span class="hljs-comment"># debug = true</span><span class="hljs-comment"># Periodically check if a new version has been released.</span><span class="hljs-comment">#</span><span class="hljs-comment"># Optional</span><span class="hljs-comment"># Default: true</span><span class="hljs-comment">#</span><span class="hljs-attr">checkNewVersion</span> = <span class="hljs-literal">false</span><span class="hljs-comment"># Backends throttle duration.</span><span class="hljs-comment">#</span><span class="hljs-comment"># Optional</span><span class="hljs-comment"># Default: "2s"</span><span class="hljs-comment">#</span><span class="hljs-comment"># providersThrottleDuration = "2s"</span><span class="hljs-comment"># Controls the maximum idle (keep-alive) connections to keep per-host.</span><span class="hljs-comment">#</span><span class="hljs-comment"># Optional</span><span class="hljs-comment"># Default: 200</span><span class="hljs-comment">#</span><span class="hljs-comment"># maxIdleConnsPerHost = 200</span><span class="hljs-comment"># If set to true invalid SSL certificates are accepted for backends.</span><span class="hljs-comment"># This disables detection of man-in-the-middle attacks so should only be used on secure backend networks.</span><span class="hljs-comment">#</span><span class="hljs-comment"># Optional</span><span class="hljs-comment"># Default: false</span><span class="hljs-comment">#</span><span class="hljs-comment"># insecureSkipVerify = true</span><span class="hljs-comment"># Register Certificates in the rootCA.</span><span class="hljs-comment">#</span><span class="hljs-comment"># Optional</span><span class="hljs-comment"># Default: []</span><span class="hljs-comment">#</span><span class="hljs-comment"># rootCAs = [ "/mycert.cert" ]</span><span class="hljs-comment"># Entrypoints to be used by frontends that do not specify any entrypoint.</span><span class="hljs-comment"># Each frontend can specify its own entrypoints.</span><span class="hljs-comment">#</span><span class="hljs-comment"># Optional</span><span class="hljs-comment"># Default: ["http"]</span><span class="hljs-comment">#</span><span class="hljs-comment"># defaultEntryPoints = ["http", "https"]</span><span class="hljs-comment"># Allow the use of 0 as server weight.</span><span class="hljs-comment"># - false: a weight 0 means internally a weight of 1.</span><span class="hljs-comment"># - true: a weight 0 means internally a weight of 0 (a server with a weight of 0 is removed from the available servers).</span><span class="hljs-comment">#</span><span class="hljs-comment"># Optional</span><span class="hljs-comment"># Default: false</span><span class="hljs-comment">#</span><span class="hljs-comment"># AllowMinWeightZero = true</span><span class="hljs-attr">logLevel</span> = <span class="hljs-string">"INFO"</span><span class="hljs-section">[traefikLog]</span>  filePath = "/var/log/traefik/traefik.log"  format   = "json"<span class="hljs-section">[accessLog]</span>  filePath = "/var/log/traefik/access.log"  format = "json"  <span class="hljs-section">[accessLog.filters]</span>    statusCodes = ["200-511"]    retryAttempts = true<span class="hljs-comment">#  [accessLog.fields]</span><span class="hljs-comment">#    defaultMode = "keep"</span><span class="hljs-comment">#    [accessLog.fields.names]</span><span class="hljs-comment">#      "ClientUsername" = "drop"</span><span class="hljs-comment">#      # ...</span><span class="hljs-comment">#</span><span class="hljs-comment">#    [accessLog.fields.headers]</span><span class="hljs-comment">#      defaultMode = "keep"</span><span class="hljs-comment">#      [accessLog.fields.headers.names]</span><span class="hljs-comment">#        "User-Agent" = "redact"</span><span class="hljs-comment">#        "Authorization" = "drop"</span><span class="hljs-comment">#        "Content-Type" = "keep"</span><span class="hljs-comment">#        # ...</span><span class="hljs-section">[entryPoints]</span>  <span class="hljs-section">[entryPoints.http]</span>    address = ":2080"    compress = true  <span class="hljs-section">[entryPoints.traefik]</span>    address = ":2180"    compress = true<span class="hljs-comment">#    [entryPoints.http.whitelist]</span><span class="hljs-comment">#      sourceRange = ["192.168.1.0/24"]</span><span class="hljs-comment">#      useXForwardedFor = true</span><span class="hljs-comment">#    [entryPoints.http.tls]</span><span class="hljs-comment">#      minVersion = "VersionTLS12"</span><span class="hljs-comment">#      cipherSuites = [</span><span class="hljs-comment">#        "TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256",</span><span class="hljs-comment">#        "TLS_RSA_WITH_AES_256_GCM_SHA384"</span><span class="hljs-comment">#       ]</span><span class="hljs-comment">#      [[entryPoints.http.tls.certificates]]</span><span class="hljs-comment">#        certFile = "path/to/my.cert"</span><span class="hljs-comment">#        keyFile = "path/to/my.key"</span><span class="hljs-comment">#      [[entryPoints.http.tls.certificates]]</span><span class="hljs-comment">#        certFile = "path/to/other.cert"</span><span class="hljs-comment">#        keyFile = "path/to/other.key"</span><span class="hljs-comment">#      # ...</span><span class="hljs-comment">#      [entryPoints.http.tls.clientCA]</span><span class="hljs-comment">#        files = ["path/to/ca1.crt", "path/to/ca2.crt"]</span><span class="hljs-comment">#        optional = false</span><span class="hljs-comment">#</span><span class="hljs-comment">#    [entryPoints.http.redirect]</span><span class="hljs-comment">#      entryPoint = "https"</span><span class="hljs-comment">#      regex = "^http://localhost/(.*)"</span><span class="hljs-comment">#      replacement = "http://mydomain/$1"</span><span class="hljs-comment">#      permanent = true</span><span class="hljs-comment">#</span><span class="hljs-comment">#    [entryPoints.http.auth]</span><span class="hljs-comment">#      headerField = "X-WebAuth-User"</span><span class="hljs-comment">#      [entryPoints.http.auth.basic]</span><span class="hljs-comment">#        users = [</span><span class="hljs-comment">#          "test:$apr1$H6uskkkW$IgXLP6ewTrSuBkTrqE8wj/",</span><span class="hljs-comment">#          "test2:$apr1$d9hr9HBB$4HxwgUir3HP4EsggP/QNo0",</span><span class="hljs-comment">#        ]</span><span class="hljs-comment">#        usersFile = "/path/to/.htpasswd"</span><span class="hljs-comment">#      [entryPoints.http.auth.digest]</span><span class="hljs-comment">#        users = [</span><span class="hljs-comment">#          "test:traefik:a2688e031edb4be6a3797f3882655c05",</span><span class="hljs-comment">#          "test2:traefik:518845800f9e2bfb1f1f740ec24f074e",</span><span class="hljs-comment">#        ]</span><span class="hljs-comment">#        usersFile = "/path/to/.htdigest"</span><span class="hljs-comment">#      [entryPoints.http.auth.forward]</span><span class="hljs-comment">#        address = "https://authserver.com/auth"</span><span class="hljs-comment">#        trustForwardHeader = true</span><span class="hljs-comment">#        [entryPoints.http.auth.forward.tls]</span><span class="hljs-comment">#          ca =  [ "path/to/local.crt"]</span><span class="hljs-comment">#          caOptional = true</span><span class="hljs-comment">#          cert = "path/to/foo.cert"</span><span class="hljs-comment">#          key = "path/to/foo.key"</span><span class="hljs-comment">#          insecureSkipVerify = true</span><span class="hljs-comment">#</span><span class="hljs-comment">#    [entryPoints.http.proxyProtocol]</span><span class="hljs-comment">#      insecure = true</span><span class="hljs-comment">#      trustedIPs = ["10.10.10.1", "10.10.10.2"]</span><span class="hljs-comment">#</span><span class="hljs-comment">#    [entryPoints.http.forwardedHeaders]</span><span class="hljs-comment">#      trustedIPs = ["10.10.10.1", "10.10.10.2"]</span><span class="hljs-comment">#</span><span class="hljs-comment">#  [entryPoints.https]</span><span class="hljs-comment">#    # ...</span><span class="hljs-comment">################################################################</span><span class="hljs-comment"># Kubernetes Ingress configuration backend</span><span class="hljs-comment">################################################################</span><span class="hljs-comment"># Enable Kubernetes Ingress configuration backend.</span><span class="hljs-section">[kubernetes]</span><span class="hljs-comment"># Kubernetes server endpoint.</span><span class="hljs-comment">#</span><span class="hljs-comment"># Optional for in-cluster configuration, required otherwise.</span><span class="hljs-comment"># Default: empty</span><span class="hljs-comment">#</span><span class="hljs-attr">endpoint</span> = <span class="hljs-string">"https://172.16.0.36:6443"</span><span class="hljs-comment"># Bearer token used for the Kubernetes client configuration.</span><span class="hljs-comment">#</span><span class="hljs-comment"># Optional</span><span class="hljs-comment"># Default: empty</span><span class="hljs-comment">#</span><span class="hljs-attr">token</span> = <span class="hljs-string">"eyJhbGciOiJSUzI1NiIsImtpZCI6IiJ9.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJrdWJlLXN5c3RlbSIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VjcmV0Lm5hbWUiOiJ0cmFlZmlrLWluZ3Jlc3MtY29udHJvbGxlci10b2tlbi1zbm5iNSIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VydmljZS1hY2NvdW50Lm5hbWUiOiJ0cmFlZmlrLWlyZ3Jlc3MtY29udHJvbGxlciIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VydmljZS1hY2NvdW50LnVpZCI6ImE4NmI3YWEzLTVmNjQtMTFlOC1hZjYxLWM4MWY2NmUwMzRhNyIsInN1YiI6InN5c3RlbTpxZXJ2aWNlYWNjb3VudDPrdWJlLXN5c3RlbTp0cmFlZmlrLWluZ3Jlc3MtY29udHJvbGxlciJ9.vOFEITuANWGnkER8gukWkTs54BmHXqNpzM55bOb5qXPmI3pZsbei3gtE6tZoqME9P5Lb85cav-8mGZJcoQqqxNBkZJ1YRqy_1O9Apkxa4jA68ipe_NB3L5-exH5cEIrU8iql_r7ycDaKwzsMnAWGPolp1dRkF31u5u8g68oLwF3GR8Z5g4_tLJlTvA53doX7k6Wd6vUygTS3EaQ_qvfXwbcIeaSdWWo2Mym6O0CvIap4jH2w21MbredGURqkRlXEPezKAgRVkr75CdvuvwORnT8YxFLVwuAJs70V-13Ib9v6HAK64GmzcqkAuJtZT8NZKl8Y4TfRGl2_RMq2tk86gD4ShDMedcrto44ZUYHQccsSlpaW5PsN2KBBNPN0-6ca3jIpOmnJojAFUYGM42Wymnx9_4XwHUeeA18-RrercmOaRMdlNq8BzBomAxQB99TqUzRIqpe6m5OotXvouCUnE7qjMwRWmQ5LHjqUGEw_A1pHcalFXQZK0sOCaJOJZIJbc_8rVX-4uxkCBxoIXmzjq8x5a_xPsN4L0aWifkP6co--agw3kOT0O6my8T_CbcZGO9e3OqYPdT4FSl92XlXW8EXHdDpCUJ10aoqJGG2vZSud7IoDxkcScpkj3n6TvyvSRVtk3CtYiIYBpgi7-X2JKkun1a7yFpLogyazz9VlUE4"</span><span class="hljs-comment"># Path to the certificate authority file.</span><span class="hljs-comment"># Used for the Kubernetes client configuration.</span><span class="hljs-comment">#</span><span class="hljs-comment"># Optional</span><span class="hljs-comment"># Default: empty</span><span class="hljs-comment">#</span><span class="hljs-attr">certAuthFilePath</span> = <span class="hljs-string">"/etc/kubernetes/ssl/k8s-root-ca.pem"</span><span class="hljs-comment"># Array of namespaces to watch.</span><span class="hljs-comment">#</span><span class="hljs-comment"># Optional</span><span class="hljs-comment"># Default: all namespaces (empty array).</span><span class="hljs-comment">#</span><span class="hljs-attr">namespaces</span> = [<span class="hljs-string">"default"</span>]<span class="hljs-comment"># Ingress label selector to filter Ingress objects that should be processed.</span><span class="hljs-comment">#</span><span class="hljs-comment"># Optional</span><span class="hljs-comment"># Default: empty (process all Ingresses)</span><span class="hljs-comment">#</span><span class="hljs-comment"># labelselector = "A and not B"</span><span class="hljs-comment"># Value of `kubernetes.io/ingress.class` annotation that identifies Ingress objects to be processed.</span><span class="hljs-comment"># If the parameter is non-empty, only Ingresses containing an annotation with the same value are processed.</span><span class="hljs-comment"># Otherwise, Ingresses missing the annotation, having an empty value, or the value `traefik` are processed.</span><span class="hljs-comment">#</span><span class="hljs-comment"># Note : `ingressClass` option must begin with the "traefik" prefix.</span><span class="hljs-comment">#</span><span class="hljs-comment"># Optional</span><span class="hljs-comment"># Default: empty</span><span class="hljs-comment">#</span><span class="hljs-comment"># ingressClass = "traefik-internal"</span><span class="hljs-comment"># Disable PassHost Headers.</span><span class="hljs-comment">#</span><span class="hljs-comment"># Optional</span><span class="hljs-comment"># Default: false</span><span class="hljs-comment">#</span><span class="hljs-comment"># disablePassHostHeaders = true</span><span class="hljs-comment"># Enable PassTLSCert Headers.</span><span class="hljs-comment">#</span><span class="hljs-comment"># Optional</span><span class="hljs-comment"># Default: false</span><span class="hljs-comment">#</span><span class="hljs-comment"># enablePassTLSCert = true</span><span class="hljs-comment"># Override default configuration template.</span><span class="hljs-comment">#</span><span class="hljs-comment"># Optional</span><span class="hljs-comment"># Default: &lt;built-in template&gt;</span><span class="hljs-comment">#</span><span class="hljs-comment"># filename = "kubernetes.tmpl"</span><span class="hljs-comment"># API definition</span><span class="hljs-section">[api]</span>  <span class="hljs-comment"># Name of the related entry point</span>  <span class="hljs-comment">#</span>  <span class="hljs-comment"># Optional</span>  <span class="hljs-comment"># Default: "traefik"</span>  <span class="hljs-comment">#</span>  entryPoint = "traefik"  <span class="hljs-comment"># Enabled Dashboard</span>  <span class="hljs-comment">#</span>  <span class="hljs-comment"># Optional</span>  <span class="hljs-comment"># Default: true</span>  <span class="hljs-comment">#</span>  dashboard = true  <span class="hljs-comment"># Enable debug mode.</span>  <span class="hljs-comment"># This will install HTTP handlers to expose Go expvars under /debug/vars and</span>  <span class="hljs-comment"># pprof profiling data under /debug/pprof.</span>  <span class="hljs-comment"># Additionally, the log level will be set to DEBUG.</span>  <span class="hljs-comment">#</span>  <span class="hljs-comment"># Optional</span>  <span class="hljs-comment"># Default: false</span>  <span class="hljs-comment">#</span>  debug = false<span class="hljs-comment"># Ping definition</span><span class="hljs-comment">#[ping]</span><span class="hljs-comment">#  # Name of the related entry point</span><span class="hljs-comment">#  #</span><span class="hljs-comment">#  # Optional</span><span class="hljs-comment">#  # Default: "traefik"</span><span class="hljs-comment">#  #</span><span class="hljs-comment">#  entryPoint = "traefik"</span></code></pre><h3 id="2-4、启动-Traefik"><a href="#2-4、启动-Traefik" class="headerlink" title="2.4、启动 Traefik"></a>2.4、启动 Traefik</h3><p>所有文件准备好以后直接执行 <code>docker-compose up -d</code> 启动即可，所有文件目录结构如下</p><pre><code class="hljs sh">traefik├── docker-compose.yaml├── k8s-root-ca.pem├── <span class="hljs-built_in">log</span>│   ├── access.log│   └── traefik.log├── rbac.yaml└── traefik.toml</code></pre><p>启动成功后可以访问 <code>http://IP:2180</code> 查看 Traefik 的控制面板</p><p><img src="https://cdn.oss.link/markdown/phrps.png" srcset="/img/loading.gif" alt="dashboard"></p><h2 id="三、增加-Ingress-配置并测试"><a href="#三、增加-Ingress-配置并测试" class="headerlink" title="三、增加 Ingress 配置并测试"></a>三、增加 Ingress 配置并测试</h2><h3 id="3-1、增加-Ingress-配置"><a href="#3-1、增加-Ingress-配置" class="headerlink" title="3.1、增加 Ingress 配置"></a>3.1、增加 Ingress 配置</h3><p>虽然这种部署方式脱离了 Kubernetes 的 Service 与 Ingress 负载，但是 Traefik 还是需要通过 Kubernetes 的 Ingress 配置来确定后端负载规则，所以 Ingress 对象我们仍需照常创建；以下为一个 Demo 项目的 deployment、service、ingress 配置示例</p><ul><li>demo.deploy.yaml</li></ul><pre><code class="hljs yaml"><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">apps/v1</span><span class="hljs-attr">kind:</span> <span class="hljs-string">Deployment</span><span class="hljs-attr">metadata:</span>  <span class="hljs-attr">name:</span> <span class="hljs-string">demo</span><span class="hljs-attr">spec:</span>  <span class="hljs-attr">replicas:</span> <span class="hljs-number">5</span>  <span class="hljs-attr">selector:</span>    <span class="hljs-attr">matchLabels:</span>      <span class="hljs-attr">app:</span> <span class="hljs-string">demo</span>  <span class="hljs-attr">template:</span>    <span class="hljs-attr">metadata:</span>      <span class="hljs-attr">labels:</span>        <span class="hljs-attr">app:</span> <span class="hljs-string">demo</span>    <span class="hljs-attr">spec:</span>      <span class="hljs-attr">containers:</span>      <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">demo</span>        <span class="hljs-attr">image:</span> <span class="hljs-string">mritd/demo</span>        <span class="hljs-attr">imagePullPolicy:</span> <span class="hljs-string">IfNotPresent</span>        <span class="hljs-attr">ports:</span>        <span class="hljs-bullet">-</span> <span class="hljs-attr">containerPort:</span> <span class="hljs-number">80</span></code></pre><ul><li>demo.svc.yaml</li></ul><pre><code class="hljs yaml"><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">v1</span><span class="hljs-attr">kind:</span> <span class="hljs-string">Service</span><span class="hljs-attr">metadata:</span>  <span class="hljs-attr">name:</span> <span class="hljs-string">demo</span>  <span class="hljs-attr">labels:</span>    <span class="hljs-attr">svc:</span> <span class="hljs-string">demo</span><span class="hljs-attr">spec:</span>  <span class="hljs-attr">ports:</span>  <span class="hljs-bullet">-</span> <span class="hljs-attr">port:</span> <span class="hljs-number">8080</span>    <span class="hljs-attr">name:</span> <span class="hljs-string">http</span>    <span class="hljs-attr">targetPort:</span> <span class="hljs-number">80</span>  <span class="hljs-attr">selector:</span>    <span class="hljs-attr">app:</span> <span class="hljs-string">demo</span></code></pre><ul><li>demo.ing.yaml</li></ul><pre><code class="hljs yaml"><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">extensions/v1beta1</span><span class="hljs-attr">kind:</span> <span class="hljs-string">Ingress</span><span class="hljs-attr">metadata:</span>  <span class="hljs-attr">name:</span> <span class="hljs-string">demo</span>  <span class="hljs-attr">annotations:</span>    <span class="hljs-attr">traefik.ingress.kubernetes.io/preserve-host:</span> <span class="hljs-string">"true"</span><span class="hljs-attr">spec:</span>  <span class="hljs-attr">rules:</span>  <span class="hljs-bullet">-</span> <span class="hljs-attr">host:</span> <span class="hljs-string">demo.mritd.me</span>    <span class="hljs-attr">http:</span>      <span class="hljs-attr">paths:</span>      <span class="hljs-bullet">-</span> <span class="hljs-attr">backend:</span>          <span class="hljs-attr">serviceName:</span> <span class="hljs-string">demo</span>          <span class="hljs-attr">servicePort:</span> <span class="hljs-number">8080</span></code></pre><h3 id="3-2、测试访问"><a href="#3-2、测试访问" class="headerlink" title="3.2、测试访问"></a>3.2、测试访问</h3><p>部署好后应当能从 Traefik 的 Dashboard 中看到新增的 demo ingress，如下所示</p><p><img src="https://cdn.oss.link/markdown/zociy.png" srcset="/img/loading.gif" alt="dashboard-demo"></p><p>最后我们使用 curl 测试即可</p><pre><code class="hljs sh"><span class="hljs-comment"># 在不使用 Host 头的情况下 Traefik 会返 404(Traefik 根据 Host 路由后端，具体配置参考官方文档)</span>test36.node ➜  ~ curl http://172.16.0.36:2080404 page not found<span class="hljs-comment"># 指定 Host 头来路由到 demo 的相关 Pod</span>test36.node ➜  ~ curl -H <span class="hljs-string">"Host: demo.mritd.me"</span> http://172.16.0.36:2080&lt;!DOCTYPE html&gt;&lt;html lang=<span class="hljs-string">"zh"</span>&gt;&lt;head&gt;    &lt;meta http-equiv=<span class="hljs-string">"Content-Type"</span> content=<span class="hljs-string">"text/html; charset=UTF-8"</span>&gt;    &lt;title&gt;Running!&lt;/title&gt;    &lt;style <span class="hljs-built_in">type</span>=<span class="hljs-string">"text/css"</span>&gt;        body &#123;            width: 100%;            min-height: 100%;            background: linear-gradient(to bottom, <span class="hljs-comment">#fff 0, #b8edff 50%, #83dfff 100%);</span>            background-attachment: fixed;        &#125;    &lt;/style&gt;&lt;/head&gt;&lt;body class=<span class="hljs-string">" hasGoogleVoiceExt"</span>&gt;&lt;div align=<span class="hljs-string">"center"</span>&gt;    &lt;h1&gt;Your container is running!&lt;/h1&gt;    &lt;img src=<span class="hljs-string">"./docker.png"</span> alt=<span class="hljs-string">"docker"</span>&gt;&lt;/div&gt;&lt;/body&gt;&lt;/html&gt;<span class="hljs-comment">#</span></code></pre><h2 id="四、其他说明"><a href="#四、其他说明" class="headerlink" title="四、其他说明"></a>四、其他说明</h2><p>写这篇文章的目的是给予一种新的服务暴露思路，这篇文章的某些配置并不适合生产使用；<strong>生产环境尽量使用独立的机器部署 Traefik，同时最好宿主机二进制方式部署；应用的 Deployment 也应当加入健康检测以防止错误的流量路由</strong>；至于 Traefik 的具体细节配置，比如访问日志、Entrypoints 配置、如何连接 Kubernets HA api 等不在本文范畴内，请自行查阅文档；</p><p>最后说一下，关于 Traefik 的 HA 只需要部署多个实例即可，还有 Traefik 本身不做日志滚动等，需要自行处理一下日志。</p>]]></content>
    
    
    <categories>
      
      <category>Kubernetes</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Kubernetes</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>为你的 GitLab 增加提交信息检测</title>
    <link href="/2018/05/11/add-commit-message-style-check-to-your-gitlab/"/>
    <url>/2018/05/11/add-commit-message-style-check-to-your-gitlab/</url>
    
    <content type="html"><![CDATA[<blockquote><p>最近准备对项目生成 Change Log，然而发现提交格式不统一根本没法处理；so 后来大家约定式遵循 GitFlow，并使用 Angular 社区规范的提交格式，同时扩展了一些前缀如 hotfix 等；但是时间长了发现还是有些提交为了 “方便” 不遵循 Angular 社区规范的提交格式，这时候我唯一能做的就是想办法在服务端增加一个提交检测；以下记录了 GitLab 增加自定义 Commit 提交格式检测的方案</p></blockquote><h2 id="一、相关文章资料"><a href="#一、相关文章资料" class="headerlink" title="一、相关文章资料"></a>一、相关文章资料</h2><p>最开始用 Google 搜索到的方案是使用 GitLab 的 Push Rules 功能，具体文档见 <a href="https://docs.gitlab.com/ee/push_rules/push_rules.html" target="_blank" rel="noopener">这里</a>，看完了我才发现这是企业版独有的，作为比较有逼格(qiong)的我们是不可能接受这种 “没技术含量” 的方式的；后来找了好多资料，发现还得借助 Git Hook 功能，文档见 <a href="https://docs.gitlab.com/ee/administration/custom_hooks.html" target="_blank" rel="noopener">Custom Git Hooks</a>；简单地说 Git Hook 就是在 git 操作的不同阶段执行的预定义脚本，<strong>GitLab 目前仅支持 <code>pre-receive</code> 这个钩子，当然他可以链式调用</strong>；所以一切操作就得从这里入手</p><h2 id="二、pre-receive-实现"><a href="#二、pre-receive-实现" class="headerlink" title="二、pre-receive 实现"></a>二、pre-receive 实现</h2><p>查阅了相关资料得出，在进行 push 时，GitLab 会调用这个钩子文件，这个钩子文件必须放在 <code>/var/opt/gitlab/git-data/repositories/&lt;group&gt;/&lt;project&gt;.git/custom_hooks</code> 目录中，当然具体路径也可能是 <code>/home/git/repositories/&lt;group&gt;/&lt;project&gt;.git/custom_hooks</code>；<code>custom_hooks</code> 目录需要自己创建，具体可以参阅文档的 <a href="https://docs.gitlab.com/ee/administration/custom_hooks.html#setup" target="_blank" rel="noopener">Setup</a>；</p><p><strong>在进行 push 操作时，GitLab 会调用这个钩子文件，并且从 stdin 输入三个参数，分别为 之前的版本 commit ID、push 的版本 commit ID 和 push 的分支；根据 commit ID 我们就可以很轻松的获取到提交信息，从而实现进一步检测动作；根据 GitLab 的文档说明，当这个 hook 执行后以非 0 状态退出则认为执行失败，从而拒绝 push；同时会将 stderr 信息返回给 client 端；</strong>说了这么多，下面就可以直接上代码了，为了方便我就直接用 go 造了一个 <a href="https://github.com/mritd/pre-receive" target="_blank" rel="noopener">pre-receive</a>，官方文档说明了不限制语言</p><pre><code class="hljs golang"><span class="hljs-keyword">package</span> main<span class="hljs-keyword">import</span> (    <span class="hljs-string">"fmt"</span>    <span class="hljs-string">"io/ioutil"</span>    <span class="hljs-string">"os"</span>    <span class="hljs-string">"os/exec"</span>    <span class="hljs-string">"regexp"</span>    <span class="hljs-string">"strings"</span>)<span class="hljs-keyword">type</span> CommitType <span class="hljs-keyword">string</span><span class="hljs-keyword">const</span> (    FEAT     CommitType = <span class="hljs-string">"feat"</span>    FIX      CommitType = <span class="hljs-string">"fix"</span>    DOCS     CommitType = <span class="hljs-string">"docs"</span>    STYLE    CommitType = <span class="hljs-string">"style"</span>    REFACTOR CommitType = <span class="hljs-string">"refactor"</span>    TEST     CommitType = <span class="hljs-string">"test"</span>    CHORE    CommitType = <span class="hljs-string">"chore"</span>    PERF     CommitType = <span class="hljs-string">"perf"</span>    HOTFIX   CommitType = <span class="hljs-string">"hotfix"</span>)<span class="hljs-keyword">const</span> CommitMessagePattern = <span class="hljs-string">`^(?:fixup!\s*)?(\w*)(\(([\w\$\.\*/-].*)\))?\: (.*)|^Merge\ branch(.*)`</span><span class="hljs-keyword">const</span> checkFailedMeassge = <span class="hljs-string">`##############################################################################</span><span class="hljs-string">##                                                                          ##</span><span class="hljs-string">## Commit message style check failed!                                       ##</span><span class="hljs-string">##                                                                          ##</span><span class="hljs-string">## Commit message style must satisfy this regular:                          ##</span><span class="hljs-string">##   ^(?:fixup!\s*)?(\w*)(\(([\w\$\.\*/-].*)\))?\: (. *)|^Merge\ branch(.*) ##</span><span class="hljs-string">##                                                                          ##</span><span class="hljs-string">## Example:                                                                 ##</span><span class="hljs-string">##   feat(test): test commit style check.                                   ##</span><span class="hljs-string">##                                                                          ##</span><span class="hljs-string">##############################################################################`</span><span class="hljs-comment">// 是否开启严格模式，严格模式下将校验所有的提交信息格式(多 commit 下)</span><span class="hljs-keyword">const</span> strictMode = <span class="hljs-literal">false</span><span class="hljs-keyword">var</span> commitMsgReg = regexp.MustCompile(CommitMessagePattern)<span class="hljs-function"><span class="hljs-keyword">func</span> <span class="hljs-title">main</span><span class="hljs-params">()</span></span> &#123;    input, _ := ioutil.ReadAll(os.Stdin)    param := strings.Fields(<span class="hljs-keyword">string</span>(input))    <span class="hljs-comment">// allow branch/tag delete</span>    <span class="hljs-keyword">if</span> param[<span class="hljs-number">1</span>] == <span class="hljs-string">"0000000000000000000000000000000000000000"</span> &#123;        os.Exit(<span class="hljs-number">0</span>)    &#125;    commitMsg := getCommitMsg(param[<span class="hljs-number">0</span>], param[<span class="hljs-number">1</span>])    <span class="hljs-keyword">for</span> _, tmpStr := <span class="hljs-keyword">range</span> commitMsg &#123;        commitTypes := commitMsgReg.FindAllStringSubmatch(tmpStr, <span class="hljs-number">-1</span>)        <span class="hljs-keyword">if</span> <span class="hljs-built_in">len</span>(commitTypes) != <span class="hljs-number">1</span> &#123;            checkFailed()        &#125; <span class="hljs-keyword">else</span> &#123;            <span class="hljs-keyword">switch</span> commitTypes[<span class="hljs-number">0</span>][<span class="hljs-number">1</span>] &#123;            <span class="hljs-keyword">case</span> <span class="hljs-keyword">string</span>(FEAT):            <span class="hljs-keyword">case</span> <span class="hljs-keyword">string</span>(FIX):            <span class="hljs-keyword">case</span> <span class="hljs-keyword">string</span>(DOCS):            <span class="hljs-keyword">case</span> <span class="hljs-keyword">string</span>(STYLE):            <span class="hljs-keyword">case</span> <span class="hljs-keyword">string</span>(REFACTOR):            <span class="hljs-keyword">case</span> <span class="hljs-keyword">string</span>(TEST):            <span class="hljs-keyword">case</span> <span class="hljs-keyword">string</span>(CHORE):            <span class="hljs-keyword">case</span> <span class="hljs-keyword">string</span>(PERF):            <span class="hljs-keyword">case</span> <span class="hljs-keyword">string</span>(HOTFIX):            <span class="hljs-keyword">default</span>:                <span class="hljs-keyword">if</span> !strings.HasPrefix(tmpStr, <span class="hljs-string">"Merge branch"</span>) &#123;                    checkFailed()                &#125;            &#125;        &#125;        <span class="hljs-keyword">if</span> !strictMode &#123;            os.Exit(<span class="hljs-number">0</span>)        &#125;    &#125;&#125;<span class="hljs-function"><span class="hljs-keyword">func</span> <span class="hljs-title">getCommitMsg</span><span class="hljs-params">(odlCommitID, commitID <span class="hljs-keyword">string</span>)</span> []<span class="hljs-title">string</span></span> &#123;    getCommitMsgCmd := exec.Command(<span class="hljs-string">"git"</span>, <span class="hljs-string">"log"</span>, odlCommitID+<span class="hljs-string">".."</span>+commitID, <span class="hljs-string">"--pretty=format:%s"</span>)    getCommitMsgCmd.Stdin = os.Stdin    getCommitMsgCmd.Stderr = os.Stderr    b, err := getCommitMsgCmd.Output()    <span class="hljs-keyword">if</span> err != <span class="hljs-literal">nil</span> &#123;        fmt.Print(err)        os.Exit(<span class="hljs-number">1</span>)    &#125;    commitMsg := strings.Split(<span class="hljs-keyword">string</span>(b), <span class="hljs-string">"\n"</span>)    <span class="hljs-keyword">return</span> commitMsg&#125;<span class="hljs-function"><span class="hljs-keyword">func</span> <span class="hljs-title">checkFailed</span><span class="hljs-params">()</span></span> &#123;    fmt.Fprintln(os.Stderr, checkFailedMeassge)    os.Exit(<span class="hljs-number">1</span>)&#125;</code></pre><h2 id="三、安装-pre-receive"><a href="#三、安装-pre-receive" class="headerlink" title="三、安装 pre-receive"></a>三、安装 pre-receive</h2><p>把以上代码编译后生成的 <code>pre-receive</code> 文件复制到对应项目的钩子目录即可；<strong>要注意的是文件名必须为 <code>pre-receive</code>，同时 <code>custom_hooks</code> 目录需要自建；<code>custom_hooks</code> 目录以及 <code>pre-receive</code> 文件用户组必须为 <code>git:git</code>；在删除分支时 commit ID 为 <code>0000000000000000000000000000000000000000</code>，此时不需要检测提交信息，否则可能导致无法删除分支/tag</strong>；最后效果如下所示</p><p><img src="https://cdn.oss.link/markdown/hs9c2.png" srcset="/img/loading.gif" alt="commit msg check"></p>]]></content>
    
    
    <categories>
      
      <category>CI/CD</category>
      
    </categories>
    
    
    <tags>
      
      <tag>CI/CD</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Kubernetes 1.10.1 集群搭建</title>
    <link href="/2018/04/19/set-up-kubernetes-1.10.1-cluster-by-hyperkube/"/>
    <url>/2018/04/19/set-up-kubernetes-1.10.1-cluster-by-hyperkube/</url>
    
    <content type="html"><![CDATA[<blockquote><p>年后比较忙，所以 1.9 也没去折腾(其实就是懒)，最近刚有点时间凑巧 1.10 发布；所以就折腾一下 1.10，感觉搭建配置没有太大变化，折腾了 2 天基本算是搞定了，这里记录一下搭建过程；本文用到的被 block 镜像已经上传至 <a href="https://pan.baidu.com/s/14W86QQ4qi8qn8JqaDMcC3g" target="_blank" rel="noopener">百度云</a> 密码: dy5p</p></blockquote><h3 id="一、环境准备"><a href="#一、环境准备" class="headerlink" title="一、环境准备"></a>一、环境准备</h3><p>目前搭建仍然采用 5 台虚拟机测试，基本环境如下</p><table><thead><tr><th>IP</th><th>Type</th><th>Docker</th><th>OS</th></tr></thead><tbody><tr><td>192.168.1.61</td><td>master、node、etcd</td><td>18.03.0-ce</td><td>ubuntu 16.04</td></tr><tr><td>192.168.1.62</td><td>master、node、etcd</td><td>18.03.0-ce</td><td>ubuntu 16.04</td></tr><tr><td>192.168.1.63</td><td>master、node、etcd</td><td>18.03.0-ce</td><td>ubuntu 16.04</td></tr><tr><td>192.168.1.64</td><td>node</td><td>18.03.0-ce</td><td>ubuntu 16.04</td></tr><tr><td>192.168.1.65</td><td>node</td><td>18.03.0-ce</td><td>ubuntu 16.04</td></tr></tbody></table><p><strong>搭建前请看完整篇文章后再操作，一些变更说明我放到后面了；还有为了尽可能的懒，也不用什么 rpm、deb 了，直接 <code>hyperkube</code> + <code>service</code> 配置，布吉岛 <code>hyperkube</code> 的请看 <a href="https://github.com/kubernetes/kubernetes/blob/master/cluster/images/hyperkube/README.md" target="_blank" rel="noopener">GitHub</a>；本篇文章基于一些小脚本搭建(懒)，所以不会写太详细的步骤，具体请参考 <a href="https://github.com/mritd/ktool" target="_blank" rel="noopener">仓库脚本</a>，如果想看更详细的每一步的作用可以参考以前的 1.7、1.8 的搭建文档</strong></p><h3 id="二、搭建-Etcd-集群"><a href="#二、搭建-Etcd-集群" class="headerlink" title="二、搭建 Etcd 集群"></a>二、搭建 Etcd 集群</h3><h4 id="2-1、安装-cfssl"><a href="#2-1、安装-cfssl" class="headerlink" title="2.1、安装 cfssl"></a>2.1、安装 cfssl</h4><p>说实话这个章节我不想写，但是考虑可能有人真的需要，所以还是写了一下；<strong>这个安装脚本使用的是我私人的 cdn，文件可能随时删除，想使用最新版本请自行从 <a href="https://github.com/cloudflare/cfssl" target="_blank" rel="noopener">Github</a> clone 并编译</strong></p><pre><code class="hljs sh">wget https://mritdftp.b0.upaiyun.com/cfssl/cfssl.tar.gztar -zxvf cfssl.tar.gzmv cfssl cfssljson /usr/<span class="hljs-built_in">local</span>/binchmod +x /usr/<span class="hljs-built_in">local</span>/bin/cfssl /usr/<span class="hljs-built_in">local</span>/bin/cfssljsonrm -f cfssl.tar.gz</code></pre><h4 id="2-2、生成-Etcd-证书"><a href="#2-2、生成-Etcd-证书" class="headerlink" title="2.2、生成 Etcd 证书"></a>2.2、生成 Etcd 证书</h4><h5 id="etcd-csr-json"><a href="#etcd-csr-json" class="headerlink" title="etcd-csr.json"></a>etcd-csr.json</h5><pre><code class="hljs json">&#123;  <span class="hljs-attr">"key"</span>: &#123;    <span class="hljs-attr">"algo"</span>: <span class="hljs-string">"rsa"</span>,    <span class="hljs-attr">"size"</span>: <span class="hljs-number">2048</span>  &#125;,  <span class="hljs-attr">"names"</span>: [    &#123;      <span class="hljs-attr">"O"</span>: <span class="hljs-string">"etcd"</span>,      <span class="hljs-attr">"OU"</span>: <span class="hljs-string">"etcd Security"</span>,      <span class="hljs-attr">"L"</span>: <span class="hljs-string">"Beijing"</span>,      <span class="hljs-attr">"ST"</span>: <span class="hljs-string">"Beijing"</span>,      <span class="hljs-attr">"C"</span>: <span class="hljs-string">"CN"</span>    &#125;  ],  <span class="hljs-attr">"CN"</span>: <span class="hljs-string">"etcd"</span>,  <span class="hljs-attr">"hosts"</span>: [    <span class="hljs-string">"127.0.0.1"</span>,    <span class="hljs-string">"localhost"</span>,    <span class="hljs-string">"192.168.1.61"</span>,    <span class="hljs-string">"192.168.1.62"</span>,    <span class="hljs-string">"192.168.1.63"</span>  ]&#125;</code></pre><h5 id="etcd-gencert-json"><a href="#etcd-gencert-json" class="headerlink" title="etcd-gencert.json"></a>etcd-gencert.json</h5><pre><code class="hljs json">&#123;  <span class="hljs-attr">"signing"</span>: &#123;    <span class="hljs-attr">"default"</span>: &#123;        <span class="hljs-attr">"usages"</span>: [          <span class="hljs-string">"signing"</span>,          <span class="hljs-string">"key encipherment"</span>,          <span class="hljs-string">"server auth"</span>,          <span class="hljs-string">"client auth"</span>        ],        <span class="hljs-attr">"expiry"</span>: <span class="hljs-string">"87600h"</span>    &#125;  &#125;&#125;</code></pre><h5 id="etcd-root-ca-csr-json"><a href="#etcd-root-ca-csr-json" class="headerlink" title="etcd-root-ca-csr.json"></a>etcd-root-ca-csr.json</h5><pre><code class="hljs json">&#123;  <span class="hljs-attr">"key"</span>: &#123;    <span class="hljs-attr">"algo"</span>: <span class="hljs-string">"rsa"</span>,    <span class="hljs-attr">"size"</span>: <span class="hljs-number">4096</span>  &#125;,  <span class="hljs-attr">"names"</span>: [    &#123;      <span class="hljs-attr">"O"</span>: <span class="hljs-string">"etcd"</span>,      <span class="hljs-attr">"OU"</span>: <span class="hljs-string">"etcd Security"</span>,      <span class="hljs-attr">"L"</span>: <span class="hljs-string">"Beijing"</span>,      <span class="hljs-attr">"ST"</span>: <span class="hljs-string">"Beijing"</span>,      <span class="hljs-attr">"C"</span>: <span class="hljs-string">"CN"</span>    &#125;  ],  <span class="hljs-attr">"CN"</span>: <span class="hljs-string">"etcd-root-ca"</span>&#125;</code></pre><h5 id="生成证书"><a href="#生成证书" class="headerlink" title="生成证书"></a>生成证书</h5><pre><code class="hljs sh">cfssl gencert --initca=<span class="hljs-literal">true</span> etcd-root-ca-csr.json | cfssljson --bare etcd-root-cacfssl gencert --ca etcd-root-ca.pem --ca-key etcd-root-ca-key.pem --config etcd-gencert.json etcd-csr.json | cfssljson --bare etcd</code></pre><p>生成后如下</p><p><img src="https://cdn.oss.link/markdown/81203.png" srcset="/img/loading.gif" alt="gen etcd certs"></p><h4 id="2-3、安装-Etcd"><a href="#2-3、安装-Etcd" class="headerlink" title="2.3、安装 Etcd"></a>2.3、安装 Etcd</h4><p>Etcd 这里采用最新的 3.2.18 版本，安装方式直接复制二进制文件、systemd service 配置即可，不过需要注意相关用户权限问题，以下脚本配置等参考了 etcd rpm 安装包</p><h5 id="etcd-service"><a href="#etcd-service" class="headerlink" title="etcd.service"></a>etcd.service</h5><pre><code class="hljs sh">[Unit]Description=Etcd ServerAfter=network.targetAfter=network-online.targetWants=network-online.target[Service]Type=notifyWorkingDirectory=/var/lib/etcd/EnvironmentFile=-/etc/etcd/etcd.confUser=etcd<span class="hljs-comment"># set GOMAXPROCS to number of processors</span>ExecStart=/bin/bash -c <span class="hljs-string">"GOMAXPROCS=<span class="hljs-variable">$(nproc)</span> /usr/local/bin/etcd --name=\"<span class="hljs-variable">$&#123;ETCD_NAME&#125;</span>\" --data-dir=\"<span class="hljs-variable">$&#123;ETCD_DATA_DIR&#125;</span>\" --listen-client-urls=\"<span class="hljs-variable">$&#123;ETCD_LISTEN_CLIENT_URLS&#125;</span>\""</span>Restart=on-failureLimitNOFILE=65536[Install]WantedBy=multi-user.target</code></pre><h5 id="etcd-conf"><a href="#etcd-conf" class="headerlink" title="etcd.conf"></a>etcd.conf</h5><pre><code class="hljs sh"><span class="hljs-comment"># [member]</span>ETCD_NAME=etcd1ETCD_DATA_DIR=<span class="hljs-string">"/var/lib/etcd/etcd1.etcd"</span>ETCD_WAL_DIR=<span class="hljs-string">"/var/lib/etcd/wal"</span>ETCD_SNAPSHOT_COUNT=<span class="hljs-string">"100"</span>ETCD_HEARTBEAT_INTERVAL=<span class="hljs-string">"100"</span>ETCD_ELECTION_TIMEOUT=<span class="hljs-string">"1000"</span>ETCD_LISTEN_PEER_URLS=<span class="hljs-string">"https://192.168.1.61:2380"</span>ETCD_LISTEN_CLIENT_URLS=<span class="hljs-string">"https://192.168.1.61:2379,http://127.0.0.1:2379"</span>ETCD_MAX_SNAPSHOTS=<span class="hljs-string">"5"</span>ETCD_MAX_WALS=<span class="hljs-string">"5"</span><span class="hljs-comment">#ETCD_CORS=""</span><span class="hljs-comment"># [cluster]</span>ETCD_INITIAL_ADVERTISE_PEER_URLS=<span class="hljs-string">"https://192.168.1.61:2380"</span><span class="hljs-comment"># if you use different ETCD_NAME (e.g. test), set ETCD_INITIAL_CLUSTER value for this name, i.e. "test=http://..."</span>ETCD_INITIAL_CLUSTER=<span class="hljs-string">"etcd1=https://192.168.1.61:2380,etcd2=https://192.168.1.62:2380,etcd3=https://192.168.1.63:2380"</span>ETCD_INITIAL_CLUSTER_STATE=<span class="hljs-string">"new"</span>ETCD_INITIAL_CLUSTER_TOKEN=<span class="hljs-string">"etcd-cluster"</span>ETCD_ADVERTISE_CLIENT_URLS=<span class="hljs-string">"https://192.168.1.61:2379"</span><span class="hljs-comment">#ETCD_DISCOVERY=""</span><span class="hljs-comment">#ETCD_DISCOVERY_SRV=""</span><span class="hljs-comment">#ETCD_DISCOVERY_FALLBACK="proxy"</span><span class="hljs-comment">#ETCD_DISCOVERY_PROXY=""</span><span class="hljs-comment">#ETCD_STRICT_RECONFIG_CHECK="false"</span><span class="hljs-comment">#ETCD_AUTO_COMPACTION_RETENTION="0"</span><span class="hljs-comment"># [proxy]</span><span class="hljs-comment">#ETCD_PROXY="off"</span><span class="hljs-comment">#ETCD_PROXY_FAILURE_WAIT="5000"</span><span class="hljs-comment">#ETCD_PROXY_REFRESH_INTERVAL="30000"</span><span class="hljs-comment">#ETCD_PROXY_DIAL_TIMEOUT="1000"</span><span class="hljs-comment">#ETCD_PROXY_WRITE_TIMEOUT="5000"</span><span class="hljs-comment">#ETCD_PROXY_READ_TIMEOUT="0"</span><span class="hljs-comment"># [security]</span>ETCD_CERT_FILE=<span class="hljs-string">"/etc/etcd/ssl/etcd.pem"</span>ETCD_KEY_FILE=<span class="hljs-string">"/etc/etcd/ssl/etcd-key.pem"</span>ETCD_CLIENT_CERT_AUTH=<span class="hljs-string">"true"</span>ETCD_TRUSTED_CA_FILE=<span class="hljs-string">"/etc/etcd/ssl/etcd-root-ca.pem"</span>ETCD_AUTO_TLS=<span class="hljs-string">"true"</span>ETCD_PEER_CERT_FILE=<span class="hljs-string">"/etc/etcd/ssl/etcd.pem"</span>ETCD_PEER_KEY_FILE=<span class="hljs-string">"/etc/etcd/ssl/etcd-key.pem"</span>ETCD_PEER_CLIENT_CERT_AUTH=<span class="hljs-string">"true"</span>ETCD_PEER_TRUSTED_CA_FILE=<span class="hljs-string">"/etc/etcd/ssl/etcd-root-ca.pem"</span>ETCD_PEER_AUTO_TLS=<span class="hljs-string">"true"</span><span class="hljs-comment"># [logging]</span><span class="hljs-comment">#ETCD_DEBUG="false"</span><span class="hljs-comment"># examples for -log-package-levels etcdserver=WARNING,security=DEBUG</span><span class="hljs-comment">#ETCD_LOG_PACKAGE_LEVELS=""</span></code></pre><h5 id="install-sh"><a href="#install-sh" class="headerlink" title="install.sh"></a>install.sh</h5><pre><code class="hljs sh"><span class="hljs-meta">#!/bin/bash</span><span class="hljs-built_in">set</span> -eETCD_VERSION=<span class="hljs-string">"3.2.18"</span><span class="hljs-keyword">function</span> <span class="hljs-function"><span class="hljs-title">download</span></span>()&#123;    <span class="hljs-keyword">if</span> [ ! -f <span class="hljs-string">"etcd-v<span class="hljs-variable">$&#123;ETCD_VERSION&#125;</span>-linux-amd64.tar.gz"</span> ]; <span class="hljs-keyword">then</span>        wget https://github.com/coreos/etcd/releases/download/v<span class="hljs-variable">$&#123;ETCD_VERSION&#125;</span>/etcd-v<span class="hljs-variable">$&#123;ETCD_VERSION&#125;</span>-linux-amd64.tar.gz        tar -zxvf etcd-v<span class="hljs-variable">$&#123;ETCD_VERSION&#125;</span>-linux-amd64.tar.gz    <span class="hljs-keyword">fi</span>&#125;<span class="hljs-keyword">function</span> <span class="hljs-function"><span class="hljs-title">preinstall</span></span>()&#123;    getent group etcd &gt;/dev/null || groupadd -r etcd    getent passwd etcd &gt;/dev/null || useradd -r -g etcd -d /var/lib/etcd -s /sbin/nologin -c <span class="hljs-string">"etcd user"</span> etcd&#125;<span class="hljs-keyword">function</span> <span class="hljs-function"><span class="hljs-title">install</span></span>()&#123;    <span class="hljs-built_in">echo</span> -e <span class="hljs-string">"\033[32mINFO: Copy etcd...\033[0m"</span>    tar -zxvf etcd-v<span class="hljs-variable">$&#123;ETCD_VERSION&#125;</span>-linux-amd64.tar.gz    cp etcd-v<span class="hljs-variable">$&#123;ETCD_VERSION&#125;</span>-linux-amd64/etcd* /usr/<span class="hljs-built_in">local</span>/bin    rm -rf etcd-v<span class="hljs-variable">$&#123;ETCD_VERSION&#125;</span>-linux-amd64    <span class="hljs-built_in">echo</span> -e <span class="hljs-string">"\033[32mINFO: Copy etcd config...\033[0m"</span>    cp -r conf /etc/etcd    chown -R etcd:etcd /etc/etcd    chmod -R 755 /etc/etcd/ssl    <span class="hljs-built_in">echo</span> -e <span class="hljs-string">"\033[32mINFO: Copy etcd systemd config...\033[0m"</span>    cp systemd/*.service /lib/systemd/system    systemctl daemon-reload&#125;<span class="hljs-keyword">function</span> <span class="hljs-function"><span class="hljs-title">postinstall</span></span>()&#123;    <span class="hljs-keyword">if</span> [ ! -d <span class="hljs-string">"/var/lib/etcd"</span> ]; <span class="hljs-keyword">then</span>        mkdir /var/lib/etcd        chown -R etcd:etcd /var/lib/etcd    <span class="hljs-keyword">fi</span>&#125;downloadpreinstallinstallpostinstall</code></pre><p><strong>脚本解释如下:</strong></p><ul><li>download: 从 Github 下载二进制文件并解压</li><li>preinstall: 为 Etcd 安装做准备，创建 etcd 用户，并指定家目录登录 shell 等</li><li>install: 将 etcd 二进制文件复制到安装目录(<code>/usr/local/bin</code>)，复制 conf 目录到 <code>/etc/etcd</code></li><li>postinstall: 安装后收尾工作，比如检测 <code>/var/lib/etcd</code> 是否存在，纠正权限等</li></ul><p>整体目录结构如下</p><pre><code class="hljs sh">etcd├── conf│   ├── etcd.conf│   └── ssl│       ├── etcd.csr│       ├── etcd-csr.json│       ├── etcd-gencert.json│       ├── etcd-key.pem│       ├── etcd.pem│       ├── etcd-root-ca.csr│       ├── etcd-root-ca-csr.json│       ├── etcd-root-ca-key.pem│       └── etcd-root-ca.pem├── etcd.service└── install.sh</code></pre><p><strong>请自行创建 conf 目录等，并放置好相关文件，保存上面脚本为 <code>install.sh</code>，直接执行即可；在每台机器上更改好对应的配置，如 etcd 名称等，etcd 估计都是轻车熟路了，这里不做过多阐述；安装后启动即可</strong></p><pre><code class="hljs sh">systemctl start etcdsystemctl <span class="hljs-built_in">enable</span> etcd</code></pre><p><strong>注意: 集群 etcd 要 3 个一起启动，集群模式下单个启动会卡半天最后失败，不要傻等；启动成功后测试如下</strong></p><pre><code class="hljs sh"><span class="hljs-built_in">export</span> ETCDCTL_API=3etcdctl --cacert=/etc/etcd/ssl/etcd-root-ca.pem --cert=/etc/etcd/ssl/etcd.pem --key=/etc/etcd/ssl/etcd-key.pem --endpoints=https://192.168.1.61:2379,https://192.168.1.62:2379,https://192.168.1.63:2379 endpoint health</code></pre><p><img src="https://cdn.oss.link/markdown/ji94m.png" srcset="/img/loading.gif" alt="check etcd"></p><h3 id="三、安装-Kubernets-集群组件"><a href="#三、安装-Kubernets-集群组件" class="headerlink" title="三、安装 Kubernets 集群组件"></a>三、安装 Kubernets 集群组件</h3><blockquote><p><strong>注意：与以前文档不同的是，这次不依赖 rpm 等特定安装包，而是基于 hyperkube 二进制手动安装，每个节点都会同时安装 Master 与 Node 配置文件，具体作为 Master 还是 Node 取决于服务开启情况</strong></p></blockquote><h4 id="3-1、生成-Kubernetes-证书"><a href="#3-1、生成-Kubernetes-证书" class="headerlink" title="3.1、生成 Kubernetes 证书"></a>3.1、生成 Kubernetes 证书</h4><p>由于 kubelet 和 kube-proxy 用到的 kubeconfig 配置文件需要借助 kubectl 来生成，所以需要先安装一下 kubectl</p><pre><code class="hljs sh">wget https://storage.googleapis.com/kubernetes-release/release/v1.10.1/bin/linux/amd64/hyperkube -O hyperkube_1.10.1chmod +x hyperkube_1.10.1cp hyperkube_1.10.1 /usr/<span class="hljs-built_in">local</span>/bin/hyperkubeln -s /usr/<span class="hljs-built_in">local</span>/bin/hyperkube /usr/<span class="hljs-built_in">local</span>/bin/kubectl</code></pre><h5 id="admin-csr-json"><a href="#admin-csr-json" class="headerlink" title="admin-csr.json"></a>admin-csr.json</h5><pre><code class="hljs json">&#123;  <span class="hljs-attr">"CN"</span>: <span class="hljs-string">"admin"</span>,  <span class="hljs-attr">"hosts"</span>: [],  <span class="hljs-attr">"key"</span>: &#123;    <span class="hljs-attr">"algo"</span>: <span class="hljs-string">"rsa"</span>,    <span class="hljs-attr">"size"</span>: <span class="hljs-number">2048</span>  &#125;,  <span class="hljs-attr">"names"</span>: [    &#123;      <span class="hljs-attr">"C"</span>: <span class="hljs-string">"CN"</span>,      <span class="hljs-attr">"ST"</span>: <span class="hljs-string">"BeiJing"</span>,      <span class="hljs-attr">"L"</span>: <span class="hljs-string">"BeiJing"</span>,      <span class="hljs-attr">"O"</span>: <span class="hljs-string">"system:masters"</span>,      <span class="hljs-attr">"OU"</span>: <span class="hljs-string">"System"</span>    &#125;  ]&#125;</code></pre><h5 id="k8s-gencert-json"><a href="#k8s-gencert-json" class="headerlink" title="k8s-gencert.json"></a>k8s-gencert.json</h5><pre><code class="hljs json">&#123;  <span class="hljs-attr">"signing"</span>: &#123;    <span class="hljs-attr">"default"</span>: &#123;      <span class="hljs-attr">"expiry"</span>: <span class="hljs-string">"87600h"</span>    &#125;,    <span class="hljs-attr">"profiles"</span>: &#123;      <span class="hljs-attr">"kubernetes"</span>: &#123;        <span class="hljs-attr">"usages"</span>: [            <span class="hljs-string">"signing"</span>,            <span class="hljs-string">"key encipherment"</span>,            <span class="hljs-string">"server auth"</span>,            <span class="hljs-string">"client auth"</span>        ],        <span class="hljs-attr">"expiry"</span>: <span class="hljs-string">"87600h"</span>      &#125;    &#125;  &#125;&#125;</code></pre><h5 id="k8s-root-ca-csr-json"><a href="#k8s-root-ca-csr-json" class="headerlink" title="k8s-root-ca-csr.json"></a>k8s-root-ca-csr.json</h5><pre><code class="hljs json">&#123;  <span class="hljs-attr">"CN"</span>: <span class="hljs-string">"kubernetes"</span>,  <span class="hljs-attr">"key"</span>: &#123;    <span class="hljs-attr">"algo"</span>: <span class="hljs-string">"rsa"</span>,    <span class="hljs-attr">"size"</span>: <span class="hljs-number">4096</span>  &#125;,  <span class="hljs-attr">"names"</span>: [    &#123;      <span class="hljs-attr">"C"</span>: <span class="hljs-string">"CN"</span>,      <span class="hljs-attr">"ST"</span>: <span class="hljs-string">"BeiJing"</span>,      <span class="hljs-attr">"L"</span>: <span class="hljs-string">"BeiJing"</span>,      <span class="hljs-attr">"O"</span>: <span class="hljs-string">"k8s"</span>,      <span class="hljs-attr">"OU"</span>: <span class="hljs-string">"System"</span>    &#125;  ]&#125;</code></pre><h5 id="kube-apiserver-csr-json"><a href="#kube-apiserver-csr-json" class="headerlink" title="kube-apiserver-csr.json"></a>kube-apiserver-csr.json</h5><p><strong>注意: 在以前的文档中这个配置叫 <code>kubernetes-csr.json</code>，为了明确划分职责，这个证书目前被重命名以表示其专属于 <code>apiserver</code> 使用；加了一个 <code>*.kubernetes.master</code> 域名以便内部私有 DNS 解析使用(可删除)；至于很多人问过 <code>kubernetes</code> 这几个能不能删掉，答案是不可以的；因为当集群创建好后，default namespace 下会创建一个叫 <code>kubenretes</code> 的 svc，有一些组件会直接连接这个 svc 来跟 api 通讯的，证书如果不包含可能会出现无法连接的情况；其他几个 <code>kubernetes</code> 开头的域名作用相同</strong></p><pre><code class="hljs json">&#123;    <span class="hljs-attr">"CN"</span>: <span class="hljs-string">"kubernetes"</span>,    <span class="hljs-attr">"hosts"</span>: [        <span class="hljs-string">"127.0.0.1"</span>,        <span class="hljs-string">"10.254.0.1"</span>,        <span class="hljs-string">"192.168.1.61"</span>,        <span class="hljs-string">"192.168.1.62"</span>,        <span class="hljs-string">"192.168.1.63"</span>,        <span class="hljs-string">"192.168.1.64"</span>,        <span class="hljs-string">"192.168.1.65"</span>,        <span class="hljs-string">"*.kubernetes.master"</span>,        <span class="hljs-string">"localhost"</span>,        <span class="hljs-string">"kubernetes"</span>,        <span class="hljs-string">"kubernetes.default"</span>,        <span class="hljs-string">"kubernetes.default.svc"</span>,        <span class="hljs-string">"kubernetes.default.svc.cluster"</span>,        <span class="hljs-string">"kubernetes.default.svc.cluster.local"</span>    ],    <span class="hljs-attr">"key"</span>: &#123;        <span class="hljs-attr">"algo"</span>: <span class="hljs-string">"rsa"</span>,        <span class="hljs-attr">"size"</span>: <span class="hljs-number">2048</span>    &#125;,    <span class="hljs-attr">"names"</span>: [        &#123;            <span class="hljs-attr">"C"</span>: <span class="hljs-string">"CN"</span>,            <span class="hljs-attr">"ST"</span>: <span class="hljs-string">"BeiJing"</span>,            <span class="hljs-attr">"L"</span>: <span class="hljs-string">"BeiJing"</span>,            <span class="hljs-attr">"O"</span>: <span class="hljs-string">"k8s"</span>,            <span class="hljs-attr">"OU"</span>: <span class="hljs-string">"System"</span>        &#125;    ]&#125;</code></pre><h5 id="kube-proxy-csr-json"><a href="#kube-proxy-csr-json" class="headerlink" title="kube-proxy-csr.json"></a>kube-proxy-csr.json</h5><pre><code class="hljs json">&#123;  <span class="hljs-attr">"CN"</span>: <span class="hljs-string">"system:kube-proxy"</span>,  <span class="hljs-attr">"hosts"</span>: [],  <span class="hljs-attr">"key"</span>: &#123;    <span class="hljs-attr">"algo"</span>: <span class="hljs-string">"rsa"</span>,    <span class="hljs-attr">"size"</span>: <span class="hljs-number">2048</span>  &#125;,  <span class="hljs-attr">"names"</span>: [    &#123;      <span class="hljs-attr">"C"</span>: <span class="hljs-string">"CN"</span>,      <span class="hljs-attr">"ST"</span>: <span class="hljs-string">"BeiJing"</span>,      <span class="hljs-attr">"L"</span>: <span class="hljs-string">"BeiJing"</span>,      <span class="hljs-attr">"O"</span>: <span class="hljs-string">"k8s"</span>,      <span class="hljs-attr">"OU"</span>: <span class="hljs-string">"System"</span>    &#125;  ]&#125;</code></pre><h5 id="生成证书及配置"><a href="#生成证书及配置" class="headerlink" title="生成证书及配置"></a>生成证书及配置</h5><pre><code class="hljs sh"><span class="hljs-comment"># 生成 CA</span>cfssl gencert --initca=<span class="hljs-literal">true</span> k8s-root-ca-csr.json | cfssljson --bare k8s-root-ca<span class="hljs-comment"># 依次生成其他组件证书</span><span class="hljs-keyword">for</span> targetName <span class="hljs-keyword">in</span> kube-apiserver admin kube-proxy; <span class="hljs-keyword">do</span>    cfssl gencert --ca k8s-root-ca.pem --ca-key k8s-root-ca-key.pem --config k8s-gencert.json --profile kubernetes <span class="hljs-variable">$targetName</span>-csr.json | cfssljson --bare <span class="hljs-variable">$targetName</span><span class="hljs-keyword">done</span><span class="hljs-comment"># 地址默认为 127.0.0.1:6443</span><span class="hljs-comment"># 如果在 master 上启用 kubelet 请在生成后的 kubeconfig 中</span><span class="hljs-comment"># 修改该地址为 当前MASTER_IP:6443</span>KUBE_APISERVER=<span class="hljs-string">"https://127.0.0.1:6443"</span>BOOTSTRAP_TOKEN=$(head -c 16 /dev/urandom | od -An -t x | tr -d <span class="hljs-string">' '</span>)<span class="hljs-built_in">echo</span> <span class="hljs-string">"Tokne: <span class="hljs-variable">$&#123;BOOTSTRAP_TOKEN&#125;</span>"</span><span class="hljs-comment"># 不要质疑 system:bootstrappers 用户组是否写错了，有疑问请参考官方文档</span><span class="hljs-comment"># https://kubernetes.io/docs/admin/kubelet-tls-bootstrapping/</span>cat &gt; token.csv &lt;&lt;EOF<span class="hljs-variable">$&#123;BOOTSTRAP_TOKEN&#125;</span>,kubelet-bootstrap,10001,<span class="hljs-string">"system:bootstrappers"</span>EOF<span class="hljs-built_in">echo</span> <span class="hljs-string">"Create kubelet bootstrapping kubeconfig..."</span><span class="hljs-comment"># 设置集群参数</span>kubectl config <span class="hljs-built_in">set</span>-cluster kubernetes \  --certificate-authority=k8s-root-ca.pem \  --embed-certs=<span class="hljs-literal">true</span> \  --server=<span class="hljs-variable">$&#123;KUBE_APISERVER&#125;</span> \  --kubeconfig=bootstrap.kubeconfig<span class="hljs-comment"># 设置客户端认证参数</span>kubectl config <span class="hljs-built_in">set</span>-credentials kubelet-bootstrap \  --token=<span class="hljs-variable">$&#123;BOOTSTRAP_TOKEN&#125;</span> \  --kubeconfig=bootstrap.kubeconfig<span class="hljs-comment"># 设置上下文参数</span>kubectl config <span class="hljs-built_in">set</span>-context default \  --cluster=kubernetes \  --user=kubelet-bootstrap \  --kubeconfig=bootstrap.kubeconfig<span class="hljs-comment"># 设置默认上下文</span>kubectl config use-context default --kubeconfig=bootstrap.kubeconfig<span class="hljs-built_in">echo</span> <span class="hljs-string">"Create kube-proxy kubeconfig..."</span><span class="hljs-comment"># 设置集群参数</span>kubectl config <span class="hljs-built_in">set</span>-cluster kubernetes \  --certificate-authority=k8s-root-ca.pem \  --embed-certs=<span class="hljs-literal">true</span> \  --server=<span class="hljs-variable">$&#123;KUBE_APISERVER&#125;</span> \  --kubeconfig=kube-proxy.kubeconfig<span class="hljs-comment"># 设置客户端认证参数</span>kubectl config <span class="hljs-built_in">set</span>-credentials kube-proxy \  --client-certificate=kube-proxy.pem \  --client-key=kube-proxy-key.pem \  --embed-certs=<span class="hljs-literal">true</span> \  --kubeconfig=kube-proxy.kubeconfig<span class="hljs-comment"># 设置上下文参数</span>kubectl config <span class="hljs-built_in">set</span>-context default \  --cluster=kubernetes \  --user=kube-proxy \  --kubeconfig=kube-proxy.kubeconfig<span class="hljs-comment"># 设置默认上下文</span>kubectl config use-context default --kubeconfig=kube-proxy.kubeconfig<span class="hljs-comment"># 创建高级审计配置</span>cat &gt;&gt; audit-policy.yaml &lt;&lt;EOF<span class="hljs-comment"># Log all requests at the Metadata level.</span>apiVersion: audit.k8s.io/v1beta1kind: Policyrules:- level: MetadataEOF</code></pre><p>生成后文件如下</p><p><img src="https://cdn.oss.link/markdown/xk8uj.png" srcset="/img/loading.gif" alt="k8s certs"></p><h4 id="3-2、准备-systemd-配置"><a href="#3-2、准备-systemd-配置" class="headerlink" title="3.2、准备 systemd 配置"></a>3.2、准备 systemd 配置</h4><p>所有组件的 <code>systemd</code> 配置如下</p><h5 id="kube-apiserver-service"><a href="#kube-apiserver-service" class="headerlink" title="kube-apiserver.service"></a>kube-apiserver.service</h5><pre><code class="hljs sh">[Unit]Description=Kubernetes API ServerDocumentation=https://github.com/GoogleCloudPlatform/kubernetesAfter=network.targetAfter=etcd.service[Service]EnvironmentFile=-/etc/kubernetes/configEnvironmentFile=-/etc/kubernetes/apiserverUser=kubeExecStart=/usr/<span class="hljs-built_in">local</span>/bin/hyperkube apiserver \            <span class="hljs-variable">$KUBE_LOGTOSTDERR</span> \            <span class="hljs-variable">$KUBE_LOG_LEVEL</span> \            <span class="hljs-variable">$KUBE_ETCD_SERVERS</span> \            <span class="hljs-variable">$KUBE_API_ADDRESS</span> \            <span class="hljs-variable">$KUBE_API_PORT</span> \            <span class="hljs-variable">$KUBELET_PORT</span> \            <span class="hljs-variable">$KUBE_ALLOW_PRIV</span> \            <span class="hljs-variable">$KUBE_SERVICE_ADDRESSES</span> \            <span class="hljs-variable">$KUBE_ADMISSION_CONTROL</span> \            <span class="hljs-variable">$KUBE_API_ARGS</span>Restart=on-failureType=notifyLimitNOFILE=65536[Install]WantedBy=multi-user.target</code></pre><h5 id="kube-controller-manager-service"><a href="#kube-controller-manager-service" class="headerlink" title="kube-controller-manager.service"></a>kube-controller-manager.service</h5><pre><code class="hljs sh">[Unit]Description=Kubernetes Controller ManagerDocumentation=https://github.com/GoogleCloudPlatform/kubernetes[Service]EnvironmentFile=-/etc/kubernetes/configEnvironmentFile=-/etc/kubernetes/controller-managerUser=kubeExecStart=/usr/<span class="hljs-built_in">local</span>/bin/hyperkube controller-manager \            <span class="hljs-variable">$KUBE_LOGTOSTDERR</span> \            <span class="hljs-variable">$KUBE_LOG_LEVEL</span> \            <span class="hljs-variable">$KUBE_MASTER</span> \            <span class="hljs-variable">$KUBE_CONTROLLER_MANAGER_ARGS</span>Restart=on-failureLimitNOFILE=65536[Install]WantedBy=multi-user.target</code></pre><h5 id="kubelet-service"><a href="#kubelet-service" class="headerlink" title="kubelet.service"></a>kubelet.service</h5><pre><code class="hljs sh">[Unit]Description=Kubernetes Kubelet ServerDocumentation=https://github.com/GoogleCloudPlatform/kubernetesAfter=docker.serviceRequires=docker.service[Service]WorkingDirectory=/var/lib/kubeletEnvironmentFile=-/etc/kubernetes/configEnvironmentFile=-/etc/kubernetes/kubeletExecStart=/usr/<span class="hljs-built_in">local</span>/bin/hyperkube kubelet \            <span class="hljs-variable">$KUBE_LOGTOSTDERR</span> \            <span class="hljs-variable">$KUBE_LOG_LEVEL</span> \            <span class="hljs-variable">$KUBELET_API_SERVER</span> \            <span class="hljs-variable">$KUBELET_ADDRESS</span> \            <span class="hljs-variable">$KUBELET_PORT</span> \            <span class="hljs-variable">$KUBELET_HOSTNAME</span> \            <span class="hljs-variable">$KUBE_ALLOW_PRIV</span> \            <span class="hljs-variable">$KUBELET_ARGS</span>Restart=on-failureKillMode=process[Install]WantedBy=multi-user.target</code></pre><h5 id="kube-proxy-service"><a href="#kube-proxy-service" class="headerlink" title="kube-proxy.service"></a>kube-proxy.service</h5><pre><code class="hljs sh">[Unit]Description=Kubernetes Kube-Proxy ServerDocumentation=https://github.com/GoogleCloudPlatform/kubernetesAfter=network.target[Service]EnvironmentFile=-/etc/kubernetes/configEnvironmentFile=-/etc/kubernetes/proxyExecStart=/usr/<span class="hljs-built_in">local</span>/bin/hyperkube proxy \            <span class="hljs-variable">$KUBE_LOGTOSTDERR</span> \            <span class="hljs-variable">$KUBE_LOG_LEVEL</span> \            <span class="hljs-variable">$KUBE_MASTER</span> \            <span class="hljs-variable">$KUBE_PROXY_ARGS</span>Restart=on-failureLimitNOFILE=65536[Install]WantedBy=multi-user.target</code></pre><h5 id="kube-scheduler-service"><a href="#kube-scheduler-service" class="headerlink" title="kube-scheduler.service"></a>kube-scheduler.service</h5><pre><code class="hljs sh">[Unit]Description=Kubernetes Scheduler PluginDocumentation=https://github.com/GoogleCloudPlatform/kubernetes[Service]EnvironmentFile=-/etc/kubernetes/configEnvironmentFile=-/etc/kubernetes/schedulerUser=kubeExecStart=/usr/<span class="hljs-built_in">local</span>/bin/hyperkube scheduler \            <span class="hljs-variable">$KUBE_LOGTOSTDERR</span> \            <span class="hljs-variable">$KUBE_LOG_LEVEL</span> \            <span class="hljs-variable">$KUBE_MASTER</span> \            <span class="hljs-variable">$KUBE_SCHEDULER_ARGS</span>Restart=on-failureLimitNOFILE=65536[Install]WantedBy=multi-user.target</code></pre><h4 id="3-3、Master-节点配置"><a href="#3-3、Master-节点配置" class="headerlink" title="3.3、Master 节点配置"></a>3.3、Master 节点配置</h4><p>Master 节点主要会运行 3 各组件: <code>kube-apiserver</code>、<code>kube-controller-manager</code>、<code>kube-scheduler</code>，其中用到的配置文件如下</p><h5 id="config"><a href="#config" class="headerlink" title="config"></a>config</h5><p><strong>config 是一个通用配置文件，值得注意的是由于安装时对于 Node、Master 节点都会包含该文件，在 Node 节点上请注释掉 <code>KUBE_MASTER</code> 变量，因为 Node 节点需要做 HA，要连接本地的 6443 加密端口；而这个变量将会覆盖 <code>kubeconfig</code> 中指定的 <code>127.0.0.1:6443</code> 地址</strong></p><pre><code class="hljs sh"><span class="hljs-comment">###</span><span class="hljs-comment"># kubernetes system config</span><span class="hljs-comment">#</span><span class="hljs-comment"># The following values are used to configure various aspects of all</span><span class="hljs-comment"># kubernetes services, including</span><span class="hljs-comment">#</span><span class="hljs-comment">#   kube-apiserver.service</span><span class="hljs-comment">#   kube-controller-manager.service</span><span class="hljs-comment">#   kube-scheduler.service</span><span class="hljs-comment">#   kubelet.service</span><span class="hljs-comment">#   kube-proxy.service</span><span class="hljs-comment"># logging to stderr means we get it in the systemd journal</span>KUBE_LOGTOSTDERR=<span class="hljs-string">"--logtostderr=true"</span><span class="hljs-comment"># journal message level, 0 is debug</span>KUBE_LOG_LEVEL=<span class="hljs-string">"--v=2"</span><span class="hljs-comment"># Should this cluster be allowed to run privileged docker containers</span>KUBE_ALLOW_PRIV=<span class="hljs-string">"--allow-privileged=true"</span><span class="hljs-comment"># How the controller-manager, scheduler, and proxy find the apiserver</span>KUBE_MASTER=<span class="hljs-string">"--master=http://127.0.0.1:8080"</span></code></pre><h5 id="apiserver"><a href="#apiserver" class="headerlink" title="apiserver"></a>apiserver</h5><p>apiserver 配置相对于 1.8 略有变动，其中准入控制器(<code>admission control</code>)选项名称变为了 <code>--enable-admission-plugins</code>，控制器列表也有相应变化，这里采用官方推荐配置，具体请参考 <a href="https://kubernetes.io/docs/admin/admission-controllers/#is-there-a-recommended-set-of-admission-controllers-to-use" target="_blank" rel="noopener">官方文档</a></p><pre><code class="hljs sh"><span class="hljs-comment">###</span><span class="hljs-comment"># kubernetes system config</span><span class="hljs-comment">#</span><span class="hljs-comment"># The following values are used to configure the kube-apiserver</span><span class="hljs-comment">#</span><span class="hljs-comment"># The address on the local server to listen to.</span>KUBE_API_ADDRESS=<span class="hljs-string">"--advertise-address=192.168.1.61 --bind-address=192.168.1.61"</span><span class="hljs-comment"># The port on the local server to listen on.</span>KUBE_API_PORT=<span class="hljs-string">"--secure-port=6443"</span><span class="hljs-comment"># Port minions listen on</span><span class="hljs-comment"># KUBELET_PORT="--kubelet-port=10250"</span><span class="hljs-comment"># Comma separated list of nodes in the etcd cluster</span>KUBE_ETCD_SERVERS=<span class="hljs-string">"--etcd-servers=https://192.168.1.61:2379,https://192.168.1.62:2379,https://192.168.1.63:2379"</span><span class="hljs-comment"># Address range to use for services</span>KUBE_SERVICE_ADDRESSES=<span class="hljs-string">"--service-cluster-ip-range=10.254.0.0/16"</span><span class="hljs-comment"># default admission control policies</span>KUBE_ADMISSION_CONTROL=<span class="hljs-string">"--enable-admission-plugins=NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota,NodeRestriction"</span><span class="hljs-comment"># Add your own!</span>KUBE_API_ARGS=<span class="hljs-string">" --anonymous-auth=false \</span><span class="hljs-string">                --apiserver-count=3 \</span><span class="hljs-string">                --audit-log-maxage=30 \</span><span class="hljs-string">                --audit-log-maxbackup=3 \</span><span class="hljs-string">                --audit-log-maxsize=100 \</span><span class="hljs-string">                --audit-log-path=/var/log/kube-audit/audit.log \</span><span class="hljs-string">                --audit-policy-file=/etc/kubernetes/audit-policy.yaml \</span><span class="hljs-string">                --authorization-mode=Node,RBAC \</span><span class="hljs-string">                --client-ca-file=/etc/kubernetes/ssl/k8s-root-ca.pem \</span><span class="hljs-string">                --enable-bootstrap-token-auth \</span><span class="hljs-string">                --enable-garbage-collector \</span><span class="hljs-string">                --enable-logs-handler \</span><span class="hljs-string">                --enable-swagger-ui \</span><span class="hljs-string">                --etcd-cafile=/etc/etcd/ssl/etcd-root-ca.pem \</span><span class="hljs-string">                --etcd-certfile=/etc/etcd/ssl/etcd.pem \</span><span class="hljs-string">                --etcd-keyfile=/etc/etcd/ssl/etcd-key.pem \</span><span class="hljs-string">                --etcd-compaction-interval=5m0s \</span><span class="hljs-string">                --etcd-count-metric-poll-period=1m0s \</span><span class="hljs-string">                --event-ttl=48h0m0s \</span><span class="hljs-string">                --kubelet-https=true \</span><span class="hljs-string">                --kubelet-timeout=3s \</span><span class="hljs-string">                --log-flush-frequency=5s \</span><span class="hljs-string">                --token-auth-file=/etc/kubernetes/token.csv \</span><span class="hljs-string">                --tls-cert-file=/etc/kubernetes/ssl/kube-apiserver.pem \</span><span class="hljs-string">                --tls-private-key-file=/etc/kubernetes/ssl/kube-apiserver-key.pem \</span><span class="hljs-string">                --service-node-port-range=30000-50000 \</span><span class="hljs-string">                --service-account-key-file=/etc/kubernetes/ssl/k8s-root-ca.pem \</span><span class="hljs-string">                --storage-backend=etcd3 \</span><span class="hljs-string">                --enable-swagger-ui=true"</span></code></pre><h5 id="controller-manager"><a href="#controller-manager" class="headerlink" title="controller-manager"></a>controller-manager</h5><p>controller manager 配置默认开启了证书轮换能力用于自动签署 kueblet 证书，并且证书时间也设置了 10 年，可自行调整；增加了 <code>--controllers</code> 选项以指定开启全部控制器</p><pre><code class="hljs sh"><span class="hljs-comment">###</span><span class="hljs-comment"># The following values are used to configure the kubernetes controller-manager</span><span class="hljs-comment"># defaults from config and apiserver should be adequate</span><span class="hljs-comment"># Add your own!</span>KUBE_CONTROLLER_MANAGER_ARGS=<span class="hljs-string">"  --bind-address=0.0.0.0 \</span><span class="hljs-string">                                --cluster-name=kubernetes \</span><span class="hljs-string">                                --cluster-signing-cert-file=/etc/kubernetes/ssl/k8s-root-ca.pem \</span><span class="hljs-string">                                --cluster-signing-key-file=/etc/kubernetes/ssl/k8s-root-ca-key.pem \</span><span class="hljs-string">                                --controllers=*,bootstrapsigner,tokencleaner \</span><span class="hljs-string">                                --deployment-controller-sync-period=10s \</span><span class="hljs-string">                                --experimental-cluster-signing-duration=86700h0m0s \</span><span class="hljs-string">                                --leader-elect=true \</span><span class="hljs-string">                                --node-monitor-grace-period=40s \</span><span class="hljs-string">                                --node-monitor-period=5s \</span><span class="hljs-string">                                --pod-eviction-timeout=5m0s \</span><span class="hljs-string">                                --terminated-pod-gc-threshold=50 \</span><span class="hljs-string">                                --root-ca-file=/etc/kubernetes/ssl/k8s-root-ca.pem \</span><span class="hljs-string">                                --service-account-private-key-file=/etc/kubernetes/ssl/k8s-root-ca-key.pem \</span><span class="hljs-string">                                --feature-gates=RotateKubeletServerCertificate=true"</span></code></pre><h5 id="scheduler"><a href="#scheduler" class="headerlink" title="scheduler"></a>scheduler</h5><pre><code class="hljs sh"><span class="hljs-comment">###</span><span class="hljs-comment"># kubernetes scheduler config</span><span class="hljs-comment"># default config should be adequate</span><span class="hljs-comment"># Add your own!</span>KUBE_SCHEDULER_ARGS=<span class="hljs-string">"   --address=0.0.0.0 \</span><span class="hljs-string">                        --leader-elect=true \</span><span class="hljs-string">                        --algorithm-provider=DefaultProvider"</span></code></pre><h4 id="3-4、Node-节点配置"><a href="#3-4、Node-节点配置" class="headerlink" title="3.4、Node 节点配置"></a>3.4、Node 节点配置</h4><p>Node 节点上主要有 <code>kubelet</code>、<code>kube-proxy</code> 组件，用到的配置如下</p><h5 id="kubelet"><a href="#kubelet" class="headerlink" title="kubelet"></a>kubelet</h5><p>kubeket 默认也开启了证书轮换能力以保证自动续签相关证书，同时增加了 <code>--node-labels</code> 选项为 node 打一个标签，关于这个标签最后部分会有讨论，<strong>如果在 master 上启动 kubelet，请将 <code>node-role.kubernetes.io/k8s-node=true</code> 修改为 <code>node-role.kubernetes.io/k8s-master=true</code></strong></p><pre><code class="hljs sh"><span class="hljs-comment">###</span><span class="hljs-comment"># kubernetes kubelet (minion) config</span><span class="hljs-comment"># The address for the info server to serve on (set to 0.0.0.0 or "" for all interfaces)</span>KUBELET_ADDRESS=<span class="hljs-string">"--node-ip=192.168.1.61"</span><span class="hljs-comment"># The port for the info server to serve on</span><span class="hljs-comment"># KUBELET_PORT="--port=10250"</span><span class="hljs-comment"># You may leave this blank to use the actual hostname</span>KUBELET_HOSTNAME=<span class="hljs-string">"--hostname-override=k1.node"</span><span class="hljs-comment"># location of the api-server</span><span class="hljs-comment"># KUBELET_API_SERVER=""</span><span class="hljs-comment"># Add your own!</span>KUBELET_ARGS=<span class="hljs-string">"  --bootstrap-kubeconfig=/etc/kubernetes/bootstrap.kubeconfig \</span><span class="hljs-string">                --cert-dir=/etc/kubernetes/ssl \</span><span class="hljs-string">                --cgroup-driver=cgroupfs \</span><span class="hljs-string">                --cluster-dns=10.254.0.2 \</span><span class="hljs-string">                --cluster-domain=cluster.local. \</span><span class="hljs-string">                --fail-swap-on=false \</span><span class="hljs-string">                --feature-gates=RotateKubeletClientCertificate=true,RotateKubeletServerCertificate=true \</span><span class="hljs-string">                --node-labels=node-role.kubernetes.io/k8s-node=true \</span><span class="hljs-string">                --image-gc-high-threshold=70 \</span><span class="hljs-string">                --image-gc-low-threshold=50 \</span><span class="hljs-string">                --kube-reserved=cpu=500m,memory=512Mi,ephemeral-storage=1Gi \</span><span class="hljs-string">                --kubeconfig=/etc/kubernetes/kubelet.kubeconfig \</span><span class="hljs-string">                --system-reserved=cpu=1000m,memory=1024Mi,ephemeral-storage=1Gi \</span><span class="hljs-string">                --serialize-image-pulls=false \</span><span class="hljs-string">                --sync-frequency=30s \</span><span class="hljs-string">                --pod-infra-container-image=k8s.gcr.io/pause-amd64:3.0 \</span><span class="hljs-string">                --resolv-conf=/etc/resolv.conf \</span><span class="hljs-string">                --rotate-certificates"</span></code></pre><h5 id="proxy"><a href="#proxy" class="headerlink" title="proxy"></a>proxy</h5><pre><code class="hljs sh"><span class="hljs-comment">###</span><span class="hljs-comment"># kubernetes proxy config</span><span class="hljs-comment"># default config should be adequate</span><span class="hljs-comment"># Add your own!</span>KUBE_PROXY_ARGS=<span class="hljs-string">"--bind-address=0.0.0.0 \</span><span class="hljs-string">                 --hostname-override=k1.node \</span><span class="hljs-string">                 --kubeconfig=/etc/kubernetes/kube-proxy.kubeconfig \</span><span class="hljs-string">                 --cluster-cidr=10.254.0.0/16"</span></code></pre><h4 id="3-5、安装集群组件"><a href="#3-5、安装集群组件" class="headerlink" title="3.5、安装集群组件"></a>3.5、安装集群组件</h4><p>上面已经准备好了相关配置文件，接下来将这些配置文件组织成如下目录结构以便后续脚本安装</p><pre><code class="hljs sh">k8s├── conf│   ├── apiserver│   ├── audit-policy.yaml│   ├── bootstrap.kubeconfig│   ├── config│   ├── controller-manager│   ├── kubelet│   ├── kube-proxy.kubeconfig│   ├── proxy│   ├── scheduler│   ├── ssl│   │   ├── admin.csr│   │   ├── admin-csr.json│   │   ├── admin-key.pem│   │   ├── admin.pem│   │   ├── k8s-gencert.json│   │   ├── k8s-root-ca.csr│   │   ├── k8s-root-ca-csr.json│   │   ├── k8s-root-ca-key.pem│   │   ├── k8s-root-ca.pem│   │   ├── kube-apiserver.csr│   │   ├── kube-apiserver-csr.json│   │   ├── kube-apiserver-key.pem│   │   ├── kube-apiserver.pem│   │   ├── kube-proxy.csr│   │   ├── kube-proxy-csr.json│   │   ├── kube-proxy-key.pem│   │   └── kube-proxy.pem│   └── token.csv├── hyperkube_1.10.1├── install.sh└── systemd    ├── kube-apiserver.service    ├── kube-controller-manager.service    ├── kubelet.service    ├── kube-proxy.service    └── kube-scheduler.service</code></pre><p>其中 <code>install.sh</code> 内容如下</p><pre><code class="hljs sh"><span class="hljs-meta">#!/bin/bash</span><span class="hljs-built_in">set</span> -eKUBE_VERSION=<span class="hljs-string">"1.10.1"</span><span class="hljs-keyword">function</span> <span class="hljs-function"><span class="hljs-title">download_k8s</span></span>()&#123;    <span class="hljs-keyword">if</span> [ ! -f <span class="hljs-string">"hyperkube_<span class="hljs-variable">$&#123;KUBE_VERSION&#125;</span>"</span> ]; <span class="hljs-keyword">then</span>        wget https://storage.googleapis.com/kubernetes-release/release/v<span class="hljs-variable">$&#123;KUBE_VERSION&#125;</span>/bin/linux/amd64/hyperkube -O hyperkube_<span class="hljs-variable">$&#123;KUBE_VERSION&#125;</span>        chmod +x hyperkube_<span class="hljs-variable">$&#123;KUBE_VERSION&#125;</span>    <span class="hljs-keyword">fi</span>&#125;<span class="hljs-keyword">function</span> <span class="hljs-function"><span class="hljs-title">preinstall</span></span>()&#123;    getent group kube &gt;/dev/null || groupadd -r kube    getent passwd kube &gt;/dev/null || useradd -r -g kube -d / -s /sbin/nologin -c <span class="hljs-string">"Kubernetes user"</span> kube&#125;<span class="hljs-keyword">function</span> <span class="hljs-function"><span class="hljs-title">install_k8s</span></span>()&#123;    <span class="hljs-built_in">echo</span> -e <span class="hljs-string">"\033[32mINFO: Copy hyperkube...\033[0m"</span>    cp hyperkube_<span class="hljs-variable">$&#123;KUBE_VERSION&#125;</span> /usr/<span class="hljs-built_in">local</span>/bin/hyperkube    <span class="hljs-built_in">echo</span> -e <span class="hljs-string">"\033[32mINFO: Create symbolic link...\033[0m"</span>    ln -sf /usr/<span class="hljs-built_in">local</span>/bin/hyperkube /usr/<span class="hljs-built_in">local</span>/bin/kubectl    <span class="hljs-built_in">echo</span> -e <span class="hljs-string">"\033[32mINFO: Copy kubernetes config...\033[0m"</span>    cp -r conf /etc/kubernetes    <span class="hljs-keyword">if</span> [ -d <span class="hljs-string">"/etc/kubernetes/ssl"</span> ]; <span class="hljs-keyword">then</span>        chown -R kube:kube /etc/kubernetes/ssl    <span class="hljs-keyword">fi</span>    <span class="hljs-built_in">echo</span> -e <span class="hljs-string">"\033[32mINFO: Copy kubernetes systemd config...\033[0m"</span>    cp systemd/*.service /lib/systemd/system    systemctl daemon-reload&#125;<span class="hljs-keyword">function</span> <span class="hljs-function"><span class="hljs-title">postinstall</span></span>()&#123;    <span class="hljs-keyword">if</span> [ ! -d <span class="hljs-string">"/var/log/kube-audit"</span> ]; <span class="hljs-keyword">then</span>        mkdir /var/<span class="hljs-built_in">log</span>/kube-audit    <span class="hljs-keyword">fi</span>    <span class="hljs-keyword">if</span> [ ! -d <span class="hljs-string">"/var/lib/kubelet"</span> ]; <span class="hljs-keyword">then</span>        mkdir /var/lib/kubelet    <span class="hljs-keyword">fi</span>    <span class="hljs-keyword">if</span> [ ! -d <span class="hljs-string">"/usr/libexec"</span> ]; <span class="hljs-keyword">then</span>        mkdir /usr/libexec    <span class="hljs-keyword">fi</span>    chown -R kube:kube /var/<span class="hljs-built_in">log</span>/kube-audit /var/lib/kubelet /usr/libexec&#125;download_k8spreinstallinstall_k8spostinstall</code></pre><p><strong>脚本解释如下:</strong></p><ul><li>download_k8s: 下载 hyperkube 二进制文件</li><li>preinstall: 安装前处理，同 etcd 一样创建 kube 普通用户指定家目录、shell 等</li><li>install_k8s: 复制 hyperkube 到安装目录，为 kubectl 创建软连接(为啥创建软连接就能执行请自行阅读 <a href="https://github.com/kubernetes/kubernetes/blob/cce67ed8e7d461657d350a1cdd55791d1637fc43/cmd/hyperkube/main.go#L69" target="_blank" rel="noopener">源码</a>)，复制相关配置到对应目录，并处理权限</li><li>postinstall: 收尾工作，创建日志目录等，并处理权限</li></ul><p>最后执行此脚本安装即可，<strong>此外，应确保每个节点安装了 <code>ipset</code>、<code>conntrack</code> 两个包，因为 kube-proxy 组件会使用其处理 iptables 规则等</strong></p><h3 id="四、启动-Kubernetes-Master-节点"><a href="#四、启动-Kubernetes-Master-节点" class="headerlink" title="四、启动 Kubernetes Master 节点"></a>四、启动 Kubernetes Master 节点</h3><p>对于 <code>master</code> 节点启动无需做过多处理，多个 <code>master</code> 只要保证 <code>apiserver</code> 等配置中的 ip 地址监听没问题后直接启动即可</p><pre><code class="hljs sh">systemctl daemon-reloadsystemctl start kube-apiserversystemctl start kube-controller-managersystemctl start kube-schedulersystemctl <span class="hljs-built_in">enable</span> kube-apiserversystemctl <span class="hljs-built_in">enable</span> kube-controller-managersystemctl <span class="hljs-built_in">enable</span> kube-scheduler</code></pre><p>成功后截图如下</p><p><img src="https://cdn.oss.link/markdown/lqur1.png" srcset="/img/loading.gif" alt="Master success"></p><h3 id="五、启动-Kubernetes-Node-节点"><a href="#五、启动-Kubernetes-Node-节点" class="headerlink" title="五、启动 Kubernetes Node 节点"></a>五、启动 Kubernetes Node 节点</h3><p>由于 HA 等功能需要，对于 Node 需要做一些处理才能启动，主要有以下两个地方需要处理</p><h4 id="5-1、nginx-proxy"><a href="#5-1、nginx-proxy" class="headerlink" title="5.1、nginx-proxy"></a>5.1、nginx-proxy</h4><p>在启动 <code>kubelet</code>、<code>kube-proxy</code> 服务之前，需要在本地启动 <code>nginx</code> 来 tcp 负载均衡 <code>apiserver</code> 6443 端口，<code>nginx-proxy</code> 使用 <code>docker</code> + <code>systemd</code> 启动，配置如下</p><p><strong>注意: 对于在 master 节点启动 kubelet 来说，不需要 nginx 做负载均衡；可以跳过此步骤，并修改 <code>kubelet.kubeconfig</code>、<code>kube-proxy.kubeconfig</code> 中的 apiserver 地址为当前 master ip 6443 端口即可</strong></p><ul><li>nginx-proxy.service</li></ul><pre><code class="hljs sh">[Unit]Description=kubernetes apiserver docker wrapperWants=docker.socketAfter=docker.service[Service]User=rootPermissionsStartOnly=<span class="hljs-literal">true</span>ExecStart=/usr/bin/docker run -p 127.0.0.1:6443:6443 \                              -v /etc/nginx:/etc/nginx \                              --name nginx-proxy \                              --net=host \                              --restart=on-failure:5 \                              --memory=512M \                              nginx:1.13.12-alpineExecStartPre=-/usr/bin/docker rm -f nginx-proxyExecStop=/usr/bin/docker stop nginx-proxyRestart=alwaysRestartSec=15sTimeoutStartSec=30s[Install]WantedBy=multi-user.target</code></pre><ul><li>nginx.conf</li></ul><pre><code class="hljs sh">error_log stderr notice;worker_processes auto;events &#123;        multi_accept on;        use epoll;        worker_connections 1024;&#125;stream &#123;    upstream kube_apiserver &#123;        least_conn;        server 192.168.1.61:6443;        server 192.168.1.62:6443;        server 192.168.1.63:6443;    &#125;    server &#123;        listen        0.0.0.0:6443;        proxy_pass    kube_apiserver;        proxy_timeout 10m;        proxy_connect_timeout 1s;    &#125;&#125;</code></pre><p><strong>启动 apiserver 的本地负载均衡</strong></p><pre><code class="hljs sh">mkdir /etc/nginxcp nginx.conf /etc/nginxcp nginx-proxy.service /lib/systemd/systemsystemctl daemon-reloadsystemctl start nginx-proxysystemctl <span class="hljs-built_in">enable</span> nginx-proxy</code></pre><h4 id="5-2、TLS-bootstrapping"><a href="#5-2、TLS-bootstrapping" class="headerlink" title="5.2、TLS bootstrapping"></a>5.2、TLS bootstrapping</h4><p>创建好 <code>nginx-proxy</code> 后不要忘记为 <code>TLS Bootstrap</code> 创建相应的 <code>RBAC</code> 规则，这些规则能实现证自动签署 <code>TLS Bootstrap</code> 发出的 <code>CSR</code> 请求，从而实现证书轮换(创建一次即可)；详情请参考 <a href="https://mritd.me/2018/01/07/kubernetes-tls-bootstrapping-note/" target="_blank" rel="noopener">Kubernetes TLS bootstrapping 那点事</a></p><ul><li>tls-bootstrapping-clusterrole.yaml(与 1.8 一样)</li></ul><pre><code class="hljs yaml"><span class="hljs-comment"># A ClusterRole which instructs the CSR approver to approve a node requesting a</span><span class="hljs-comment"># serving cert matching its client cert.</span><span class="hljs-attr">kind:</span> <span class="hljs-string">ClusterRole</span><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">rbac.authorization.k8s.io/v1</span><span class="hljs-attr">metadata:</span>  <span class="hljs-attr">name:</span> <span class="hljs-string">system:certificates.k8s.io:certificatesigningrequests:selfnodeserver</span><span class="hljs-attr">rules:</span><span class="hljs-bullet">-</span> <span class="hljs-attr">apiGroups:</span> <span class="hljs-string">["certificates.k8s.io"]</span>  <span class="hljs-attr">resources:</span> <span class="hljs-string">["certificatesigningrequests/selfnodeserver"]</span>  <span class="hljs-attr">verbs:</span> <span class="hljs-string">["create"]</span></code></pre><p><strong>在 master 执行创建</strong></p><pre><code class="hljs sh"><span class="hljs-comment"># 给与 kubelet-bootstrap 用户进行 node-bootstrapper 的权限</span>kubectl create clusterrolebinding kubelet-bootstrap \    --clusterrole=system:node-bootstrapper \    --user=kubelet-bootstrapkubectl create -f tls-bootstrapping-clusterrole.yaml<span class="hljs-comment"># 自动批准 system:bootstrappers 组用户 TLS bootstrapping 首次申请证书的 CSR 请求</span>kubectl create clusterrolebinding node-client-auto-approve-csr \        --clusterrole=system:certificates.k8s.io:certificatesigningrequests:nodeclient \        --group=system:bootstrappers<span class="hljs-comment"># 自动批准 system:nodes 组用户更新 kubelet 自身与 apiserver 通讯证书的 CSR 请求</span>kubectl create clusterrolebinding node-client-auto-renew-crt \        --clusterrole=system:certificates.k8s.io:certificatesigningrequests:selfnodeclient \        --group=system:nodes<span class="hljs-comment"># 自动批准 system:nodes 组用户更新 kubelet 10250 api 端口证书的 CSR 请求</span>kubectl create clusterrolebinding node-server-auto-renew-crt \        --clusterrole=system:certificates.k8s.io:certificatesigningrequests:selfnodeserver \        --group=system:nodes</code></pre><h4 id="5-3、执行启动"><a href="#5-3、执行启动" class="headerlink" title="5.3、执行启动"></a>5.3、执行启动</h4><p>多节点部署时先启动好 <code>nginx-proxy</code>，然后修改好相应配置的 ip 地址等配置，最终直接启动即可(master 上启动 kubelet 不要忘了修改 kubeconfig 中的 apiserver 地址，还有对应的 kubelet 的 node label)</p><pre><code class="hljs sh">systemctl daemon-reloadsystemctl start kubeletsystemctl start kube-proxysystemctl <span class="hljs-built_in">enable</span> kubeletsystemctl <span class="hljs-built_in">enable</span> kube-proxy</code></pre><p>最后启动成功后如下</p><p><img src="https://cdn.oss.link/markdown/r4s34.png" srcset="/img/loading.gif" alt="cluster started"></p><h3 id="五、安装-Calico"><a href="#五、安装-Calico" class="headerlink" title="五、安装 Calico"></a>五、安装 Calico</h3><p>Calico 安装仍然延续以前的方案，使用 Daemonset 安装 cni 组件，使用 systemd 控制 calico-node 以确保 calico-node 能正确的拿到主机名等</p><h4 id="5-1、修改-Calico-配置"><a href="#5-1、修改-Calico-配置" class="headerlink" title="5.1、修改 Calico 配置"></a>5.1、修改 Calico 配置</h4><pre><code class="hljs sh">wget https://docs.projectcalico.org/v3.1/getting-started/kubernetes/installation/hosted/calico.yaml -O calico.example.yamlETCD_CERT=`cat /etc/etcd/ssl/etcd.pem | base64 | tr -d <span class="hljs-string">'\n'</span>`ETCD_KEY=`cat /etc/etcd/ssl/etcd-key.pem | base64 | tr -d <span class="hljs-string">'\n'</span>`ETCD_CA=`cat /etc/etcd/ssl/etcd-root-ca.pem | base64 | tr -d <span class="hljs-string">'\n'</span>`ETCD_ENDPOINTS=<span class="hljs-string">"https://192.168.1.61:2379,https://192.168.1.62:2379,https://192.168.1.63:2379"</span>cp calico.example.yaml calico.yamlsed -i <span class="hljs-string">"s@.*etcd_endpoints:.*@\ \ etcd_endpoints:\ \"<span class="hljs-variable">$&#123;ETCD_ENDPOINTS&#125;</span>\"@gi"</span> calico.yamlsed -i <span class="hljs-string">"s@.*etcd-cert:.*@\ \ etcd-cert:\ <span class="hljs-variable">$&#123;ETCD_CERT&#125;</span>@gi"</span> calico.yamlsed -i <span class="hljs-string">"s@.*etcd-key:.*@\ \ etcd-key:\ <span class="hljs-variable">$&#123;ETCD_KEY&#125;</span>@gi"</span> calico.yamlsed -i <span class="hljs-string">"s@.*etcd-ca:.*@\ \ etcd-ca:\ <span class="hljs-variable">$&#123;ETCD_CA&#125;</span>@gi"</span> calico.yamlsed -i <span class="hljs-string">'s@.*etcd_ca:.*@\ \ etcd_ca:\ "/calico-secrets/etcd-ca"@gi'</span> calico.yamlsed -i <span class="hljs-string">'s@.*etcd_cert:.*@\ \ etcd_cert:\ "/calico-secrets/etcd-cert"@gi'</span> calico.yamlsed -i <span class="hljs-string">'s@.*etcd_key:.*@\ \ etcd_key:\ "/calico-secrets/etcd-key"@gi'</span> calico.yaml<span class="hljs-comment"># 注释掉 calico-node 部分(由 Systemd 接管)</span>sed -i <span class="hljs-string">'123,219s@.*@#&amp;@gi'</span> calico.yaml</code></pre><h4 id="5-2、创建-Systemd-文件"><a href="#5-2、创建-Systemd-文件" class="headerlink" title="5.2、创建 Systemd 文件"></a>5.2、创建 Systemd 文件</h4><p><strong>注意: 创建 systemd service 配置文件要在每个节点上都执行</strong></p><pre><code class="hljs sh">K8S_MASTER_IP=<span class="hljs-string">"192.168.1.61"</span>HOSTNAME=`cat /etc/hostname`ETCD_ENDPOINTS=<span class="hljs-string">"https://192.168.1.61:2379,https://192.168.1.62:2379,https://192.168.1.63:2379"</span>cat &gt; /lib/systemd/system/calico-node.service &lt;&lt;EOF[Unit]Description=calico nodeAfter=docker.serviceRequires=docker.service[Service]User=rootEnvironment=ETCD_ENDPOINTS=<span class="hljs-variable">$&#123;ETCD_ENDPOINTS&#125;</span>PermissionsStartOnly=<span class="hljs-literal">true</span>ExecStart=/usr/bin/docker run   --net=host --privileged --name=calico-node \\                                -e ETCD_ENDPOINTS=\<span class="hljs-variable">$&#123;ETCD_ENDPOINTS&#125;</span> \\                                -e ETCD_CA_CERT_FILE=/etc/etcd/ssl/etcd-root-ca.pem \\                                -e ETCD_CERT_FILE=/etc/etcd/ssl/etcd.pem \\                                -e ETCD_KEY_FILE=/etc/etcd/ssl/etcd-key.pem \\                                -e NODENAME=<span class="hljs-variable">$&#123;HOSTNAME&#125;</span> \\                                -e IP= \\                                -e IP_AUTODETECTION_METHOD=can-reach=<span class="hljs-variable">$&#123;K8S_MASTER_IP&#125;</span> \\                                -e AS=64512 \\                                -e CLUSTER_TYPE=k8s,bgp \\                                -e CALICO_IPV4POOL_CIDR=10.20.0.0/16 \\                                -e CALICO_IPV4POOL_IPIP=always \\                                -e CALICO_LIBNETWORK_ENABLED=<span class="hljs-literal">true</span> \\                                -e CALICO_NETWORKING_BACKEND=bird \\                                -e CALICO_DISABLE_FILE_LOGGING=<span class="hljs-literal">true</span> \\                                -e FELIX_IPV6SUPPORT=<span class="hljs-literal">false</span> \\                                -e FELIX_DEFAULTENDPOINTTOHOSTACTION=ACCEPT \\                                -e FELIX_LOGSEVERITYSCREEN=info \\                                -e FELIX_IPINIPMTU=1440 \\                                -e FELIX_HEALTHENABLED=<span class="hljs-literal">true</span> \\                                -e CALICO_K8S_NODE_REF=<span class="hljs-variable">$&#123;HOSTNAME&#125;</span> \\                                -v /etc/calico/etcd-root-ca.pem:/etc/etcd/ssl/etcd-root-ca.pem \\                                -v /etc/calico/etcd.pem:/etc/etcd/ssl/etcd.pem \\                                -v /etc/calico/etcd-key.pem:/etc/etcd/ssl/etcd-key.pem \\                                -v /lib/modules:/lib/modules \\                                -v /var/lib/calico:/var/lib/calico \\                                -v /var/run/calico:/var/run/calico \\                                quay.io/calico/node:v3.1.0ExecStop=/usr/bin/docker rm -f calico-nodeRestart=alwaysRestartSec=10[Install]WantedBy=multi-user.targetEOF</code></pre><p><strong>对于以上脚本中的 <code>K8S_MASTER_IP</code> 变量，只需要填写一个 master ip 即可，这个变量用于 calico 自动选择 IP 使用；在宿主机有多张网卡的情况下，calcio node 会自动获取一个 IP，获取原则就是尝试是否能够联通这个 master ip</strong></p><p>由于 calico 需要使用 etcd 存储数据，所以需要复制 etcd 证书到相关目录，<strong><code>/etc/calico</code> 需要在每个节点都有</strong></p><pre><code class="hljs sh">cp -r /etc/etcd/ssl /etc/calico</code></pre><h4 id="5-3、修改-kubelet-配置"><a href="#5-3、修改-kubelet-配置" class="headerlink" title="5.3、修改 kubelet 配置"></a>5.3、修改 kubelet 配置</h4><p>使用 Calico 后需要修改 kubelet 配置增加 CNI 设置(<code>--network-plugin=cni</code>)，修改后配置如下</p><pre><code class="hljs sh"><span class="hljs-comment">###</span><span class="hljs-comment"># kubernetes kubelet (minion) config</span><span class="hljs-comment"># The address for the info server to serve on (set to 0.0.0.0 or "" for all interfaces)</span>KUBELET_ADDRESS=<span class="hljs-string">"--node-ip=192.168.1.61"</span><span class="hljs-comment"># The port for the info server to serve on</span><span class="hljs-comment"># KUBELET_PORT="--port=10250"</span><span class="hljs-comment"># You may leave this blank to use the actual hostname</span>KUBELET_HOSTNAME=<span class="hljs-string">"--hostname-override=k1.node"</span><span class="hljs-comment"># location of the api-server</span><span class="hljs-comment"># KUBELET_API_SERVER=""</span><span class="hljs-comment"># Add your own!</span>KUBELET_ARGS=<span class="hljs-string">"  --bootstrap-kubeconfig=/etc/kubernetes/bootstrap.kubeconfig \</span><span class="hljs-string">                --cert-dir=/etc/kubernetes/ssl \</span><span class="hljs-string">                --cgroup-driver=cgroupfs \</span><span class="hljs-string">                --network-plugin=cni \</span><span class="hljs-string">                --cluster-dns=10.254.0.2 \</span><span class="hljs-string">                --cluster-domain=cluster.local. \</span><span class="hljs-string">                --fail-swap-on=false \</span><span class="hljs-string">                --feature-gates=RotateKubeletClientCertificate=true,RotateKubeletServerCertificate=true \</span><span class="hljs-string">                --node-labels=node-role.kubernetes.io/k8s-master=true \</span><span class="hljs-string">                --image-gc-high-threshold=70 \</span><span class="hljs-string">                --image-gc-low-threshold=50 \</span><span class="hljs-string">                --kube-reserved=cpu=500m,memory=512Mi,ephemeral-storage=1Gi \</span><span class="hljs-string">                --kubeconfig=/etc/kubernetes/kubelet.kubeconfig \</span><span class="hljs-string">                --system-reserved=cpu=1000m,memory=1024Mi,ephemeral-storage=1Gi \</span><span class="hljs-string">                --serialize-image-pulls=false \</span><span class="hljs-string">                --sync-frequency=30s \</span><span class="hljs-string">                --pod-infra-container-image=k8s.gcr.io/pause-amd64:3.0 \</span><span class="hljs-string">                --resolv-conf=/etc/resolv.conf \</span><span class="hljs-string">                --rotate-certificates"</span></code></pre><h4 id="5-4、创建-Calico-Daemonset"><a href="#5-4、创建-Calico-Daemonset" class="headerlink" title="5.4、创建 Calico Daemonset"></a>5.4、创建 Calico Daemonset</h4><pre><code class="hljs sh"><span class="hljs-comment"># 先创建 RBAC</span>kubectl apply -f \https://docs.projectcalico.org/v3.1/getting-started/kubernetes/installation/rbac.yaml<span class="hljs-comment"># 再创建 Calico Daemonset</span>kubectl create -f calico.yaml</code></pre><h4 id="5-5、启动-Calico-Node"><a href="#5-5、启动-Calico-Node" class="headerlink" title="5.5、启动 Calico Node"></a>5.5、启动 Calico Node</h4><pre><code class="hljs sh">systemctl daemon-reloadsystemctl restart calico-nodesystemctl <span class="hljs-built_in">enable</span> calico-node<span class="hljs-comment"># 等待 20s 拉取镜像</span>sleep 20systemctl restart kubelet</code></pre><h4 id="5-6、测试网络"><a href="#5-6、测试网络" class="headerlink" title="5.6、测试网络"></a>5.6、测试网络</h4><p>网络测试与其他几篇文章一样，创建几个 pod 测试即可</p><pre><code class="hljs sh"><span class="hljs-comment"># 创建 deployment</span>cat &lt;&lt; EOF &gt;&gt; demo.deploy.ymlapiVersion: apps/v1kind: Deploymentmetadata:  name: demo-deploymentspec:  replicas: 5  selector:    matchLabels:      app: demo  template:    metadata:      labels:        app: demo    spec:      containers:      - name: demo        image: mritd/demo        imagePullPolicy: IfNotPresent        ports:        - containerPort: 80EOFkubectl create -f demo.deploy.yml</code></pre><p>测试结果如图所示</p><p><img src="https://cdn.oss.link/markdown/u9j3v.png" srcset="/img/loading.gif" alt="test calico"></p><h3 id="六、部署集群-DNS"><a href="#六、部署集群-DNS" class="headerlink" title="六、部署集群 DNS"></a>六、部署集群 DNS</h3><h4 id="6-1、部署-CoreDNS"><a href="#6-1、部署-CoreDNS" class="headerlink" title="6.1、部署 CoreDNS"></a>6.1、部署 CoreDNS</h4><p>CoreDNS 给出了标准的 deployment 配置，如下</p><ul><li>coredns.yaml.sed</li></ul><pre><code class="hljs sh">apiVersion: v1kind: ServiceAccountmetadata:  name: coredns  namespace: kube-system---apiVersion: rbac.authorization.k8s.io/v1beta1kind: ClusterRolemetadata:  labels:    kubernetes.io/bootstrapping: rbac-defaults  name: system:corednsrules:- apiGroups:  - <span class="hljs-string">""</span>  resources:  - endpoints  - services  - pods  - namespaces  verbs:  - list  - watch---apiVersion: rbac.authorization.k8s.io/v1beta1kind: ClusterRoleBindingmetadata:  annotations:    rbac.authorization.kubernetes.io/autoupdate: <span class="hljs-string">"true"</span>  labels:    kubernetes.io/bootstrapping: rbac-defaults  name: system:corednsroleRef:  apiGroup: rbac.authorization.k8s.io  kind: ClusterRole  name: system:corednssubjects:- kind: ServiceAccount  name: coredns  namespace: kube-system---apiVersion: v1kind: ConfigMapmetadata:  name: coredns  namespace: kube-systemdata:  Corefile: |    .:53 &#123;        errors        health        kubernetes CLUSTER_DOMAIN REVERSE_CIDRS &#123;          pods insecure          upstream          fallthrough <span class="hljs-keyword">in</span>-addr.arpa ip6.arpa        &#125;        prometheus :9153        proxy . /etc/resolv.conf        cache 30    &#125;---apiVersion: extensions/v1beta1kind: Deploymentmetadata:  name: coredns  namespace: kube-system  labels:    k8s-app: kube-dns    kubernetes.io/name: <span class="hljs-string">"CoreDNS"</span>spec:  replicas: 2  strategy:    <span class="hljs-built_in">type</span>: RollingUpdate    rollingUpdate:      maxUnavailable: 1  selector:    matchLabels:      k8s-app: kube-dns  template:    metadata:      labels:        k8s-app: kube-dns    spec:      serviceAccountName: coredns      tolerations:        - key: <span class="hljs-string">"CriticalAddonsOnly"</span>          operator: <span class="hljs-string">"Exists"</span>      containers:      - name: coredns        image: coredns/coredns:1.1.1        imagePullPolicy: IfNotPresent        args: [ <span class="hljs-string">"-conf"</span>, <span class="hljs-string">"/etc/coredns/Corefile"</span> ]        volumeMounts:        - name: config-volume          mountPath: /etc/coredns        ports:        - containerPort: 53          name: dns          protocol: UDP        - containerPort: 53          name: dns-tcp          protocol: TCP        - containerPort: 9153          name: metrics          protocol: TCP        livenessProbe:          httpGet:            path: /health            port: 8080            scheme: HTTP          initialDelaySeconds: 60          timeoutSeconds: 5          successThreshold: 1          failureThreshold: 5      dnsPolicy: Default      volumes:        - name: config-volume          configMap:            name: coredns            items:            - key: Corefile              path: Corefile---apiVersion: v1kind: Servicemetadata:  name: kube-dns  namespace: kube-system  annotations:    prometheus.io/scrape: <span class="hljs-string">"true"</span>  labels:    k8s-app: kube-dns    kubernetes.io/cluster-service: <span class="hljs-string">"true"</span>    kubernetes.io/name: <span class="hljs-string">"CoreDNS"</span>spec:  selector:    k8s-app: kube-dns  clusterIP: CLUSTER_DNS_IP  ports:  - name: dns    port: 53    protocol: UDP  - name: dns-tcp    port: 53    protocol: TCP</code></pre><p>然后直接使用脚本替换即可(脚本变量我已经修改了)</p><pre><code class="hljs sh"><span class="hljs-meta">#!/bin/bash</span><span class="hljs-comment"># Deploys CoreDNS to a cluster currently running Kube-DNS.</span>SERVICE_CIDR=<span class="hljs-variable">$&#123;1:-10.254.0.0/16&#125;</span>POD_CIDR=<span class="hljs-variable">$&#123;2:-10.20.0.0/16&#125;</span>CLUSTER_DNS_IP=<span class="hljs-variable">$&#123;3:-10.254.0.2&#125;</span>CLUSTER_DOMAIN=<span class="hljs-variable">$&#123;4:-cluster.local&#125;</span>YAML_TEMPLATE=<span class="hljs-variable">$&#123;5:-`pwd`/coredns.yaml.sed&#125;</span>sed -e s/CLUSTER_DNS_IP/<span class="hljs-variable">$CLUSTER_DNS_IP</span>/g -e s/CLUSTER_DOMAIN/<span class="hljs-variable">$CLUSTER_DOMAIN</span>/g -e s?SERVICE_CIDR?<span class="hljs-variable">$SERVICE_CIDR</span>?g -e s?POD_CIDR?<span class="hljs-variable">$POD_CIDR</span>?g <span class="hljs-variable">$YAML_TEMPLATE</span> &gt; coredns.yaml</code></pre><p>最后使用 <code>kubectl</code> 创建一下</p><pre><code class="hljs sh"><span class="hljs-comment"># 执行上面的替换脚本</span>./deploy.sh<span class="hljs-comment"># 创建 CoreDNS</span>kubectl create -f coredns.yaml</code></pre><p>测试截图如下</p><p><img src="https://cdn.oss.link/markdown/v1jdc.png" srcset="/img/loading.gif" alt="test dns"></p><h4 id="6-2、部署-DNS-自动扩容"><a href="#6-2、部署-DNS-自动扩容" class="headerlink" title="6.2、部署 DNS 自动扩容"></a>6.2、部署 DNS 自动扩容</h4><p>自动扩容跟以往一样，yaml 创建一下就行</p><ul><li>dns-horizontal-autoscaler.yaml</li></ul><pre><code class="hljs sh"><span class="hljs-comment"># Copyright 2016 The Kubernetes Authors.</span><span class="hljs-comment">#</span><span class="hljs-comment"># Licensed under the Apache License, Version 2.0 (the "License");</span><span class="hljs-comment"># you may not use this file except in compliance with the License.</span><span class="hljs-comment"># You may obtain a copy of the License at</span><span class="hljs-comment">#</span><span class="hljs-comment">#     http://www.apache.org/licenses/LICENSE-2.0</span><span class="hljs-comment">#</span><span class="hljs-comment"># Unless required by applicable law or agreed to in writing, software</span><span class="hljs-comment"># distributed under the License is distributed on an "AS IS" BASIS,</span><span class="hljs-comment"># WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.</span><span class="hljs-comment"># See the License for the specific language governing permissions and</span><span class="hljs-comment"># limitations under the License.</span>kind: ServiceAccountapiVersion: v1metadata:  name: kube-dns-autoscaler  namespace: kube-system  labels:    addonmanager.kubernetes.io/mode: Reconcile---kind: ClusterRoleapiVersion: rbac.authorization.k8s.io/v1metadata:  name: system:kube-dns-autoscaler  labels:    addonmanager.kubernetes.io/mode: Reconcilerules:  - apiGroups: [<span class="hljs-string">""</span>]    resources: [<span class="hljs-string">"nodes"</span>]    verbs: [<span class="hljs-string">"list"</span>]  - apiGroups: [<span class="hljs-string">""</span>]    resources: [<span class="hljs-string">"replicationcontrollers/scale"</span>]    verbs: [<span class="hljs-string">"get"</span>, <span class="hljs-string">"update"</span>]  - apiGroups: [<span class="hljs-string">"extensions"</span>]    resources: [<span class="hljs-string">"deployments/scale"</span>, <span class="hljs-string">"replicasets/scale"</span>]    verbs: [<span class="hljs-string">"get"</span>, <span class="hljs-string">"update"</span>]<span class="hljs-comment"># Remove the configmaps rule once below issue is fixed:</span><span class="hljs-comment"># kubernetes-incubator/cluster-proportional-autoscaler#16</span>  - apiGroups: [<span class="hljs-string">""</span>]    resources: [<span class="hljs-string">"configmaps"</span>]    verbs: [<span class="hljs-string">"get"</span>, <span class="hljs-string">"create"</span>]---kind: ClusterRoleBindingapiVersion: rbac.authorization.k8s.io/v1metadata:  name: system:kube-dns-autoscaler  labels:    addonmanager.kubernetes.io/mode: Reconcilesubjects:  - kind: ServiceAccount    name: kube-dns-autoscaler    namespace: kube-systemroleRef:  kind: ClusterRole  name: system:kube-dns-autoscaler  apiGroup: rbac.authorization.k8s.io---apiVersion: apps/v1kind: Deploymentmetadata:  name: kube-dns-autoscaler  namespace: kube-system  labels:    k8s-app: kube-dns-autoscaler    kubernetes.io/cluster-service: <span class="hljs-string">"true"</span>    addonmanager.kubernetes.io/mode: Reconcilespec:  selector:    matchLabels:      k8s-app: kube-dns-autoscaler  template:    metadata:      labels:        k8s-app: kube-dns-autoscaler      annotations:        scheduler.alpha.kubernetes.io/critical-pod: <span class="hljs-string">''</span>    spec:      priorityClassName: system-cluster-critical      containers:      - name: autoscaler        image: k8s.gcr.io/cluster-proportional-autoscaler-amd64:1.1.2-r2        resources:            requests:                cpu: <span class="hljs-string">"20m"</span>                memory: <span class="hljs-string">"10Mi"</span>        <span class="hljs-built_in">command</span>:          - /cluster-proportional-autoscaler          - --namespace=kube-system          - --configmap=kube-dns-autoscaler          <span class="hljs-comment"># Should keep target in sync with cluster/addons/dns/kube-dns.yaml.base</span>          - --target=Deployment/kube-dns          <span class="hljs-comment"># When cluster is using large nodes(with more cores), "coresPerReplica" should dominate.</span>          <span class="hljs-comment"># If using small nodes, "nodesPerReplica" should dominate.</span>          - --default-params=&#123;<span class="hljs-string">"linear"</span>:&#123;<span class="hljs-string">"coresPerReplica"</span>:256,<span class="hljs-string">"nodesPerReplica"</span>:16,<span class="hljs-string">"preventSinglePointFailure"</span>:<span class="hljs-literal">true</span>&#125;&#125;          - --logtostderr=<span class="hljs-literal">true</span>          - --v=2      tolerations:      - key: <span class="hljs-string">"CriticalAddonsOnly"</span>        operator: <span class="hljs-string">"Exists"</span>      serviceAccountName: kube-dns-autoscaler</code></pre><h3 id="七、部署-heapster"><a href="#七、部署-heapster" class="headerlink" title="七、部署 heapster"></a>七、部署 heapster</h3><p>heapster 部署相对简单的多，yaml 创建一下就可以了</p><pre><code class="hljs sh">kubectl create -f https://raw.githubusercontent.com/kubernetes/heapster/master/deploy/kube-config/influxdb/grafana.yamlkubectl create -f https://raw.githubusercontent.com/kubernetes/heapster/master/deploy/kube-config/influxdb/heapster.yamlkubectl create -f https://raw.githubusercontent.com/kubernetes/heapster/master/deploy/kube-config/influxdb/influxdb.yamlkubectl create -f https://raw.githubusercontent.com/kubernetes/heapster/master/deploy/kube-config/rbac/heapster-rbac.yaml</code></pre><h3 id="八、部署-Dashboard"><a href="#八、部署-Dashboard" class="headerlink" title="八、部署 Dashboard"></a>八、部署 Dashboard</h3><h4 id="8-1、部署-Dashboard"><a href="#8-1、部署-Dashboard" class="headerlink" title="8.1、部署 Dashboard"></a>8.1、部署 Dashboard</h4><p>Dashboard 部署同 heapster 一样，不过为了方便访问，我设置了 NodePort，还注意到一点是 yaml 拉取策略已经没有比较傻的 <code>Always</code> 了</p><pre><code class="hljs sh">wget https://raw.githubusercontent.com/kubernetes/dashboard/master/src/deploy/recommended/kubernetes-dashboard.yaml -O kubernetes-dashboard.yaml</code></pre><p>将最后部分的端口暴露修改如下</p><pre><code class="hljs yaml"><span class="hljs-comment"># ------------------- Dashboard Service ------------------- #</span><span class="hljs-attr">kind:</span> <span class="hljs-string">Service</span><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">v1</span><span class="hljs-attr">metadata:</span>  <span class="hljs-attr">labels:</span>    <span class="hljs-attr">k8s-app:</span> <span class="hljs-string">kubernetes-dashboard</span>  <span class="hljs-attr">name:</span> <span class="hljs-string">kubernetes-dashboard</span>  <span class="hljs-attr">namespace:</span> <span class="hljs-string">kube-system</span><span class="hljs-attr">spec:</span>  <span class="hljs-attr">type:</span> <span class="hljs-string">NodePort</span>  <span class="hljs-attr">ports:</span>    <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">dashboard-tls</span>      <span class="hljs-attr">port:</span> <span class="hljs-number">443</span>      <span class="hljs-attr">targetPort:</span> <span class="hljs-number">8443</span>      <span class="hljs-attr">nodePort:</span> <span class="hljs-number">30000</span>      <span class="hljs-attr">protocol:</span> <span class="hljs-string">TCP</span>  <span class="hljs-attr">selector:</span>    <span class="hljs-attr">k8s-app:</span> <span class="hljs-string">kubernetes-dashboard</span></code></pre><p>然后执行 <code>kubectl create -f kubernetes-dashboard.yaml</code> 即可</p><h4 id="8-2、创建-admin-账户"><a href="#8-2、创建-admin-账户" class="headerlink" title="8.2、创建 admin 账户"></a>8.2、创建 admin 账户</h4><p>默认情况下部署成功后可以直接访问 <code>https://NODE_IP:30000</code> 访问，但是想要登录进去查看的话需要使用 kubeconfig 或者 access token 的方式；实际上这个就是 RBAC 授权控制，以下提供一个创建 admin access token 的脚本，更细节的权限控制比如只读用户可以参考 <a href="https://mritd.me/2018/03/20/use-rbac-to-control-kubectl-permissions/" target="_blank" rel="noopener">使用 RBAC 控制 kubectl 权限</a>，RBAC 权限控制原理是一样的</p><pre><code class="hljs sh"><span class="hljs-meta">#!/bin/bash</span><span class="hljs-keyword">if</span> kubectl get sa dashboard-admin -n kube-system &amp;&gt; /dev/null;<span class="hljs-keyword">then</span>    <span class="hljs-built_in">echo</span> -e <span class="hljs-string">"\033[33mWARNING: ServiceAccount dashboard-admin exist!\033[0m"</span><span class="hljs-keyword">else</span>    kubectl create sa dashboard-admin -n kube-system    kubectl create clusterrolebinding dashboard-admin --clusterrole=cluster-admin --serviceaccount=kube-system:dashboard-admin<span class="hljs-keyword">fi</span>kubectl describe secret -n kube-system $(kubectl get secrets -n kube-system | grep dashboard-admin | cut -f1 -d <span class="hljs-string">' '</span>) | grep -E <span class="hljs-string">'^token'</span></code></pre><p>将以上脚本保存为 <code>create_dashboard_sa.sh</code> 执行即可，成功后访问截图如下(<strong>如果访问不了的话请检查下 iptable FORWARD 默认规则是否为 DROP，如果是将其改为 ACCEPT 即可</strong>)</p><p><img src="https://cdn.oss.link/markdown/oxmms.png" srcset="/img/loading.gif" alt="create_dashboard_sa"></p><p><img src="https://cdn.oss.link/markdown/pyplb.png" srcset="/img/loading.gif" alt="dashboard"></p><h3 id="九、其他说明"><a href="#九、其他说明" class="headerlink" title="九、其他说明"></a>九、其他说明</h3><h4 id="9-1、选项-label-等说明"><a href="#9-1、选项-label-等说明" class="headerlink" title="9.1、选项 label 等说明"></a>9.1、选项 label 等说明</h4><p>部署过程中注意到一些选项已经做了名称更改，比如 <code>--network-plugin-dir</code> 变更为 <code>--cni-bin-dir</code> 等，具体的那些选项做了变更请自行对比配置，以及查看官方文档；</p><p>对于 Node label <code>--node-labels=node-role.kubernetes.io/k8s-node=true</code> 这个选项，它的作用只是在 <code>kubectl get node</code> 时 ROLES 栏显示是什么节点；不过需要注意 <strong>master 上的 kubelet 不要将 <code>node-role.kubernetes.io/k8s-master=true</code> 更改成 <code>node-role.kubernetes.io/master=xxxx</code>；后面这个 <code>node-role.kubernetes.io/master</code> 是 kubeadm 用的，这个 label 会告诉 k8s 调度器当前节点为 master，从而执行一些特定动作，比如 <code>node-role.kubernetes.io/master:NoSchedule</code> 此节点将不会被分配 pod；具体参见 <a href="https://github.com/kubernetes-incubator/kubespray/issues/2108" target="_blank" rel="noopener">kubespray issue</a> 以及 <a href="https://github.com/kubernetes/kubeadm/blob/master/docs/design/design_v1.9.md#mark-master" target="_blank" rel="noopener">官方设计文档</a></strong></p><p>很多人可能会发现大约 1 小时候 <code>kubectl get csr</code> 看不到任何 csr 了，这是因为最新版本增加了 csr 清理功能，<strong>默认对于 <code>approved</code> 和 <code>denied</code> 状态的 csr 一小时后会被清理，对于 <code>pending</code> 状态的 csr 24 小时后会被清理，想问时间从哪来的请看 <a href="https://github.com/kubernetes/kubernetes/blob/fa85bf7094a8a503fede964b7038eed51360ffc7/pkg/controller/certificates/cleaner/cleaner.go#L47" target="_blank" rel="noopener">代码</a>；PR issue 我忘记了，增加这个功能的起因大致就是因为当开启了证书轮换后，csr 会不断增加，所以需要增加一个清理功能</strong></p><h4 id="9-2、异常及警告说明"><a href="#9-2、异常及警告说明" class="headerlink" title="9.2、异常及警告说明"></a>9.2、异常及警告说明</h4><p>在部署过程中我记录了一些异常警告等，以下做一下统一说明</p><pre><code class="hljs sh"><span class="hljs-comment"># https://github.com/kubernetes/kubernetes/issues/42158</span><span class="hljs-comment"># 这个问题还没解决，PR 没有合并被关闭了，可以关注一下上面这个 issue，被关闭的 PR 在下面</span><span class="hljs-comment"># https://github.com/kubernetes/kubernetes/pull/49567</span>Failed to update statusUpdateNeeded field <span class="hljs-keyword">in</span> actual state of world: Failed to <span class="hljs-built_in">set</span> statusUpdateNeeded to needed <span class="hljs-literal">true</span>, because nodeName=...<span class="hljs-comment"># https://github.com/kubernetes/kubernetes/issues/59993</span><span class="hljs-comment"># 这个似乎已经解决了，没时间测试，PR 地址在下面，我大致 debug 一下 好像是 cAdvisor 的问题</span><span class="hljs-comment"># https://github.com/opencontainers/runc/pull/1722</span>Failed to get system container stats <span class="hljs-keyword">for</span> <span class="hljs-string">"/kubepods"</span>: failed to get cgroup stats <span class="hljs-keyword">for</span> <span class="hljs-string">"/kubepods"</span>: failed to get container info <span class="hljs-keyword">for</span> <span class="hljs-string">"/kubepods"</span>: unknown containe <span class="hljs-string">"/kubepods"</span><span class="hljs-comment"># https://github.com/kubernetes/kubernetes/issues/58217</span><span class="hljs-comment"># 注意: 这个问题现在仍未解决，可关注上面的 issue，这个问题可能影响 node image gc</span><span class="hljs-comment"># 强烈依赖于 kubelet 做 宿主机 image gc 的需要注意一下</span>Image garbage collection failed once. Stats initialization may not have completed yet: failed to get imageFs info: unable to find data <span class="hljs-keyword">for</span> container /<span class="hljs-comment"># 没找到太多资料，不过感觉跟上面问题类似</span>failed to construct signal: <span class="hljs-string">"allocatableMemory.available"</span> error: system container <span class="hljs-string">"pods"</span> not found <span class="hljs-keyword">in</span> metrics</code></pre>]]></content>
    
    
    <categories>
      
      <category>Kubernetes</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Kubernetes</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Drone CI 搭建</title>
    <link href="/2018/03/30/set-up-drone-ci/"/>
    <url>/2018/03/30/set-up-drone-ci/</url>
    
    <content type="html"><![CDATA[<blockquote><p>最近感觉 GitLab CI 稍有繁琐，所以尝试了一下 Drone CI，这里记录一下搭建过程；虽然 Drone CI 看似简单，但是坑还是有不少的</p></blockquote><h2 id="一、环境准备"><a href="#一、环境准备" class="headerlink" title="一、环境准备"></a>一、环境准备</h2><p>基本环境如下:</p><ul><li>Docker: 17.09.0-ce</li><li>GitLab: 10.4.3-ce.0</li><li>Drone: 0.8.5</li></ul><p>其中 GitLab 采用 TLS 链接，为了方便使用 git 协议 clone 代码，所以 docker compose 部署时采用了 macvlan 网络获取独立 IP</p><h2 id="二、GitLab-配置"><a href="#二、GitLab-配置" class="headerlink" title="二、GitLab 配置"></a>二、GitLab 配置</h2><h3 id="2-1、GitLab-搭建"><a href="#2-1、GitLab-搭建" class="headerlink" title="2.1、GitLab 搭建"></a>2.1、GitLab 搭建</h3><p>为了测试 CI build 需要一个 GitLab 服务器以及测试项目，GitLab 这里直接采用 docker compose 启动，同时为了方便 git clone，网络使用了 macvlan 方式，macvlan 网络接口、IP 等参数请自行修改</p><pre><code class="hljs sh"><span class="hljs-comment"># config refs ==&gt; https://gitlab.com/gitlab-org/omnibus-gitlab/blob/master/files/gitlab-config-template/gitlab.rb.template</span>version: <span class="hljs-string">'3'</span>services:  gitlab:    image: <span class="hljs-string">'gitlab/gitlab-ce:10.4.3-ce.0'</span>    container_name: gitlab    restart: always    hostname: <span class="hljs-string">'gitlab.mritd.me'</span>    environment:      GITLAB_OMNIBUS_CONFIG: |        external_url <span class="hljs-string">'https://gitlab.mritd.me'</span>        nginx[<span class="hljs-string">'redirect_http_to_https'</span>] = <span class="hljs-literal">true</span>        nginx[<span class="hljs-string">'ssl_certificate'</span>] = <span class="hljs-string">"/etc/gitlab/ssl/mritd.me.cer"</span>        nginx[<span class="hljs-string">'ssl_certificate_key'</span>] = <span class="hljs-string">"/etc/gitlab/ssl/mritd.me.key"</span>        nginx[<span class="hljs-string">'real_ip_header'</span>] = <span class="hljs-string">'X-Real-IP'</span>        nginx[<span class="hljs-string">'real_ip_recursive'</span>] = <span class="hljs-string">'on'</span>        <span class="hljs-comment">#gitlab_rails['ldap_enabled'] = true</span>        <span class="hljs-comment">#gitlab_rails['ldap_servers'] = YAML.load &lt;&lt;-EOS # remember to close this block with 'EOS' below</span>        <span class="hljs-comment">#main: # 'main' is the GitLab 'provider ID' of this LDAP server</span>        <span class="hljs-comment">#  ## label</span>        <span class="hljs-comment">#  #</span>        <span class="hljs-comment">#  # A human-friendly name for your LDAP server. It is OK to change the label later,</span>        <span class="hljs-comment">#  # for instance if you find out it is too large to fit on the web page.</span>        <span class="hljs-comment">#  #</span>        <span class="hljs-comment">#  # Example: 'Paris' or 'Acme, Ltd.'</span>        <span class="hljs-comment">#  label: 'LDAP'</span>        <span class="hljs-comment">#  host: 'mail.mritd.me'</span>        <span class="hljs-comment">#  port: 389 # or 636</span>        <span class="hljs-comment">#  uid: 'uid'</span>        <span class="hljs-comment">#  method: 'plain' # "tls" or "ssl" or "plain"</span>        <span class="hljs-comment">#  bind_dn: 'uid=zimbra,cn=admins,cn=zimbra'</span>        <span class="hljs-comment">#  password: 'PASSWORD'</span>        <span class="hljs-comment">#  # This setting specifies if LDAP server is Active Directory LDAP server.</span>        <span class="hljs-comment">#  # For non AD servers it skips the AD specific queries.</span>        <span class="hljs-comment">#  # If your LDAP server is not AD, set this to false.</span>        <span class="hljs-comment">#  active_directory: true</span>        <span class="hljs-comment">#  # If allow_username_or_email_login is enabled, GitLab will ignore everything</span>        <span class="hljs-comment">#  # after the first '@' in the LDAP username submitted by the user on login.</span>        <span class="hljs-comment">#  #</span>        <span class="hljs-comment">#  # Example:</span>        <span class="hljs-comment">#  # - the user enters 'jane.doe@example.com' and 'p@ssw0rd' as LDAP credentials;</span>        <span class="hljs-comment">#  # - GitLab queries the LDAP server with 'jane.doe' and 'p@ssw0rd'.</span>        <span class="hljs-comment">#  #</span>        <span class="hljs-comment">#  # If you are using "uid: 'userPrincipalName'" on ActiveDirectory you need to</span>        <span class="hljs-comment">#  # disable this setting, because the userPrincipalName contains an '@'.</span>        <span class="hljs-comment">#  allow_username_or_email_login: true</span>        <span class="hljs-comment">#  # Base where we can search for users</span>        <span class="hljs-comment">#  #</span>        <span class="hljs-comment">#  #   Ex. ou=People,dc=gitlab,dc=example</span>        <span class="hljs-comment">#  #</span>        <span class="hljs-comment">#  base: ''</span>        <span class="hljs-comment">#  # Filter LDAP users</span>        <span class="hljs-comment">#  #</span>        <span class="hljs-comment">#  #   Format: RFC 4515 http://tools.ietf.org/search/rfc4515</span>        <span class="hljs-comment">#  #   Ex. (employeeType=developer)</span>        <span class="hljs-comment">#  #</span>        <span class="hljs-comment">#  #   Note: GitLab does not support omniauth-ldap's custom filter syntax.</span>        <span class="hljs-comment">#  #</span>        <span class="hljs-comment">#  user_filter: ''</span>        <span class="hljs-comment">#EOS</span>        gitlab_rails[<span class="hljs-string">'log_directory'</span>] = <span class="hljs-string">"/var/log/gitlab/gitlab-rails"</span>        unicorn[<span class="hljs-string">'log_directory'</span>] = <span class="hljs-string">"/var/log/gitlab/unicorn"</span>        registry[<span class="hljs-string">'log_directory'</span>] = <span class="hljs-string">"/var/log/gitlab/registry"</span>        <span class="hljs-comment"># Below are some of the default settings</span>        logging[<span class="hljs-string">'logrotate_frequency'</span>] = <span class="hljs-string">"daily"</span> <span class="hljs-comment"># rotate logs daily</span>        logging[<span class="hljs-string">'logrotate_size'</span>] = nil <span class="hljs-comment"># do not rotate by size by default</span>        logging[<span class="hljs-string">'logrotate_rotate'</span>] = 30 <span class="hljs-comment"># keep 30 rotated logs</span>        logging[<span class="hljs-string">'logrotate_compress'</span>] = <span class="hljs-string">"compress"</span> <span class="hljs-comment"># see 'man logrotate'</span>        logging[<span class="hljs-string">'logrotate_method'</span>] = <span class="hljs-string">"copytruncate"</span> <span class="hljs-comment"># see 'man logrotate'</span>        logging[<span class="hljs-string">'logrotate_postrotate'</span>] = nil <span class="hljs-comment"># no postrotate command by default</span>        logging[<span class="hljs-string">'logrotate_dateformat'</span>] = nil <span class="hljs-comment"># use date extensions for rotated files rather than numbers e.g. a value of "-%Y-%m-%d" would give rotated files like p</span>        <span class="hljs-comment"># You can add overrides per service</span>        nginx[<span class="hljs-string">'logrotate_frequency'</span>] = nil        nginx[<span class="hljs-string">'logrotate_size'</span>] = <span class="hljs-string">"200M"</span>        <span class="hljs-comment"># You can also disable the built-in logrotate service if you want</span>        logrotate[<span class="hljs-string">'enable'</span>] = <span class="hljs-literal">false</span>        gitlab_rails[<span class="hljs-string">'smtp_enable'</span>] = <span class="hljs-literal">true</span>        gitlab_rails[<span class="hljs-string">'smtp_address'</span>] = <span class="hljs-string">"mail.mritd.me"</span>        gitlab_rails[<span class="hljs-string">'smtp_port'</span>] = 25        gitlab_rails[<span class="hljs-string">'smtp_user_name'</span>] = <span class="hljs-string">"no-reply@mritd.me"</span>        gitlab_rails[<span class="hljs-string">'smtp_password'</span>] = <span class="hljs-string">"PASSWORD"</span>        gitlab_rails[<span class="hljs-string">'smtp_domain'</span>] = <span class="hljs-string">"mritd.me"</span>        gitlab_rails[<span class="hljs-string">'smtp_authentication'</span>] = <span class="hljs-string">"login"</span>        gitlab_rails[<span class="hljs-string">'smtp_enable_starttls_auto'</span>] = <span class="hljs-literal">true</span>        gitlab_rails[<span class="hljs-string">'smtp_openssl_verify_mode'</span>] = <span class="hljs-string">'peer'</span>        <span class="hljs-comment"># If your SMTP server does not like the default 'From: gitlab@localhost' you</span>        <span class="hljs-comment"># can change the 'From' with this setting.</span>        gitlab_rails[<span class="hljs-string">'gitlab_email_from'</span>] = <span class="hljs-string">'gitlab@mritd.me'</span>        gitlab_rails[<span class="hljs-string">'gitlab_email_reply_to'</span>] = <span class="hljs-string">'no-reply@mritd.me'</span>        gitlab_rails[<span class="hljs-string">'initial_root_password'</span>] = <span class="hljs-string">'PASSWORD'</span>        gitlab_rails[<span class="hljs-string">'initial_shared_runners_registration_token'</span>] = <span class="hljs-string">"iuLaUhGZYyFgTxAyZ6HbdFUZ"</span>    networks:      macvlan:        ipv4_address: 172.16.0.70    ports:      - <span class="hljs-string">'80:80'</span>      - <span class="hljs-string">'443:443'</span>      - <span class="hljs-string">'22:22'</span>    volumes:      - config:/etc/gitlab      - logs:/var/<span class="hljs-built_in">log</span>/gitlab      - data:/var/opt/gitlabnetworks:  macvlan:    driver: macvlan    driver_opts:      parent: ens18    ipam:      config:      - subnet: 172.16.0.0/19volumes:  config:  logs:  data:</code></pre><h3 id="2-2、创建-Drone-App"><a href="#2-2、创建-Drone-App" class="headerlink" title="2.2、创建 Drone App"></a>2.2、创建 Drone App</h3><p>Drone CI 工作时需要接入 GitLab 以完成项目同步等功能，所以在搭建好 GitLab 后需要为其创建 Application，创建方式如下所示</p><p><img src="https://cdn.oss.link/markdown/lzm4j.png" srcset="/img/loading.gif" alt="create drone app"></p><p>创建 Application 时请自行更换回调地址域名，创建好后如下所示(后续 Drone CI 需要使用这两个 key)</p><p><img src="https://cdn.oss.link/markdown/sl4yl.png" srcset="/img/loading.gif" alt="drone app create success"></p><h2 id="三、Drone-服务端配置"><a href="#三、Drone-服务端配置" class="headerlink" title="三、Drone 服务端配置"></a>三、Drone 服务端配置</h2><h3 id="3-1、Drone-CI-搭建"><a href="#3-1、Drone-CI-搭建" class="headerlink" title="3.1、Drone CI 搭建"></a>3.1、Drone CI 搭建</h3><p>Drone CI 服务器与 GitLab 等传统 CI 相似，都是 CS 模式，为了方便测试这里将 Agent 与 Server 端都放在一个 docker compose 中启动；docker compose 配置如下</p><pre><code class="hljs sh">version: <span class="hljs-string">'3'</span>services:  drone-server:    image: drone/drone:0.8-alpine    container_name: drone-server    ports:      - 8000:8000      - 9000:9000    volumes:      - data:/var/lib/drone/    restart: always    environment:      - DRONE_OPEN=<span class="hljs-literal">true</span>      - DRONE_ADMIN=drone,mritd      - DRONE_HOST=https://drone.mritd.me      - DRONE_GITLAB=<span class="hljs-literal">true</span>      - DRONE_GITLAB_PRIVATE_MODE=<span class="hljs-literal">true</span>      - DRONE_GITLAB_URL=https://gitlab.mritd.me      - DRONE_GITLAB_CLIENT=76155ab75bafd73d4ebfe0a02d9d6284a032f7d8667d558e3f929a64805d1fa1      - DRONE_GITLAB_SECRET=6957b06f53b80d4dd17051ceb36f9139ae83b9077e345a404f476e317b0c8f3d      - DRONE_SECRET=XsJnj4DmzuXBKkcgHeUAJQxq  drone-agent:    image: drone/agent:0.8    container_name: drone-agent    <span class="hljs-built_in">command</span>: agent    restart: always    volumes:      - /var/run/docker.sock:/var/run/docker.sock    environment:      - DRONE_SERVER=172.16.0.36:9000      - DRONE_SECRET=XsJnj4DmzuXBKkcgHeUAJQxqvolumes:  data:</code></pre><p>docker compose 中 <code>DRONE_GITLAB_CLIENT</code> 为 GitLab 创建 Application 时的 <code>Application Id</code>，<code>DRONE_GITLAB_SECRET</code> 为 <code>Secret</code>；其他环境变量解释如下:</p><ul><li>DRONE_OPEN: 是否允许开放注册</li><li>DRONE_ADMIN: 注册后的管理员用户</li><li>DRONE_HOST: Server 地址</li><li>DRONE_GITLAB: 声明 Drone CI 对接为 GitLab</li><li>DRONE_GITLAB_PRIVATE_MODE: GitLab 私有化部署</li><li>DRONE_GITLAB_URL: GitLab 地址</li><li>DRONE_SECRET: Server 端认证秘钥，Agent 连接时需要</li></ul><p>实际上 Agent 可以与 Server 分离部署，不过需要注意 Server 端 9000 端口走的是 grpc 协议基于 HTTP2，nginx 等反向代理时需要做好对应处理</p><p>搭建成功这里外面套了一层 nginx 用来反向代理 Drone Server 的 8000 端口，Nginx 配置如下:</p><pre><code class="hljs sh">upstream drone&#123;    server 172.16.0.36:8000;&#125;server &#123;    listen 80;    listen [::]:80;    server_name drone.mritd.me;    <span class="hljs-comment"># Redirect all HTTP requests to HTTPS with a 301 Moved Permanently response.</span>    <span class="hljs-built_in">return</span> 301 https://<span class="hljs-variable">$host</span><span class="hljs-variable">$request_uri</span>;&#125;server &#123;    listen 443 ssl http2;    listen [::]:443 ssl http2;    server_name drone.mritd.me;    <span class="hljs-comment"># certs sent to the client in SERVER HELLO are concatenated in ssl_certificate</span>    ssl_certificate /etc/nginx/ssl/mritd.me.cer;    ssl_certificate_key /etc/nginx/ssl/mritd.me.key;    ssl_session_timeout 1d;    ssl_session_cache shared:SSL:50m;    ssl_session_tickets off;        <span class="hljs-comment"># Diffie-Hellman parameter for DHE ciphersuites, recommended 2048 bits</span>    ssl_dhparam /etc/nginx/ssl/dhparam.pem;    <span class="hljs-comment"># intermediate configuration. tweak to your needs.</span>    ssl_protocols TLSv1 TLSv1.1 TLSv1.2;    ssl_ciphers <span class="hljs-string">'ECDHE-ECDSA-CHACHA20-POLY1305:ECDHE-RSA-CHACHA20-POLY1305:ECDHE-ECDSA-AES128-GCM-SHA256:ECDHE-RSA-AES128-GCM-SHA256:ECDHE-ECDSA-AES256-GCM-SHA384:EC</span><span class="hljs-string">DHE-RSA-AES256-GCM-SHA384:DHE-RSA-AES128-GCM-SHA256:DHE-RSA-AES256-GCM-SHA384:ECDHE-ECDSA-AES128-SHA256:ECDHE-RSA-AES128-SHA256:ECDHE-ECDSA-AES128-SHA:ECDHE-RSA-AES2</span><span class="hljs-string">56-SHA384:ECDHE-RSA-AES128-SHA:ECDHE-ECDSA-AES256-SHA384:ECDHE-ECDSA-AES256-SHA:ECDHE-RSA-AES256-SHA:DHE-RSA-AES128-SHA256:DHE-RSA-AES128-SHA:DHE-RSA-AES256-SHA256:D</span><span class="hljs-string">HE-RSA-AES256-SHA:ECDHE-ECDSA-DES-CBC3-SHA:ECDHE-RSA-DES-CBC3-SHA:EDH-RSA-DES-CBC3-SHA:AES128-GCM-SHA256:AES256-GCM-SHA384:AES128-SHA256:AES256-SHA256:AES128-SHA:AES</span><span class="hljs-string">256-SHA:DES-CBC3-SHA:!DSS'</span>;    ssl_prefer_server_ciphers on;    <span class="hljs-comment"># HSTS (ngx_http_headers_module is required) (15768000 seconds = 6 months)</span>    add_header Strict-Transport-Security max-age=15768000;    <span class="hljs-comment"># OCSP Stapling ---</span>    <span class="hljs-comment"># fetch OCSP records from URL in ssl_certificate and cache them</span>    ssl_stapling on;    ssl_stapling_verify on;    <span class="hljs-comment">## verify chain of trust of OCSP response using Root CA and Intermediate certs</span>    ssl_trusted_certificate /etc/nginx/ssl/mritd-ca.cer;    <span class="hljs-comment">#resolver &lt;IP DNS resolver&gt;;</span>    location / &#123;        log_not_found on;        proxy_set_header X-Forwarded-For <span class="hljs-variable">$remote_addr</span>;        proxy_set_header X-Forwarded-Proto <span class="hljs-variable">$scheme</span>;        proxy_set_header Host <span class="hljs-variable">$http_host</span>;        proxy_pass http://drone;        proxy_redirect off;        proxy_http_version 1.1;        proxy_buffering off;        chunked_transfer_encoding off;    &#125;&#125;</code></pre><p>然后访问 <code>https://YOUR_DRONE_SERVER</code> 将会自动跳转到 GitLab Auth2 授权界面，授权登录即可；随后将会返回 Drone CI 界面，界面上会列出相应的项目列表，点击后面的开关按钮来开启对应项目的 Drone CI 服务</p><p><img src="https://cdn.oss.link/markdown/6u4fk.png" srcset="/img/loading.gif" alt="drone ci project list"></p><h3 id="3-2、创建示例项目"><a href="#3-2、创建示例项目" class="headerlink" title="3.2、创建示例项目"></a>3.2、创建示例项目</h3><p>这里的示例项目为 Java 项目，采用 Gradle 构建，项目整体结构如下所示，源码可以从 <a href="">GitHub</a> 下载</p><p><img src="https://cdn.oss.link/markdown/ybrjc.png" srcset="/img/loading.gif" alt="drone test project"></p><p>将此项目推送到 GitLab 就会触发 Drone CI 自动构建(第一次肯定构建失败，具体看下面配置)</p><h3 id="3-3、Drone-CLI"><a href="#3-3、Drone-CLI" class="headerlink" title="3.3、Drone CLI"></a>3.3、Drone CLI</h3><p>这里不得不说一下官方文档真的很烂，有些东西只能自己摸索，而且各种错误提示也是烂的不能再烂，经常遇到 <code>Client Error 404:</code> 这种错误，后面任何提示信息也没有；官方文档中介绍了有些操作只能通过 cli 执行，CLI 下载需要到 GitHub 下载页下载，地址 <a href="https://github.com/drone/drone-cli/releases" target="_blank" rel="noopener">点这里</a></p><p>cli 工具下载后需要进行配置，目前只支持读取环境变量，使用前需要 <code>export</code> 以下两个变量</p><ul><li>DRONE_SERVER: Drone CI 地址</li><li>DRONE_TOKEN: cli 控制 Server 端使用的用户 Token</li></ul><p>其中 Token 可以在用户设置页面找到，如下</p><p><img src="https://cdn.oss.link/markdown/5fkvi.png" srcset="/img/loading.gif" alt="drone user token"></p><p>配置好以后就可以使用 cli 操作 CI Server 了</p><h3 id="3-4、Drone-CI-配置文件"><a href="#3-4、Drone-CI-配置文件" class="headerlink" title="3.4、Drone CI 配置文件"></a>3.4、Drone CI 配置文件</h3><p>Drone CI 对一个项目进行 CI 构建取决于两个因素，第一必须保证该项目在 Drone 控制面板中开启了构建(构建按钮开启)，第二保证项目根目录下存在 <code>.drone.yml</code>；满足这两点后每次提交 Drone 就会根据 <code>.drone.yml</code> 中配置进行按步骤构建；本示例中 <code>.drone.yml</code> 配置如下</p><pre><code class="hljs sh"><span class="hljs-built_in">clone</span>:  git:    image: plugins/gitpipeline:  backend:    image: reg.mritd.me/base/build:2.1.5    commands:      - gradle --no-daemon clean assemble    when:      branch:        event: [ push, pull_request ]        include: [ master ]        exclude: [ develop ]<span class="hljs-comment">#  rebuild-cache:</span><span class="hljs-comment">#    image: drillster/drone-volume-cache</span><span class="hljs-comment">#    rebuild: true</span><span class="hljs-comment">#    mount:</span><span class="hljs-comment">#      - ./build</span><span class="hljs-comment">#    volumes:</span><span class="hljs-comment">#      - /data/drone/$DRONE_COMMIT_SHA:/cache</span>  docker:    image: mritd/docker-kubectl:v1.8.8    commands:      - bash build_image.sh    volumes:      - /var/run/docker.sock:/var/run/docker.sock<span class="hljs-comment"># Pipeline Conditions</span>branches:  include: [ master, feature/* ]  exclude: [ develop, <span class="hljs-built_in">test</span>/* ]</code></pre><p>Drone CI 配置文件为 docker compose 的超集，<strong>Drone CI 构建思想是使用不同的阶段定义完成对 CI 流程的整体划分，然后每个阶段内定义不同的任务(task)，这些任务所有操作无论是 build、package 等全部由单独的 Docker 镜像完成，同时以 <code>plugins</code> 开头的 image 被解释为内部插件；其他的插件实际上可以看做为标准的 Docker image</strong></p><p>第一段 <code>clone</code> 配置声明了源码版本控制系统拉取方式，具体参见 <a href="http://docs.drone.io/cloning" target="_blank" rel="noopener">cloning</a>部分，定义后 Drone CI 将自动拉取源码</p><p>此后的 <code>pipeline</code> 配置段为定义整个 CI 流程段，该段中可以自定义具体 task，比如后端构建可以取名字为 <code>backend</code>，前端构建可以叫做 <code>frontend</code>；中间可以穿插辅助的如打包 docker 镜像等 task；同 GitLab CI 一样，Agent 在使用 Docker 进行构建时必然涉及到拉取私有镜像，Drone CI 想要拉取私有镜像目前仅能通过 cli 命令行进行设置，而且仅针对项目级设置(全局需要企业版…这也行)</p><pre><code class="hljs sh">drone registry add --repository drone/DroneCI-TestProject --hostname reg.mritd.me --username gitlab --password 123456</code></pre><p>在构建时需要注意一点，Drone CI 不同的 task 之间共享源码文件，<strong>也就是说如果你在第一个 task 中对源码或者编译后的发布物做了什么更改，在下一个 task 中同样可见，Drone CI 并没有 GitLab CI 在每个 task 中都进行还原的机制</strong></p><p>除此之外，某些特殊性的挂载行为默认也是不被允许的，需要在 Drone CI 中对项目做 <code>Trusted</code> 设置</p><p><img src="https://cdn.oss.link/markdown/gd60v.png" srcset="/img/loading.gif" alt="Drone Project Trusted Setting"></p><h2 id="四、与-GitLab-CI-对比"><a href="#四、与-GitLab-CI-对比" class="headerlink" title="四、与 GitLab CI 对比"></a>四、与 GitLab CI 对比</h2><p>写到这里基本接近尾声了，可能常看我博客的人现在想喷我，这篇文章确实有点水…因为我真不推荐用这玩意，未来发展倒是不确定；下面对比一下与 GitLab CI 的区别</p><p>先说一下 Drone CI 的优点，Drone CI 更加轻量级，而且也支持 HA 等设置，配置文件使用 docker compose 的方式对于玩容器多的人确实很爽，启动速度等感觉也比 GitLab CI 要快；而且我个人用 GitLab CI Docker build 的方式时也是尽量将不同功能交给不同的镜像，通过切换镜像实现不同的功能；这个思想在 Drone CI 中表现的非常明显</p><p>至于 Drone CI 的缺点，目前我最大的吐槽就是文档烂，报错烂；很多时候搞得莫名其妙，比如上来安装讲的那个管理员账户配置，我现在也没明白怎么能关闭注册启动然后添加用户(可能是我笨)；还有就是报错问题，感觉就像写代码不打 log 一样，比如 CI Server 在没有 agent 链接时，如果触发了 build 任务，Drone CI 不会报错，只会在任务上显示一个小闹钟，也没有超时…我傻傻的等了 1 小时；其他的比如全局变量、全局加密参数等都需要企业版才能支持，同时一些细节东西也缺失，比如查看当前 Server 连接的 Agent，对 Agent 打标签实现不同 task 分配等等</p><p>总结: Drone CI 目前还是个小玩具阶段，与传统 CI 基本没有抗衡之力，文档功能呢也是缺失比较严重，出问题很难排查</p>]]></content>
    
    
    <categories>
      
      <category>CI/CD</category>
      
      <category>Docker</category>
      
    </categories>
    
    
    <tags>
      
      <tag>CI/CD</tag>
      
      <tag>Linux</tag>
      
      <tag>Docker</tag>
      
      <tag>Drone</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Unix 平台下各种加速配置</title>
    <link href="/2018/03/28/unix-proxy-setting/"/>
    <url>/2018/03/28/unix-proxy-setting/</url>
    
    <content type="html"><![CDATA[<blockquote><p>本文主要阐述在 *Uinx 平台下，各种常用开发工具的加速配置，<strong>加速前提是你需要有一个能够加速的 socks5 端口，常用工具请自行搭建</strong>；本文档包括 docker、terminal、git、chrome 常用加速配置，其他工具可能后续补充</p></blockquote><h3 id="一、加速类型"><a href="#一、加速类型" class="headerlink" title="一、加速类型"></a>一、加速类型</h3><p>目前大部分工具在原始版本都是只提供 socks5 加速，常用平台一些工具已经支持手动设置加速端口，如 telegram、mega 同步客户端等等；但是某些工具并不支持 socks5，通用的加速目前各个平台只支持 http、https 设置(包括 terminal 下)；<strong>综上所述，在设置之前你至少需要保证有一个 socks5 端口能够进行加速，然后根据以下教程将 socks5 转换成 http，最后配置各个软件或系统的加速方式为 http，这也是我们常用的某些带有图形化客户端实际的背后实现</strong></p><h3 id="二、socks5-to-http"><a href="#二、socks5-to-http" class="headerlink" title="二、socks5 to http"></a>二、socks5 to http</h3><p>sock5 转 http 这里采用 privoxy 进行转换，根据各个平台不同，安装方式可能不同，主要就是包管理器的区别，以下只列举 Ubuntu、Mac 下的命令，其他平台自行 Google</p><ul><li>Mac: <code>brew install privoxy</code></li><li>Ubuntu: <code>apt-get -y install privoxy</code></li></ul><p>安装成功后，需要修改配置以指定 socks5 端口以及不代理的白名单，配置文件位置如下:</p><ul><li>Mac: <code>/usr/local/etc/privoxy/config</code></li><li>Ubuntu: <code>/etc/privoxy/config</code></li></ul><p>在修改之前请备份默认配置文件，这是个好习惯，备份后修改内容如下:</p><pre><code class="hljs sh"><span class="hljs-comment"># 转发地址</span>forward-socks5   /               127.0.0.1:1080 .<span class="hljs-comment"># 监听地址</span>listen-address  localhost:8118<span class="hljs-comment"># local network do not use proxy</span>forward         192.168.*.*/     .forward            10.*.*.*/     .forward           127.*.*.*/     .</code></pre><p><strong>其中 <code>127.0.0.1:1080</code> 为你的 socks5 ip 及 端口，<code>localhost:8118</code> 为你转换后的 http 监听地址和端口</strong>；配置完成后启动 privoxy 即可，启动命令如下:</p><ul><li>Mac: <code>brew services start privoxy</code></li><li>Ubuntu: <code>systemctl start privoxy</code></li></ul><h3 id="三、Docker-加速拉取-gcr-io-镜像"><a href="#三、Docker-加速拉取-gcr-io-镜像" class="headerlink" title="三、Docker 加速拉取 gcr.io 镜像"></a>三、Docker 加速拉取 gcr.io 镜像</h3><p>对于 docker 来说，terminal 下执行 <code>docker pull</code> 等命令实质上都是通过调用 docker daemon 操作的；而 docker daemon 是由 systemd 启动的(就目前来讲，别跟我掰什么 service start…)；对于 docker daemon 来说，一旦它启动以后就不会再接受加速设置，所以我们需要在 systemd 的 service 配置中配置它的加速。</p><p>目前 docker daemon 接受标准的终端加速设置(读取 <code>http_proxy</code>、<code>https_proxy</code>)，同时也支持 socks5 加速；为了保证配置清晰方便修改，这里采用创建单独配置文件的方式来配置 daemon 的 socks5 加速，配置脚本如下(Ubuntu、CentOS):</p><pre><code class="hljs sh"><span class="hljs-meta">#!/bin/bash</span><span class="hljs-built_in">set</span> -eOS_TYPE=<span class="hljs-variable">$1</span>PROXY_ADDRESS=<span class="hljs-variable">$2</span><span class="hljs-keyword">if</span> [ <span class="hljs-string">"<span class="hljs-variable">$&#123;PROXY_ADDRESS&#125;</span>"</span> == <span class="hljs-string">""</span> ]; <span class="hljs-keyword">then</span>    <span class="hljs-built_in">echo</span> -e <span class="hljs-string">"\033[31mError: PROXY_ADDRESS is blank!\033[0m"</span>    <span class="hljs-built_in">echo</span> -e <span class="hljs-string">"\033[32mUse: sudo <span class="hljs-variable">$0</span> centos|ubuntu 1.2.3.4:1080\033[0m"</span>    <span class="hljs-built_in">exit</span> 1<span class="hljs-keyword">fi</span><span class="hljs-keyword">if</span> [ <span class="hljs-string">"<span class="hljs-variable">$&#123;OS_TYPE&#125;</span>"</span> == <span class="hljs-string">""</span> ];<span class="hljs-keyword">then</span>    <span class="hljs-built_in">echo</span> -e <span class="hljs-string">"\033[31mError: OS_TYPE is blank!\033[0m"</span>    <span class="hljs-built_in">echo</span> -e <span class="hljs-string">"\033[32mUse: sudo <span class="hljs-variable">$0</span> centos|ubuntu\033[0m"</span>    <span class="hljs-built_in">exit</span> 1<span class="hljs-keyword">elif</span> [ <span class="hljs-string">"<span class="hljs-variable">$&#123;OS_TYPE&#125;</span>"</span> == <span class="hljs-string">"centos"</span> ];<span class="hljs-keyword">then</span>    mkdir /etc/systemd/system/docker.service.d || <span class="hljs-literal">true</span>    tee /etc/systemd/system/docker.service.d/socks5-proxy.conf &lt;&lt;-EOF[Service]Environment=<span class="hljs-string">"ALL_PROXY=socks5://<span class="hljs-variable">$&#123;PROXY_ADDRESS&#125;</span>"</span>EOF<span class="hljs-keyword">elif</span> [ <span class="hljs-string">"<span class="hljs-variable">$&#123;OS_TYPE&#125;</span>"</span> == <span class="hljs-string">"ubuntu"</span> ];<span class="hljs-keyword">then</span>    mkdir /lib/systemd/system/docker.service.d || <span class="hljs-literal">true</span>    tee /lib/systemd/system/docker.service.d/socks5-proxy.conf &lt;&lt;-EOF[Service]Environment=<span class="hljs-string">"ALL_PROXY=socks5://<span class="hljs-variable">$&#123;PROXY_ADDRESS&#125;</span>"</span>EOF<span class="hljs-keyword">fi</span>systemctl daemon-reloadsystemctl restart dockersystemctl show docker --property Environment</code></pre><p>将该脚本内容保存为 <code>docker_proxy.sh</code>，终端执行 <code>bash docker_proxy.sh ubuntu 1.2.3.4:1080</code> 即可(自行替换 socks5 地址)；脚本实际上很简单，就是创建一个与 <code>docker.service</code> 文件同级的 <code>docker.service.d</code> 目录，然后在里面写入一个 <code>socks5-proxy.conf</code>，配置内容只有两行:</p><pre><code class="hljs sh">[Service]Environment=<span class="hljs-string">"ALL_PROXY=socks5://1.2.3.4:1080</span></code></pre><p>这样 systemd 会自动读取，只需要 reload 一下，然后 restart docker daemon 即可，此后  docker 就可以通过加速端口直接 pull <code>gcr.io</code> 的镜像；<strong>注意: 配置加速后，docker 将无法 pull 私服镜像(一般私服都是内网 DNS 解析)，但是不会影响容器启动以及启动后的容器中的网络</strong></p><h3 id="四、Chrome-加速访问"><a href="#四、Chrome-加速访问" class="headerlink" title="四、Chrome 加速访问"></a>四、Chrome 加速访问</h3><p>对于 Chrome 浏览器来说，目前有比较好的插件实现用来配置根据策略的加速访问；这里使用的插件为 <code>SwitchyOmega</code></p><h4 id="4-1、SwitchyOmega-下载"><a href="#4-1、SwitchyOmega-下载" class="headerlink" title="4.1、SwitchyOmega 下载"></a>4.1、SwitchyOmega 下载</h4><p>默认情况下 <code>SwitchyOmega</code> 可以通过 Chrome 进行在线安装，但是众所周知的原因这是不可能的，不过国内有一些网站提供代理下载 Chrome 扩展的服务，如 <code>https://chrome-extension-downloader.com</code>、<code>http://yurl.sinaapp.com/crx.php</code>，这些网站只需要提供插件 ID 即可帮你下载下来；<strong><code>SwitchyOmega</code> 插件的 ID 为 <code>padekgcemlokbadohgkifijomclgjgif</code>，注意下载时不要使用 chrome 下载，因为他自身的防护机制会阻止你下载扩展程序</strong>；下载后打开 chrome 的扩展设置页，将 crx 文件拖入安装即可，如下所示:</p><p><img src="https://cdn.oss.link/markdown/zruoq.png" srcset="/img/loading.gif" alt="install chrome plugin"></p><h4 id="4-2、SwitchyOmega-配置"><a href="#4-2、SwitchyOmega-配置" class="headerlink" title="4.2、SwitchyOmega 配置"></a>4.2、SwitchyOmega 配置</h4><p>SwitchyOmega 安装成功后在 Chrome 右上角有显示，右键点击该图标，进入选项设置后如下所示:</p><p><img src="https://cdn.oss.link/markdown/ouh48.png" srcset="/img/loading.gif" alt="SwitchyOmega detail"></p><p>默认情况下左侧只有两个加速模式，一个叫做 <code>proxy</code> 另一个叫做 <code>autoproxy</code>；根据加速模式不同 SwitchyOmega 在浏览网页时选择的加速通道也不同，不同的加速方式可以通过点击 <strong>新建情景模式</strong> 按钮创建，下面介绍一下常用的两种情景模式:</p><p><strong>代理服务器:</strong> 这种情景模式创建后需要填写一个代理地址，该地址可以是 http(s)/socks5(4) 类型；创建成功后，浏览器右上角切换到该情景模式，<strong>浏览器访问所有网页的流量全部通过该代理地址发出</strong>，不论你是访问百度还是 Google</p><p> <img src="https://cdn.oss.link/markdown/idbi4.png" srcset="/img/loading.gif" alt="create test proxy1"></p><p> <img src="https://cdn.oss.link/markdown/52m7b.png" srcset="/img/loading.gif" alt="create test proxy2"></p><p><strong>自动切换模式:</strong> 这种情景模式并不需要填写实际的代理地址，而是需要填写一些规则；创建完成后插件中选择此种情景模式时，浏览器访问所有网页流量会根据填写的规则自动路由，然后选择合适的代理情景模式；可以实现智能切换代理</p><p> <img src="https://cdn.oss.link/markdown/7u6mv.png" srcset="/img/loading.gif" alt="create test auto proxy1"></p><p> <img src="https://cdn.oss.link/markdown/m5x36.png" srcset="/img/loading.gif" alt="create test auto proxy2"></p><p>综上所述，首先应该创建(或者修改默认的 proxy 情景模式)一个代理服务器的情景模式，然后填写好你的加速 IP 和对应的协议端口；接下来在浏览器中切换到该情景模式尝试访问 kubenretes.io 等网站测试加速效果；成功后再次新建一个自动切换情景模式，<strong>保证 <code>规则列表规则</code> 一栏后面的下拉列表对应到你刚刚创建的代理服务器情景模式，<code>默认情景模式</code> 后面的下拉列表对应到直接连接情景模式，然后点击下面的 <code>添加规则列表</code> 按钮，选择 <code>AutoProxy</code> 单选框，<code>规则列表网址</code> 填写 <code>https://raw.githubusercontent.com/gfwlist/gfwlist/master/gfwlist.txt</code>(这是一个开源项目收集的需要加速的网址列表)</strong>；最后在浏览器中切换到自动切换情景模式，然后访问 kubernetes.io、baidu.com 等网站测试是否能自动切换情景模式</p><h3 id="五、Terminal-加速"><a href="#五、Terminal-加速" class="headerlink" title="五、Terminal 加速"></a>五、Terminal 加速</h3><h4 id="5-1、脚本方式"><a href="#5-1、脚本方式" class="headerlink" title="5.1、脚本方式"></a>5.1、脚本方式</h4><p>对于终端下的应用程序，百分之九十的程序都会识别 <code>http_proxy</code> 和 <code>https_proxy</code> 两个变量；所以终端加速最简单的方式就是在执行命令前声明这两个变量即可，为了方便起见也可以写个小脚本，示例如下:</p><pre><code class="hljs sh">sudo tee /usr/<span class="hljs-built_in">local</span>/bin/proxy &lt;&lt;-EOF<span class="hljs-meta">#!/bin/bash</span>http_proxy=http://1.2.3.4:8118 https_proxy=http://1.2.3.4:8118 \$*EOFsudo chmod +x /usr/<span class="hljs-built_in">local</span>/bin/proxy</code></pre><p>将上面的地址自行更换成你的 http 加速地址后，终端运行 <code>proxy curl ip.cn</code> 即可测试加速效果</p><h4 id="5-2、proxychains-ng"><a href="#5-2、proxychains-ng" class="headerlink" title="5.2、proxychains-ng"></a>5.2、proxychains-ng</h4><p>proxychains-ng 是一个终端下的工具，它可以 hook libc 下的网络相关方法实现加速效果；目前支持后端为 http(s)/socks5(4a)，前段协议仅支持对 TCP 加速；</p><p>Mac 下安装方式:</p><pre><code class="hljs sh">brew install proxychains-ng</code></pre><p>Ubuntu 等平台下需要手动编译安装:</p><pre><code class="hljs sh"><span class="hljs-comment"># 安装编译依赖</span>apt-get -y install gcc make git<span class="hljs-comment"># 下载源码</span>git <span class="hljs-built_in">clone</span> https://github.com/rofl0r/proxychains-ng.git<span class="hljs-comment"># 编译安装</span><span class="hljs-built_in">cd</span> /proxychains-ng./configure --prefix=/usr --sysconfdir=/etcsudo make installsudo make install-config</code></pre><p>安装完成后编辑配置使用即可，Mac 下配置位于 <code>/usr/local/etc/proxychains.conf</code>，Ubuntu 下配置位于 <code>/etc/proxychains.conf</code>；配置修改如下:</p><pre><code class="hljs sh"><span class="hljs-comment"># 主要修改 [ProxyList] 下的加速地址</span>[ProxyList]socks5 1.2.3.4 1080</code></pre><p>然后命令行使用 <code>proxychains4 curl ip.cn</code> 测试即可</p><h3 id="六、Git-加速"><a href="#六、Git-加速" class="headerlink" title="六、Git 加速"></a>六、Git 加速</h3><p>目前 Git 的协议大致上只有三种 <code>https</code>、<code>ssh</code> 和 <code>git</code>，对于使用 <code>https</code> 方式进行 clone 和 push 操作时，可以使用第五部分 Terminal 加速方案即可实现对 Git 的加速；对于 <code>ssh</code>、<code>git</code> 协议，实际上都在调用 ssh 协议相关进行通讯(具体细节请 Google，这里的描述可能不精准)，此时同样可以使用 <code>proxychains-ng</code> 进行加速，<strong>不过需要注意 <code>proxychains-ng</code> 要自行编译安装，同时 <code>./configure</code> 增加 <code>--fat-binary</code> 选项，具体参考 <a href="https://github.com/rofl0r/proxychains-ng/issues/109" target="_blank" rel="noopener">GitHub Issue</a></strong>；<code>ssh</code>、<code>git</code> 由于都在调用 ssh 协议进行通讯，所以实际上还可以通过设置 ssh 的 <code>ProxyCommand</code> 来实现，具体操作如下:</p><pre><code class="hljs sh">sudo tee /usr/<span class="hljs-built_in">local</span>/bin/proxy-wrapper &lt;&lt;-EOF<span class="hljs-meta">#!/bin/bash</span>nc -x1.2.3.4:1080 -X5 \$*<span class="hljs-comment">#connect-proxy -S 1.2.3.4:1080 \$*</span>EOFsudo chmod +x /usr/<span class="hljs-built_in">local</span>/bin/proxy-wrappersudo tee ~/.ssh/config &lt;&lt;-EOFHost github.com    ProxyCommand /usr/<span class="hljs-built_in">local</span>/bin/proxy-wrapper <span class="hljs-string">'%h %p'</span>EOF</code></pre><p>需要注意: <strong>nc 命令是 netcat-openbsd 版本，Mac 下默认提供，Ubuntu 下需要使用 <code>apt-get install -y netcat-openbsd</code> 安装；CentOS 没有 netcat-openbsd，需要安装 EPEL 源，然后安装 connect-proxy 包，使用 connect-proxy 命令替代</strong></p>]]></content>
    
    
    <categories>
      
      <category>Linux</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Linux</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>使用 RBAC 控制 kubectl 权限</title>
    <link href="/2018/03/20/use-rbac-to-control-kubectl-permissions/"/>
    <url>/2018/03/20/use-rbac-to-control-kubectl-permissions/</url>
    
    <content type="html"><![CDATA[<blockquote><p>好久没写文章了，过年以后就有点懒… 最近也在学习 golang，再加上不断造轮子所以没太多时间；凑巧最近想控制一下 kubectl 权限，这里便记录一下。</p></blockquote><h3 id="一、RBAC-相关"><a href="#一、RBAC-相关" class="headerlink" title="一、RBAC 相关"></a>一、RBAC 相关</h3><p>相信现在大部分人用的集群已经都是 1.6 版本以上，而且在安装各种组件的时候也已经或多或少的处理过 RBAC 的东西，所以这里不做太细节性的讲述，RBAC 文档我以前胡乱翻译过一篇，请看 <a href="https://mritd.me/2017/07/17/kubernetes-rbac-chinese-translation/" target="_blank" rel="noopener">这里</a>，以下内容仅说主要的</p><h4 id="1-1、RBAC-用户角色相关"><a href="#1-1、RBAC-用户角色相关" class="headerlink" title="1.1、RBAC 用户角色相关"></a>1.1、RBAC 用户角色相关</h4><p>我在第一次接触 Kubernetes RBAC 的时候，对于基于角色控制权限这种做法是有了解的，基本结构主要就是三个:</p><ul><li>权限: 即对系统中指定资源的增删改查权限</li><li>角色: 将一定的权限组合在一起产生权限组，如管理员角色</li><li>用户: 具体的使用者，具有唯一身份标识(ID)，其后与角色绑定便拥有角色的对应权限</li></ul><p>但是翻了一会文档，最晕的就是 <strong>这个用户标识(ID)存在哪</strong>，因为传统的授权模型都是下面这样</p><p><img src="https://cdn.oss.link/markdown/sn1qp.png" srcset="/img/loading.gif" alt="ctrole"></p><p>不论怎样，在进行授权时总要有个地方存放用户信息(DB/文件)，但是在 Kubernetes 里却没找到；后来翻阅文档，找到<a href="https://kubernetes.io/docs/admin/authentication/" target="_blank" rel="noopener">这么一段</a></p><pre><code class="hljs routeros">Normal<span class="hljs-built_in"> users </span>are assumed <span class="hljs-keyword">to</span> be managed by an outside, independent service. An admin distributing private keys, a<span class="hljs-built_in"> user </span>store like Keystone <span class="hljs-keyword">or</span> Google Accounts, even a file with a list of usernames <span class="hljs-keyword">and</span> passwords.</code></pre><p><strong>也就是说，Kubernetes 是不负责维护存储用户数据的；对于 Kubernetes 来说，它识别或者说认识一个用户主要就几种方式</strong></p><ul><li>X509 Client Certs: 使用由 k8s 根 CA 签发的证书，提取 O 字段</li><li>Static Token File: 预先在 API Server 放置 Token 文件(bootstrap 阶段使用过)</li><li>Bootstrap Tokens: 一种在集群内创建的 Bootstrap 专用 Token(新的 Bootstarp 推荐)</li><li>Static Password File: 跟静态 Token 类似</li><li>Service Account Tokens: 使用 Service Account 的 Token</li></ul><p>其他不再一一列举，具体请看文档 <a href="https://kubernetes.io/docs/admin/authentication/" target="_blank" rel="noopener">Authenticating</a>；了解了这些，后面我们使用 RBAC 控制 kubectl 权限的时候就要使用如上几种方法创建对应用户</p><h4 id="1-2、RBAC-权限相关"><a href="#1-2、RBAC-权限相关" class="headerlink" title="1.2、RBAC 权限相关"></a>1.2、RBAC 权限相关</h4><p>RBAC 权限定义部分主要有三个层级</p><ul><li>apiGroups: 指定那个 API 组下的权限</li><li>resources: 该组下具体资源，如 pod 等</li><li>verbs: 指对该资源具体执行哪些动作</li></ul><p>定义一组权限(角色)时要根据其所需的真正需求做最细粒度的划分</p><h3 id="二、创建一个只读的用户"><a href="#二、创建一个只读的用户" class="headerlink" title="二、创建一个只读的用户"></a>二、创建一个只读的用户</h3><h4 id="2-1、创建用户"><a href="#2-1、创建用户" class="headerlink" title="2.1、创建用户"></a>2.1、创建用户</h4><p>首先根据上文可以得知，Kubernetes 不存储用户具体细节信息，也就是说只要通过它的那几种方式能进来的用户，Kubernetes 就认为它是合法的；那么为了让 kubectl 只读，所以我们需要先给它创建一个用来承载只读权限的用户；这里用户创建我们选择使用证书方式</p><pre><code class="hljs sh"><span class="hljs-comment"># 首先先创建一个用于签发证书的 json(证书创建使用 cfssl)</span>&#123;  <span class="hljs-string">"CN"</span>: <span class="hljs-string">"readonly"</span>,  <span class="hljs-string">"hosts"</span>: [],  <span class="hljs-string">"key"</span>: &#123;    <span class="hljs-string">"algo"</span>: <span class="hljs-string">"rsa"</span>,    <span class="hljs-string">"size"</span>: 2048  &#125;,  <span class="hljs-string">"names"</span>: [    &#123;      <span class="hljs-string">"C"</span>: <span class="hljs-string">"CN"</span>,      <span class="hljs-string">"ST"</span>: <span class="hljs-string">"BeiJing"</span>,      <span class="hljs-string">"L"</span>: <span class="hljs-string">"BeiJing"</span>,      <span class="hljs-string">"O"</span>: <span class="hljs-string">"develop:readonly"</span>,      <span class="hljs-string">"OU"</span>: <span class="hljs-string">"develop"</span>    &#125;  ]&#125;</code></pre><p>然后基于以 Kubernetes CA 证书创建只读用户的证书</p><pre><code class="hljs sh">cfssl gencert --ca /etc/kubernetes/ssl/k8s-root-ca.pem \              --ca-key /etc/kubernetes/ssl/k8s-root-ca-key.pem \              --config k8s-gencert.json \              --profile kubernetes readonly.json | \              cfssljson --bare <span class="hljs-built_in">readonly</span></code></pre><p>以上命令会生成 <code>readonly-key.pem</code>、<code>readonly.pem</code> 两个证书文件以及一个 csr 请求文件</p><h4 id="2-2、创建-kubeconfig"><a href="#2-2、创建-kubeconfig" class="headerlink" title="2.2、创建 kubeconfig"></a>2.2、创建 kubeconfig</h4><p>有了用于证明身份的证书以后，接下来创建一个 kubeconfig 文件方便 kubectl 使用</p><pre><code class="hljs sh"><span class="hljs-meta">#!/bin/bash</span>KUBE_API_SERVER=<span class="hljs-string">"https://172.16.0.18:6443"</span>CERT_DIR=<span class="hljs-variable">$&#123;2:-"/etc/kubernetes/ssl"&#125;</span>kubectl config <span class="hljs-built_in">set</span>-cluster default-cluster --server=<span class="hljs-variable">$&#123;KUBE_API_SERVER&#125;</span> \    --certificate-authority=<span class="hljs-variable">$&#123;CERT_DIR&#125;</span>/k8s-root-ca.pem \    --embed-certs=<span class="hljs-literal">true</span> \    --kubeconfig=readonly.kubeconfigkubectl config <span class="hljs-built_in">set</span>-credentials develop-readonly \    --certificate-authority=<span class="hljs-variable">$&#123;CERT_DIR&#125;</span>/k8s-root-ca.pem \    --embed-certs=<span class="hljs-literal">true</span> \    --client-key=<span class="hljs-built_in">readonly</span>-key.pem \    --client-certificate=readonly.pem \    --kubeconfig=readonly.kubeconfigkubectl config <span class="hljs-built_in">set</span>-context default-system --cluster=default-cluster \    --user=develop-readonly \    --kubeconfig=readonly.kubeconfigkubectl config use-context default-system --kubeconfig=readonly.kubeconfig</code></pre><p>这条命令会将证书也写入到 readonly.kubeconfig 配置文件中，将该文件放在 <code>~/.kube/config</code> 位置，kubectl 会自动读取</p><h4 id="2-3、创建-ClusterRole"><a href="#2-3、创建-ClusterRole" class="headerlink" title="2.3、创建 ClusterRole"></a>2.3、创建 ClusterRole</h4><p>本示例创建的只读用户权限范围为 Cluster 集群范围，所以先创建一个只读权限的 ClusterRole；创建 ClusterRole 不知道都有哪些权限的话，最简单的办法是将集群的 admin ClusterRole 保存出来，然后做修改</p><pre><code class="hljs sh"><span class="hljs-comment"># 导出 admin ClusterRole</span>kubectl get clusterrole admin -o yaml &gt; readonly.yaml</code></pre><p>这个 admin ClusterRole 是默认存在的，导出后我们根据自己需求修改就行；最基本的原则就是像 update、delete 这种权限必须删掉(我们要创建只读用户)，修改后如下</p><pre><code class="hljs yaml"><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">rbac.authorization.k8s.io/v1beta1</span><span class="hljs-attr">kind:</span> <span class="hljs-string">ClusterRole</span><span class="hljs-attr">metadata:</span>  <span class="hljs-attr">name:</span> <span class="hljs-string">cluster-readonly</span><span class="hljs-attr">rules:</span><span class="hljs-bullet">-</span> <span class="hljs-attr">apiGroups:</span>  <span class="hljs-bullet">-</span> <span class="hljs-string">""</span>  <span class="hljs-attr">resources:</span>  <span class="hljs-bullet">-</span> <span class="hljs-string">pods</span>  <span class="hljs-bullet">-</span> <span class="hljs-string">pods/attach</span>  <span class="hljs-bullet">-</span> <span class="hljs-string">pods/exec</span>  <span class="hljs-bullet">-</span> <span class="hljs-string">pods/portforward</span>  <span class="hljs-bullet">-</span> <span class="hljs-string">pods/proxy</span>  <span class="hljs-attr">verbs:</span>  <span class="hljs-bullet">-</span> <span class="hljs-string">get</span>  <span class="hljs-bullet">-</span> <span class="hljs-string">list</span>  <span class="hljs-bullet">-</span> <span class="hljs-string">watch</span><span class="hljs-bullet">-</span> <span class="hljs-attr">apiGroups:</span>  <span class="hljs-bullet">-</span> <span class="hljs-string">""</span>  <span class="hljs-attr">resources:</span>  <span class="hljs-bullet">-</span> <span class="hljs-string">configmaps</span>  <span class="hljs-bullet">-</span> <span class="hljs-string">endpoints</span>  <span class="hljs-bullet">-</span> <span class="hljs-string">persistentvolumeclaims</span>  <span class="hljs-bullet">-</span> <span class="hljs-string">replicationcontrollers</span>  <span class="hljs-bullet">-</span> <span class="hljs-string">replicationcontrollers/scale</span>  <span class="hljs-bullet">-</span> <span class="hljs-string">secrets</span>  <span class="hljs-bullet">-</span> <span class="hljs-string">serviceaccounts</span>  <span class="hljs-bullet">-</span> <span class="hljs-string">services</span>  <span class="hljs-bullet">-</span> <span class="hljs-string">services/proxy</span>  <span class="hljs-attr">verbs:</span>  <span class="hljs-bullet">-</span> <span class="hljs-string">get</span>  <span class="hljs-bullet">-</span> <span class="hljs-string">list</span>  <span class="hljs-bullet">-</span> <span class="hljs-string">watch</span><span class="hljs-bullet">-</span> <span class="hljs-attr">apiGroups:</span>  <span class="hljs-bullet">-</span> <span class="hljs-string">""</span>  <span class="hljs-attr">resources:</span>  <span class="hljs-bullet">-</span> <span class="hljs-string">bindings</span>  <span class="hljs-bullet">-</span> <span class="hljs-string">events</span>  <span class="hljs-bullet">-</span> <span class="hljs-string">limitranges</span>  <span class="hljs-bullet">-</span> <span class="hljs-string">namespaces/status</span>  <span class="hljs-bullet">-</span> <span class="hljs-string">pods/log</span>  <span class="hljs-bullet">-</span> <span class="hljs-string">pods/status</span>  <span class="hljs-bullet">-</span> <span class="hljs-string">replicationcontrollers/status</span>  <span class="hljs-bullet">-</span> <span class="hljs-string">resourcequotas</span>  <span class="hljs-bullet">-</span> <span class="hljs-string">resourcequotas/status</span>  <span class="hljs-attr">verbs:</span>  <span class="hljs-bullet">-</span> <span class="hljs-string">get</span>  <span class="hljs-bullet">-</span> <span class="hljs-string">list</span>  <span class="hljs-bullet">-</span> <span class="hljs-string">watch</span><span class="hljs-bullet">-</span> <span class="hljs-attr">apiGroups:</span>  <span class="hljs-bullet">-</span> <span class="hljs-string">""</span>  <span class="hljs-attr">resources:</span>  <span class="hljs-bullet">-</span> <span class="hljs-string">namespaces</span>  <span class="hljs-attr">verbs:</span>  <span class="hljs-bullet">-</span> <span class="hljs-string">get</span>  <span class="hljs-bullet">-</span> <span class="hljs-string">list</span>  <span class="hljs-bullet">-</span> <span class="hljs-string">watch</span><span class="hljs-bullet">-</span> <span class="hljs-attr">apiGroups:</span>  <span class="hljs-bullet">-</span> <span class="hljs-string">apps</span>  <span class="hljs-attr">resources:</span>  <span class="hljs-bullet">-</span> <span class="hljs-string">deployments</span>  <span class="hljs-bullet">-</span> <span class="hljs-string">deployments/rollback</span>  <span class="hljs-bullet">-</span> <span class="hljs-string">deployments/scale</span>  <span class="hljs-bullet">-</span> <span class="hljs-string">statefulsets</span>  <span class="hljs-attr">verbs:</span>  <span class="hljs-bullet">-</span> <span class="hljs-string">get</span>  <span class="hljs-bullet">-</span> <span class="hljs-string">list</span>  <span class="hljs-bullet">-</span> <span class="hljs-string">watch</span><span class="hljs-bullet">-</span> <span class="hljs-attr">apiGroups:</span>  <span class="hljs-bullet">-</span> <span class="hljs-string">autoscaling</span>  <span class="hljs-attr">resources:</span>  <span class="hljs-bullet">-</span> <span class="hljs-string">horizontalpodautoscalers</span>  <span class="hljs-attr">verbs:</span>  <span class="hljs-bullet">-</span> <span class="hljs-string">get</span>  <span class="hljs-bullet">-</span> <span class="hljs-string">list</span>  <span class="hljs-bullet">-</span> <span class="hljs-string">watch</span><span class="hljs-bullet">-</span> <span class="hljs-attr">apiGroups:</span>  <span class="hljs-bullet">-</span> <span class="hljs-string">batch</span>  <span class="hljs-attr">resources:</span>  <span class="hljs-bullet">-</span> <span class="hljs-string">cronjobs</span>  <span class="hljs-bullet">-</span> <span class="hljs-string">jobs</span>  <span class="hljs-bullet">-</span> <span class="hljs-string">scheduledjobs</span>  <span class="hljs-attr">verbs:</span>  <span class="hljs-bullet">-</span> <span class="hljs-string">get</span>  <span class="hljs-bullet">-</span> <span class="hljs-string">list</span>  <span class="hljs-bullet">-</span> <span class="hljs-string">watch</span><span class="hljs-bullet">-</span> <span class="hljs-attr">apiGroups:</span>  <span class="hljs-bullet">-</span> <span class="hljs-string">extensions</span>  <span class="hljs-attr">resources:</span>  <span class="hljs-bullet">-</span> <span class="hljs-string">daemonsets</span>  <span class="hljs-bullet">-</span> <span class="hljs-string">deployments</span>  <span class="hljs-bullet">-</span> <span class="hljs-string">ingresses</span>  <span class="hljs-bullet">-</span> <span class="hljs-string">replicasets</span>  <span class="hljs-attr">verbs:</span>  <span class="hljs-bullet">-</span> <span class="hljs-string">get</span>  <span class="hljs-bullet">-</span> <span class="hljs-string">list</span>  <span class="hljs-bullet">-</span> <span class="hljs-string">watch</span></code></pre><p>最后执行 <code>kubectl create -f readonly.yaml</code> 创建即可</p><h4 id="2-4、创建-ClusterRoleBinding"><a href="#2-4、创建-ClusterRoleBinding" class="headerlink" title="2.4、创建 ClusterRoleBinding"></a>2.4、创建 ClusterRoleBinding</h4><p>用户已经创建完成，集群权限也有了，接下来使用 ClusterRoleBinding 绑定到一起即可</p><pre><code class="hljs yaml"><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">rbac.authorization.k8s.io/v1beta1</span><span class="hljs-attr">kind:</span> <span class="hljs-string">ClusterRoleBinding</span><span class="hljs-attr">metadata:</span>  <span class="hljs-attr">name:</span> <span class="hljs-string">cluster-readonly</span><span class="hljs-attr">roleRef:</span>  <span class="hljs-attr">apiGroup:</span> <span class="hljs-string">rbac.authorization.k8s.io</span>  <span class="hljs-attr">kind:</span> <span class="hljs-string">ClusterRole</span>  <span class="hljs-attr">name:</span> <span class="hljs-string">cluster-readonly</span><span class="hljs-attr">subjects:</span><span class="hljs-bullet">-</span> <span class="hljs-attr">apiGroup:</span> <span class="hljs-string">rbac.authorization.k8s.io</span>  <span class="hljs-attr">kind:</span> <span class="hljs-string">Group</span>  <span class="hljs-attr">name:</span> <span class="hljs-string">develop:readonly</span></code></pre><p>将以上保存为 <code>readonly-bind.yaml</code> 执行 <code>kubectl create -f readonly-bind.yaml</code> 即可</p><h4 id="2-5、测试权限"><a href="#2-5、测试权限" class="headerlink" title="2.5、测试权限"></a>2.5、测试权限</h4><p>将最初创建的 kubeconfig 放到 <code>~/.kube/config</code> 或者直接使用 <code>--kubeconfig</code> 选项测试读取、删除 pod 等权限即可，测试后如下所示</p><p><img src="https://cdn.oss.link/markdown/68ukm.png" srcset="/img/loading.gif" alt="test readonly"></p>]]></content>
    
    
    <categories>
      
      <category>Kubernetes</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Linux</tag>
      
      <tag>Docker</tag>
      
      <tag>Kubernetes</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Kubernetes TLS bootstrapping 那点事</title>
    <link href="/2018/01/07/kubernetes-tls-bootstrapping-note/"/>
    <url>/2018/01/07/kubernetes-tls-bootstrapping-note/</url>
    
    <content type="html"><![CDATA[<blockquote><p>前段时间撸了一会 Kubernetes 官方文档，在查看 TLS bootstrapping 这块是发现已经跟 1.4 的时候完全不一样了；目前所有搭建文档也都保留着 1.4 时代的配置，在看完文档后发现目前配置有很多问题，同时也埋下了 <strong>隐藏炸弹</strong>，这个问题可能会在一年后爆发…..后果就是集群 node 全部掉线；所以仔细的撸了一下这个文档，从元旦到写此文章的时间都在测试这个 TLS bootstrapping，以下记录一下这次的成果</p></blockquote><p>阅读本文章前，请先阅读一下本文参考的相关文档:</p><ul><li><a href="https://kubernetes.io/docs/admin/kubelet-tls-bootstrapping/" target="_blank" rel="noopener">TLS bootstrapping</a></li><li><a href="https://github.com/jcbsmpsn/community/blob/a843295a4f7594d41e66a8342e174f48d06b4f9f/contributors/design-proposals/kubelet-server-certificate-bootstrap-rotation.md" target="_blank" rel="noopener">Kubelet Server Certificate Bootstrap &amp; Rotation</a></li><li><a href="https://kubernetes.io/docs/admin/authorization/rbac/" target="_blank" rel="noopener">Using RBAC Authorization</a></li></ul><h3 id="一、TLS-bootstrapping-简介"><a href="#一、TLS-bootstrapping-简介" class="headerlink" title="一、TLS bootstrapping 简介"></a>一、TLS bootstrapping 简介</h3><p>Kubernetes 在 1.4 版本(我记着是)推出了 TLS bootstrapping 功能；这个功能主要解决了以下问题:</p><p>当集群开启了 TLS 认证后，每个节点的 kubelet 组件都要使用由 apiserver 使用的 CA 签发的有效证书才能与 apiserver 通讯；此时如果节点多起来，为每个节点单独签署证书将是一件非常繁琐的事情；TLS bootstrapping 功能就是让 kubelet 先使用一个预定的低权限用户连接到 apiserver，然后向 apiserver 申请证书，kubelet 的证书由 apiserver 动态签署；在配合 RBAC 授权模型下的工作流程大致如下所示(不完整，下面细说)</p><p><img src="https://cdn.oss.link/markdown/ixtwd.png" srcset="/img/loading.gif" alt="tls_bootstrapping"></p><h3 id="二、TLS-bootstrapping-相关术语"><a href="#二、TLS-bootstrapping-相关术语" class="headerlink" title="二、TLS bootstrapping 相关术语"></a>二、TLS bootstrapping 相关术语</h3><h4 id="2-1、kubelet-server"><a href="#2-1、kubelet-server" class="headerlink" title="2.1、kubelet server"></a>2.1、kubelet server</h4><p>在官方 TLS bootstrapping 文档中多次提到过 <code>kubelet server</code> 这个东西；在经过翻阅大量文档以及 TLS bootstrapping 设计文档后得出，<strong><code>kubelet server</code> 指的应该是 kubelet 的 10250 端口；</strong></p><p><strong>kubelet 组件在工作时，采用主动的查询机制，即定期请求 apiserver 获取自己所应当处理的任务，如哪些 pod 分配到了自己身上，从而去处理这些任务；同时 kubelet 自己还会暴露出两个本身 api 的端口，用于将自己本身的私有 api 暴露出去，这两个端口分别是 10250 与 10255；对于 10250 端口，kubelet 会在其上采用 TLS 加密以提供适当的鉴权功能；对于 10255 端口，kubelet 会以只读形式暴露组件本身的私有 api，并且不做鉴权处理</strong></p><p><strong>总结一下，就是说 kubelet 上实际上有两个地方用到证书，一个是用于与 API server 通讯所用到的证书，另一个是 kubelet 的 10250 私有 api 端口需要用到的证书</strong></p><h4 id="2-2、CSR-请求类型"><a href="#2-2、CSR-请求类型" class="headerlink" title="2.2、CSR 请求类型"></a>2.2、CSR 请求类型</h4><p>kubelet 发起的 CSR 请求都是由 controller manager 来做实际签署的，对于 controller manager 来说，TLS bootstrapping 下 kubelet 发起的 CSR 请求大致分为以下三种</p><ul><li>nodeclient: kubelet 以 <code>O=system:nodes</code> 和 <code>CN=system:node:(node name)</code> 形式发起的 CSR 请求</li><li>selfnodeclient: kubelet client renew 自己的证书发起的 CSR 请求(与上一个证书就有相同的 O 和 CN)</li><li>selfnodeserver: kubelet server renew 自己的证书发起的 CSR 请求</li></ul><p><strong>大白话加自己测试得出的结果: nodeclient 类型的 CSR 仅在第一次启动时会产生，selfnodeclient 类型的 CSR 请求实际上就是 kubelet renew 自己作为 client 跟 apiserver 通讯时使用的证书产生的，selfnodeserver 类型的 CSR 请求则是 kubelet 首次申请或后续 renew 自己的 10250 api 端口证书时产生的</strong></p><h3 id="三、TLS-bootstrapping-具体引导过程"><a href="#三、TLS-bootstrapping-具体引导过程" class="headerlink" title="三、TLS bootstrapping 具体引导过程"></a>三、TLS bootstrapping 具体引导过程</h3><h4 id="3-1、Kubernetes-TLS-与-RBAC-认证"><a href="#3-1、Kubernetes-TLS-与-RBAC-认证" class="headerlink" title="3.1、Kubernetes TLS 与 RBAC 认证"></a>3.1、Kubernetes TLS 与 RBAC 认证</h4><p>在说具体的引导过程之前先谈一下 TLS 和 RBAC，因为这两个事不整明白下面的都不用谈；</p><ul><li>TLS 作用</li></ul><p>众所周知 TLS 的作用就是对通讯加密，防止中间人窃听；同时如果证书不信任的话根本就无法与 apiserver 建立连接，更不用提有没有权限向 apiserver 请求指定内容</p><ul><li>RBAC 作用</li></ul><p>当 TLS 解决了通讯问题后，那么权限问题就应由 RBAC 解决(可以使用其他权限模型，如 ABAC)；RBAC 中规定了一个用户或者用户组(subject)具有请求哪些 api 的权限；<strong>在配合 TLS 加密的时候，实际上 apiserver 读取客户端证书的 CN 字段作为用户名，读取 O 字段作为用户组</strong></p><p>从以上两点上可以总结出两点: 第一，想要与 apiserver 通讯就必须采用由 apiserver CA 签发的证书，这样才能形成信任关系，建立 TLS 连接；第二，可以通过证书的 CN、O 字段来提供 RBAC 所需的用户与用户组</p><h4 id="3-2、kubelet-首次启动流程"><a href="#3-2、kubelet-首次启动流程" class="headerlink" title="3.2、kubelet 首次启动流程"></a>3.2、kubelet 首次启动流程</h4><p>看完上面的介绍，不知道有没有人想过，既然 TLS bootstrapping 功能是让 kubelet 组件去 apiserver 申请证书，然后用于连接 apiserver；<strong>那么第一次启动时没有证书如何连接 apiserver ?</strong></p><p>这个问题实际上可以去查看一下 <code>bootstrap.kubeconfig</code> 和 <code>token.csv</code> 得到答案: <strong>在 apiserver 配置中指定了一个 <code>token.csv</code> 文件，该文件中是一个预设的用户配置；同时该用户的 Token 和 apiserver 的 CA 证书被写入了 kubelet 所使用的 <code>bootstrap.kubeconfig</code> 配置文件中；这样在首次请求时，kubelet 使用 <code>bootstrap.kubeconfig</code> 中的 apiserver CA 证书来与 apiserver 建立 TLS 通讯，使用 <code>bootstrap.kubeconfig</code> 中的用户 Token 来向 apiserver 声明自己的 RBAC 授权身份</strong>，如下图所示</p><p><img src="https://cdn.oss.link/markdown/ji5ug.png" srcset="/img/loading.gif" alt="first_request"></p><p>在有些用户首次启动时，可能与遇到 kubelet 报 401 无权访问 apiserver 的错误；<strong>这是因为在默认情况下，kubelet 通过 <code>bootstrap.kubeconfig</code> 中的预设用户 Token 声明了自己的身份，然后创建 CSR 请求；但是不要忘记这个用户在我们不处理的情况下他没任何权限的，包括创建 CSR 请求；所以需要如下命令创建一个 ClusterRoleBinding，将预设用户 <code>kubelet-bootstrap</code> 与内置的 ClusterRole <code>system:node-bootstrapper</code> 绑定到一起，使其能够发起 CSR 请求</strong></p><pre><code class="hljs sh">kubectl create clusterrolebinding kubelet-bootstrap \  --clusterrole=system:node-bootstrapper \  --user=kubelet-bootstrap</code></pre><h4 id="3-3、手动签发证书"><a href="#3-3、手动签发证书" class="headerlink" title="3.3、手动签发证书"></a>3.3、手动签发证书</h4><p>在 kubelet 首次启动后，如果用户 Token 没问题，并且 RBAC 也做了相应的设置，那么此时在集群内应该能看到 kubelet 发起的 CSR 请求</p><p><img src="https://cdn.oss.link/markdown/n9bbw.png" srcset="/img/loading.gif" alt="bootstrap_csr"></p><p>出现 CSR 请求后，可以使用 kubectl 手动签发(允许) kubelet 的证书</p><p><img src="https://cdn.oss.link/markdown/5ssf8.png" srcset="/img/loading.gif" alt="bootstrap_approve_crt"></p><p><strong>当成功签发证书后，目标节点的 kubelet 会将证书写入到 <code>--cert-dir=</code> 选项指定的目录中；注意此时如果不做其他设置应当生成四个文件</strong></p><p><img src="https://cdn.oss.link/markdown/a25ip.png" srcset="/img/loading.gif" alt="bootstrap_crt"></p><p><strong>而 kubelet 与 apiserver 通讯所使用的证书为 <code>kubelet-client.crt</code>，剩下的 <code>kubelet.crt</code> 将会被用于 <code>kubelet server</code>(10250) 做鉴权使用；注意，此时 <code>kubelet.crt</code> 这个证书是个独立于 apiserver CA 的自签 CA，并且删除后 kubelet 组件会重新生成它</strong></p><h3 id="四、TLS-bootstrapping-证书自动续期"><a href="#四、TLS-bootstrapping-证书自动续期" class="headerlink" title="四、TLS bootstrapping 证书自动续期"></a>四、TLS bootstrapping 证书自动续期</h3><blockquote><p>单独把这部分拿出来写，是因为个人觉得上面已经有点乱了；这部分实际上更复杂，只好单独写一下了，因为这部分涉及的东西比较多，所以也不想草率的几笔带过</p></blockquote><h4 id="4-1、RBAC-授权"><a href="#4-1、RBAC-授权" class="headerlink" title="4.1、RBAC 授权"></a>4.1、RBAC 授权</h4><p>首先…首先好几次了…嗯，就是说 kubelet 所发起的 CSR 请求是由 controller manager 签署的；如果想要是实现自动续期，就需要让 controller manager 能够在 kubelet 发起证书请求的时候自动帮助其签署证书；那么 controller manager 不可能对所有的 CSR 证书申请都自动签署，这时候就需要配置 RBAC 规则，<strong>保证 controller manager 只对 kubelet 发起的特定 CSR 请求自动批准即可</strong>；在 TLS bootstrapping 官方文档中，针对上面 2.2 章节提出的 3 种 CSR 请求分别给出了 3 种对应的 ClusterRole，如下所示</p><pre><code class="hljs yaml"><span class="hljs-comment"># A ClusterRole which instructs the CSR approver to approve a user requesting</span><span class="hljs-comment"># node client credentials.</span><span class="hljs-attr">kind:</span> <span class="hljs-string">ClusterRole</span><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">rbac.authorization.k8s.io/v1</span><span class="hljs-attr">metadata:</span>  <span class="hljs-attr">name:</span> <span class="hljs-string">approve-node-client-csr</span><span class="hljs-attr">rules:</span><span class="hljs-bullet">-</span> <span class="hljs-attr">apiGroups:</span> <span class="hljs-string">["certificates.k8s.io"]</span>  <span class="hljs-attr">resources:</span> <span class="hljs-string">["certificatesigningrequests/nodeclient"]</span>  <span class="hljs-attr">verbs:</span> <span class="hljs-string">["create"]</span><span class="hljs-meta">---</span><span class="hljs-comment"># A ClusterRole which instructs the CSR approver to approve a node renewing its</span><span class="hljs-comment"># own client credentials.</span><span class="hljs-attr">kind:</span> <span class="hljs-string">ClusterRole</span><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">rbac.authorization.k8s.io/v1</span><span class="hljs-attr">metadata:</span>  <span class="hljs-attr">name:</span> <span class="hljs-string">approve-node-client-renewal-csr</span><span class="hljs-attr">rules:</span><span class="hljs-bullet">-</span> <span class="hljs-attr">apiGroups:</span> <span class="hljs-string">["certificates.k8s.io"]</span>  <span class="hljs-attr">resources:</span> <span class="hljs-string">["certificatesigningrequests/selfnodeclient"]</span>  <span class="hljs-attr">verbs:</span> <span class="hljs-string">["create"]</span><span class="hljs-meta">---</span><span class="hljs-comment"># A ClusterRole which instructs the CSR approver to approve a node requesting a</span><span class="hljs-comment"># serving cert matching its client cert.</span><span class="hljs-attr">kind:</span> <span class="hljs-string">ClusterRole</span><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">rbac.authorization.k8s.io/v1</span><span class="hljs-attr">metadata:</span>  <span class="hljs-attr">name:</span> <span class="hljs-string">approve-node-server-renewal-csr</span><span class="hljs-attr">rules:</span><span class="hljs-bullet">-</span> <span class="hljs-attr">apiGroups:</span> <span class="hljs-string">["certificates.k8s.io"]</span>  <span class="hljs-attr">resources:</span> <span class="hljs-string">["certificatesigningrequests/selfnodeserver"]</span>  <span class="hljs-attr">verbs:</span> <span class="hljs-string">["create"]</span></code></pre><p>RBAC 中 ClusterRole 只是描述或者说定义一种集群范围内的能力，这三个 ClusterRole 在 1.7 之前需要自己手动创建，在 1.8 后 apiserver 会自动创建前两个(1.8 以后名称有改变，自己查看文档)；以上三个 ClusterRole 含义如下</p><ul><li>approve-node-client-csr: 具有自动批准 nodeclient 类型 CSR 请求的能力</li><li>approve-node-client-renewal-csr: 具有自动批准 selfnodeclient 类型 CSR 请求的能力</li><li>approve-node-server-renewal-csr: 具有自动批准 selfnodeserver 类型 CSR 请求的能力</li></ul><p><strong>所以，如果想要 kubelet 能够自动续期，那么就应当将适当的 ClusterRole 绑定到 kubelet 自动续期时所所采用的用户或者用户组身上</strong></p><h4 id="4-2、自动续期下的引导过程"><a href="#4-2、自动续期下的引导过程" class="headerlink" title="4.2、自动续期下的引导过程"></a>4.2、自动续期下的引导过程</h4><p>在自动续期下引导过程与单纯的手动批准 CSR 有点差异，具体的引导流程地址如下</p><ul><li>kubelet 读取 bootstrap.kubeconfig，使用其 CA 与 Token 向 apiserver 发起第一次 CSR 请求(nodeclient)</li><li>apiserver 根据 RBAC 规则自动批准首次 CSR 请求(approve-node-client-csr)，并下发证书(kubelet-client.crt)</li><li>kubelet <strong>使用刚刚签发的证书(O=system:nodes, CN=system:node:NODE_NAME)</strong>与 apiserver 通讯，并发起申请 10250 server 所使用证书的 CSR 请求</li><li>apiserver 根据 RBAC 规则自动批准 kubelet 为其 10250 端口申请的证书(kubelet-server-current.crt)</li><li>证书即将到期时，kubelet 自动向 apiserver 发起用于与 apiserver 通讯所用证书的 renew CSR 请求和 renew 本身 10250 端口所用证书的 CSR 请求</li><li>apiserver 根据 RBAC 规则自动批准两个证书</li><li>kubelet 拿到新证书后关闭所有连接，reload 新证书，以后便一直如此</li></ul><p><strong>从以上流程我们可以看出，我们如果要创建 RBAC 规则，则至少能满足四种情况:</strong></p><ul><li>自动批准 kubelet 首次用于与 apiserver 通讯证书的 CSR 请求(nodeclient)</li><li>自动批准 kubelet 首次用于 10250 端口鉴权的 CSR 请求(实际上这个请求走的也是 selfnodeserver 类型 CSR)</li><li>自动批准 kubelet 后续 renew 用于与 apiserver 通讯证书的 CSR 请求(selfnodeclient)</li><li>自动批准 kubelet 后续 renew 用于 10250 端口鉴权的 CSR 请求(selfnodeserver)</li></ul><p>基于以上四种情况，我们需要创建 3 个 ClusterRoleBinding，创建如下</p><pre><code class="hljs sh"><span class="hljs-comment"># 自动批准 kubelet 的首次 CSR 请求(用于与 apiserver 通讯的证书)</span>kubectl create clusterrolebinding node-client-auto-approve-csr --clusterrole=approve-node-client-csr --group=system:bootstrappers<span class="hljs-comment"># 自动批准 kubelet 后续 renew 用于与 apiserver 通讯证书的 CSR 请求</span>kubectl create clusterrolebinding node-client-auto-renew-crt --clusterrole=approve-node-client-renewal-csr --group=system:nodes<span class="hljs-comment"># 自动批准 kubelet 发起的用于 10250 端口鉴权证书的 CSR 请求(包括后续 renew)</span>kubectl create clusterrolebinding node-server-auto-renew-crt --clusterrole=approve-node-server-renewal-csr --group=system:nodes</code></pre><h4 id="4-3、开启自动续期"><a href="#4-3、开启自动续期" class="headerlink" title="4.3、开启自动续期"></a>4.3、开启自动续期</h4><p>在 1.7 后，kubelet 启动时增加 <code>--feature-gates=RotateKubeletClientCertificate=true,RotateKubeletServerCertificate=true</code> 选项，则 kubelet 在证书即将到期时会自动发起一个 renew 自己证书的 CSR 请求；同时 controller manager 需要在启动时增加 <code>--feature-gates=RotateKubeletServerCertificate=true</code> 参数，再配合上面创建好的 ClusterRoleBinding，kubelet client 和 kubelet server 证才书会被自动签署；</p><p><strong>注意，1.7 版本设置自动续期参数后，新的 renew 请求不会立即开始，而是在证书总有效期的 <code>70%~90%</code> 的时间时发起；而且经测试 1.7 版本即使自动签发了证书，kubelet 在不重启的情况下不会重新应用新证书；在 1.8 后 kubelet 组件在增加一个 <code>--rotate-certificates</code> 参数后，kubelet 才会自动重载新证书</strong></p><h4 id="4-3、证书过期问题"><a href="#4-3、证书过期问题" class="headerlink" title="4.3、证书过期问题"></a>4.3、证书过期问题</h4><p>需要重复强调一个问题是: <strong>TLS bootstrapping 时的证书实际是由 kube-controller-manager 组件来签署的，也就是说证书有效期是 kube-controller-manager 组件控制的</strong>；所以在 1.7 版本以后(我查文档发现的从1.7开始有) kube-controller-manager 组件提供了一个 <code>--experimental-cluster-signing-duration</code> 参数来设置签署的证书有效时间；默认为 <code>8760h0m0s</code>，将其改为 <code>87600h0m0s</code> 即 10 年后再进行 TLS bootstrapping 签署证书即可。</p><h3 id="五、TLS-bootstrapping-总结以及详细操作"><a href="#五、TLS-bootstrapping-总结以及详细操作" class="headerlink" title="五、TLS bootstrapping 总结以及详细操作"></a>五、TLS bootstrapping 总结以及详细操作</h3><h4 id="5-1、主要流程细节"><a href="#5-1、主要流程细节" class="headerlink" title="5.1、主要流程细节"></a>5.1、主要流程细节</h4><p>kubelet 首次启动通过加载 <code>bootstrap.kubeconfig</code> 中的用户 Token 和 apiserver CA 证书发起首次 CSR 请求，这个 Token 被预先内置在 apiserver 节点的 token.csv 中，其身份为 <code>kubelet-bootstrap</code> 用户和 <code>system:bootstrappers</code> 用户组；想要首次 CSR 请求能成功(成功指的是不会被 apiserver 401 拒绝)，则需要先将 <code>kubelet-bootstrap</code> 用户和 <code>system:node-bootstrapper</code> 内置 ClusterRole 绑定；</p><p>对于首次 CSR 请求可以手动批准，也可以将 <code>system:bootstrappers</code> 用户组与 <code>approve-node-client-csr</code> ClusterRole 绑定实现自动批准(1.8 之前这个 ClusterRole 需要手动创建，1.8 后 apiserver 自动创建，并更名为 <code>system:certificates.k8s.io:certificatesigningrequests:nodeclient</code>)</p><p>默认签署的的证书只有 1 年有效期，如果想要调整证书有效期可以通过设置 kube-controller-manager 的 <code>--experimental-cluster-signing-duration</code> 参数实现，该参数默认值为 <code>8760h0m0s</code></p><p>对于证书自动续签，需要通过协调两个方面实现；第一，想要 kubelet 在证书到期后自动发起续期请求，则需要在 kubelet 启动时增加 <code>--feature-gates=RotateKubeletClientCertificate=true,RotateKubeletServerCertificate=true</code> 来实现；第二，想要让 controller manager 自动批准续签的 CSR 请求需要在 controller manager 启动时增加 <code>--feature-gates=RotateKubeletServerCertificate=true</code> 参数，并绑定对应的 RBAC 规则；<strong>同时需要注意的是 1.7 版本的 kubelet 自动续签后需要手动重启 kubelet 以使其重新加载新证书，而 1.8 后只需要在 kublet 启动时附带 <code>--rotate-certificates</code> 选项就会自动重新加载新证书</strong></p><h4 id="5-2、证书及配置文件作用"><a href="#5-2、证书及配置文件作用" class="headerlink" title="5.2、证书及配置文件作用"></a>5.2、证书及配置文件作用</h4><ul><li>token.csv</li></ul><p>该文件为一个用户的描述文件，基本格式为 <code>Token,用户名,UID,用户组</code>；这个文件在 apiserver 启动时被 apiserver 加载，然后就相当于在集群内创建了一个这个用户；接下来就可以用 RBAC 给他授权；持有这个用户 Token 的组件访问 apiserver 的时候，apiserver 根据 RBAC 定义的该用户应当具有的权限来处理相应请求</p><ul><li>bootstarp.kubeconfig</li></ul><p>该文件中内置了 token.csv 中用户的 Token，以及 apiserver CA 证书；kubelet 首次启动会加载此文件，使用 apiserver CA 证书建立与 apiserver 的 TLS 通讯，使用其中的用户 Token 作为身份标识像 apiserver 发起 CSR 请求</p><ul><li>kubelet-client.crt</li></ul><p>该文件在 kubelet 完成 TLS bootstrapping 后生成，此证书是由 controller manager 签署的，此后 kubelet 将会加载该证书，用于与 apiserver 建立 TLS 通讯，同时使用该证书的 CN 字段作为用户名，O 字段作为用户组向 apiserver 发起其他请求</p><ul><li>kubelet.crt</li></ul><p>该文件在 kubelet 完成 TLS bootstrapping 后并且<strong>没有配置 <code>--feature-gates=RotateKubeletServerCertificate=true</code> 时才会生成</strong>；这种情况下该文件为一个独立于 apiserver CA 的自签 CA 证书，有效期为 1 年；被用作 kubelet 10250 api 端口</p><ul><li>kubelet-server.crt</li></ul><p>该文件在 kubelet 完成 TLS bootstrapping 后并且<strong>配置了 <code>--feature-gates=RotateKubeletServerCertificate=true</code> 时才会生成</strong>；这种情况下该证书由 apiserver CA 签署，默认有效期同样是 1 年，被用作 kubelet 10250 api 端口鉴权</p><ul><li>kubelet-client-current.pem</li></ul><p>这是一个软连接文件，当 kubelet 配置了 <code>--feature-gates=RotateKubeletClientCertificate=true</code> 选项后，会在证书总有效期的 <code>70%~90%</code> 的时间内发起续期请求，请求被批准后会生成一个 <code>kubelet-client-时间戳.pem</code>；<code>kubelet-client-current.pem</code> 文件则始终软连接到最新的真实证书文件，除首次启动外，kubelet 一直会使用这个证书同  apiserver 通讯</p><ul><li>kubelet-server-current.pem</li></ul><p>同样是一个软连接文件，当 kubelet 配置了 <code>--feature-gates=RotateKubeletServerCertificate=true</code> 选项后，会在证书总有效期的 <code>70%~90%</code> 的时间内发起续期请求，请求被批准后会生成一个 <code>kubelet-server-时间戳.pem</code>；<code>kubelet-server-current.pem</code> 文件则始终软连接到最新的真实证书文件，该文件将会一直被用于 kubelet 10250 api 端口鉴权</p><h4 id="5-3、1-7-TLS-bootstrapping-配置"><a href="#5-3、1-7-TLS-bootstrapping-配置" class="headerlink" title="5.3、1.7 TLS bootstrapping 配置"></a>5.3、1.7 TLS bootstrapping 配置</h4><p>apiserver 预先放置 token.csv，内容样例如下</p><pre><code class="hljs sh">6df3c701f979cee17732c30958745947,kubelet-bootstrap,10001,<span class="hljs-string">"system:bootstrappers"</span></code></pre><p>允许 kubelet-bootstrap 用户创建首次启动的 CSR 请求</p><pre><code class="hljs sh">kubectl create clusterrolebinding kubelet-bootstrap \  --clusterrole=system:node-bootstrapper \  --user=kubelet-bootstrap</code></pre><p>配置 kubelet 自动续期，<strong>RotateKubeletClientCertificate 用于自动续期 kubelet 连接 apiserver 所用的证书(kubelet-client-xxxx.pem)，RotateKubeletServerCertificate 用于自动续期 kubelet 10250 api 端口所使用的证书(kubelet-server-xxxx.pem)</strong></p><pre><code class="hljs sh">KUBELET_ARGS=<span class="hljs-string">"--cgroup-driver=cgroupfs \</span><span class="hljs-string">              --cluster-dns=10.254.0.2 \</span><span class="hljs-string">              --resolv-conf=/etc/resolv.conf \</span><span class="hljs-string">              --experimental-bootstrap-kubeconfig=/etc/kubernetes/bootstrap.kubeconfig \</span><span class="hljs-string">              --feature-gates=RotateKubeletClientCertificate=true,RotateKubeletServerCertificate=true \</span><span class="hljs-string">              --kubeconfig=/etc/kubernetes/kubelet.kubeconfig \</span><span class="hljs-string">              --fail-swap-on=false \</span><span class="hljs-string">              --cert-dir=/etc/kubernetes/ssl \</span><span class="hljs-string">              --cluster-domain=cluster.local. \</span><span class="hljs-string">              --hairpin-mode=promiscuous-bridge \</span><span class="hljs-string">              --serialize-image-pulls=false \</span><span class="hljs-string">              --pod-infra-container-image=gcr.io/google_containers/pause-amd64:3.0"</span></code></pre><p>配置 controller manager 自动批准相关 CSR 请求，<strong>如果不配置 <code>--feature-gates=RotateKubeletServerCertificate=true</code> 参数，则即使配置了相关的 RBAC 规则，也只会自动批准 kubelet client 的 renew 请求</strong> </p><pre><code class="hljs sh">KUBE_CONTROLLER_MANAGER_ARGS=<span class="hljs-string">"--address=0.0.0.0 \</span><span class="hljs-string">                              --service-cluster-ip-range=10.254.0.0/16 \</span><span class="hljs-string">                              --feature-gates=RotateKubeletServerCertificate=true \</span><span class="hljs-string">                              --cluster-name=kubernetes \</span><span class="hljs-string">                              --cluster-signing-cert-file=/etc/kubernetes/ssl/k8s-root-ca.pem \</span><span class="hljs-string">                              --cluster-signing-key-file=/etc/kubernetes/ssl/k8s-root-ca-key.pem \</span><span class="hljs-string">                              --service-account-private-key-file=/etc/kubernetes/ssl/k8s-root-ca-key.pem \</span><span class="hljs-string">                              --root-ca-file=/etc/kubernetes/ssl/k8s-root-ca.pem \</span><span class="hljs-string">                              --leader-elect=true \</span><span class="hljs-string">                              --node-monitor-grace-period=40s \</span><span class="hljs-string">                              --node-monitor-period=5s \</span><span class="hljs-string">                              --pod-eviction-timeout=5m0s"</span></code></pre><p>创建自动批准相关 CSR 请求的 ClusterRole</p><pre><code class="hljs yaml"><span class="hljs-comment"># A ClusterRole which instructs the CSR approver to approve a user requesting</span><span class="hljs-comment"># node client credentials.</span><span class="hljs-attr">kind:</span> <span class="hljs-string">ClusterRole</span><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">rbac.authorization.k8s.io/v1</span><span class="hljs-attr">metadata:</span>  <span class="hljs-attr">name:</span> <span class="hljs-string">approve-node-client-csr</span><span class="hljs-attr">rules:</span><span class="hljs-bullet">-</span> <span class="hljs-attr">apiGroups:</span> <span class="hljs-string">["certificates.k8s.io"]</span>  <span class="hljs-attr">resources:</span> <span class="hljs-string">["certificatesigningrequests/nodeclient"]</span>  <span class="hljs-attr">verbs:</span> <span class="hljs-string">["create"]</span><span class="hljs-meta">---</span><span class="hljs-comment"># A ClusterRole which instructs the CSR approver to approve a node renewing its</span><span class="hljs-comment"># own client credentials.</span><span class="hljs-attr">kind:</span> <span class="hljs-string">ClusterRole</span><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">rbac.authorization.k8s.io/v1</span><span class="hljs-attr">metadata:</span>  <span class="hljs-attr">name:</span> <span class="hljs-string">approve-node-client-renewal-csr</span><span class="hljs-attr">rules:</span><span class="hljs-bullet">-</span> <span class="hljs-attr">apiGroups:</span> <span class="hljs-string">["certificates.k8s.io"]</span>  <span class="hljs-attr">resources:</span> <span class="hljs-string">["certificatesigningrequests/selfnodeclient"]</span>  <span class="hljs-attr">verbs:</span> <span class="hljs-string">["create"]</span><span class="hljs-meta">---</span><span class="hljs-comment"># A ClusterRole which instructs the CSR approver to approve a node requesting a</span><span class="hljs-comment"># serving cert matching its client cert.</span><span class="hljs-attr">kind:</span> <span class="hljs-string">ClusterRole</span><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">rbac.authorization.k8s.io/v1</span><span class="hljs-attr">metadata:</span>  <span class="hljs-attr">name:</span> <span class="hljs-string">approve-node-server-renewal-csr</span><span class="hljs-attr">rules:</span><span class="hljs-bullet">-</span> <span class="hljs-attr">apiGroups:</span> <span class="hljs-string">["certificates.k8s.io"]</span>  <span class="hljs-attr">resources:</span> <span class="hljs-string">["certificatesigningrequests/selfnodeserver"]</span>  <span class="hljs-attr">verbs:</span> <span class="hljs-string">["create"]</span></code></pre><p>将 ClusterRole 绑定到适当的用户组，以完成自动批准相关 CSR 请求</p><pre><code class="hljs sh"><span class="hljs-comment"># 自动批准 system:bootstrappers 组用户 TLS bootstrapping 首次申请证书的 CSR 请求</span>kubectl create clusterrolebinding node-client-auto-approve-csr --clusterrole=approve-node-client-csr --group=system:bootstrappers<span class="hljs-comment"># 自动批准 system:nodes 组用户更新 kubelet 自身与 apiserver 通讯证书的 CSR 请求</span>kubectl create clusterrolebinding node-client-auto-renew-crt --clusterrole=approve-node-client-renewal-csr --group=system:nodes<span class="hljs-comment"># 自动批准 system:nodes 组用户更新 kubelet 10250 api 端口证书的 CSR 请求</span>kubectl create clusterrolebinding node-server-auto-renew-crt --clusterrole=approve-node-server-renewal-csr --group=system:nodes</code></pre><p><strong>一切就绪后启动 kubelet 组件即可，不过需要注意的是 1.7 版本 kubelet 不会自动重载 renew 的证书，需要自己手动重启</strong></p><h4 id="5-4、1-8-TLS-bootstrapping-配置"><a href="#5-4、1-8-TLS-bootstrapping-配置" class="headerlink" title="5.4、1.8 TLS bootstrapping 配置"></a>5.4、1.8 TLS bootstrapping 配置</h4><p>apiserver 预先放置 token.csv，内容样例如下</p><pre><code class="hljs sh">6df3c701f979cee17732c30958745947,kubelet-bootstrap,10001,<span class="hljs-string">"system:bootstrappers"</span></code></pre><p>允许 kubelet-bootstrap 用户创建首次启动的 CSR 请求</p><pre><code class="hljs sh">kubectl create clusterrolebinding kubelet-bootstrap \  --clusterrole=system:node-bootstrapper \  --user=kubelet-bootstrap</code></pre><p>配置 kubelet 自动续期，<strong>RotateKubeletClientCertificate 用于自动续期 kubelet 连接 apiserver 所用的证书(kubelet-client-xxxx.pem)，RotateKubeletServerCertificate 用于自动续期 kubelet 10250 api 端口所使用的证书(kubelet-server-xxxx.pem)，<code>--rotate-certificates</code> 选项使得 kubelet 能够自动重载新证书</strong></p><pre><code class="hljs sh">KUBELET_ARGS=<span class="hljs-string">"--cgroup-driver=cgroupfs \</span><span class="hljs-string">              --cluster-dns=10.254.0.2 \</span><span class="hljs-string">              --resolv-conf=/etc/resolv.conf \</span><span class="hljs-string">              --experimental-bootstrap-kubeconfig=/etc/kubernetes/bootstrap.kubeconfig \</span><span class="hljs-string">              --feature-gates=RotateKubeletClientCertificate=true,RotateKubeletServerCertificate=true \</span><span class="hljs-string">              --rotate-certificates \</span><span class="hljs-string">              --kubeconfig=/etc/kubernetes/kubelet.kubeconfig \</span><span class="hljs-string">              --fail-swap-on=false \</span><span class="hljs-string">              --cert-dir=/etc/kubernetes/ssl \</span><span class="hljs-string">              --cluster-domain=cluster.local. \</span><span class="hljs-string">              --hairpin-mode=promiscuous-bridge \</span><span class="hljs-string">              --serialize-image-pulls=false \</span><span class="hljs-string">              --pod-infra-container-image=gcr.io/google_containers/pause-amd64:3.0"</span></code></pre><p>配置 controller manager 自动批准相关 CSR 请求，<strong>如果不配置 <code>--feature-gates=RotateKubeletServerCertificate=true</code> 参数，则即使配置了相关的 RBAC 规则，也只会自动批准 kubelet client 的 renew 请求</strong> </p><pre><code class="hljs sh">KUBE_CONTROLLER_MANAGER_ARGS=<span class="hljs-string">"--address=0.0.0.0 \</span><span class="hljs-string">                              --service-cluster-ip-range=10.254.0.0/16 \</span><span class="hljs-string">                              --cluster-name=kubernetes \</span><span class="hljs-string">                              --cluster-signing-cert-file=/etc/kubernetes/ssl/k8s-root-ca.pem \</span><span class="hljs-string">                              --cluster-signing-key-file=/etc/kubernetes/ssl/k8s-root-ca-key.pem \</span><span class="hljs-string">                              --service-account-private-key-file=/etc/kubernetes/ssl/k8s-root-ca-key.pem \</span><span class="hljs-string">                              --feature-gates=RotateKubeletServerCertificate=true \</span><span class="hljs-string">                              --root-ca-file=/etc/kubernetes/ssl/k8s-root-ca.pem \</span><span class="hljs-string">                              --leader-elect=true \</span><span class="hljs-string">                              --experimental-cluster-signing-duration 10m0s \</span><span class="hljs-string">                              --node-monitor-grace-period=40s \</span><span class="hljs-string">                              --node-monitor-period=5s \</span><span class="hljs-string">                              --pod-eviction-timeout=5m0s"</span></code></pre><p>创建自动批准相关 CSR 请求的 ClusterRole，相对于 1.7 版本，1.8 的 apiserver 自动创建了前两条 ClusterRole，所以只需要创建一条就行了</p><pre><code class="hljs yaml"><span class="hljs-comment"># A ClusterRole which instructs the CSR approver to approve a node requesting a</span><span class="hljs-comment"># serving cert matching its client cert.</span><span class="hljs-attr">kind:</span> <span class="hljs-string">ClusterRole</span><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">rbac.authorization.k8s.io/v1</span><span class="hljs-attr">metadata:</span>  <span class="hljs-attr">name:</span> <span class="hljs-string">system:certificates.k8s.io:certificatesigningrequests:selfnodeserver</span><span class="hljs-attr">rules:</span><span class="hljs-bullet">-</span> <span class="hljs-attr">apiGroups:</span> <span class="hljs-string">["certificates.k8s.io"]</span>  <span class="hljs-attr">resources:</span> <span class="hljs-string">["certificatesigningrequests/selfnodeserver"]</span>  <span class="hljs-attr">verbs:</span> <span class="hljs-string">["create"]</span></code></pre><p>将 ClusterRole 绑定到适当的用户组，以完成自动批准相关 CSR 请求</p><pre><code class="hljs sh"><span class="hljs-comment"># 自动批准 system:bootstrappers 组用户 TLS bootstrapping 首次申请证书的 CSR 请求</span>kubectl create clusterrolebinding node-client-auto-approve-csr --clusterrole=system:certificates.k8s.io:certificatesigningrequests:nodeclient --group=system:bootstrappers<span class="hljs-comment"># 自动批准 system:nodes 组用户更新 kubelet 自身与 apiserver 通讯证书的 CSR 请求</span>kubectl create clusterrolebinding node-client-auto-renew-crt --clusterrole=system:certificates.k8s.io:certificatesigningrequests:selfnodeclient --group=system:nodes<span class="hljs-comment"># 自动批准 system:nodes 组用户更新 kubelet 10250 api 端口证书的 CSR 请求</span>kubectl create clusterrolebinding node-server-auto-renew-crt --clusterrole=system:certificates.k8s.io:certificatesigningrequests:selfnodeserver --group=system:nodes</code></pre><p><strong>一切就绪后启动 kubelet 组件即可，1.8 版本 kubelet 会自动重载证书，以下为 1.8 版本在运行一段时间后的相关证书截图</strong></p><p><img src="https://cdn.oss.link/markdown/570wk.png" srcset="/img/loading.gif" alt="tls_bootstrapping_crts"></p>]]></content>
    
    
    <categories>
      
      <category>Kubernetes</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Linux</tag>
      
      <tag>Docker</tag>
      
      <tag>Kubernetes</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>CI/CD 之 GitLab CI</title>
    <link href="/2017/11/28/ci-cd-gitlab-ci/"/>
    <url>/2017/11/28/ci-cd-gitlab-ci/</url>
    
    <content type="html"><![CDATA[<blockquote><p>接着上篇文章整理，这篇文章主要介绍一下 GitLab CI 相关功能，并通过 GitLab CI 实现自动化构建项目；项目中所用的示例项目已经上传到了 <a href="https://github.com/mritd/GitLabCI-TestProject" target="_blank" rel="noopener">GitHub</a></p></blockquote><h3 id="一、环境准备"><a href="#一、环境准备" class="headerlink" title="一、环境准备"></a>一、环境准备</h3><p>首先需要有一台 GitLab 服务器，然后需要有个项目；这里示例项目以 Spring Boot 项目为例，然后最好有一台专门用来 Build 的机器，实际生产中如果 Build 任务不频繁可适当用一些业务机器进行 Build；本文示例所有组件将采用 Docker 启动， GitLab HA 等不在本文阐述范围内</p><ul><li>Docker Version : 1.13.1</li><li>GitLab Version : 10.1.4-ce.0</li><li>GitLab Runner Version : 10.1.0</li><li>GitLab IP : 172.16.0.37</li><li>GitLab Runner IP : 172.16.0.36</li></ul><h3 id="二、GitLab-CI-简介"><a href="#二、GitLab-CI-简介" class="headerlink" title="二、GitLab CI 简介"></a>二、GitLab CI 简介</h3><p>GitLab CI 是 GitLab 默认集成的 CI 功能，GitLab CI 通过在项目内 <code>.gitlab-ci.yaml</code> 配置文件读取 CI 任务并进行相应处理；GitLab CI 通过其称为 GitLab Runner 的 Agent 端进行 build 操作；Runner 本身可以使用多种方式安装，比如使用 Docker 镜像启动等；Runner 在进行 build 操作时也可以选择多种 build 环境提供者；比如直接在 Runner 所在宿主机 build、通过新创建虚拟机(vmware、virtualbox)进行 build等；同时 Runner 支持 Docker 作为 build 提供者，即每次 build 新启动容器进行 build；GitLab CI 其大致架构如下</p><p><img src="https://cdn.oss.link/markdown/wejnz.png" srcset="/img/loading.gif" alt="GitLab"></p><h3 id="三、搭建-GitLab-服务器"><a href="#三、搭建-GitLab-服务器" class="headerlink" title="三、搭建 GitLab 服务器"></a>三、搭建 GitLab 服务器</h3><h4 id="3-1、GitLab-搭建"><a href="#3-1、GitLab-搭建" class="headerlink" title="3.1、GitLab 搭建"></a>3.1、GitLab 搭建</h4><p>GitLab 搭建这里直接使用 docker compose 启动，compose 配置如下</p><pre><code class="hljs sh">version: <span class="hljs-string">'2'</span>services:  gitlab:    image: <span class="hljs-string">'gitlab/gitlab-ce:10.1.4-ce.0'</span>    restart: always    container_name: gitlab    hostname: <span class="hljs-string">'git.mritd.me'</span>    environment:      GITLAB_OMNIBUS_CONFIG: |        external_url <span class="hljs-string">'http://git.mritd.me'</span>        <span class="hljs-comment"># Add any other gitlab.rb configuration here, each on its own line</span>    ports:      - <span class="hljs-string">'80:80'</span>      - <span class="hljs-string">'443:443'</span>      - <span class="hljs-string">'8022:22'</span>    volumes:      - <span class="hljs-string">'./data/gitlab/config:/etc/gitlab'</span>      - <span class="hljs-string">'./data/gitlab/logs:/var/log/gitlab'</span>      - <span class="hljs-string">'./data/gitlab/data:/var/opt/gitlab'</span></code></pre><p>直接启动后，首次登陆需要设置初始密码如下，默认用户为 <code>root</code></p><p><img src="https://cdn.oss.link/markdown/5go94.png" srcset="/img/loading.gif" alt="gitkab init"></p><p>登陆成功后创建一个用户(该用户最好给予 Admin 权限，以后操作以该用户为例)，并且创建一个测试 Group 和 Project，如下所示</p><p><img src="https://cdn.oss.link/markdown/vtyhi.png" srcset="/img/loading.gif" alt="Create User"></p><p><img src="https://cdn.oss.link/markdown/3b7gl.png" srcset="/img/loading.gif" alt="Test Project"></p><h4 id="3-2、增加示例项目"><a href="#3-2、增加示例项目" class="headerlink" title="3.2、增加示例项目"></a>3.2、增加示例项目</h4><p>这里示例项目采用 Java 的 SpringBoot 项目，并采用 Gradle 构建，其他语言原理一样；<strong>如果不熟悉 Java 的没必要死磕此步配置，任意语言(最好 Java)整一个能用的 Web 项目就行，并不强求一定 Java 并且使用 Gradle 构建，以下只是一个样例项目</strong>；SpringBoot 可以采用 <a href="https://start.spring.io/" target="_blank" rel="noopener">Spring Initializr</a> 直接生成(依赖要加入 WEB)，如下所示</p><p><img src="https://cdn.oss.link/markdown/0wx6d.png" srcset="/img/loading.gif" alt="Spring Initializr"></p><p>将项目导入 IDEA，然后创建一个 index 示例页面，主要修改如下</p><ul><li>build.gradle</li></ul><pre><code class="hljs groovy">buildscript &#123;    ext &#123;        springBootVersion = <span class="hljs-string">'1.5.8.RELEASE'</span>    &#125;    repositories &#123;        mavenCentral()    &#125;    dependencies &#123;        classpath(<span class="hljs-string">"org.springframework.boot:spring-boot-gradle-plugin:$&#123;springBootVersion&#125;"</span>)    &#125;&#125;apply <span class="hljs-string">plugin:</span> <span class="hljs-string">'java'</span>apply <span class="hljs-string">plugin:</span> <span class="hljs-string">'eclipse'</span>apply <span class="hljs-string">plugin:</span> <span class="hljs-string">'idea'</span>apply <span class="hljs-string">plugin:</span> <span class="hljs-string">'org.springframework.boot'</span>group = <span class="hljs-string">'me.mritd'</span>version = <span class="hljs-string">'0.0.1-SNAPSHOT'</span>sourceCompatibility = <span class="hljs-number">1.8</span>repositories &#123;    mavenCentral()&#125;dependencies &#123;    compile(<span class="hljs-string">'org.springframework.boot:spring-boot-starter'</span>)    compile(<span class="hljs-string">'org.springframework.boot:spring-boot-starter-web'</span>)    compile(<span class="hljs-string">'org.springframework.boot:spring-boot-starter-thymeleaf'</span>)    testCompile(<span class="hljs-string">'org.springframework.boot:spring-boot-starter-test'</span>)&#125;</code></pre><ul><li>新建一个 HomeController</li></ul><pre><code class="hljs java"><span class="hljs-keyword">package</span> me.mritd.TestProject;<span class="hljs-keyword">import</span> org.springframework.stereotype.Controller;<span class="hljs-keyword">import</span> org.springframework.web.bind.annotation.RequestMapping;<span class="hljs-comment">/*******************************************************************************</span><span class="hljs-comment"> * Copyright (c) 2005-2017 Mritd, Inc.</span><span class="hljs-comment"> * TestProject</span><span class="hljs-comment"> * me.mritd.TestProject</span><span class="hljs-comment"> * Created by mritd on 2017/11/24 下午12:23.</span><span class="hljs-comment"> * Description: </span><span class="hljs-comment"> *******************************************************************************/</span><span class="hljs-meta">@Controller</span><span class="hljs-keyword">public</span> <span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">HomeController</span> </span>&#123;    <span class="hljs-meta">@RequestMapping</span>(<span class="hljs-string">"/"</span>)    <span class="hljs-function"><span class="hljs-keyword">public</span> String <span class="hljs-title">home</span><span class="hljs-params">()</span></span>&#123;        <span class="hljs-keyword">return</span> <span class="hljs-string">"index"</span>;    &#125;&#125;</code></pre><ul><li>templates 下新建 index.html</li></ul><pre><code class="hljs html"><span class="hljs-meta">&lt;!DOCTYPE <span class="hljs-meta-keyword">html</span>&gt;</span><span class="hljs-tag">&lt;<span class="hljs-name">html</span> <span class="hljs-attr">lang</span>=<span class="hljs-string">"en"</span>&gt;</span><span class="hljs-tag">&lt;<span class="hljs-name">head</span>&gt;</span>    <span class="hljs-tag">&lt;<span class="hljs-name">meta</span> <span class="hljs-attr">charset</span>=<span class="hljs-string">"UTF-8"</span>/&gt;</span>    <span class="hljs-tag">&lt;<span class="hljs-name">title</span>&gt;</span>Title<span class="hljs-tag">&lt;/<span class="hljs-name">title</span>&gt;</span><span class="hljs-tag">&lt;/<span class="hljs-name">head</span>&gt;</span><span class="hljs-tag">&lt;<span class="hljs-name">body</span>&gt;</span><span class="hljs-tag">&lt;<span class="hljs-name">h1</span>&gt;</span>Test...<span class="hljs-tag">&lt;/<span class="hljs-name">h1</span>&gt;</span><span class="hljs-tag">&lt;/<span class="hljs-name">body</span>&gt;</span><span class="hljs-tag">&lt;/<span class="hljs-name">html</span>&gt;</span></code></pre><p>最后项目整体结构如下</p><p><img src="https://cdn.oss.link/markdown/5k12p.png" srcset="/img/loading.gif" alt="TestProject"></p><p>执行 <code>assemble</code> Task 打包出可执行 jar 包，并运行 <code>java -jar TestProject-0.0.1-SNAPSHOT.jar</code> 测试下能启动访问页面即可</p><p><img src="https://cdn.oss.link/markdown/xoj3d.png" srcset="/img/loading.gif" alt="TestProject assemble"></p><p>最后将项目提交到 GitLab 后如下</p><p><img src="https://cdn.oss.link/markdown/1fuex.png" srcset="/img/loading.gif" alt="init Project"></p><h3 id="四、GitLab-CI-配置"><a href="#四、GitLab-CI-配置" class="headerlink" title="四、GitLab CI 配置"></a>四、GitLab CI 配置</h3><blockquote><p>针对这一章节创建基础镜像以及项目镜像，这里仅以 Java 项目为例；其他语言原理相通，按照其他语言对应的运行环境修改即可</p></blockquote><h4 id="4-1、增加-Runner"><a href="#4-1、增加-Runner" class="headerlink" title="4.1、增加 Runner"></a>4.1、增加 Runner</h4><p>GitLab CI 在进行构建时会将任务下发给 Runner，让 Runner 去执行；所以先要添加一个 Runner，Runner 这里采用 Docker Compose 启动，build 方式也使用 Docker 方式 Build；compose 文件如下</p><pre><code class="hljs yaml"><span class="hljs-attr">version:</span> <span class="hljs-string">'2'</span><span class="hljs-attr">services:</span>  <span class="hljs-attr">gitlab-runner:</span>    <span class="hljs-attr">container_name:</span> <span class="hljs-string">gitlab-runner</span>    <span class="hljs-attr">image:</span> <span class="hljs-string">gitlab/gitlab-runner:alpine-v10.1.0</span>    <span class="hljs-attr">restart:</span> <span class="hljs-string">always</span>    <span class="hljs-attr">network_mode:</span> <span class="hljs-string">"host"</span>    <span class="hljs-attr">volumes:</span>      <span class="hljs-bullet">-</span> <span class="hljs-string">/var/run/docker.sock:/var/run/docker.sock</span>      <span class="hljs-bullet">-</span> <span class="hljs-string">./config.toml:/etc/gitlab-runner/config.toml</span>    <span class="hljs-attr">extra_hosts:</span>      <span class="hljs-bullet">-</span> <span class="hljs-string">"git.mritd.me:172.16.0.37"</span></code></pre><p><strong>在启动前，我们需要先 touch 一下这个 config.toml 配置文件</strong>；该文件是 Runner 的运行配置，此后 Runner 所有配置都会写入这个文件(不 touch 出来 docker-compose 发现不存在会挂载一个目录进去，导致 Runner 启动失败)；启动 docker-compose 后，<strong>需要进入容器执行注册，让 Runner 主动去连接 GitLab 服务器</strong></p><pre><code class="hljs sh"><span class="hljs-comment"># 生成 Runner 配置文件</span>touch config.toml<span class="hljs-comment"># 启动 Runner</span>docker-compose up -d<span class="hljs-comment"># 激活 Runner</span>docker <span class="hljs-built_in">exec</span> -it gitlab-runner gitlab-runner register</code></pre><p>在执行上一条激活命令后，会按照提示让你输入一些信息；<strong>首先输入 GitLab 地址，然后是 Runner Token，Runner Token 可以从 GitLab 设置中查看</strong>，如下所示</p><p><img src="https://cdn.oss.link/markdown/mfqg7.png" srcset="/img/loading.gif" alt="Runner Token"></p><p>整体注册流程如下</p><p><img src="https://cdn.oss.link/markdown/r7xay.png" srcset="/img/loading.gif" alt="Runner registry"></p><p>注册完成后，在 GitLab Runner 设置中就可以看到刚刚注册的 Runner，如下所示</p><p><img src="https://cdn.oss.link/markdown/xv03e.png" srcset="/img/loading.gif" alt="Runner List"></p><p><strong>Runner 注册成功后会将配置写入到 config.toml 配置文件；由于两个测试宿主机都没有配置内网 DNS，所以为了保证 runner 在使用 docker build 时能正确的找到 GitLab 仓库地址，还需要增加一个 docker 的 host 映射( <code>extra_hosts</code> )；同时为了能调用 宿主机 Docker 和持久化 build 的一些缓存还挂载了一些文件和目录；完整的 配置如下(配置文件可以做一些更高级的配置，具体参考 <a href="https://docs.gitlab.com/runner/configuration/advanced-configuration.html" target="_blank" rel="noopener">官方文档</a> )</strong></p><ul><li>config.toml</li></ul><pre><code class="hljs toml"><span class="hljs-attr">concurrent</span> = <span class="hljs-number">1</span><span class="hljs-attr">check_interval</span> = <span class="hljs-number">0</span><span class="hljs-section">[[runners]]</span>  name = "Test Runner"  url = "http://git.mritd.me"  token = "c279ec1ac08aec98c7141c7cf2d474"  executor = "docker"  builds_dir = "/gitlab/runner-builds"  cache_dir = "/gitlab/runner-cache"  <span class="hljs-section">[runners.docker]</span>    tls_verify = false    image = "debian"    privileged = false    disable_cache = false    shm_size = 0    volumes = ["/data/gitlab-runner:/gitlab","/var/run/docker.sock:/var/run/docker.sock","/data/maven_repo:/data/repo","/data/maven_repo:/data/maven","/data/gradle:/data/gradle","/data/sonar_cache:/root/.sonar","/data/androidsdk:/usr/local/android","/data/node_modules:/data/node_modules"]    extra_hosts = ["git.mritd.me:172.16.0.37"]  <span class="hljs-section">[runners.cache]</span></code></pre><p><strong>注意，这里声明的 Volumes 会在每个运行的容器中都生效；也就是说 build 时新开启的每个容器都会被挂载这些目录</strong>；修改完成后重启 runner 容器即可，由于 runner 中没啥可保存的东西，所以可以直接 <code>docker-compose down &amp;&amp; docker-compose up -d</code> 重启</p><h4 id="4-2、创建基础镜像"><a href="#4-2、创建基础镜像" class="headerlink" title="4.2、创建基础镜像"></a>4.2、创建基础镜像</h4><p>由于示例项目是一个 Java 项目，而且是采用 Spring Boot 的，所以该项目想要运行起来只需要一个 java 环境即可，中间件已经被打包到了 jar 包中；以下是一个作为基础运行环境的 openjdk 镜像的 Dockerfile</p><pre><code class="hljs sh">FROM alpine:edge LABEL maintainer=<span class="hljs-string">"mritd &lt;mritd1234@gmail.com&gt;"</span>ENV JAVA_HOME /usr/lib/jvm/java-1.8-openjdkENV PATH <span class="hljs-variable">$PATH</span>:/usr/lib/jvm/java-1.8-openjdk/jre/bin:/usr/lib/jvm/java-1.8-openjdk/binRUN apk add --update bash curl tar wget ca-certificates unzip \        openjdk8 font-adobe-100dpi ttf-dejavu fontconfig \    &amp;&amp; rm -rf /var/cache/apk/* \CMD [<span class="hljs-string">"bash"</span>]</code></pre><p><strong>这个 openjdk Dockerfile 升级到了 8.151 版本，并且集成了一些字体相关的软件，以解决在 Java 中某些验证码库无法运行问题，详见 <a href="https://mritd.me/2017/09/27/alpine-3.6-openjdk-8-bug/" target="_blank" rel="noopener">Alpine 3.6 OpenJDK 8 Bug</a></strong>；使用这个 Dockerfile，在当前目录执行 <code>docker build -t mritd/openjdk:8 .</code> build 一个 openjdk8 的基础镜像，然后将其推送到私服，或者 Docker Hub 即可</p><h4 id="4-3、创建项目镜像"><a href="#4-3、创建项目镜像" class="headerlink" title="4.3、创建项目镜像"></a>4.3、创建项目镜像</h4><p>有了基本的 openjdk 的 docker 镜像后，针对于项目每次 build 都应该生成一个包含发布物的 docker 镜像，所以对于项目来说还需要一个项目本身的 Dockerfile；<strong>项目的 Dockerfile 有两种使用方式；一种是动态生成 Dockerfile，然后每次使用新生成的 Dockerfile 去 build；还有一种是写一个通用的 Dockerfile，build 时利用 ARG 参数传入变量</strong>；这里采用第二种方式，以下为一个可以反复使用的 Dockerfile</p><pre><code class="hljs sh">FROM mritd/openjdk:8-144-01MAINTAINER mritd &lt;mritd1234@gmail.com&gt;ARG PROJECT_BUILD_FINALNAMEENV TZ <span class="hljs-string">'Asia/Shanghai'</span>ENV PROJECT_BUILD_FINALNAME <span class="hljs-variable">$&#123;PROJECT_BUILD_FINALNAME&#125;</span>COPY build/libs/<span class="hljs-variable">$&#123;PROJECT_BUILD_FINALNAME&#125;</span>.jar /<span class="hljs-variable">$&#123;PROJECT_BUILD_FINALNAME&#125;</span>.jarCMD [<span class="hljs-string">"bash"</span>,<span class="hljs-string">"-c"</span>,<span class="hljs-string">"java -jar /<span class="hljs-variable">$&#123;PROJECT_BUILD_FINALNAME&#125;</span>.jar"</span>]</code></pre><p><strong>该 Dockerfile 通过声明一个 <code>PROJECT_BUILD_FINALNAME</code> 变量来表示项目的发布物名称；然后将其复制到根目录下，最终利用 java 执行这个 jar 包；所以每次 build 之前只要能拿到项目发布物的名称即可</strong></p><h4 id="4-4、Gradle-修改"><a href="#4-4、Gradle-修改" class="headerlink" title="4.4、Gradle 修改"></a>4.4、Gradle 修改</h4><p>上面已经创建了一个标准的通用型 Dockerfile，每次 build 镜像只要传入 <code>PROJECT_BUILD_FINALNAME</code> 这个最终发布物名称即可；对于发布物名称来说，最好不要固定死；当然不论是 Java 还是其他语言的项目我们都能将最终发布物变成一个固定名字，最不济可以写脚本重命名一下；但是不建议那么干，最好保留版本号信息，以便于异常情况下进入容器能够分辨；对于当前 Java 项目来说，想要拿到 <code>PROJECT_BUILD_FINALNAME</code> 很简单，我们只需要略微修改一下 Gradle 的 build 脚本，让其每次打包 jar 包时将项目的名称及版本号导出到文件中即可；同时这里也加入了镜像版本号的处理，Gradle 脚本修改如下</p><ul><li>build.gradle 最后面增加如下</li></ul><pre><code class="hljs groovy">bootRepackage &#123;    mainClass = <span class="hljs-string">'me.mritd.TestProject.TestProjectApplication'</span>    executable = <span class="hljs-literal">true</span>    doLast &#123;        File envFile = <span class="hljs-keyword">new</span> File(<span class="hljs-string">"build/tmp/PROJECT_ENV"</span>)        println(<span class="hljs-string">"Create $&#123;archivesBaseName&#125; ENV File ===&gt; "</span> + envFile.createNewFile())        println(<span class="hljs-string">"Export $&#123;archivesBaseName&#125; Build Version ===&gt; $&#123;version&#125;"</span>)        envFile.write(<span class="hljs-string">"export PROJECT_BUILD_FINALNAME=$&#123;archivesBaseName&#125;-$&#123;version&#125;\n"</span>)        println(<span class="hljs-string">"Generate Docker image tag..."</span>)        envFile.append(<span class="hljs-string">"export BUILD_DATE=`date +%Y%m%d%H%M%S`\n"</span>)        envFile.append(<span class="hljs-string">"export IMAGE_NAME=mritd/test:`echo \$&#123;CI_BUILD_REF_NAME&#125; | tr '/' '-'`-`echo \$&#123;CI_COMMIT_SHA&#125; | cut -c1-8`-\$&#123;BUILD_DATE&#125;\n"</span>)        envFile.append(<span class="hljs-string">"export LATEST_IMAGE_NAME=mritd/test:latest\n"</span>)    &#125;&#125;</code></pre><p><strong>这一步操作实际上是修改了 <code>bootRepackage</code> 这个 Task(不了解 Gradle 或者不是 Java 项目的请忽略)，在其结束后创建了一个叫 <code>PROJECT_ENV</code> 的文件，里面实际上就是写入了一些 bash 环境变量声明，以方便后面 source 一下这个文件拿到一些变量，然后用户 build 镜像使用</strong>，<code>PROJECT_ENV</code> 最终生成如下</p><pre><code class="hljs sh"><span class="hljs-built_in">export</span> PROJECT_BUILD_FINALNAME=TestProject-0.0.1-SNAPSHOT<span class="hljs-built_in">export</span> BUILD_DATE=`date +%Y%m%d%H%M%S`<span class="hljs-built_in">export</span> IMAGE_NAME=mritd/<span class="hljs-built_in">test</span>:`<span class="hljs-built_in">echo</span> <span class="hljs-variable">$&#123;CI_BUILD_REF_NAME&#125;</span> | tr <span class="hljs-string">'/'</span> <span class="hljs-string">'-'</span>`-`<span class="hljs-built_in">echo</span> <span class="hljs-variable">$&#123;CI_COMMIT_SHA&#125;</span> | cut -c1-8`-<span class="hljs-variable">$&#123;BUILD_DATE&#125;</span><span class="hljs-built_in">export</span> LATEST_IMAGE_NAME=mritd/<span class="hljs-built_in">test</span>:latest</code></pre><p><img src="https://cdn.oss.link/markdown/gr6kc.png" srcset="/img/loading.gif" alt="PROJECT_ENV"></p><h4 id="4-5、创建-CI-配置文件"><a href="#4-5、创建-CI-配置文件" class="headerlink" title="4.5、创建 CI 配置文件"></a>4.5、创建 CI 配置文件</h4><p>一切准备就绪以后，就可以编写 CI 脚本了；GitLab 依靠读取项目根目录下的 <code>.gitlab-ci.yml</code> 文件来执行相应的 CI 操作；以下为测试项目的 <code>.gitlab-ci.yml</code> 配置</p><pre><code class="hljs yaml"><span class="hljs-comment"># 调试开启</span><span class="hljs-comment">#before_script:</span><span class="hljs-comment">#  - pwd</span><span class="hljs-comment">#  - env</span><span class="hljs-attr">cache:</span>  <span class="hljs-attr">key:</span> <span class="hljs-string">$CI_PROJECT_NAME/$CI_COMMIT_REF_NAME-$CI_COMMIT_SHA</span>  <span class="hljs-attr">paths:</span>    <span class="hljs-bullet">-</span> <span class="hljs-string">build</span><span class="hljs-attr">stages:</span>  <span class="hljs-bullet">-</span> <span class="hljs-string">build</span>  <span class="hljs-bullet">-</span> <span class="hljs-string">deploy</span><span class="hljs-attr">auto-build:</span>  <span class="hljs-attr">image:</span> <span class="hljs-string">mritd/build:2.1.1</span>  <span class="hljs-attr">stage:</span> <span class="hljs-string">build</span>  <span class="hljs-attr">script:</span>    <span class="hljs-bullet">-</span> <span class="hljs-string">gradle</span> <span class="hljs-string">--no-daemon</span> <span class="hljs-string">clean</span> <span class="hljs-string">assemble</span>  <span class="hljs-attr">tags:</span>    <span class="hljs-bullet">-</span> <span class="hljs-string">test</span><span class="hljs-attr">deploy:</span>  <span class="hljs-attr">image:</span> <span class="hljs-string">mritd/docker-kubectl:v1.7.4</span>  <span class="hljs-attr">stage:</span> <span class="hljs-string">deploy</span>  <span class="hljs-attr">script:</span>    <span class="hljs-bullet">-</span> <span class="hljs-string">source</span> <span class="hljs-string">build/tmp/PROJECT_ENV</span>    <span class="hljs-bullet">-</span> <span class="hljs-string">echo</span> <span class="hljs-string">"Build Docker Image ==&gt; $&#123;IMAGE_NAME&#125;"</span>    <span class="hljs-bullet">-</span> <span class="hljs-string">docker</span> <span class="hljs-string">build</span> <span class="hljs-string">-t</span> <span class="hljs-string">$&#123;IMAGE_NAME&#125;</span> <span class="hljs-string">--build-arg</span> <span class="hljs-string">PROJECT_BUILD_FINALNAME=$&#123;PROJECT_BUILD_FINALNAME&#125;</span> <span class="hljs-string">.</span><span class="hljs-comment">#    - docker push $&#123;IMAGE_NAME&#125;</span>    <span class="hljs-bullet">-</span> <span class="hljs-string">docker</span> <span class="hljs-string">tag</span> <span class="hljs-string">$&#123;IMAGE_NAME&#125;</span> <span class="hljs-string">$&#123;LATEST_IMAGE_NAME&#125;</span><span class="hljs-comment">#    - docker push $&#123;LATEST_IMAGE_NAME&#125;</span><span class="hljs-comment">#    - docker rmi $&#123;IMAGE_NAME&#125; $&#123;LATEST_IMAGE_NAME&#125;</span><span class="hljs-comment">#    - kubectl --kubeconfig $&#123;KUBE_CONFIG&#125; set image deployment/test test=$IMAGE_NAME</span>  <span class="hljs-attr">tags:</span>    <span class="hljs-bullet">-</span> <span class="hljs-string">test</span>  <span class="hljs-attr">only:</span>    <span class="hljs-bullet">-</span> <span class="hljs-string">master</span>    <span class="hljs-bullet">-</span> <span class="hljs-string">develop</span>    <span class="hljs-bullet">-</span> <span class="hljs-string">/^chore.*$/</span></code></pre><p><strong>关于 CI 配置的一些简要说明如下</strong></p><h5 id="stages"><a href="#stages" class="headerlink" title="stages"></a>stages</h5><p>stages 字段定义了整个 CI 一共有哪些阶段流程，以上的 CI 配置中，定义了该项目的 CI 总共分为 <code>build</code>、<code>deploy</code> 两个阶段；GitLab CI 会根据其顺序执行对应阶段下的所有任务；<strong>在正常生产环境流程可以定义很多个，比如可以有 <code>test</code>、<code>publish</code>，甚至可能有代码扫描的 <code>sonar</code> 阶段等；这些阶段没有任何限制，完全是自定义的</strong>，上面的阶段定义好后在 CI 中表现如下图</p><p><img src="https://cdn.oss.link/markdown/8c7gs.png" srcset="/img/loading.gif" alt="stages"></p><h5 id="task"><a href="#task" class="headerlink" title="task"></a>task</h5><p>task 隶属于 stages 之下；也就是说一个阶段可以有多个任务，任务执行顺序默认不指定会并发执行；对于上面的 CI 配置来说 <code>auto-build</code> 和 <code>deploy</code> 都是 task，他们通过 <code>stage: xxxx</code> 这个标签来指定他们隶属于哪个 stage；当 Runner 使用 Docker 作为 build 提供者时，我们可以在 task 的 <code>image</code> 标签下声明该 task 要使用哪个镜像运行，不指定则默认为 Runner 注册时的镜像(这里是 debian)；<strong>同时 task 还有一个 <code>tags</code> 的标签，该标签指明了这个任务将可以在哪些 Runner 上运行；这个标签可以从 Runner 页面看到，实际上就是 Runner 注册时输入的哪个 tag；对于某些特殊的项目，比如 IOS 项目，则必须在特定机器上执行，所以此时指定 tags 标签很有用</strong>，当 task 运行后如下图所示</p><p><img src="https://cdn.oss.link/markdown/qzvlh.png" srcset="/img/loading.gif" alt="Task"></p><p>除此之外 task 还能指定 <code>only</code> 标签用于限定那些分支才能触发这个 task，如果分支名字不满足则不会触发；<strong>默认情况下，这些 task 都是自动执行的，如果感觉某些任务太过危险，则可以通过增加 <code>when: manual</code> 改为手动执行；注意: 手动执行被 GitLab 认为是高权限的写操作，所以只有项目管理员才能手动运行一个 task，直白的说就是管理员才能点击</strong>；手动执行如下图所示</p><p><img src="https://cdn.oss.link/markdown/vcjci.png" srcset="/img/loading.gif" alt="manual task"></p><h5 id="cache"><a href="#cache" class="headerlink" title="cache"></a>cache</h5><p>cache 这个参数用于定义全局那些文件将被 cache；<strong>在 GitLab CI 中，跨 stage 是不能保存东西的；也就是说在第一步 build 的操作生成的 jar 包，到第二部打包 docker image 时就会被删除；GitLab 会保证每个 stage 中任务在执行时都将工作目录(Docker 容器 中)还原到跟 GitLab 代码仓库中一模一样，多余文件及变更都会被删除</strong>；正常情况下，第一步 build 生成 jar 包应当立即推送到 nexus 私服；但是这里测试没有搭建，所以只能放到本地；但是放到本地下一个 task 就会删除它，所以利用 <code>cache</code> 这个参数将 <code>build</code> 目录 cache 住，保证其跨 stage 也能存在</p><p><strong>关于 <code>.gitlab-ci.yml</code> 具体配置更完整的请参考 <a href="https://docs.gitlab.com/ee/ci/yaml/" target="_blank" rel="noopener">官方文档</a></strong></p><h3 id="五、其他相关"><a href="#五、其他相关" class="headerlink" title="五、其他相关"></a>五、其他相关</h3><h4 id="5-1、GitLab-内置环境变量"><a href="#5-1、GitLab-内置环境变量" class="headerlink" title="5.1、GitLab 内置环境变量"></a>5.1、GitLab 内置环境变量</h4><p>上面已经基本搞定了一个项目的 CI，但是有些变量可能并未说清楚；比如在创建的 <code>PROJECT_ENV</code> 文件中引用了 <code>${CI_COMMIT_SHA}</code> 变量；这种变量其实是 GitLab CI 的内置隐藏变量，这些变量在每次 CI 调用 Runner 运行某个任务时都会传递到对应的 Runner 的执行环境中；<strong>也就是说这些变量在每次的任务容器 SHELL 环境中都会存在，可以直接引用</strong>，具体的完整环境变量列表可以从 <a href="https://docs.gitlab.com/ee/ci/variables/" target="_blank" rel="noopener">官方文档</a> 中获取；如果想知道环境变量具体的值，实际上可以通过在任务执行前用 <code>env</code> 指令打印出来，如下所示</p><p><img src="https://cdn.oss.link/markdown/la9kn.png" srcset="/img/loading.gif" alt="env"></p><p><img src="https://cdn.oss.link/markdown/0175j.png" srcset="/img/loading.gif" alt="env task"></p><h4 id="5-2、GitLab-自定义环境变量"><a href="#5-2、GitLab-自定义环境变量" class="headerlink" title="5.2、GitLab 自定义环境变量"></a>5.2、GitLab 自定义环境变量</h4><p>在某些情况下，我们希望 CI 能自动的发布或者修改一些东西；比如将 jar 包上传到 nexus、将 docker 镜像 push 到私服；这些动作往往需要一个高权限或者说有可写入对应仓库权限的账户来支持，但是这些账户又不想写到项目的 CI 配置里；因为这样很不安全，谁都能看到；此时我们可以将这些敏感变量写入到 GitLab 自定义环境变量中，GitLab 会像对待内置变量一样将其传送到 Runner 端，以供我们使用；GitLab 中自定义的环境变量可以有两种，一种是项目级别的，只能够在当前项目使用，如下</p><p><img src="https://cdn.oss.link/markdown/ennug.png" srcset="/img/loading.gif" alt="project env"></p><p>另一种是组级别的，可以在整个组内的所有项目中使用，如下</p><p><img src="https://cdn.oss.link/markdown/si8ig.png" srcset="/img/loading.gif" alt="group env"></p><p>这两种变量添加后都可以在 CI 的脚本中直接引用</p><h4 id="5-3、Kubernetes-集成"><a href="#5-3、Kubernetes-集成" class="headerlink" title="5.3、Kubernetes 集成"></a>5.3、Kubernetes 集成</h4><p>对于 Kubernetes 集成实际上有两种方案，一种是对接 Kubernetes 的 api，纯代码实现；另一种取巧的方案是调用 kubectl 工具，用 kubectl 工具来实现滚动升级；这里采用后一种取巧的方式，将 kubectl 二进制文件封装到镜像中，然后在 deploy 阶段使用这个镜像直接部署就可以</p><p><img src="https://cdn.oss.link/markdown/bu17r.png" srcset="/img/loading.gif" alt="kubectl"></p><p>其中 <code>mritd/docker-kubectl:v1.7.4</code> 这个镜像的 Dockerfile 如下</p><pre><code class="hljs sh">FROM docker:dind LABEL maintainer=<span class="hljs-string">"mritd &lt;mritd1234@gmail.com&gt;"</span>ARG TZ=<span class="hljs-string">"Asia/Shanghai"</span>ENV TZ <span class="hljs-variable">$&#123;TZ&#125;</span>ENV KUBE_VERSION v1.8.0RUN apk upgrade --update \    &amp;&amp; apk add bash tzdata wget ca-certificates \    &amp;&amp; wget https://storage.googleapis.com/kubernetes-release/release/<span class="hljs-variable">$&#123;KUBE_VERSION&#125;</span>/bin/linux/amd64/kubectl -O /usr/<span class="hljs-built_in">local</span>/bin/kubectl \    &amp;&amp; chmod +x /usr/<span class="hljs-built_in">local</span>/bin/kubectl \    &amp;&amp; ln -sf /usr/share/zoneinfo/<span class="hljs-variable">$&#123;TZ&#125;</span> /etc/localtime \    &amp;&amp; <span class="hljs-built_in">echo</span> <span class="hljs-variable">$&#123;TZ&#125;</span> &gt; /etc/timezone \    &amp;&amp; rm -rf /var/cache/apk/*CMD [<span class="hljs-string">"/bin/bash"</span>]</code></pre><p>这里面的 <code>${KUBE_CONFIG}</code> 是一个自定义的环境变量，对于测试环境我将配置文件直接挂载入了容器中，然后 <code>${KUBE_CONFIG}</code> 只是指定了一个配置文件位置，实际生产环境中可以选择将配置文件变成自定义环境变量使用</p><h4 id="5-4、GitLab-CI-总结"><a href="#5-4、GitLab-CI-总结" class="headerlink" title="5.4、GitLab CI 总结"></a>5.4、GitLab CI 总结</h4><p>关于 GitLab CI 上面已经讲了很多，但是并不全面，也不算太细致；因为这东西说起来实际太多了，现在目测已经 1W 多字了；以下总结一下 GitLab CI 的总体思想，当思路清晰了以后，我想后面的只是查查文档自己试一试就行了</p><p><strong>CS 架构</strong></p><p>GitLab 作为 Server 端，控制 Runner 端执行一系列的 CI 任务；代码 clone 等无需关心，GitLab 会自动处理好一切；Runner 每次都会启动新的容器执行 CI 任务</p><p><strong>容器即环境</strong></p><p>在 Runner 使用 Docker build 的前提下；<strong>所有依赖切换、环境切换应当由切换不同镜像实现，即 build 那就使用 build 的镜像，deploy 就用带有 deploy 功能的镜像；通过不同镜像容器实现完整的环境隔离</strong></p><p><strong>CI即脚本</strong></p><p>不同的 CI 任务实际上就是在使用不同镜像的容器中执行 SHELL 命令，自动化 CI 就是执行预先写好的一些小脚本</p><p><strong>敏感信息走环境变量</strong></p><p>一切重要的敏感信息，如账户密码等，不要写到 CI 配置中，直接放到 GitLab 的环境变量中；GitLab 会保证将其推送到远端 Runner 的 SHELL 变量中</p>]]></content>
    
    
    <categories>
      
      <category>CI/CD</category>
      
    </categories>
    
    
    <tags>
      
      <tag>CI/CD</tag>
      
      <tag>Linux</tag>
      
      <tag>Docker</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>CI/CD 之 Dockerfile</title>
    <link href="/2017/11/12/ci-cd-dockerfile/"/>
    <url>/2017/11/12/ci-cd-dockerfile/</url>
    
    <content type="html"><![CDATA[<blockquote><p>最近准备整理一下关于 CI/CD 的相关文档，写一个关于 CI/CD 的系列文章，这篇先从最基本的 Dockerfile 书写开始，本系列文章默认读者已经熟悉 Docker、Kubernetes 相关工具</p></blockquote><h3 id="一、基础镜像选择"><a href="#一、基础镜像选择" class="headerlink" title="一、基础镜像选择"></a>一、基础镜像选择</h3><p>这里的基础镜像指的是实际项目运行时的基础环境镜像，比如 Java 的 JDK 基础镜像、Nodejs 的基础镜像等；在制作项目的基础镜像时，我个人认为应当考虑一下几点因素:</p><h4 id="1-1、可维护性"><a href="#1-1、可维护性" class="headerlink" title="1.1、可维护性"></a>1.1、可维护性</h4><p>可维护性应当放在首要位置，如果在制作基础镜像时，选择了一个你根本不熟悉的基础镜像，或者说你完全不知道这个基础镜像里有哪些环境变量、Entrypoint 脚本做了什么时，请果断放弃这个基础镜像，选择一个你自己更加熟悉的基础镜像，不要为以后挖坑；还有就是如果对应的应用已经有官方镜像，那么尽量采用官方的，因为你可以省去维护 <strong>自己造的轮子</strong> 的精力，<strong>除非你对基础镜像制作已经得心应手，否则请不要造轮子</strong></p><h4 id="1-2、稳定性"><a href="#1-2、稳定性" class="headerlink" title="1.2、稳定性"></a>1.2、稳定性</h4><p>基础镜像稳定性实际上是个很微妙的话题，因为普遍来说成熟的 Linux 发行版都很稳定；但是对于不同发行版镜像之间还是存在差异的，比如 alpine 的镜像用的是 musl libc，而 debian 用的是 glibc，某些依赖 glibc 的程序可能无法在 alpine 上工作；alpine 版本的 nginx 能使用 http2，debian 版本 nginx 则不行，因为 openssl 版本不同；甚至在相同发行版不同版本之间也会有差异，譬如 openjdk alpine 3.6 版本 java 某些图形库无法工作，在 alpine edge 上安装最新的 openjdk 却没问题等；所以稳定性这个话题对于基础镜像自己来说，他永远稳定，但是对于你的应用来说，则不同基础镜像会产生不同的稳定性；<strong>最后，如果你完全熟悉你的应用，甚至应用层代码也是你写的，那么你可以根据你的习惯和喜好去选择基础镜像，因为你能把控应用运行时依赖；否则的话，请尽量选择 debian 这种比较成熟的发行版作为基础镜像，因为它在普遍上兼容性更好一点；还有尽量不要使用 CentOS 作为基础镜像，因为他的体积将会成为大规模网络分发瓶颈</strong></p><h4 id="1-3、易用性"><a href="#1-3、易用性" class="headerlink" title="1.3、易用性"></a>1.3、易用性</h4><p>易用性简单地说就是是否可调试，因为有些极端情况下，并不是应用只要运行起来就没事了；可能出现一些很棘手的问题需要你进入容器进行调试，此时你的镜像易用性就会体现出来；譬如一个 Java 项目你的基础镜像是 JRE，那么 JDK 的调试工具将完全不可用，还有就是如果你的基础镜像选择了 alpine，那么它默认没有 bash，可能你的脚本无法在里面工作；<strong>所有在选择基础镜像的时候最好也考虑一下未来极端情况的可调试性</strong></p><h3 id="二、格式化及注意事项"><a href="#二、格式化及注意事项" class="headerlink" title="二、格式化及注意事项"></a>二、格式化及注意事项</h3><h4 id="2-1、书写格式"><a href="#2-1、书写格式" class="headerlink" title="2.1、书写格式"></a>2.1、书写格式</h4><p>Dockerfile 类似一堆 shell 命令的堆砌，实际上在构建阶段也可以简单的看做是一个 shell 脚本；但是为了更高效的利用缓存层，通常都会在一个 RUN 命令中连续书写大量的脚本命令，这时候一个良好的书写格式可以使 Dockerfile 看起来更加清晰易懂，也方便以后维护；我个人比较推崇的格式是按照 <a href="https://github.com/nginxinc/docker-nginx/blob/master/mainline/alpine/Dockerfile" target="_blank" rel="noopener">nginx-alpine官方 Dockerfile</a> 的样式来书写，这个 Dockerfile 大致包括了以下规则:</p><ul><li>换行以 <code>&amp;&amp;</code> 开头保持每行对齐，看起来干净又舒服</li><li>安装大量软件包时，每个包一行并添加换行符，虽然会造成很多行，但是看起来很清晰；也可根据实际需要增加每行软件包个数，但是建议不要超过 5 个</li><li>configure 的配置尽量放在统一的变量里，并做好合理换行，方便以后集中化修改</li><li>注释同样和对应命令对齐，并保持单行长度不超出视野，即不能造成拉动滚动条才能看完你的注释</li><li>alpine 作为基础镜像的话，必要时可以使用 scanelf 来减少安装依赖</li></ul><p>除了以上规则，说下我个人的一些小习惯，仅供参考:</p><ul><li>当需要编译时，尽量避免多次 <code>cd</code> 目录，必须进入目录编译时可以开启子 shell 使其完成后还停留但在当前目录，避免 <code>cd</code> 进去再 <code>cd</code> 回来，如</li></ul><pre><code class="hljs sh"><span class="hljs-built_in">cd</span> xxxx \&amp;&amp; ./configure \&amp;&amp; make \&amp;&amp; make install \&amp;&amp; <span class="hljs-built_in">cd</span> ../</code></pre><p>可以变为</p><pre><code class="hljs sh">(<span class="hljs-built_in">cd</span> xxx \&amp;&amp; ./configure \&amp;&amp; make \&amp;&amp; make install)</code></pre><ul><li>同样意义的操作统一放在相邻行处理，比如镜像需要安装两个软件，做两次 <code>wget</code>，那么没必要安装完一个删除一个安装包，可以在最后统一的进行清理动作，简而言之是 <strong>合并具有相同目的的命令</strong></li><li>尽量使用网络资源，也就是说尽量不要在当前目录下放置那种二进制文件，然后进行 <code>ADD</code>/<code>COPY</code> 操作，因为一般 Dockerfile 都是存放到 git 仓库的，同目录下的二进制变动会给 git 仓库带来很大负担</li><li>调整好镜像时区，最好内置一下 bash，可能以后临时进入容器会处理一些东西</li><li><code>FROM</code> 时指定具体的版本号，防止后续升级或者更换主机 build 造成不可预知的结果</li></ul><h4 id="2-2、合理利用缓存"><a href="#2-2、合理利用缓存" class="headerlink" title="2.2、合理利用缓存"></a>2.2、合理利用缓存</h4><p>Docker 在 build 或者说是拉取镜像时是以层为单位作为缓存的；通俗的讲，一个 Dockerfile 命令就会形成一个镜像层(不绝对)，尤其是 <code>RUN</code> 命令形成的镜像层可能会很大；此时应当合理组织 Dockerfile，以便每次拉取或者 build 时高效的利用缓存层</p><ul><li>重复 build 的缓存利用</li></ul><p>Docker 在进行 build 操作时，对于同一个 Dockerfile 来说，<strong>只要执行过一次 build，那么下次 build 将从命令更改处开始</strong>；简单的例子如下</p><pre><code class="hljs sh">FROM alpine:3.6COPY test.jar /test.jarRUN apk add openjdk8 --no-cacheCMD [<span class="hljs-string">"java"</span>,<span class="hljs-string">"-jar"</span>,<span class="hljs-string">"/test.tar"</span>]</code></pre><p>假设我们的项目发布物为 <code>test.jar</code>，那么以上 Dockerfile 放到 CI 里每次 build 都会相当慢，原因就是 <strong>每次更改的发布物为 <code>test.jar</code>，那么也就是相当于每次 build 失效位置从 <code>COPY</code> 命令开始，这将导致下面的 <code>RUN</code> 命令每次都会不走缓存重复执行，当 <code>RUN</code> 命令涉及网络下载等复杂动作时这会极大拖慢 build 进度</strong>，解决方案很简单，移动一下 <code>COPY</code> 命令即可</p><pre><code class="hljs sh">FROM alpine:3.6RUN apk add openjdk8 --no-cacheCOPY test.jar /test.jarCMD [<span class="hljs-string">"java"</span>,<span class="hljs-string">"-jar"</span>,<span class="hljs-string">"/test.tar"</span>]</code></pre><p>此时每次 build 失效位置仍然是 <code>COPY</code> 命令，但是上面的 <code>RUN</code> 命令层已经被 build 过，而且无任何改变，那么每次 build 时 <code>RUN</code> 命令都会命中缓存层从而秒过</p><ul><li>多次拉取的缓存利用</li></ul><p>同上面的 build 一个原理，在 Docker 进行 pull 操作时，也是按照镜像层来进行缓存；当项目进行更新版本，那么只要当前主机 pull 过一次上一个版本的项目，那么下一次将会直接 pull 变更的层，也就是说上面安装 openjdk 的层将会复用；这种情况为了看起来清晰一点也可以将 Dockerfile 拆分成两个</p><p><strong>OpenJDK8 base</strong></p><pre><code class="hljs sh">FROM alpine:3.6RUN RUN apk add openjdk8 --no-cache</code></pre><p><strong>Java Web image</strong></p><pre><code class="hljs sh">FROM xxx.com/base/openjdk8COPY test.jar /test.jarCMD [<span class="hljs-string">"java"</span>,<span class="hljs-string">"-jar"</span>,<span class="hljs-string">"/test.tar"</span>]</code></pre><h3 id="三、镜像安全"><a href="#三、镜像安全" class="headerlink" title="三、镜像安全"></a>三、镜像安全</h3><h4 id="3-1、用户切换"><a href="#3-1、用户切换" class="headerlink" title="3.1、用户切换"></a>3.1、用户切换</h4><p>当我们不在 Dockerfile 中指定内部用户时，那么默认以 root 用户运行；由于 Linux 系统权限判定是根据 UID、GID 来进行的，也就是说 <strong>容器里面的 root 用户有权限访问宿主机 root 用户的东西；所以一旦挂载错误(比如将 <code>/root/.ssh</code> 目录挂载进去)，并且里面的用户具有高权限那么就很危险</strong>；通常习惯是遵从最小权限原则，也就是说尽量保证容器里的程序以低权限运行，此时可以在 Dockerfile 中通过 <code>USER</code> 命令指定后续运行命令所使用的账户，通过 <code>WORKDIR</code> 指定后续命令在那个目录下执行</p><pre><code class="hljs sh">FROM alpine:3.6RUN apk add openjdk8 --no-cacheCOPY test.jar /test.jarUSER testuser:testuserWORKDIR /tmpCMD [<span class="hljs-string">"java"</span>,<span class="hljs-string">"-jar"</span>,<span class="hljs-string">"/test.tar"</span>]</code></pre><p>有时直接使用 <code>USER</code> 指令来切换用户并不算方便，比如你的镜像需要挂载外部存储，如果外部存储中文件权限被意外修改，你的程序接下来可能就会启动失败；此时可以使用一下两个小工具来动态切换用户，巧妙的做法是 <strong>在正式运行程序之前先使用 root 用户进行权限修复，然后使用以下工具切换到具体用户运行</strong></p><ul><li><a href="https://github.com/tianon/gosu" target="_blank" rel="noopener">gosu</a> Golang 实现的一个切换用户身份执行其他程序的小工具</li><li><a href="https://github.com/hlovdal/su-exec" target="_blank" rel="noopener">su-exec</a> C 实现的一个更轻量级的用户切换工具</li></ul><p>具体的 Dockerfile 可以参见我写的 elasticsearch 的 <a href="https://github.com/mritd/dockerfile/blob/master/elasticsearch/docker-entrypoint.sh" target="_blank" rel="noopener">entrypoint 脚本</a></p><h4 id="3-2、容器运行时"><a href="#3-2、容器运行时" class="headerlink" title="3.2、容器运行时"></a>3.2、容器运行时</h4><p>并不是每个容器都一定能切换到低权限用户来运行的，可能某些程序就希望在 root 下运行，此时一定要确认好容器是否需要 <strong>特权模式</strong> 运行；因为一旦开启了特权模式运行的容器将有能力修改宿主机内核参数等重要设置；具体的 Docker 容器运行设置前请参考 <a href="https://docs.docker.com/engine/reference/run/#runtime-privilege-and-linux-capabilities" target="_blank" rel="noopener">官方文档</a></p><p>关于 Dockerfile 方面暂时总结出这些，可能也会有遗漏，待后续补充吧；同时欢迎各位提出相关修改意见 😊</p>]]></content>
    
    
    <categories>
      
      <category>CI/CD</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Linux</tag>
      
      <tag>Docker</tag>
      
      <tag>Kubernetes</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Teleport 跳板机部署</title>
    <link href="/2017/11/09/set-up-teleport/"/>
    <url>/2017/11/09/set-up-teleport/</url>
    
    <content type="html"><![CDATA[<blockquote><p>由于业务需求，以前账号管理混乱，所以很多人有生产服务器的 root 权限；所以目前需要一个能 ssh 登录线上服务器的工具，同时具有简单的审计功能；找了好久找到了这个小工具，以下记录一下搭建教程</p></blockquote><h3 id="一、环境准备"><a href="#一、环境准备" class="headerlink" title="一、环境准备"></a>一、环境准备</h3><p>目前准备了 3 台虚拟机，两台位于内网 NAT 之后，一台位于公网可以直接链接；使用时客户端通过工具连接到公网跳板机上，然后实现自动跳转到内网任意主机；并且具有相应的操作回放审计，通过宿主机账户限制用户权限</p><table><thead><tr><th>ip</th><th>节点</th></tr></thead><tbody><tr><td>92.223.67.84</td><td>公网 Master</td></tr><tr><td>172.16.0.80</td><td>内网 Master</td></tr><tr><td>172.16.0.81</td><td>内网 Node</td></tr></tbody></table><h3 id="二、Teleport-工作模式"><a href="#二、Teleport-工作模式" class="headerlink" title="二、Teleport 工作模式"></a>二、Teleport 工作模式</h3><p>Teleport 工作时从宏观上看是以集群为单位，也就是说<strong>公网算作一个集群，内网算作另一个集群，内网集群通过 ssh 隧道保持跟公网的链接状态，同时内网机群允许公网集群用户连接</strong>，大体工作模式如下</p><p><img src="https://cdn.oss.link/markdown/hsnj8.png" srcset="/img/loading.gif" alt="Teleport 工作模式"></p><h3 id="三、搭建公网-Master"><a href="#三、搭建公网-Master" class="headerlink" title="三、搭建公网 Master"></a>三、搭建公网 Master</h3><h4 id="3-1、配置-Systemd"><a href="#3-1、配置-Systemd" class="headerlink" title="3.1、配置 Systemd"></a>3.1、配置 Systemd</h4><p>首先下载相关可执行文件并复制到 Path 目录下，然后创建一下配置目录等</p><pre><code class="hljs sh">wget https://github.com/gravitational/teleport/releases/download/v2.3.5/teleport-v2.3.5-linux-amd64-bin.tar.gztar -zxvf teleport-v2.3.5-linux-amd64-bin.tar.gzmv teleport/tctl teleport/teleport teleport/tsh /usr/<span class="hljs-built_in">local</span>/binmkdir -p /etc/teleport /data/teleport</code></pre><p>然后为了让服务后台运行创建一个 systemd service 配置文件</p><pre><code class="hljs sh">cat &gt; /etc/systemd/system/teleport.service &lt;&lt;EOF[Unit]Description=Teleport SSH ServiceAfter=network.target[Service]Type=simpleRestart=alwaysExecStart=/usr/<span class="hljs-built_in">local</span>/bin/teleport start -c /etc/teleport/teleport.yaml[Install]WantedBy=multi-user.targetEOF</code></pre><h4 id="3-2、配置-Teleport"><a href="#3-2、配置-Teleport" class="headerlink" title="3.2、配置 Teleport"></a>3.2、配置 Teleport</h4><p>Systemd 配置完成后，就需要写一个 Teleport 的配置文件来让 Teleport 启动，具体选项含义可以参考 <a href="https://gravitational.com/teleport/docs/2.3/admin-guide/" target="_blank" rel="noopener">官方文档</a>；以下为我的配置样例</p><pre><code class="hljs yaml"><span class="hljs-comment"># By default, this file should be stored in /etc/teleport.yaml</span><span class="hljs-comment"># This section of the configuration file applies to all teleport</span><span class="hljs-comment"># services.</span><span class="hljs-attr">teleport:</span>    <span class="hljs-comment"># nodename allows to assign an alternative name this node can be reached by.</span>    <span class="hljs-comment"># by default it's equal to hostname</span>    <span class="hljs-attr">nodename:</span> <span class="hljs-string">mritd.master</span>    <span class="hljs-comment"># Data directory where Teleport keeps its data, like keys/users for</span>    <span class="hljs-comment"># authentication (if using the default BoltDB back-end)</span>    <span class="hljs-attr">data_dir:</span> <span class="hljs-string">/data/teleport</span>    <span class="hljs-comment"># one-time invitation token used to join a cluster. it is not used on</span>    <span class="hljs-comment"># subsequent starts</span>    <span class="hljs-attr">auth_token:</span> <span class="hljs-string">jYektagNTmhjv9Dh</span>    <span class="hljs-comment"># when running in multi-homed or NATed environments Teleport nodes need</span>    <span class="hljs-comment"># to know which IP it will be reachable at by other nodes</span>    <span class="hljs-attr">advertise_ip:</span> <span class="hljs-number">92.223</span><span class="hljs-number">.67</span><span class="hljs-number">.84</span>    <span class="hljs-comment"># list of auth servers in a cluster. you will have more than one auth server</span>    <span class="hljs-comment"># if you configure teleport auth to run in HA configuration</span>    <span class="hljs-attr">auth_servers:</span>        <span class="hljs-bullet">-</span> <span class="hljs-number">0.0</span><span class="hljs-number">.0</span><span class="hljs-number">.0</span><span class="hljs-string">:3025</span>        <span class="hljs-bullet">-</span> <span class="hljs-number">0.0</span><span class="hljs-number">.0</span><span class="hljs-number">.0</span><span class="hljs-string">:3025</span>    <span class="hljs-comment"># Teleport throttles all connections to avoid abuse. These settings allow</span>    <span class="hljs-comment"># you to adjust the default limits</span>    <span class="hljs-attr">connection_limits:</span>        <span class="hljs-attr">max_connections:</span> <span class="hljs-number">1000</span>        <span class="hljs-attr">max_users:</span> <span class="hljs-number">250</span>    <span class="hljs-comment"># Logging configuration. Possible output values are 'stdout', 'stderr' and</span>    <span class="hljs-comment"># 'syslog'. Possible severity values are INFO, WARN and ERROR (default).</span>    <span class="hljs-attr">log:</span>        <span class="hljs-attr">output:</span> <span class="hljs-string">stdout</span>        <span class="hljs-attr">severity:</span> <span class="hljs-string">INFO</span>    <span class="hljs-comment"># Type of storage used for keys. You need to configure this to use etcd</span>    <span class="hljs-comment"># backend if you want to run Teleport in HA configuration.</span>    <span class="hljs-attr">storage:</span>        <span class="hljs-attr">type:</span> <span class="hljs-string">bolt</span>    <span class="hljs-comment"># Cipher algorithms that the server supports. This section only needs to be</span>    <span class="hljs-comment"># set if you want to override the defaults.</span>    <span class="hljs-attr">ciphers:</span>      <span class="hljs-bullet">-</span> <span class="hljs-string">aes128-ctr</span>      <span class="hljs-bullet">-</span> <span class="hljs-string">aes192-ctr</span>      <span class="hljs-bullet">-</span> <span class="hljs-string">aes256-ctr</span>      <span class="hljs-bullet">-</span> <span class="hljs-string">aes128-gcm@openssh.com</span>      <span class="hljs-bullet">-</span> <span class="hljs-string">arcfour256</span>      <span class="hljs-bullet">-</span> <span class="hljs-string">arcfour128</span>    <span class="hljs-comment"># Key exchange algorithms that the server supports. This section only needs</span>    <span class="hljs-comment"># to be set if you want to override the defaults.</span>    <span class="hljs-attr">kex_algos:</span>      <span class="hljs-bullet">-</span> <span class="hljs-string">curve25519-sha256@libssh.org</span>      <span class="hljs-bullet">-</span> <span class="hljs-string">ecdh-sha2-nistp256</span>      <span class="hljs-bullet">-</span> <span class="hljs-string">ecdh-sha2-nistp384</span>      <span class="hljs-bullet">-</span> <span class="hljs-string">ecdh-sha2-nistp521</span>      <span class="hljs-bullet">-</span> <span class="hljs-string">diffie-hellman-group14-sha1</span>      <span class="hljs-bullet">-</span> <span class="hljs-string">diffie-hellman-group1-sha1</span>    <span class="hljs-comment"># Message authentication code (MAC) algorithms that the server supports.</span>    <span class="hljs-comment"># This section only needs to be set if you want to override the defaults.</span>    <span class="hljs-attr">mac_algos:</span>      <span class="hljs-bullet">-</span> <span class="hljs-string">hmac-sha2-256-etm@openssh.com</span>      <span class="hljs-bullet">-</span> <span class="hljs-string">hmac-sha2-256</span>      <span class="hljs-bullet">-</span> <span class="hljs-string">hmac-sha1</span>      <span class="hljs-bullet">-</span> <span class="hljs-string">hmac-sha1-96</span><span class="hljs-comment"># This section configures the 'auth service':</span><span class="hljs-attr">auth_service:</span>    <span class="hljs-comment"># Turns 'auth' role on. Default is 'yes'</span>    <span class="hljs-attr">enabled:</span> <span class="hljs-literal">yes</span>    <span class="hljs-attr">authentication:</span>        <span class="hljs-comment"># default authentication type. possible values are 'local', 'oidc' and 'saml'</span>        <span class="hljs-comment"># only local authentication (Teleport's own user DB) is supported in the open</span>        <span class="hljs-comment"># source version</span>        <span class="hljs-attr">type:</span> <span class="hljs-string">local</span>        <span class="hljs-comment"># second_factor can be off, otp, or u2f</span>        <span class="hljs-attr">second_factor:</span> <span class="hljs-string">otp</span>        <span class="hljs-comment"># this section is used if second_factor is set to 'u2f'</span>        <span class="hljs-comment">#u2f:</span>        <span class="hljs-comment">#    # app_id must point to the URL of the Teleport Web UI (proxy) accessible</span>        <span class="hljs-comment">#    # by the end users</span>        <span class="hljs-comment">#    app_id: https://localhost:3080</span>        <span class="hljs-comment">#    # facets must list all proxy servers if there are more than one deployed</span>        <span class="hljs-comment">#    facets:</span>        <span class="hljs-comment">#    - https://localhost:3080</span>    <span class="hljs-comment"># IP and the port to bind to. Other Teleport nodes will be connecting to</span>    <span class="hljs-comment"># this port (AKA "Auth API" or "Cluster API") to validate client</span>    <span class="hljs-comment"># certificates</span>    <span class="hljs-attr">listen_addr:</span> <span class="hljs-number">0.0</span><span class="hljs-number">.0</span><span class="hljs-number">.0</span><span class="hljs-string">:3025</span>    <span class="hljs-comment"># Pre-defined tokens for adding new nodes to a cluster. Each token specifies</span>    <span class="hljs-comment"># the role a new node will be allowed to assume. The more secure way to</span>    <span class="hljs-comment"># add nodes is to use `ttl node add --ttl` command to generate auto-expiring</span>    <span class="hljs-comment"># tokens.</span>    <span class="hljs-comment">#</span>    <span class="hljs-comment"># We recommend to use tools like `pwgen` to generate sufficiently random</span>    <span class="hljs-comment"># tokens of 32+ byte length.</span>    <span class="hljs-attr">tokens:</span>        <span class="hljs-bullet">-</span> <span class="hljs-string">"proxy,node:jYektagNTmhjv9Dh"</span>        <span class="hljs-bullet">-</span> <span class="hljs-string">"auth:jYektagNTmhjv9Dh"</span>    <span class="hljs-comment"># Optional "cluster name" is needed when configuring trust between multiple</span>    <span class="hljs-comment"># auth servers. A cluster name is used as part of a signature in certificates</span>    <span class="hljs-comment"># generated by this CA.</span>    <span class="hljs-comment">#</span>    <span class="hljs-comment"># By default an automatically generated GUID is used.</span>    <span class="hljs-comment">#</span>    <span class="hljs-comment"># IMPORTANT: if you change cluster_name, it will invalidate all generated</span>    <span class="hljs-comment"># certificates and keys (may need to wipe out /var/lib/teleport directory)</span>    <span class="hljs-attr">cluster_name:</span> <span class="hljs-string">"mritd"</span><span class="hljs-comment"># This section configures the 'node service':</span><span class="hljs-attr">ssh_service:</span>    <span class="hljs-comment"># Turns 'ssh' role on. Default is 'yes'</span>    <span class="hljs-attr">enabled:</span> <span class="hljs-literal">yes</span>    <span class="hljs-comment"># IP and the port for SSH service to bind to.</span>    <span class="hljs-attr">listen_addr:</span> <span class="hljs-number">0.0</span><span class="hljs-number">.0</span><span class="hljs-number">.0</span><span class="hljs-string">:3022</span>    <span class="hljs-comment"># See explanation of labels in "Labeling Nodes" section below</span>    <span class="hljs-attr">labels:</span>        <span class="hljs-attr">role:</span> <span class="hljs-string">master</span>    <span class="hljs-comment"># List of the commands to periodically execute. Their output will be used as node labels.</span>    <span class="hljs-comment"># See "Labeling Nodes" section below for more information.</span>    <span class="hljs-attr">commands:</span>    <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">arch</span>             <span class="hljs-comment"># this command will add a label like 'arch=x86_64' to a node</span>      <span class="hljs-attr">command:</span> <span class="hljs-string">[uname,</span> <span class="hljs-string">-p]</span>      <span class="hljs-attr">period:</span> <span class="hljs-string">1h0m0s</span>    <span class="hljs-comment"># enables reading ~/.tsh/environment before creating a session. by default</span>    <span class="hljs-comment"># set to false, can be set true here or as a command line flag.</span>    <span class="hljs-attr">permit_user_env:</span> <span class="hljs-literal">false</span><span class="hljs-comment"># This section configures the 'proxy servie'</span><span class="hljs-attr">proxy_service:</span>    <span class="hljs-comment"># Turns 'proxy' role on. Default is 'yes'</span>    <span class="hljs-attr">enabled:</span> <span class="hljs-literal">yes</span>    <span class="hljs-comment"># SSH forwarding/proxy address. Command line (CLI) clients always begin their</span>    <span class="hljs-comment"># SSH sessions by connecting to this port</span>    <span class="hljs-attr">listen_addr:</span> <span class="hljs-number">0.0</span><span class="hljs-number">.0</span><span class="hljs-number">.0</span><span class="hljs-string">:3023</span>    <span class="hljs-comment"># Reverse tunnel listening address. An auth server (CA) can establish an</span>    <span class="hljs-comment"># outbound (from behind the firewall) connection to this address.</span>    <span class="hljs-comment"># This will allow users of the outside CA to connect to behind-the-firewall</span>    <span class="hljs-comment"># nodes.</span>    <span class="hljs-attr">tunnel_listen_addr:</span> <span class="hljs-number">0.0</span><span class="hljs-number">.0</span><span class="hljs-number">.0</span><span class="hljs-string">:3024</span>    <span class="hljs-comment"># The HTTPS listen address to serve the Web UI and also to authenticate the</span>    <span class="hljs-comment"># command line (CLI) users via password+HOTP</span>    <span class="hljs-attr">web_listen_addr:</span> <span class="hljs-number">0.0</span><span class="hljs-number">.0</span><span class="hljs-number">.0</span><span class="hljs-string">:3080</span>    <span class="hljs-comment"># TLS certificate for the HTTPS connection. Configuring these properly is</span>    <span class="hljs-comment"># critical for Teleport security.</span>    <span class="hljs-comment">#https_key_file: /var/lib/teleport/webproxy_key.pem</span>    <span class="hljs-comment">#https_cert_file: /var/lib/teleport/webproxy_cert.pem</span></code></pre><p>然后启动 Teleport 即可</p><pre><code class="hljs sh">systemctl <span class="hljs-built_in">enable</span> teleportsystemctl start teleport</code></pre><p>如果启动出现如下错误</p><pre><code class="hljs sh">error: Could not load host key: /etc/ssh/ssh_host_ecdsa_keyerror: Could not load host key: /etc/ssh/ssh_host_ed25519_key</code></pre><p>请执行 ssh-keygen 命令自行生成相关秘钥</p><pre><code class="hljs sh">ssh-keygen -t ecdsa -f /etc/ssh/ssh_host_ecdsa_keyssh-keygen -t ed25519 -f /etc/ssh/ssh_host_ed25519_key</code></pre><h4 id="3-3、添加用户"><a href="#3-3、添加用户" class="headerlink" title="3.3、添加用户"></a>3.3、添加用户</h4><p>公网这台 Teleport 将会作为主要的接入机器，所以在此节点内添加的用户将有权限登录所有集群，包括内网的另一个集群；所以为了方便以后操作先添加一个用户</p><pre><code class="hljs sh"><span class="hljs-comment"># 添加一个用户名为 mritd 的用户，该用户在所有集群具有 root 用户权限</span>tctl --config /etc/teleport/teleport.yaml users add mritd root</code></pre><p>添加成功后会返回一个 OTP 认证初始化地址，浏览器访问后可以使用 Google 扫描 OTP 二维码从而在登录时增加一层 OTP 认证</p><p><img src="https://cdn.oss.link/markdown/chuyf.png" srcset="/img/loading.gif" alt="OTP CMD"></p><p>访问该地址后初始化密码及 OTP</p><p><img src="https://cdn.oss.link/markdown/czwmd.png" srcset="/img/loading.gif" alt="init OTP"></p><h3 id="四、搭建内网-Master"><a href="#四、搭建内网-Master" class="headerlink" title="四、搭建内网 Master"></a>四、搭建内网 Master</h3><p>内网搭建 Master 和公网类似，只不过为了安全将所有 <code>0.0.0.0</code> 的地址全部换成内网 IP 即可，以下为内网的配置信息</p><pre><code class="hljs yaml"><span class="hljs-comment"># By default, this file should be stored in /etc/teleport.yaml</span><span class="hljs-comment"># This section of the configuration file applies to all teleport</span><span class="hljs-comment"># services.</span><span class="hljs-attr">teleport:</span>    <span class="hljs-comment"># nodename allows to assign an alternative name this node can be reached by.</span>    <span class="hljs-comment"># by default it's equal to hostname</span>    <span class="hljs-attr">nodename:</span> <span class="hljs-string">mritd.test1</span>    <span class="hljs-comment"># Data directory where Teleport keeps its data, like keys/users for</span>    <span class="hljs-comment"># authentication (if using the default BoltDB back-end)</span>    <span class="hljs-attr">data_dir:</span> <span class="hljs-string">/data/teleport</span>    <span class="hljs-comment"># one-time invitation token used to join a cluster. it is not used on</span>    <span class="hljs-comment"># subsequent starts</span>    <span class="hljs-attr">auth_token:</span> <span class="hljs-string">jYektagNTmhjv9Dh</span>    <span class="hljs-comment"># when running in multi-homed or NATed environments Teleport nodes need</span>    <span class="hljs-comment"># to know which IP it will be reachable at by other nodes</span>    <span class="hljs-attr">advertise_ip:</span> <span class="hljs-number">172.16</span><span class="hljs-number">.0</span><span class="hljs-number">.80</span>    <span class="hljs-comment"># list of auth servers in a cluster. you will have more than one auth server</span>    <span class="hljs-comment"># if you configure teleport auth to run in HA configuration</span>    <span class="hljs-attr">auth_servers:</span>        <span class="hljs-bullet">-</span> <span class="hljs-number">172.16</span><span class="hljs-number">.0</span><span class="hljs-number">.80</span><span class="hljs-string">:3025</span>    <span class="hljs-comment"># Teleport throttles all connections to avoid abuse. These settings allow</span>    <span class="hljs-comment"># you to adjust the default limits</span>    <span class="hljs-attr">connection_limits:</span>        <span class="hljs-attr">max_connections:</span> <span class="hljs-number">1000</span>        <span class="hljs-attr">max_users:</span> <span class="hljs-number">250</span>    <span class="hljs-comment"># Logging configuration. Possible output values are 'stdout', 'stderr' and</span>    <span class="hljs-comment"># 'syslog'. Possible severity values are INFO, WARN and ERROR (default).</span>    <span class="hljs-attr">log:</span>        <span class="hljs-attr">output:</span> <span class="hljs-string">stdout</span>        <span class="hljs-attr">severity:</span> <span class="hljs-string">INFO</span>    <span class="hljs-comment"># Type of storage used for keys. You need to configure this to use etcd</span>    <span class="hljs-comment"># backend if you want to run Teleport in HA configuration.</span>    <span class="hljs-attr">storage:</span>        <span class="hljs-attr">type:</span> <span class="hljs-string">bolt</span>    <span class="hljs-comment"># Cipher algorithms that the server supports. This section only needs to be</span>    <span class="hljs-comment"># set if you want to override the defaults. </span>    <span class="hljs-attr">ciphers:</span>      <span class="hljs-bullet">-</span> <span class="hljs-string">aes128-ctr</span>      <span class="hljs-bullet">-</span> <span class="hljs-string">aes192-ctr</span>      <span class="hljs-bullet">-</span> <span class="hljs-string">aes256-ctr</span>      <span class="hljs-bullet">-</span> <span class="hljs-string">aes128-gcm@openssh.com</span>      <span class="hljs-bullet">-</span> <span class="hljs-string">arcfour256</span>      <span class="hljs-bullet">-</span> <span class="hljs-string">arcfour128</span>    <span class="hljs-comment"># Key exchange algorithms that the server supports. This section only needs</span>    <span class="hljs-comment"># to be set if you want to override the defaults.</span>    <span class="hljs-attr">kex_algos:</span>      <span class="hljs-bullet">-</span> <span class="hljs-string">curve25519-sha256@libssh.org</span>      <span class="hljs-bullet">-</span> <span class="hljs-string">ecdh-sha2-nistp256</span>      <span class="hljs-bullet">-</span> <span class="hljs-string">ecdh-sha2-nistp384</span>      <span class="hljs-bullet">-</span> <span class="hljs-string">ecdh-sha2-nistp521</span>      <span class="hljs-bullet">-</span> <span class="hljs-string">diffie-hellman-group14-sha1</span>      <span class="hljs-bullet">-</span> <span class="hljs-string">diffie-hellman-group1-sha1</span>    <span class="hljs-comment"># Message authentication code (MAC) algorithms that the server supports.</span>    <span class="hljs-comment"># This section only needs to be set if you want to override the defaults.</span>    <span class="hljs-attr">mac_algos:</span>      <span class="hljs-bullet">-</span> <span class="hljs-string">hmac-sha2-256-etm@openssh.com</span>      <span class="hljs-bullet">-</span> <span class="hljs-string">hmac-sha2-256</span>      <span class="hljs-bullet">-</span> <span class="hljs-string">hmac-sha1</span>      <span class="hljs-bullet">-</span> <span class="hljs-string">hmac-sha1-96</span><span class="hljs-comment"># This section configures the 'auth service':</span><span class="hljs-attr">auth_service:</span>    <span class="hljs-comment"># Turns 'auth' role on. Default is 'yes'</span>    <span class="hljs-attr">enabled:</span> <span class="hljs-literal">yes</span>    <span class="hljs-attr">authentication:</span>        <span class="hljs-comment"># default authentication type. possible values are 'local', 'oidc' and 'saml'</span>        <span class="hljs-comment"># only local authentication (Teleport's own user DB) is supported in the open</span>        <span class="hljs-comment"># source version</span>        <span class="hljs-attr">type:</span> <span class="hljs-string">local</span>        <span class="hljs-comment"># second_factor can be off, otp, or u2f</span>        <span class="hljs-attr">second_factor:</span> <span class="hljs-string">otp</span>        <span class="hljs-comment"># this section is used if second_factor is set to 'u2f'</span>        <span class="hljs-comment">#u2f:</span>        <span class="hljs-comment">#    # app_id must point to the URL of the Teleport Web UI (proxy) accessible</span>        <span class="hljs-comment">#    # by the end users</span>        <span class="hljs-comment">#    app_id: https://localhost:3080</span>        <span class="hljs-comment">#    # facets must list all proxy servers if there are more than one deployed</span>        <span class="hljs-comment">#    facets:</span>        <span class="hljs-comment">#    - https://localhost:3080</span>    <span class="hljs-comment"># IP and the port to bind to. Other Teleport nodes will be connecting to</span>    <span class="hljs-comment"># this port (AKA "Auth API" or "Cluster API") to validate client</span>    <span class="hljs-comment"># certificates</span>    <span class="hljs-attr">listen_addr:</span> <span class="hljs-number">172.16</span><span class="hljs-number">.0</span><span class="hljs-number">.80</span><span class="hljs-string">:3025</span>    <span class="hljs-comment"># Pre-defined tokens for adding new nodes to a cluster. Each token specifies</span>    <span class="hljs-comment"># the role a new node will be allowed to assume. The more secure way to</span>    <span class="hljs-comment"># add nodes is to use `ttl node add --ttl` command to generate auto-expiring</span>    <span class="hljs-comment"># tokens.</span>    <span class="hljs-comment">#</span>    <span class="hljs-comment"># We recommend to use tools like `pwgen` to generate sufficiently random</span>    <span class="hljs-comment"># tokens of 32+ byte length.</span>    <span class="hljs-attr">tokens:</span>        <span class="hljs-bullet">-</span> <span class="hljs-string">"proxy,node:jYektagNTmhjv9Dh"</span>        <span class="hljs-bullet">-</span> <span class="hljs-string">"auth:jYektagNTmhjv9Dh"</span>    <span class="hljs-comment"># Optional "cluster name" is needed when configuring trust between multiple</span>    <span class="hljs-comment"># auth servers. A cluster name is used as part of a signature in certificates</span>    <span class="hljs-comment"># generated by this CA.</span>    <span class="hljs-comment">#</span>    <span class="hljs-comment"># By default an automatically generated GUID is used.</span>    <span class="hljs-comment">#</span>    <span class="hljs-comment"># IMPORTANT: if you change cluster_name, it will invalidate all generated</span>    <span class="hljs-comment"># certificates and keys (may need to wipe out /var/lib/teleport directory)</span>    <span class="hljs-attr">cluster_name:</span> <span class="hljs-string">"nat"</span><span class="hljs-comment"># This section configures the 'node service':</span><span class="hljs-attr">ssh_service:</span>    <span class="hljs-comment"># Turns 'ssh' role on. Default is 'yes'</span>    <span class="hljs-attr">enabled:</span> <span class="hljs-literal">yes</span>    <span class="hljs-comment"># IP and the port for SSH service to bind to.</span>    <span class="hljs-attr">listen_addr:</span> <span class="hljs-number">172.16</span><span class="hljs-number">.0</span><span class="hljs-number">.80</span><span class="hljs-string">:3022</span>    <span class="hljs-comment"># See explanation of labels in "Labeling Nodes" section below</span>    <span class="hljs-attr">labels:</span>        <span class="hljs-attr">role:</span> <span class="hljs-string">master</span>    <span class="hljs-comment"># List of the commands to periodically execute. Their output will be used as node labels.</span>    <span class="hljs-comment"># See "Labeling Nodes" section below for more information.</span>    <span class="hljs-attr">commands:</span>    <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">arch</span>             <span class="hljs-comment"># this command will add a label like 'arch=x86_64' to a node</span>      <span class="hljs-attr">command:</span> <span class="hljs-string">[uname,</span> <span class="hljs-string">-p]</span>      <span class="hljs-attr">period:</span> <span class="hljs-string">1h0m0s</span>    <span class="hljs-comment"># enables reading ~/.tsh/environment before creating a session. by default</span>    <span class="hljs-comment"># set to false, can be set true here or as a command line flag.</span>    <span class="hljs-attr">permit_user_env:</span> <span class="hljs-literal">false</span><span class="hljs-comment"># This section configures the 'proxy servie'</span><span class="hljs-attr">proxy_service:</span>    <span class="hljs-comment"># Turns 'proxy' role on. Default is 'yes'</span>    <span class="hljs-attr">enabled:</span> <span class="hljs-literal">yes</span>    <span class="hljs-comment"># SSH forwarding/proxy address. Command line (CLI) clients always begin their</span>    <span class="hljs-comment"># SSH sessions by connecting to this port</span>    <span class="hljs-attr">listen_addr:</span> <span class="hljs-number">172.16</span><span class="hljs-number">.0</span><span class="hljs-number">.80</span><span class="hljs-string">:3023</span>    <span class="hljs-comment"># Reverse tunnel listening address. An auth server (CA) can establish an</span>    <span class="hljs-comment"># outbound (from behind the firewall) connection to this address.</span>    <span class="hljs-comment"># This will allow users of the outside CA to connect to behind-the-firewall</span>    <span class="hljs-comment"># nodes.</span>    <span class="hljs-attr">tunnel_listen_addr:</span> <span class="hljs-number">172.16</span><span class="hljs-number">.0</span><span class="hljs-number">.80</span><span class="hljs-string">:3024</span>    <span class="hljs-comment"># The HTTPS listen address to serve the Web UI and also to authenticate the</span>    <span class="hljs-comment"># command line (CLI) users via password+HOTP</span>    <span class="hljs-attr">web_listen_addr:</span> <span class="hljs-number">172.16</span><span class="hljs-number">.0</span><span class="hljs-number">.80</span><span class="hljs-string">:3080</span>    <span class="hljs-comment"># TLS certificate for the HTTPS connection. Configuring these properly is</span>    <span class="hljs-comment"># critical for Teleport security.</span>    <span class="hljs-comment">#https_key_file: /var/lib/teleport/webproxy_key.pem</span>    <span class="hljs-comment">#https_cert_file: /var/lib/teleport/webproxy_cert.pem</span></code></pre><p>配置完成后直接启动即可</p><pre><code class="hljs sh">systemctl <span class="hljs-built_in">enable</span> teleportsystemctl start teleport</code></pre><h3 id="五、将内网集群链接至公网"><a href="#五、将内网集群链接至公网" class="headerlink" title="五、将内网集群链接至公网"></a>五、将内网集群链接至公网</h3><p>上文已经讲过，Teleport 通过公网链接内网主机的方式是让内网集群向公网打通一条 ssh 隧道，然后再进行通讯；具体配置如下</p><h4 id="5-1、公网-Master-开启授信集群"><a href="#5-1、公网-Master-开启授信集群" class="headerlink" title="5.1、公网 Master 开启授信集群"></a>5.1、公网 Master 开启授信集群</h4><p>在公网 Master 增加 Token 配置，以允许持有该 Token 的其他内网集群连接到此，修改 <code>/etc/teleport/teleport.yaml</code> 增加一个 token 即可</p><pre><code class="hljs sh">tokens:    - <span class="hljs-string">"proxy,node:jYektagNTmhjv9Dh"</span>    - <span class="hljs-string">"auth:jYektagNTmhjv9Dh"</span>    - <span class="hljs-string">"trusted_cluster:xiomwWcrKinFw4Vs"</span></code></pre><p>然后重启 Teleport</p><pre><code class="hljs sh">systemctl restart teleport</code></pre><h4 id="5-2、内网-Master-链接公网-Master"><a href="#5-2、内网-Master-链接公网-Master" class="headerlink" title="5.2、内网 Master 链接公网 Master"></a>5.2、内网 Master 链接公网 Master</h4><p>当公网集群开启了允许其他集群链接后，内网集群只需要创建配置进行连接即可，创建配置(cluster.yaml)如下</p><pre><code class="hljs yaml"><span class="hljs-comment"># cluster.yaml</span><span class="hljs-attr">kind:</span> <span class="hljs-string">trusted_cluster</span><span class="hljs-attr">version:</span> <span class="hljs-string">v2</span><span class="hljs-attr">metadata:</span>  <span class="hljs-comment"># the trusted cluster name MUST match the 'cluster_name' setting of the</span>  <span class="hljs-comment"># cluster</span>  <span class="hljs-attr">name:</span> <span class="hljs-string">local_cluster</span><span class="hljs-attr">spec:</span>  <span class="hljs-comment"># this field allows to create tunnels that are disabled, but can be enabled later.</span>  <span class="hljs-attr">enabled:</span> <span class="hljs-literal">true</span>  <span class="hljs-comment"># the token expected by the "main" cluster:</span>  <span class="hljs-attr">token:</span> <span class="hljs-string">xiomwWcrKinFw4Vs</span>  <span class="hljs-comment"># the address in 'host:port' form of the reverse tunnel listening port on the</span>  <span class="hljs-comment"># "master" proxy server:</span>  <span class="hljs-attr">tunnel_addr:</span> <span class="hljs-number">92.223</span><span class="hljs-number">.67</span><span class="hljs-number">.84</span><span class="hljs-string">:3024</span>  <span class="hljs-comment"># the address in 'host:port' form of the web listening port on the</span>  <span class="hljs-comment"># "master" proxy server:</span>  <span class="hljs-attr">web_proxy_addr:</span> <span class="hljs-number">92.223</span><span class="hljs-number">.67</span><span class="hljs-number">.84</span><span class="hljs-string">:3080</span></code></pre><p>执行以下命令使内网集群通过 ssh 隧道连接到公网集群</p><pre><code class="hljs sh">tctl --config /etc/teleport/teleport.yaml create /etc/teleport/cluster.yaml</code></pre><p><strong>注意，如果在启动公网和内网集群时没有指定受信的证书( <code>https_cert_file</code>、<code>https_key_file</code> )，那么默认 Teleport 将会生成一个自签名证书，此时在 create 受信集群时将会产生如下错误:</strong></p><pre><code class="hljs sh">the trusted cluster uses misconfigured HTTP/TLS certificate</code></pre><p>此时需要在 <strong>待添加集群(内网)</strong> 启动时增加 <code>--insecure</code> 参数，即 Systemd 配置修改如下</p><pre><code class="hljs sh">[Unit]Description=Teleport SSH ServiceAfter=network.target[Service]Type=simpleRestart=alwaysExecStart=/usr/<span class="hljs-built_in">local</span>/bin/teleport start --insecure -c /etc/teleport/teleport.yaml[Install]WantedBy=multi-user.target</code></pre><p>然后再进行 create 就不会报错</p><h3 id="六、添加其他节点"><a href="#六、添加其他节点" class="headerlink" title="六、添加其他节点"></a>六、添加其他节点</h3><p>两台节点打通后，此时如果有其他机器则可以将其加入到对应集群中，以下以另一台内网机器为例</p><p>由于在主节点 <code>auth_service</code> 中已经预先指定了一个 static Token 用于其他节点加入( <code>proxy,node:jYektagNTmhjv9Dh</code> )，所以其他节点只需要使用这个 Token 加入即可，在另一台内网主机上修改 Systemd 配置如下，然后启动即可</p><pre><code class="hljs sh">[Unit]Description=Teleport SSH ServiceAfter=network.target[Service]Type=simpleRestart=alwaysExecStart=/usr/<span class="hljs-built_in">local</span>/bin/teleport start --roles=node,proxy \                                        --token=jYektagNTmhjv9Dh \                                        --auth-server=172.16.0.80[Install]WantedBy=multi-user.target</code></pre><p>此时在内网的 Master 上可以查看到 Node 已经加入</p><pre><code class="hljs sh">test1.node ➜ tctl --config /etc/teleport/teleport.yaml nodes lsHostname    UUID                                 Address          Labels----------- ------------------------------------ ---------------- -----------------------test2.node  abc786fe-9a60-4480-80f7-8edc20710e58 172.16.0.81:3022mritd.test1 be9080fb-bdba-4823-9fb6-294e0b0dcce3 172.16.0.80:3022 arch=x86_64,role=master</code></pre><h3 id="七、连接测试"><a href="#七、连接测试" class="headerlink" title="七、连接测试"></a>七、连接测试</h3><h4 id="7-1、Web-测试"><a href="#7-1、Web-测试" class="headerlink" title="7.1、Web 测试"></a>7.1、Web 测试</h4><p>Teleport 支持 Web 页面访问，直接访问 <code>https://公网IP:3080</code>，然后登陆即可，登陆后如下</p><p><img src="https://cdn.oss.link/markdown/9yf6k.png" srcset="/img/loading.gif" alt="Web login"></p><p>通过 Cluster 选项可以切换不同集群，点击后面的用户名可以选择不同用户登录到不同主机(用户授权在添加用户时控制)，登陆成功后如下</p><p><img src="https://cdn.oss.link/markdown/m7hz5.png" srcset="/img/loading.gif" alt="Login Success"></p><p>通过 Teleport 进行的所有操作可以通过审计菜单进行操作回放</p><p><img src="https://cdn.oss.link/markdown/c8a74.png" srcset="/img/loading.gif" alt="Audit"></p><h4 id="7-2、命令行测试"><a href="#7-2、命令行测试" class="headerlink" title="7.2、命令行测试"></a>7.2、命令行测试</h4><p>类 Uninx 系统下我们还是习惯使用终端登录，终端登录需要借助 Teleport 的命令行工具 <code>tsh</code>，<code>tsh</code> 在下载的 release 压缩版中已经有了，具体使用文档请自行 help 和参考官方文档，以下为简单的使用示例</p><ul><li>登录跳板机: 短时间内只需要登录一次即可，登录时需要输入密码及 OTP 口令</li></ul><pre><code class="hljs sh"><span class="hljs-built_in">export</span> TELEPORT_PROXY=92.223.67.84<span class="hljs-built_in">export</span> TELEPORT_USER=mritdtsh login --insecure</code></pre><ul><li>登录主机: 完成上一步 login 后就可以免密码登录任意主机</li></ul><pre><code class="hljs sh"><span class="hljs-comment"># cluster 名字是上面设置的，在 web 界面也能看到</span>tsh ssh --cluster nat root@test2.node</code></pre><ul><li>复制文件: <strong>复制文件时不显示进度，并非卡死</strong></li></ul><pre><code class="hljs sh">tsh scp --cluster nat teleport-v2.3.5-linux-amd64-bin.tar.gz root@test2.node:/-&gt; teleport-v2.3.5-linux-amd64-bin.tar.gz (16797035)</code></pre>]]></content>
    
    
    <categories>
      
      <category>Linux</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Linux</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Kubernetes 深度学习笔记</title>
    <link href="/2017/11/03/deep-learning-on-kubernetes/"/>
    <url>/2017/11/03/deep-learning-on-kubernetes/</url>
    
    <content type="html"><![CDATA[<blockquote><p>本文主要记录下 Kubernetes 下运行深度学习框架如 Tensorflow、Caffe2 等一些坑，纯总结性文档</p></blockquote><h3 id="一、先决条件"><a href="#一、先决条件" class="headerlink" title="一、先决条件"></a>一、先决条件</h3><p>Kubernetes 运行深度学习应用实际上要解决的唯一问题就是 GPU 调用，以下只描述 Nvidia 相关的问题以及解决方法；要想完成 Kubernetes 对 GPU 调用，首先要满足以下条件:</p><ul><li>Nvidia 显卡驱动安装正确</li><li>CUDA 安装正确</li><li>Nvidia Docker 安装正确</li></ul><p>关于 Nvidia 驱动和 CUDA 请自行查找安装方法，如果这两部都搞不定，那么不用继续了</p><p><strong>还有一点需要注意: <code>/var/lib</code> 这个目录不能处于单独分区中，具体原因下面阐述</strong></p><h3 id="二、Nvidia-Docker-安装"><a href="#二、Nvidia-Docker-安装" class="headerlink" title="二、Nvidia Docker 安装"></a>二、Nvidia Docker 安装</h3><p>在安装 Nvidia Docker 之前，请确保 Nvidia 驱动以及 CUDA 安装成功，并且 <code>nvidia-smi</code> 能正确显示，如下图所示(来源于网络)</p><p><img src="https://cdn.oss.link/markdown/tdpbk.jpg" srcset="/img/loading.gif" alt="nvidia-smi"></p><p>Nvidia Docker 安装极其简单，具体可参考 <a href="https://github.com/NVIDIA/nvidia-docker" target="_blank" rel="noopener">官方文档</a>，安装完成后请自行按照官方文档描述进行测试，这一步一般不会出现问题</p><p>如果测试成功后，<strong>请查看 <code>/var/lib/nvidia-docker/volumes</code></strong> 目录下是否有文件，<strong>如果没有，那就意味着 Nvidia Docker 并未生成相关的驱动文件成功，需要单独执行 <code>docker volume create --driver=nvidia-docker --name=nvidia_driver_$(modinfo -F version nvidia)</code> 以生成该文件；该命令生成的方式是将已经安装到系统的相关文件硬链接至此，所以要求 <code>/var/lib</code> 目录不能在单独的分区</strong>；驱动生成完成后应该会产生类似 <code>/var/lib/nvidia-docker/volumes/nvidia_driver/375.66</code> 的目录结构</p><h3 id="三、Kubernetes-配置"><a href="#三、Kubernetes-配置" class="headerlink" title="三、Kubernetes 配置"></a>三、Kubernetes 配置</h3><p>当所有基础环境就绪后，最后需要开启 Kubernetes 对 GPU 支持；Kubernetes GPU 文档可以参考 <a href="https://kubernetes.io/docs/tasks/manage-gpus/scheduling-gpus" target="_blank" rel="noopener">这里</a>，实际主要就是在 kubelet 启动时增加 <code>--feature-gates=&quot;Accelerators=true&quot;</code> 参数，如下所示</p><p><img src="https://cdn.oss.link/markdown/gifs3.jpg" srcset="/img/loading.gif" alt="Accelerators"></p><p>所有节点全部修改完成后重启 kubelet 即可，<strong>如果一台机器上有不同型号的显卡，同时希望 Pod 能区别使用不同的 GPU 则可以按照 <a href="https://kubernetes.io/docs/tasks/manage-gpus/scheduling-gpus/#api" target="_blank" rel="noopener">官方文档</a> 增加相应设置</strong></p><h3 id="四、Deployment-设置"><a href="#四、Deployment-设置" class="headerlink" title="四、Deployment 设置"></a>四、Deployment 设置</h3><p>Deployment 部署采用一个 Tensorflow 镜像作为示例，部署配置如下</p><pre><code class="hljs sh">apiVersion: apps/v1beta1kind: Deploymentmetadata:  name: tensorflow  labels:    name: tensorflowspec:  replicas: 1  template:    metadata:      labels:        name: tensorflow    spec:      containers:        - name: tensorflow          image: tensorflow/tensorflow:1.4.0-rc0-gpu          imagePullPolicy: IfNotPresent          <span class="hljs-built_in">command</span>: [<span class="hljs-string">"bash"</span>,<span class="hljs-string">"-c"</span>,<span class="hljs-string">"sleep 999999"</span>]          ports:            - name: tensorflow              containerPort: 8888          resources:             limits:               alpha.kubernetes.io/nvidia-gpu: 1          volumeMounts:            - mountPath: /usr/<span class="hljs-built_in">local</span>/nvidia              name: nvidia-driver            - mountPath: /dev/nvidia0              name: nvidia0            - mountPath: /dev/nvidia-uvm              name: nvidia-uvm            - mountPath: /dev/nvidia-uvm-tools              name: nvidia-uvm-tools            - mountPath: /dev/nvidiactl              name: nvidiactl      volumes:        - name: nvidia-driver          hostPath:            path: /var/lib/nvidia-docker/volumes/nvidia_driver/375.66        - name: nvidia0          hostPath:            path: /dev/nvidia0        - name: nvidia-uvm          hostPath:            path: /dev/nvidia-uvm        - name: nvidia-uvm-tools          hostPath:            path: /dev/nvidia-uvm-tools        - name: nvidiactl          hostPath:            path: /dev/nvidiactl</code></pre><p><strong>Deployment 中运行的 Pod 需要挂载对应的宿主机设备文件以及驱动文件才能正确的调用宿主机 GPU，所以一定要确保前几步生成的相关驱动文件等没问题；如果有多个 nvidia 显卡的话可能需要挂载多个 nvidia 设备</strong></p><p>Pod 运行成功后可执行以下代码测试 GPU 调用</p><pre><code class="hljs python"><span class="hljs-keyword">import</span> tensorflow <span class="hljs-keyword">as</span> tfhello = tf.constant(<span class="hljs-string">'Hello, TensorFlow!'</span>)sess = tf.Session()print(sess.run(hello))a = tf.constant(<span class="hljs-number">10</span>)b = tf.constant(<span class="hljs-number">32</span>)print(sess.run(a + b))</code></pre><p>成功后截图如下</p><p><img src="https://cdn.oss.link/markdown/l7ufl.jpg" srcset="/img/loading.gif" alt="Tensorflow"></p>]]></content>
    
    
    <categories>
      
      <category>Docker</category>
      
      <category>Kubernetes</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Linux</tag>
      
      <tag>Docker</tag>
      
      <tag>Kubernetes</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Kubernetes 1.8 kube-proxy 开启 ipvs</title>
    <link href="/2017/10/10/kube-proxy-use-ipvs-on-kubernetes-1.8/"/>
    <url>/2017/10/10/kube-proxy-use-ipvs-on-kubernetes-1.8/</url>
    
    <content type="html"><![CDATA[<blockquote><p>Kubernetes 1.8 发布已经好几天，1.8 对于 kube-proxy 组件增加了 ipvs 支持，以下记录一下 kube-proxy ipvs 开启教程</p></blockquote><h3 id="一、环境准备"><a href="#一、环境准备" class="headerlink" title="一、环境准备"></a>一、环境准备</h3><p>目前测试为 5 台虚拟机，CentOS 系统，etcd、kubernetes 全部采用 rpm 安装，使用 systemd 来做管理，网络组件采用 calico，Master 实现了 HA；基本环境如下</p><table><thead><tr><th>IP</th><th>组件</th></tr></thead><tbody><tr><td>10.10.1.5</td><td>Master、Node、etcd</td></tr><tr><td>10.10.1.6</td><td>Master、Node、etcd</td></tr><tr><td>10.10.1.7</td><td>Master、Node、etcd</td></tr><tr><td>10.10.1.8</td><td>Node</td></tr><tr><td>10.10.1.9</td><td>Node</td></tr></tbody></table><h3 id="二、注意事项"><a href="#二、注意事项" class="headerlink" title="二、注意事项"></a>二、注意事项</h3><p>之所以把这个单独写一个标题是因为坑有点多，为了避免下面出现问题，先说一下注意事项:</p><h4 id="2-1、SELinux"><a href="#2-1、SELinux" class="headerlink" title="2.1、SELinux"></a>2.1、SELinux</h4><p>如果对 SELinux 玩的不溜的朋友，我建议先关闭  SELinux，关闭方法如下</p><pre><code class="hljs sh"><span class="hljs-comment"># 编辑 /etc/selinux/config 文件；确保 SELINUX=disabled</span>docker1.node ➜  ~ cat /etc/selinux/config<span class="hljs-comment"># This file controls the state of SELinux on the system.</span><span class="hljs-comment"># SELINUX= can take one of these three values:</span><span class="hljs-comment">#     enforcing - SELinux security policy is enforced.</span><span class="hljs-comment">#     permissive - SELinux prints warnings instead of enforcing.</span><span class="hljs-comment">#     disabled - No SELinux policy is loaded.</span>SELINUX=disabled<span class="hljs-comment"># SELINUXTYPE= can take one of three two values:</span><span class="hljs-comment">#     targeted - Targeted processes are protected,</span><span class="hljs-comment">#     minimum - Modification of targeted policy. Only selected processes are protected.</span><span class="hljs-comment">#     mls - Multi Level Security protection.</span>SELINUXTYPE=targeted</code></pre><p><strong>然后重启机器并验证</strong></p><pre><code class="hljs sh">docker1.node ➜  ~ sestatusSELinux status:                 disabled</code></pre><h4 id="2-2、Firewall"><a href="#2-2、Firewall" class="headerlink" title="2.2、Firewall"></a>2.2、Firewall</h4><p>搭建时尽量关闭防火墙，如果你玩的很溜，那么请在测试没问题后再开启防火墙</p><pre><code class="hljs sh">systemctl stop firewalldsystemctl <span class="hljs-built_in">disable</span> firewalld</code></pre><h4 id="2-3、内核参数调整"><a href="#2-3、内核参数调整" class="headerlink" title="2.3、内核参数调整"></a>2.3、内核参数调整</h4><p>确保内核已经开启如下参数，或者说确保 <code>/etc/sysctl.conf</code> 有如下配置</p><pre><code class="hljs sh">docker1.node ➜  ~ cat /etc/sysctl.conf<span class="hljs-comment"># sysctl settings are defined through files in</span><span class="hljs-comment"># /usr/lib/sysctl.d/, /run/sysctl.d/, and /etc/sysctl.d/.</span><span class="hljs-comment">#</span><span class="hljs-comment"># Vendors settings live in /usr/lib/sysctl.d/.</span><span class="hljs-comment"># To override a whole file, create a new file with the same in</span><span class="hljs-comment"># /etc/sysctl.d/ and put new settings there. To override</span><span class="hljs-comment"># only specific settings, add a file with a lexically later</span><span class="hljs-comment"># name in /etc/sysctl.d/ and put new settings there.</span><span class="hljs-comment">#</span><span class="hljs-comment"># For more information, see sysctl.conf(5) and sysctl.d(5).</span>net.ipv4.ip_forward=1net.bridge.bridge-nf-call-iptables=1net.bridge.bridge-nf-call-ip6tables=1</code></pre><p>然后执行 <code>sysctl -p</code> 使之生效</p><pre><code class="hljs sh">docker1.node ➜  ~ sysctl -pnet.ipv4.ip_forward = 1net.bridge.bridge-nf-call-iptables = 1net.bridge.bridge-nf-call-ip6tables = 1</code></pre><h4 id="2-4、内核模块加载"><a href="#2-4、内核模块加载" class="headerlink" title="2.4、内核模块加载"></a>2.4、内核模块加载</h4><p>由于 ipvs 已经加入到内核主干，所以需要内核模块支持，请确保内核已经加载了相应模块；如不确定，执行以下脚本，以确保内核加载相应模块，<strong>否则会出现 <code>failed to load kernel modules: [ip_vs_rr ip_vs_sh ip_vs_wrr]</code> 错误</strong></p><pre><code class="hljs sh">cat &gt; /etc/sysconfig/modules/ipvs.modules &lt;&lt;EOF<span class="hljs-meta">#!/bin/bash</span>ipvs_modules=<span class="hljs-string">"ip_vs ip_vs_lc ip_vs_wlc ip_vs_rr ip_vs_wrr ip_vs_lblc ip_vs_lblcr ip_vs_dh ip_vs_sh ip_vs_fo ip_vs_nq ip_vs_sed ip_vs_ftp nf_conntrack_ipv4"</span><span class="hljs-keyword">for</span> kernel_module <span class="hljs-keyword">in</span> \<span class="hljs-variable">$&#123;ipvs_modules&#125;</span>; <span class="hljs-keyword">do</span>    /sbin/modinfo -F filename \<span class="hljs-variable">$&#123;kernel_module&#125;</span> &gt; /dev/null 2&gt;&amp;1    <span class="hljs-keyword">if</span> [ $? -eq 0 ]; <span class="hljs-keyword">then</span>        /sbin/modprobe \<span class="hljs-variable">$&#123;kernel_module&#125;</span>    <span class="hljs-keyword">fi</span><span class="hljs-keyword">done</span>EOFchmod 755 /etc/sysconfig/modules/ipvs.modules &amp;&amp; bash /etc/sysconfig/modules/ipvs.modules &amp;&amp; lsmod | grep ip_vs</code></pre><p>执行后应该如下图所示，<strong>如果 <code>lsmod | grep ip_vs</code> 并未出现 <code>ip_vs_rr</code> 等模块；那么请更换内核(一般不会，2.6 以后 ipvs 好像已经就合并进主干了)</strong></p><p><img src="https://cdn.oss.link/markdown/49wbb.jpg" srcset="/img/loading.gif" alt="Load kernel modules"></p><h3 id="三、开启-ipvs-支持"><a href="#三、开启-ipvs-支持" class="headerlink" title="三、开启 ipvs 支持"></a>三、开启 ipvs 支持</h3><h4 id="3-1、修改配置"><a href="#3-1、修改配置" class="headerlink" title="3.1、修改配置"></a>3.1、修改配置</h4><p>修改 <code>/etc/kubernetes/proxy</code> 配置如下</p><pre><code class="hljs sh"><span class="hljs-comment">###</span><span class="hljs-comment"># kubernetes proxy config</span><span class="hljs-comment"># default config should be adequate</span><span class="hljs-comment"># Add your own!</span>KUBE_PROXY_ARGS=<span class="hljs-string">"--bind-address=10.10.1.8 \</span><span class="hljs-string">                 --hostname-override=docker4.node \</span><span class="hljs-string">                 --masquerade-all \</span><span class="hljs-string">                 --feature-gates=SupportIPVSProxyMode=true \</span><span class="hljs-string">                 --proxy-mode=ipvs \</span><span class="hljs-string">                 --ipvs-min-sync-period=5s \</span><span class="hljs-string">                 --ipvs-sync-period=5s \</span><span class="hljs-string">                 --ipvs-scheduler=rr \</span><span class="hljs-string">                 --kubeconfig=/etc/kubernetes/kube-proxy.kubeconfig \</span><span class="hljs-string">                 --cluster-cidr=10.254.0.0/16"</span></code></pre><p><strong>启用 ipvs 后与 1.7 版本的配置差异如下：</strong></p><ul><li>增加 <code>--feature-gates=SupportIPVSProxyMode=true</code> 选项，用于告诉 kube-proxy 开启 ipvs 支持，因为目前 ipvs 并未稳定</li><li>增加 <code>ipvs-min-sync-period</code>、<code>--ipvs-sync-period</code>、<code>--ipvs-scheduler</code> 三个参数用于调整 ipvs，具体参数值请自行查阅 ipvs 文档</li><li><strong>增加 <code>--masquerade-all</code> 选项，以确保反向流量通过</strong></li></ul><p><strong>重点说一下 <code>--masquerade-all</code> 选项: kube-proxy ipvs 是基于 NAT 实现的，当创建一个 service 后，kubernetes 会在每个节点上创建一个网卡，同时帮你将 Service IP(VIP) 绑定上，此时相当于每个 Node 都是一个 ds，而其他任何 Node 上的 Pod，甚至是宿主机服务(比如 kube-apiserver 的 6443)都可能成为 rs；按照正常的 lvs nat 模型，所有 rs 应该将 ds 设置成为默认网关，以便数据包在返回时能被 ds 正确修改；在 kubernetes 将 vip 设置到每个 Node 后，默认路由显然不可行，所以要设置 <code>--masquerade-all</code> 选项，以便反向数据包能通过</strong></p><p>以上描述可能并不精准，具体请看 <a href="https://docs.google.com/document/d/1YEBWR4EWeCEWwxufXzRM0e82l_lYYzIXQiSayGaVQ8M/edit?usp=sharing" target="_blank" rel="noopener">Google 文档</a></p><h4 id="3-2、测试-ipvs"><a href="#3-2、测试-ipvs" class="headerlink" title="3.2、测试 ipvs"></a>3.2、测试 ipvs</h4><p>修改完成后，重启 kube-proxy 使其生效</p><pre><code class="hljs sh">systemctl daemon-reloadsystemctl restart kube-proxy</code></pre><p>重启后日志中应该能看到如下输出，不应该有其他提示 ipvs 的错误信息出现</p><p><img src="https://cdn.oss.link/markdown/o05rq.jpg" srcset="/img/loading.gif" alt="kube-proxy ipvs log"></p><p>同时使用 ipvsadm 命令应该能看到相应的 service 的 ipvs 规则(ipvsadm 自己安装一下)</p><p><img src="https://cdn.oss.link/markdown/d1ilk.jpg" srcset="/img/loading.gif" alt="ipvs role"></p><p>然后进入 Pod 测试</p><p><img src="https://cdn.oss.link/markdown/42pjm.jpg" srcset="/img/loading.gif" alt="test ipvs1"></p><p><strong>最后说一点: ipvs 尚未稳定，请慎用；而且 <code>--masquerade-all</code> 选项与 Calico 安全策略控制不兼容，请酌情考虑使用(Calico 在做网络策略限制的时候要求不能开启此选项)</strong></p>]]></content>
    
    
    <categories>
      
      <category>Kubernetes</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Linux</tag>
      
      <tag>Docker</tag>
      
      <tag>Kubernetes</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Kubernetes 1.8 集群搭建</title>
    <link href="/2017/10/09/set-up-kubernetes-1.8-ha-cluster/"/>
    <url>/2017/10/09/set-up-kubernetes-1.8-ha-cluster/</url>
    
    <content type="html"><![CDATA[<blockquote><p>目前 Kubernetes 1.8.0 已经发布，1.8.0增加了很多新特性，比如 kube-proxy 组建的 ipvs 模式等，同时 RBAC 授权也做了一些调整，国庆没事干，所以试了一下；以下记录了 Kubernetes 1.8.0 的搭建过程</p></blockquote><h3 id="一、环境准备"><a href="#一、环境准备" class="headerlink" title="一、环境准备"></a>一、环境准备</h3><p>目前测试为 5 台虚拟机，etcd、kubernetes 全部采用 rpm 安装，使用 systemd 来做管理，网络组件采用 calico，Master 实现了 HA；基本环境如下</p><table><thead><tr><th>IP</th><th>组件</th></tr></thead><tbody><tr><td>10.10.1.5</td><td>Master、Node、etcd</td></tr><tr><td>10.10.1.6</td><td>Master、Node、etcd</td></tr><tr><td>10.10.1.7</td><td>Master、Node、etcd</td></tr><tr><td>10.10.1.8</td><td>Node</td></tr><tr><td>10.10.1.9</td><td>Node</td></tr></tbody></table><p><strong>本文尽量以实际操作为主，因为写过一篇 <a href="https://mritd.me/2017/07/21/set-up-kubernetes-ha-cluster-by-binary/" target="_blank" rel="noopener">Kubernetes 1.7 搭建文档</a>，所以以下细节部分不在详细阐述，不懂得可以参考上一篇文章；本文所有安装工具均已打包上传到了 <a href="https://pan.baidu.com/s/1nvwZCfv" target="_blank" rel="noopener">百度云</a> 密码: <code>4zaz</code>，可直接下载重复搭建过程，搭建前请自行 load 好 images 目录下的相关 docker 镜像</strong></p><h3 id="二、搭建-Etcd-集群"><a href="#二、搭建-Etcd-集群" class="headerlink" title="二、搭建 Etcd 集群"></a>二、搭建 Etcd 集群</h3><h4 id="2-1、生成-Etcd-证书"><a href="#2-1、生成-Etcd-证书" class="headerlink" title="2.1、生成 Etcd 证书"></a>2.1、生成 Etcd 证书</h4><p>同样证书工具仍使用的是 <a href="https://pkg.cfssl.org/" target="_blank" rel="noopener">cfssl</a>，百度云的压缩包里已经包含了，下面直接上配置(<strong>注意，所有证书生成只需要在任意一台主机上生成一遍即可，我这里在 Master 上操作的</strong>)</p><h5 id="etcd-csr-json"><a href="#etcd-csr-json" class="headerlink" title="etcd-csr.json"></a>etcd-csr.json</h5><pre><code class="hljs json">&#123;  <span class="hljs-attr">"key"</span>: &#123;    <span class="hljs-attr">"algo"</span>: <span class="hljs-string">"rsa"</span>,    <span class="hljs-attr">"size"</span>: <span class="hljs-number">2048</span>  &#125;,  <span class="hljs-attr">"names"</span>: [    &#123;      <span class="hljs-attr">"O"</span>: <span class="hljs-string">"etcd"</span>,      <span class="hljs-attr">"OU"</span>: <span class="hljs-string">"etcd Security"</span>,      <span class="hljs-attr">"L"</span>: <span class="hljs-string">"Beijing"</span>,      <span class="hljs-attr">"ST"</span>: <span class="hljs-string">"Beijing"</span>,      <span class="hljs-attr">"C"</span>: <span class="hljs-string">"CN"</span>    &#125;  ],  <span class="hljs-attr">"CN"</span>: <span class="hljs-string">"etcd"</span>,  <span class="hljs-attr">"hosts"</span>: [    <span class="hljs-string">"127.0.0.1"</span>,    <span class="hljs-string">"localhost"</span>,    <span class="hljs-string">"10.10.1.5"</span>,    <span class="hljs-string">"10.10.1.6"</span>,    <span class="hljs-string">"10.10.1.7"</span>,    <span class="hljs-string">"10.10.1.8"</span>,    <span class="hljs-string">"10.10.1.9"</span>  ]&#125;</code></pre><h5 id="etcd-gencert-json"><a href="#etcd-gencert-json" class="headerlink" title="etcd-gencert.json"></a>etcd-gencert.json</h5><pre><code class="hljs json">&#123;  <span class="hljs-attr">"signing"</span>: &#123;    <span class="hljs-attr">"default"</span>: &#123;        <span class="hljs-attr">"usages"</span>: [          <span class="hljs-string">"signing"</span>,          <span class="hljs-string">"key encipherment"</span>,          <span class="hljs-string">"server auth"</span>,          <span class="hljs-string">"client auth"</span>        ],        <span class="hljs-attr">"expiry"</span>: <span class="hljs-string">"87600h"</span>    &#125;  &#125;&#125;</code></pre><h5 id="etcd-root-ca-csr-json"><a href="#etcd-root-ca-csr-json" class="headerlink" title="etcd-root-ca-csr.json"></a>etcd-root-ca-csr.json</h5><pre><code class="hljs json">&#123;  <span class="hljs-attr">"key"</span>: &#123;    <span class="hljs-attr">"algo"</span>: <span class="hljs-string">"rsa"</span>,    <span class="hljs-attr">"size"</span>: <span class="hljs-number">4096</span>  &#125;,  <span class="hljs-attr">"names"</span>: [    &#123;      <span class="hljs-attr">"O"</span>: <span class="hljs-string">"etcd"</span>,      <span class="hljs-attr">"OU"</span>: <span class="hljs-string">"etcd Security"</span>,      <span class="hljs-attr">"L"</span>: <span class="hljs-string">"Beijing"</span>,      <span class="hljs-attr">"ST"</span>: <span class="hljs-string">"Beijing"</span>,      <span class="hljs-attr">"C"</span>: <span class="hljs-string">"CN"</span>    &#125;  ],  <span class="hljs-attr">"CN"</span>: <span class="hljs-string">"etcd-root-ca"</span>&#125;</code></pre><p><strong>最后生成证书</strong></p><pre><code class="hljs sh">cfssl gencert --initca=<span class="hljs-literal">true</span> etcd-root-ca-csr.json | cfssljson --bare etcd-root-cacfssl gencert --ca etcd-root-ca.pem --ca-key etcd-root-ca-key.pem --config etcd-gencert.json etcd-csr.json | cfssljson --bare etcd</code></pre><p>证书生成后截图如下</p><p><img src="https://cdn.oss.link/markdown/6mn6y.jpg" srcset="/img/loading.gif" alt="Gen Etcd Cert"></p><h4 id="2-2、搭建集群"><a href="#2-2、搭建集群" class="headerlink" title="2.2、搭建集群"></a>2.2、搭建集群</h4><p>首先分发证书及 rpm 包</p><pre><code class="hljs sh"><span class="hljs-comment"># 分发 rpm</span><span class="hljs-keyword">for</span> IP <span class="hljs-keyword">in</span> `seq 5 7`; <span class="hljs-keyword">do</span>    scp etcd-3.2.7-1.fc28.x86_64.rpm root@10.10.1.<span class="hljs-variable">$IP</span>:~    ssh root@10.10.1.<span class="hljs-variable">$IP</span> rpm -ivh etcd-3.2.7-1.fc28.x86_64.rpm<span class="hljs-keyword">done</span><span class="hljs-comment"># 分发证书</span><span class="hljs-keyword">for</span> IP <span class="hljs-keyword">in</span> `seq 5 7`;<span class="hljs-keyword">do</span>    ssh root@10.10.1.<span class="hljs-variable">$IP</span> mkdir /etc/etcd/ssl    scp *.pem root@10.10.1.<span class="hljs-variable">$IP</span>:/etc/etcd/ssl    ssh root@10.10.1.<span class="hljs-variable">$IP</span> chown -R etcd:etcd /etc/etcd/ssl    ssh root@10.10.1.<span class="hljs-variable">$IP</span> chmod -R 644 /etc/etcd/ssl/*    ssh root@10.10.1.<span class="hljs-variable">$IP</span> chmod 755 /etc/etcd/ssl<span class="hljs-keyword">done</span></code></pre><pre><code class="hljs sh"><span class="hljs-comment"># 修改 etcd 数据目录权限组</span><span class="hljs-keyword">for</span> IP <span class="hljs-keyword">in</span> `seq 5 7`;<span class="hljs-keyword">do</span>    ssh root@10.10.1.<span class="hljs-variable">$IP</span> chown -R etcd:etcd /var/lib/etcd<span class="hljs-keyword">done</span></code></pre><p><strong>然后修改配置如下(其他两个节点类似，只需要改监听地址和 Etcd Name 即可)</strong></p><pre><code class="hljs sh">docker1.node ➜  ~ cat /etc/etcd/etcd.conf<span class="hljs-comment"># [member]</span>ETCD_NAME=etcd1ETCD_DATA_DIR=<span class="hljs-string">"/var/lib/etcd/etcd1.etcd"</span>ETCD_WAL_DIR=<span class="hljs-string">"/var/lib/etcd/wal"</span>ETCD_SNAPSHOT_COUNT=<span class="hljs-string">"100"</span>ETCD_HEARTBEAT_INTERVAL=<span class="hljs-string">"100"</span>ETCD_ELECTION_TIMEOUT=<span class="hljs-string">"1000"</span>ETCD_LISTEN_PEER_URLS=<span class="hljs-string">"https://10.10.1.5:2380"</span>ETCD_LISTEN_CLIENT_URLS=<span class="hljs-string">"https://10.10.1.5:2379,http://127.0.0.1:2379"</span>ETCD_MAX_SNAPSHOTS=<span class="hljs-string">"5"</span>ETCD_MAX_WALS=<span class="hljs-string">"5"</span><span class="hljs-comment">#ETCD_CORS=""</span><span class="hljs-comment"># [cluster]</span>ETCD_INITIAL_ADVERTISE_PEER_URLS=<span class="hljs-string">"https://10.10.1.5:2380"</span><span class="hljs-comment"># if you use different ETCD_NAME (e.g. test), set ETCD_INITIAL_CLUSTER value for this name, i.e. "test=http://..."</span>ETCD_INITIAL_CLUSTER=<span class="hljs-string">"etcd1=https://10.10.1.5:2380,etcd2=https://10.10.1.6:2380,etcd3=https://10.10.1.7:2380"</span>ETCD_INITIAL_CLUSTER_STATE=<span class="hljs-string">"new"</span>ETCD_INITIAL_CLUSTER_TOKEN=<span class="hljs-string">"etcd-cluster"</span>ETCD_ADVERTISE_CLIENT_URLS=<span class="hljs-string">"https://10.10.1.5:2379"</span><span class="hljs-comment">#ETCD_DISCOVERY=""</span><span class="hljs-comment">#ETCD_DISCOVERY_SRV=""</span><span class="hljs-comment">#ETCD_DISCOVERY_FALLBACK="proxy"</span><span class="hljs-comment">#ETCD_DISCOVERY_PROXY=""</span><span class="hljs-comment">#ETCD_STRICT_RECONFIG_CHECK="false"</span><span class="hljs-comment">#ETCD_AUTO_COMPACTION_RETENTION="0"</span><span class="hljs-comment"># [proxy]</span><span class="hljs-comment">#ETCD_PROXY="off"</span><span class="hljs-comment">#ETCD_PROXY_FAILURE_WAIT="5000"</span><span class="hljs-comment">#ETCD_PROXY_REFRESH_INTERVAL="30000"</span><span class="hljs-comment">#ETCD_PROXY_DIAL_TIMEOUT="1000"</span><span class="hljs-comment">#ETCD_PROXY_WRITE_TIMEOUT="5000"</span><span class="hljs-comment">#ETCD_PROXY_READ_TIMEOUT="0"</span><span class="hljs-comment"># [security]</span>ETCD_CERT_FILE=<span class="hljs-string">"/etc/etcd/ssl/etcd.pem"</span>ETCD_KEY_FILE=<span class="hljs-string">"/etc/etcd/ssl/etcd-key.pem"</span>ETCD_CLIENT_CERT_AUTH=<span class="hljs-string">"true"</span>ETCD_TRUSTED_CA_FILE=<span class="hljs-string">"/etc/etcd/ssl/etcd-root-ca.pem"</span>ETCD_AUTO_TLS=<span class="hljs-string">"true"</span>ETCD_PEER_CERT_FILE=<span class="hljs-string">"/etc/etcd/ssl/etcd.pem"</span>ETCD_PEER_KEY_FILE=<span class="hljs-string">"/etc/etcd/ssl/etcd-key.pem"</span>ETCD_PEER_CLIENT_CERT_AUTH=<span class="hljs-string">"true"</span>ETCD_PEER_TRUSTED_CA_FILE=<span class="hljs-string">"/etc/etcd/ssl/etcd-root-ca.pem"</span>ETCD_PEER_AUTO_TLS=<span class="hljs-string">"true"</span><span class="hljs-comment"># [logging]</span><span class="hljs-comment">#ETCD_DEBUG="false"</span><span class="hljs-comment"># examples for -log-package-levels etcdserver=WARNING,security=DEBUG</span><span class="hljs-comment">#ETCD_LOG_PACKAGE_LEVELS=""</span></code></pre><p>最后启动集群并测试如下</p><pre><code class="hljs sh">systemctl daemon-reloadsystemctl start etcdsystemctl <span class="hljs-built_in">enable</span> etcd<span class="hljs-built_in">export</span> ETCDCTL_API=3etcdctl --cacert=/etc/etcd/ssl/etcd-root-ca.pem --cert=/etc/etcd/ssl/etcd.pem --key=/etc/etcd/ssl/etcd-key.pem --endpoints=https://10.10.1.5:2379,https://10.10.1.6:2379,https://10.10.1.7:2379 endpoint health</code></pre><p><img src="https://cdn.oss.link/markdown/ecrgr.jpg" srcset="/img/loading.gif" alt="check etcd"></p><h3 id="三、搭建-Master-节点"><a href="#三、搭建-Master-节点" class="headerlink" title="三、搭建 Master 节点"></a>三、搭建 Master 节点</h3><h4 id="3-1、生成-Kubernetes-证书"><a href="#3-1、生成-Kubernetes-证书" class="headerlink" title="3.1、生成 Kubernetes 证书"></a>3.1、生成 Kubernetes 证书</h4><p><strong>生成证书配置文件需要借助 kubectl，所以先要安装一下 kubernetes-client 包</strong></p><pre><code class="hljs sh">rpm -ivh kubernetes-client-1.8.0-1.el7.centos.x86_64.rpm</code></pre><p>生成证书配置如下</p><h5 id="admin-csr-json"><a href="#admin-csr-json" class="headerlink" title="admin-csr.json"></a>admin-csr.json</h5><pre><code class="hljs json">&#123;  <span class="hljs-attr">"CN"</span>: <span class="hljs-string">"admin"</span>,  <span class="hljs-attr">"hosts"</span>: [],  <span class="hljs-attr">"key"</span>: &#123;    <span class="hljs-attr">"algo"</span>: <span class="hljs-string">"rsa"</span>,    <span class="hljs-attr">"size"</span>: <span class="hljs-number">2048</span>  &#125;,  <span class="hljs-attr">"names"</span>: [    &#123;      <span class="hljs-attr">"C"</span>: <span class="hljs-string">"CN"</span>,      <span class="hljs-attr">"ST"</span>: <span class="hljs-string">"BeiJing"</span>,      <span class="hljs-attr">"L"</span>: <span class="hljs-string">"BeiJing"</span>,      <span class="hljs-attr">"O"</span>: <span class="hljs-string">"system:masters"</span>,      <span class="hljs-attr">"OU"</span>: <span class="hljs-string">"System"</span>    &#125;  ]&#125;</code></pre><h5 id="k8s-gencert-json"><a href="#k8s-gencert-json" class="headerlink" title="k8s-gencert.json"></a>k8s-gencert.json</h5><pre><code class="hljs json">&#123;  <span class="hljs-attr">"signing"</span>: &#123;    <span class="hljs-attr">"default"</span>: &#123;      <span class="hljs-attr">"expiry"</span>: <span class="hljs-string">"87600h"</span>    &#125;,    <span class="hljs-attr">"profiles"</span>: &#123;      <span class="hljs-attr">"kubernetes"</span>: &#123;        <span class="hljs-attr">"usages"</span>: [            <span class="hljs-string">"signing"</span>,            <span class="hljs-string">"key encipherment"</span>,            <span class="hljs-string">"server auth"</span>,            <span class="hljs-string">"client auth"</span>        ],        <span class="hljs-attr">"expiry"</span>: <span class="hljs-string">"87600h"</span>      &#125;    &#125;  &#125;&#125;</code></pre><h5 id="k8s-root-ca-csr-json"><a href="#k8s-root-ca-csr-json" class="headerlink" title="k8s-root-ca-csr.json"></a>k8s-root-ca-csr.json</h5><pre><code class="hljs json">&#123;  <span class="hljs-attr">"CN"</span>: <span class="hljs-string">"kubernetes"</span>,  <span class="hljs-attr">"key"</span>: &#123;    <span class="hljs-attr">"algo"</span>: <span class="hljs-string">"rsa"</span>,    <span class="hljs-attr">"size"</span>: <span class="hljs-number">4096</span>  &#125;,  <span class="hljs-attr">"names"</span>: [    &#123;      <span class="hljs-attr">"C"</span>: <span class="hljs-string">"CN"</span>,      <span class="hljs-attr">"ST"</span>: <span class="hljs-string">"BeiJing"</span>,      <span class="hljs-attr">"L"</span>: <span class="hljs-string">"BeiJing"</span>,      <span class="hljs-attr">"O"</span>: <span class="hljs-string">"k8s"</span>,      <span class="hljs-attr">"OU"</span>: <span class="hljs-string">"System"</span>    &#125;  ]&#125;</code></pre><h5 id="kube-proxy-csr-json"><a href="#kube-proxy-csr-json" class="headerlink" title="kube-proxy-csr.json"></a>kube-proxy-csr.json</h5><pre><code class="hljs json">&#123;  <span class="hljs-attr">"CN"</span>: <span class="hljs-string">"system:kube-proxy"</span>,  <span class="hljs-attr">"hosts"</span>: [],  <span class="hljs-attr">"key"</span>: &#123;    <span class="hljs-attr">"algo"</span>: <span class="hljs-string">"rsa"</span>,    <span class="hljs-attr">"size"</span>: <span class="hljs-number">2048</span>  &#125;,  <span class="hljs-attr">"names"</span>: [    &#123;      <span class="hljs-attr">"C"</span>: <span class="hljs-string">"CN"</span>,      <span class="hljs-attr">"ST"</span>: <span class="hljs-string">"BeiJing"</span>,      <span class="hljs-attr">"L"</span>: <span class="hljs-string">"BeiJing"</span>,      <span class="hljs-attr">"O"</span>: <span class="hljs-string">"k8s"</span>,      <span class="hljs-attr">"OU"</span>: <span class="hljs-string">"System"</span>    &#125;  ]&#125;</code></pre><h5 id="kubernetes-csr-json"><a href="#kubernetes-csr-json" class="headerlink" title="kubernetes-csr.json"></a>kubernetes-csr.json</h5><pre><code class="hljs json">&#123;    <span class="hljs-attr">"CN"</span>: <span class="hljs-string">"kubernetes"</span>,    <span class="hljs-attr">"hosts"</span>: [        <span class="hljs-string">"127.0.0.1"</span>,        <span class="hljs-string">"10.254.0.1"</span>,        <span class="hljs-string">"10.10.1.5"</span>,        <span class="hljs-string">"10.10.1.6"</span>,        <span class="hljs-string">"10.10.1.7"</span>,        <span class="hljs-string">"10.10.1.8"</span>,        <span class="hljs-string">"10.10.1.9"</span>,        <span class="hljs-string">"localhost"</span>,        <span class="hljs-string">"kubernetes"</span>,        <span class="hljs-string">"kubernetes.default"</span>,        <span class="hljs-string">"kubernetes.default.svc"</span>,        <span class="hljs-string">"kubernetes.default.svc.cluster"</span>,        <span class="hljs-string">"kubernetes.default.svc.cluster.local"</span>    ],    <span class="hljs-attr">"key"</span>: &#123;        <span class="hljs-attr">"algo"</span>: <span class="hljs-string">"rsa"</span>,        <span class="hljs-attr">"size"</span>: <span class="hljs-number">2048</span>    &#125;,    <span class="hljs-attr">"names"</span>: [        &#123;            <span class="hljs-attr">"C"</span>: <span class="hljs-string">"CN"</span>,            <span class="hljs-attr">"ST"</span>: <span class="hljs-string">"BeiJing"</span>,            <span class="hljs-attr">"L"</span>: <span class="hljs-string">"BeiJing"</span>,            <span class="hljs-attr">"O"</span>: <span class="hljs-string">"k8s"</span>,            <span class="hljs-attr">"OU"</span>: <span class="hljs-string">"System"</span>        &#125;    ]&#125;</code></pre><p>最后生成证书及配置文件</p><pre><code class="hljs sh"><span class="hljs-comment"># 生成证书</span>cfssl gencert --initca=<span class="hljs-literal">true</span> k8s-root-ca-csr.json | cfssljson --bare k8s-root-ca<span class="hljs-keyword">for</span> targetName <span class="hljs-keyword">in</span> kubernetes admin kube-proxy; <span class="hljs-keyword">do</span>    cfssl gencert --ca k8s-root-ca.pem --ca-key k8s-root-ca-key.pem --config k8s-gencert.json --profile kubernetes <span class="hljs-variable">$targetName</span>-csr.json | cfssljson --bare <span class="hljs-variable">$targetName</span><span class="hljs-keyword">done</span><span class="hljs-comment"># 生成配置</span><span class="hljs-built_in">export</span> KUBE_APISERVER=<span class="hljs-string">"https://127.0.0.1:6443"</span><span class="hljs-built_in">export</span> BOOTSTRAP_TOKEN=$(head -c 16 /dev/urandom | od -An -t x | tr -d <span class="hljs-string">' '</span>)<span class="hljs-built_in">echo</span> <span class="hljs-string">"Tokne: <span class="hljs-variable">$&#123;BOOTSTRAP_TOKEN&#125;</span>"</span>cat &gt; token.csv &lt;&lt;EOF<span class="hljs-variable">$&#123;BOOTSTRAP_TOKEN&#125;</span>,kubelet-bootstrap,10001,<span class="hljs-string">"system:kubelet-bootstrap"</span>EOF<span class="hljs-built_in">echo</span> <span class="hljs-string">"Create kubelet bootstrapping kubeconfig..."</span>kubectl config <span class="hljs-built_in">set</span>-cluster kubernetes \  --certificate-authority=k8s-root-ca.pem \  --embed-certs=<span class="hljs-literal">true</span> \  --server=<span class="hljs-variable">$&#123;KUBE_APISERVER&#125;</span> \  --kubeconfig=bootstrap.kubeconfigkubectl config <span class="hljs-built_in">set</span>-credentials kubelet-bootstrap \  --token=<span class="hljs-variable">$&#123;BOOTSTRAP_TOKEN&#125;</span> \  --kubeconfig=bootstrap.kubeconfigkubectl config <span class="hljs-built_in">set</span>-context default \  --cluster=kubernetes \  --user=kubelet-bootstrap \  --kubeconfig=bootstrap.kubeconfigkubectl config use-context default --kubeconfig=bootstrap.kubeconfig<span class="hljs-built_in">echo</span> <span class="hljs-string">"Create kube-proxy kubeconfig..."</span>kubectl config <span class="hljs-built_in">set</span>-cluster kubernetes \  --certificate-authority=k8s-root-ca.pem \  --embed-certs=<span class="hljs-literal">true</span> \  --server=<span class="hljs-variable">$&#123;KUBE_APISERVER&#125;</span> \  --kubeconfig=kube-proxy.kubeconfigkubectl config <span class="hljs-built_in">set</span>-credentials kube-proxy \  --client-certificate=kube-proxy.pem \  --client-key=kube-proxy-key.pem \  --embed-certs=<span class="hljs-literal">true</span> \  --kubeconfig=kube-proxy.kubeconfigkubectl config <span class="hljs-built_in">set</span>-context default \  --cluster=kubernetes \  --user=kube-proxy \  --kubeconfig=kube-proxy.kubeconfigkubectl config use-context default --kubeconfig=kube-proxy.kubeconfig<span class="hljs-comment"># 生成高级审计配置</span>cat &gt;&gt; audit-policy.yaml &lt;&lt;EOF<span class="hljs-comment"># Log all requests at the Metadata level.</span>apiVersion: audit.k8s.io/v1beta1kind: Policyrules:- level: MetadataEOF</code></pre><h4 id="3-2、分发-rpm-及证书"><a href="#3-2、分发-rpm-及证书" class="headerlink" title="3.2、分发 rpm 及证书"></a>3.2、分发 rpm 及证书</h4><p>创建好证书以后就要进行分发，同时由于 Master 也作为 Node 使用，所以以下命令中在 Master 上也安装了 kubelet、kube-proxy 组件</p><pre><code class="hljs sh"><span class="hljs-comment"># 分发并安装 rpm</span><span class="hljs-keyword">for</span> IP <span class="hljs-keyword">in</span> `seq 5 7`; <span class="hljs-keyword">do</span>    scp kubernetes*.rpm root@10.10.1.<span class="hljs-variable">$IP</span>:~;     ssh root@10.10.1.<span class="hljs-variable">$IP</span> yum install -y kubernetes*.rpm<span class="hljs-keyword">done</span><span class="hljs-comment"># 分发证书</span><span class="hljs-keyword">for</span> IP <span class="hljs-keyword">in</span> `seq 5 7`;<span class="hljs-keyword">do</span>    ssh root@10.10.1.<span class="hljs-variable">$IP</span> mkdir /etc/kubernetes/ssl    scp *.pem root@10.10.1.<span class="hljs-variable">$IP</span>:/etc/kubernetes/ssl    scp *.kubeconfig token.csv audit-policy.yaml root@10.10.1.<span class="hljs-variable">$IP</span>:/etc/kubernetes    ssh root@10.10.1.<span class="hljs-variable">$IP</span> chown -R kube:kube /etc/kubernetes/ssl<span class="hljs-keyword">done</span><span class="hljs-comment"># 设置 log 目录权限</span><span class="hljs-keyword">for</span> IP <span class="hljs-keyword">in</span> `seq 5 7`;<span class="hljs-keyword">do</span>    ssh root@10.10.1.<span class="hljs-variable">$IP</span> mkdir -p /var/<span class="hljs-built_in">log</span>/kube-audit /usr/libexec/kubernetes    ssh root@10.10.1.<span class="hljs-variable">$IP</span> chown -R kube:kube /var/<span class="hljs-built_in">log</span>/kube-audit /usr/libexec/kubernetes    ssh root@10.10.1.<span class="hljs-variable">$IP</span> chmod -R 755 /var/<span class="hljs-built_in">log</span>/kube-audit /usr/libexec/kubernetes<span class="hljs-keyword">done</span></code></pre><h4 id="3-3、-搭建-Master-节点"><a href="#3-3、-搭建-Master-节点" class="headerlink" title="3.3、 搭建 Master 节点"></a>3.3、 搭建 Master 节点</h4><p>证书与 rpm 都安装完成后，只需要修改配置(配置位于 <code>/etc/kubernetes</code> 目录)后启动相关组件即可</p><ul><li>config 通用配置</li></ul><pre><code class="hljs sh"><span class="hljs-comment">###</span><span class="hljs-comment"># kubernetes system config</span><span class="hljs-comment">#</span><span class="hljs-comment"># The following values are used to configure various aspects of all</span><span class="hljs-comment"># kubernetes services, including</span><span class="hljs-comment">#</span><span class="hljs-comment">#   kube-apiserver.service</span><span class="hljs-comment">#   kube-controller-manager.service</span><span class="hljs-comment">#   kube-scheduler.service</span><span class="hljs-comment">#   kubelet.service</span><span class="hljs-comment">#   kube-proxy.service</span><span class="hljs-comment"># logging to stderr means we get it in the systemd journal</span>KUBE_LOGTOSTDERR=<span class="hljs-string">"--logtostderr=true"</span><span class="hljs-comment"># journal message level, 0 is debug</span>KUBE_LOG_LEVEL=<span class="hljs-string">"--v=2"</span><span class="hljs-comment"># Should this cluster be allowed to run privileged docker containers</span>KUBE_ALLOW_PRIV=<span class="hljs-string">"--allow-privileged=true"</span><span class="hljs-comment"># How the controller-manager, scheduler, and proxy find the apiserver</span>KUBE_MASTER=<span class="hljs-string">"--master=http://127.0.0.1:8080"</span></code></pre><h5 id="apiserver-配置"><a href="#apiserver-配置" class="headerlink" title="apiserver 配置"></a>apiserver 配置</h5><pre><code class="hljs sh"><span class="hljs-comment">###</span><span class="hljs-comment"># kubernetes system config</span><span class="hljs-comment">#</span><span class="hljs-comment"># The following values are used to configure the kube-apiserver</span><span class="hljs-comment">#</span><span class="hljs-comment"># The address on the local server to listen to.</span>KUBE_API_ADDRESS=<span class="hljs-string">"--advertise-address=10.10.1.5 --insecure-bind-address=127.0.0.1 --bind-address=10.10.1.5"</span><span class="hljs-comment"># The port on the local server to listen on.</span>KUBE_API_PORT=<span class="hljs-string">"--insecure-port=8080 --secure-port=6443"</span><span class="hljs-comment"># Port minions listen on</span><span class="hljs-comment"># KUBELET_PORT="--kubelet-port=10250"</span><span class="hljs-comment"># Comma separated list of nodes in the etcd cluster</span>KUBE_ETCD_SERVERS=<span class="hljs-string">"--etcd-servers=https://10.10.1.5:2379,https://10.10.1.6:2379,https://10.10.1.7:2379"</span><span class="hljs-comment"># Address range to use for services</span>KUBE_SERVICE_ADDRESSES=<span class="hljs-string">"--service-cluster-ip-range=10.254.0.0/16"</span><span class="hljs-comment"># default admission control policies</span>KUBE_ADMISSION_CONTROL=<span class="hljs-string">"--admission-control=NamespaceLifecycle,LimitRanger,SecurityContextDeny,ServiceAccount,ResourceQuota,NodeRestriction"</span><span class="hljs-comment"># Add your own!</span>KUBE_API_ARGS=<span class="hljs-string">"--authorization-mode=RBAC,Node \</span><span class="hljs-string">               --anonymous-auth=false \</span><span class="hljs-string">               --kubelet-https=true \</span><span class="hljs-string">               --enable-bootstrap-token-auth \</span><span class="hljs-string">               --token-auth-file=/etc/kubernetes/token.csv \</span><span class="hljs-string">               --service-node-port-range=30000-50000 \</span><span class="hljs-string">               --tls-cert-file=/etc/kubernetes/ssl/kubernetes.pem \</span><span class="hljs-string">               --tls-private-key-file=/etc/kubernetes/ssl/kubernetes-key.pem \</span><span class="hljs-string">               --client-ca-file=/etc/kubernetes/ssl/k8s-root-ca.pem \</span><span class="hljs-string">               --service-account-key-file=/etc/kubernetes/ssl/k8s-root-ca.pem \</span><span class="hljs-string">               --etcd-quorum-read=true \</span><span class="hljs-string">               --storage-backend=etcd3 \</span><span class="hljs-string">               --etcd-cafile=/etc/etcd/ssl/etcd-root-ca.pem \</span><span class="hljs-string">               --etcd-certfile=/etc/etcd/ssl/etcd.pem \</span><span class="hljs-string">               --etcd-keyfile=/etc/etcd/ssl/etcd-key.pem \</span><span class="hljs-string">               --enable-swagger-ui=true \</span><span class="hljs-string">               --apiserver-count=3 \</span><span class="hljs-string">               --audit-policy-file=/etc/kubernetes/audit-policy.yaml \</span><span class="hljs-string">               --audit-log-maxage=30 \</span><span class="hljs-string">               --audit-log-maxbackup=3 \</span><span class="hljs-string">               --audit-log-maxsize=100 \</span><span class="hljs-string">               --audit-log-path=/var/log/kube-audit/audit.log \</span><span class="hljs-string">               --event-ttl=1h"</span></code></pre><p><strong>注意：API SERVER 对比 1.7 配置出现几项变动:</strong></p><ul><li>移除了 <code>--runtime-config=rbac.authorization.k8s.io/v1beta1</code> 配置，因为 RBAC 已经稳定，被纳入了 v1 api，不再需要指定开启</li><li><code>--authorization-mode</code> 授权模型增加了 <code>Node</code> 参数，因为 1.8 后默认 <code>system:node</code> role 不会自动授予 <code>system:nodes</code> 组，具体请参看 <a href="https://github.com/kubernetes/kubernetes/blob/master/CHANGELOG.md#before-upgrading" target="_blank" rel="noopener">CHANGELOG</a>(before-upgrading 段最后一条说明)</li><li>由于以上原因，<code>--admission-control</code> 同时增加了 <code>NodeRestriction</code> 参数，关于关于节点授权器请参考 <a href="https://kubernetes.io/docs/admin/authorization/node/" target="_blank" rel="noopener">Using Node Authorization</a></li><li>增加 <code>--audit-policy-file</code> 参数用于指定高级审计配置，具体可参考 <a href="https://github.com/kubernetes/kubernetes/blob/master/CHANGELOG.md#before-upgrading" target="_blank" rel="noopener">CHANGELOG</a>(before-upgrading 第四条)、<a href="https://kubernetes.io/docs/tasks/debug-application-cluster/audit/#advanced-audit" target="_blank" rel="noopener">Advanced audit</a></li><li>移除 <code>--experimental-bootstrap-token-auth</code> 参数，更换为 <code>--enable-bootstrap-token-auth</code>，详情参考 <a href="https://github.com/kubernetes/kubernetes/blob/master/CHANGELOG.md#auth" target="_blank" rel="noopener">CHANGELOG</a>(Auth 第二条)</li></ul><h5 id="controller-manager-配置"><a href="#controller-manager-配置" class="headerlink" title="controller-manager 配置"></a>controller-manager 配置</h5><pre><code class="hljs sh"><span class="hljs-comment">###</span><span class="hljs-comment"># The following values are used to configure the kubernetes controller-manager</span><span class="hljs-comment"># defaults from config and apiserver should be adequate</span><span class="hljs-comment"># Add your own!</span>KUBE_CONTROLLER_MANAGER_ARGS=<span class="hljs-string">"--address=0.0.0.0 \</span><span class="hljs-string">                              --service-cluster-ip-range=10.254.0.0/16 \</span><span class="hljs-string">                              --cluster-name=kubernetes \</span><span class="hljs-string">                              --cluster-signing-cert-file=/etc/kubernetes/ssl/k8s-root-ca.pem \</span><span class="hljs-string">                              --cluster-signing-key-file=/etc/kubernetes/ssl/k8s-root-ca-key.pem \</span><span class="hljs-string">                              --experimental-cluster-signing-duration=87600h0m0s \</span><span class="hljs-string">                              --service-account-private-key-file=/etc/kubernetes/ssl/k8s-root-ca-key.pem \</span><span class="hljs-string">                              --root-ca-file=/etc/kubernetes/ssl/k8s-root-ca.pem \</span><span class="hljs-string">                              --leader-elect=true \</span><span class="hljs-string">                              --node-monitor-grace-period=40s \</span><span class="hljs-string">                              --node-monitor-period=5s \</span><span class="hljs-string">                              --pod-eviction-timeout=5m0s"</span></code></pre><h5 id="scheduler-配置"><a href="#scheduler-配置" class="headerlink" title="scheduler 配置"></a>scheduler 配置</h5><pre><code class="hljs sh"><span class="hljs-comment">###</span><span class="hljs-comment"># kubernetes scheduler config</span><span class="hljs-comment"># default config should be adequate</span><span class="hljs-comment"># Add your own!</span>KUBE_SCHEDULER_ARGS=<span class="hljs-string">"--leader-elect=true --address=0.0.0.0"</span></code></pre><p>最后启动 Master 相关组件并验证</p><pre><code class="hljs sh">systemctl daemon-reloadsystemctl start kube-apiserversystemctl start kube-controller-managersystemctl start kube-schedulersystemctl <span class="hljs-built_in">enable</span> kube-apiserversystemctl <span class="hljs-built_in">enable</span> kube-controller-managersystemctl <span class="hljs-built_in">enable</span> kube-scheduler</code></pre><p><img src="https://cdn.oss.link/markdown/klnwa.jpg" srcset="/img/loading.gif" alt="Master Success"></p><h3 id="四、搭建-Node-节点"><a href="#四、搭建-Node-节点" class="headerlink" title="四、搭建 Node 节点"></a>四、搭建 Node 节点</h3><h4 id="4-1、分发-rpm-及证书"><a href="#4-1、分发-rpm-及证书" class="headerlink" title="4.1、分发 rpm 及证书"></a>4.1、分发 rpm 及证书</h4><p>对于 Node 节点，只需要安装 <code>kubernetes-node</code> 即可，同时为了方便使用，这里也安装了 <code>kubernetes-client</code>，如下</p><pre><code class="hljs sh"><span class="hljs-keyword">for</span> IP <span class="hljs-keyword">in</span> `seq 8 9`;<span class="hljs-keyword">do</span>    scp kubernetes-node-1.8.0-1.el7.centos.x86_64.rpm kubernetes-client-1.8.0-1.el7.centos.x86_64.rpm root@10.10.1.<span class="hljs-variable">$IP</span>:~    ssh root@10.10.1.<span class="hljs-variable">$IP</span> yum install -y kubernetes-node-1.8.0-1.el7.centos.x86_64.rpm kubernetes-client-1.8.0-1.el7.centos.x86_64.rpm<span class="hljs-keyword">done</span></code></pre><p>同时还要分发相关证书；这里将 Etcd 证书已进行了分发，是因为 <strong>虽然 Node 节点上没有 Etcd，但是如果部署网络组件，如 calico、flannel 等时，网络组件需要联通 Etcd 就会用到 Etcd 的相关证书。</strong></p><pre><code class="hljs sh"><span class="hljs-comment"># 分发 Kubernetes 证书</span><span class="hljs-keyword">for</span> IP <span class="hljs-keyword">in</span> `seq 8 9`;<span class="hljs-keyword">do</span>    ssh root@10.10.1.<span class="hljs-variable">$IP</span> mkdir /etc/kubernetes/ssl    scp *.pem root@10.10.1.<span class="hljs-variable">$IP</span>:/etc/kubernetes/ssl    scp *.kubeconfig token.csv audit-policy.yaml root@10.10.1.<span class="hljs-variable">$IP</span>:/etc/kubernetes    ssh root@10.10.1.<span class="hljs-variable">$IP</span> chown -R kube:kube /etc/kubernetes/ssl<span class="hljs-keyword">done</span><span class="hljs-comment"># 分发 Etcd 证书</span><span class="hljs-keyword">for</span> IP <span class="hljs-keyword">in</span> `seq 8 9`;<span class="hljs-keyword">do</span>    ssh root@10.10.1.<span class="hljs-variable">$IP</span> mkdir -p /etc/etcd/ssl    scp *.pem root@10.10.1.<span class="hljs-variable">$IP</span>:/etc/etcd/ssl    ssh root@10.10.1.<span class="hljs-variable">$IP</span> chmod -R 644 /etc/etcd/ssl/*    ssh root@10.10.1.<span class="hljs-variable">$IP</span> chmod 755 /etc/etcd/ssl<span class="hljs-keyword">done</span></code></pre><h4 id="4-2、修改-Node-配置"><a href="#4-2、修改-Node-配置" class="headerlink" title="4.2、修改 Node 配置"></a>4.2、修改 Node 配置</h4><p>Node 上只需要修改 kubelet 和 kube-proxy 的配置即可</p><h5 id="config-通用配置"><a href="#config-通用配置" class="headerlink" title="config 通用配置"></a>config 通用配置</h5><pre><code class="hljs sh"><span class="hljs-comment">###</span><span class="hljs-comment"># kubernetes system config</span><span class="hljs-comment">#</span><span class="hljs-comment"># The following values are used to configure various aspects of all</span><span class="hljs-comment"># kubernetes services, including</span><span class="hljs-comment">#</span><span class="hljs-comment">#   kube-apiserver.service</span><span class="hljs-comment">#   kube-controller-manager.service</span><span class="hljs-comment">#   kube-scheduler.service</span><span class="hljs-comment">#   kubelet.service</span><span class="hljs-comment">#   kube-proxy.service</span><span class="hljs-comment"># logging to stderr means we get it in the systemd journal</span>KUBE_LOGTOSTDERR=<span class="hljs-string">"--logtostderr=true"</span><span class="hljs-comment"># journal message level, 0 is debug</span>KUBE_LOG_LEVEL=<span class="hljs-string">"--v=2"</span><span class="hljs-comment"># Should this cluster be allowed to run privileged docker containers</span>KUBE_ALLOW_PRIV=<span class="hljs-string">"--allow-privileged=true"</span><span class="hljs-comment"># How the controller-manager, scheduler, and proxy find the apiserver</span><span class="hljs-comment"># KUBE_MASTER="--master=http://127.0.0.1:8080"</span></code></pre><h5 id="kubelet-配置"><a href="#kubelet-配置" class="headerlink" title="kubelet 配置"></a>kubelet 配置</h5><pre><code class="hljs sh"><span class="hljs-comment">###</span><span class="hljs-comment"># kubernetes kubelet (minion) config</span><span class="hljs-comment"># The address for the info server to serve on (set to 0.0.0.0 or "" for all interfaces)</span>KUBELET_ADDRESS=<span class="hljs-string">"--address=10.10.1.8"</span><span class="hljs-comment"># The port for the info server to serve on</span><span class="hljs-comment"># KUBELET_PORT="--port=10250"</span><span class="hljs-comment"># You may leave this blank to use the actual hostname</span>KUBELET_HOSTNAME=<span class="hljs-string">"--hostname-override=docker4.node"</span><span class="hljs-comment"># location of the api-server</span><span class="hljs-comment"># KUBELET_API_SERVER=""</span><span class="hljs-comment"># Add your own!</span>KUBELET_ARGS=<span class="hljs-string">"--cgroup-driver=cgroupfs \</span><span class="hljs-string">              --cluster-dns=10.254.0.2 \</span><span class="hljs-string">              --resolv-conf=/etc/resolv.conf \</span><span class="hljs-string">              --experimental-bootstrap-kubeconfig=/etc/kubernetes/bootstrap.kubeconfig \</span><span class="hljs-string">              --kubeconfig=/etc/kubernetes/kubelet.kubeconfig \</span><span class="hljs-string">              --fail-swap-on=false \</span><span class="hljs-string">              --cert-dir=/etc/kubernetes/ssl \</span><span class="hljs-string">              --cluster-domain=cluster.local. \</span><span class="hljs-string">              --hairpin-mode=promiscuous-bridge \</span><span class="hljs-string">              --serialize-image-pulls=false \</span><span class="hljs-string">              --pod-infra-container-image=gcr.io/google_containers/pause-amd64:3.0"</span></code></pre><p><strong>注意: kubelet 配置与 1.7 版本有一定改动</strong></p><ul><li>增加 <code>--fail-swap-on=false</code> 选项，否则可能导致在开启 swap 分区的机器上无法启动 kubelet，详细可参考 <a href="https://github.com/kubernetes/kubernetes/blob/master/CHANGELOG.md#before-upgrading" target="_blank" rel="noopener">CHANGELOG</a>(before-upgrading 第一条)</li><li>移除 <code>--require-kubeconfig</code> 选项，已经过时废弃</li></ul><h5 id="proxy-配置"><a href="#proxy-配置" class="headerlink" title="proxy 配置"></a>proxy 配置</h5><pre><code class="hljs sh"><span class="hljs-comment">###</span><span class="hljs-comment"># kubernetes proxy config</span><span class="hljs-comment"># default config should be adequate</span><span class="hljs-comment"># Add your own!</span>KUBE_PROXY_ARGS=<span class="hljs-string">"--bind-address=10.10.1.8 \</span><span class="hljs-string">                 --hostname-override=docker4.node \</span><span class="hljs-string">                 --kubeconfig=/etc/kubernetes/kube-proxy.kubeconfig \</span><span class="hljs-string">                 --cluster-cidr=10.254.0.0/16"</span></code></pre><p><strong>kube-proxy 配置与 1.7 并无改变，最新 1.8 的 ipvs 模式将单独写一篇文章，这里不做介绍</strong></p><h4 id="4-3、创建-Nginx-代理"><a href="#4-3、创建-Nginx-代理" class="headerlink" title="4.3、创建 Nginx 代理"></a>4.3、创建 Nginx 代理</h4><p>由于 HA 方案基于 Nginx 反代实现，所以每个 Node 要启动一个 Nginx 负载均衡 Master，具体参考 <a href="https://mritd.me/2017/07/21/set-up-kubernetes-ha-cluster-by-binary/#41ha-master-%E7%AE%80%E8%BF%B0" target="_blank" rel="noopener">HA Master 简述</a></p><h5 id="nginx-conf"><a href="#nginx-conf" class="headerlink" title="nginx.conf"></a>nginx.conf</h5><pre><code class="hljs sh"><span class="hljs-comment"># 创建配置目录</span>mkdir -p /etc/nginx<span class="hljs-comment"># 写入代理配置</span>cat &lt;&lt; EOF &gt;&gt; /etc/nginx/nginx.conferror_log stderr notice;worker_processes auto;events &#123;  multi_accept on;  use epoll;  worker_connections 1024;&#125;stream &#123;    upstream kube_apiserver &#123;        least_conn;        server 10.10.1.5:6443;        server 10.10.1.6:6443;        server 10.10.1.7:6443;    &#125;    server &#123;        listen        0.0.0.0:6443;        proxy_pass    kube_apiserver;        proxy_timeout 10m;        proxy_connect_timeout 1s;    &#125;&#125;EOF<span class="hljs-comment"># 更新权限</span>chmod +r /etc/nginx/nginx.conf</code></pre><h5 id="nginx-proxy-service"><a href="#nginx-proxy-service" class="headerlink" title="nginx-proxy.service"></a>nginx-proxy.service</h5><pre><code class="hljs sh">cat &lt;&lt; EOF &gt;&gt; /etc/systemd/system/nginx-proxy.service[Unit]Description=kubernetes apiserver docker wrapperWants=docker.socketAfter=docker.service[Service]User=rootPermissionsStartOnly=<span class="hljs-literal">true</span>ExecStart=/usr/bin/docker run -p 127.0.0.1:6443:6443 \\                              -v /etc/nginx:/etc/nginx \\                              --name nginx-proxy \\                              --net=host \\                              --restart=on-failure:5 \\                              --memory=512M \\                              nginx:1.13.5-alpineExecStartPre=-/usr/bin/docker rm -f nginx-proxyExecStop=/usr/bin/docker stop nginx-proxyRestart=alwaysRestartSec=15sTimeoutStartSec=30s[Install]WantedBy=multi-user.targetEOF</code></pre><p><strong>最后启动 Nginx 代理即可</strong></p><pre><code class="hljs sh">systemctl daemon-reloadsystemctl start nginx-proxysystemctl <span class="hljs-built_in">enable</span> nginx-proxy</code></pre><h4 id="4-4、添加-Node"><a href="#4-4、添加-Node" class="headerlink" title="4.4、添加 Node"></a>4.4、添加 Node</h4><p>一切准备就绪后就可以添加 Node 了，首先由于我们采用了 <a href="https://kubernetes.io/docs/admin/kubelet-tls-bootstrapping/" target="_blank" rel="noopener">TLS Bootstrapping</a>，所以需要先创建一个 ClusterRoleBinding</p><pre><code class="hljs sh"><span class="hljs-comment"># 在任意 master 执行即可</span>kubectl create clusterrolebinding kubelet-bootstrap \  --clusterrole=system:node-bootstrapper \  --user=kubelet-bootstrap</code></pre><p>然后启动 kubelet</p><pre><code class="hljs sh">systemctl daemon-reloadsystemctl start kubeletsystemctl <span class="hljs-built_in">enable</span> kubelet</code></pre><p>由于采用了 TLS Bootstrapping，所以 kubelet 启动后不会立即加入集群，而是进行证书申请，从日志中可以看到如下输出</p><pre><code class="hljs sh">10月 06 19:53:23 docker4.node kubelet[3797]: I1006 19:53:23.917261    3797 bootstrap.go:57] Using bootstrap kubeconfig to generate TLS client cert, key and kubeconfig file</code></pre><p>此时只需要在 master 允许其证书申请即可</p><pre><code class="hljs sh">kubectl get csr | grep Pending | awk <span class="hljs-string">'&#123;print $1&#125;'</span> | xargs kubectl certificate approve</code></pre><p>此时可以看到 Node 已经加入了</p><pre><code class="hljs sh">docker1.node ➜  ~ kubectl get nodeNAME           STATUS    ROLES     AGE       VERSIONdocker4.node   Ready     &lt;none&gt;    14m       v1.8.0docker5.node   Ready     &lt;none&gt;    3m        v1.8.0</code></pre><p>最后再启动 kube-proxy 即可</p><pre><code class="hljs sh">systemctl start kube-proxysystemctl <span class="hljs-built_in">enable</span> kube-proxy</code></pre><p><strong>再次提醒: 如果 kubelet 启动出现了类似 <code>system:node:xxxx</code> 用户没有权限访问 API 的 RBAC 错误，那么一定是 API Server 授权控制器、准入控制配置有问题，请仔细阅读上面的文档进行更改</strong></p><h4 id="4-5、Master-作为-Node"><a href="#4-5、Master-作为-Node" class="headerlink" title="4.5、Master 作为 Node"></a>4.5、Master 作为 Node</h4><p>如果想讲 Master 也作为 Node 的话，请在 Master 上安装 kubernete-node rpm 包，配置与上面基本一致；<strong>区别于 Master 上不需要启动 nginx 做负载均衡，同时 <code>bootstrap.kubeconfig</code>、<code>kube-proxy.kubeconfig</code> 中的 API Server 地址改成当前 Master IP 即可。</strong></p><p>最终成功后如下图所示</p><p><img src="https://cdn.oss.link/markdown/c4dde.jpg" srcset="/img/loading.gif" alt="cluster success"></p><h3 id="五、部署-Calico"><a href="#五、部署-Calico" class="headerlink" title="五、部署 Calico"></a>五、部署 Calico</h3><h4 id="5-1、修改-Calico-配置"><a href="#5-1、修改-Calico-配置" class="headerlink" title="5.1、修改 Calico 配置"></a>5.1、修改 Calico 配置</h4><p>Calico 部署仍然采用 “混搭” 方式，即 Systemd 控制 calico node，cni 等由 kubernetes daemonset 安装，具体请参考 <a href="https://mritd.me/2017/07/31/calico-yml-bug/" target="_blank" rel="noopener">Calico 部署踩坑记录</a>，以下直接上代码</p><pre><code class="hljs sh"><span class="hljs-comment"># 获取 calico.yaml</span>wget https://docs.projectcalico.org/v2.6/getting-started/kubernetes/installation/hosted/calico.yaml<span class="hljs-comment"># 替换 Etcd 地址</span>sed -i <span class="hljs-string">'s@.*etcd_endpoints:.*@\ \ etcd_endpoints:\ \"https://10.10.1.5:2379,https://10.10.1.6:2379,https://10.10.1.7:2379\"@gi'</span> calico.yaml<span class="hljs-comment"># 替换 Etcd 证书</span><span class="hljs-built_in">export</span> ETCD_CERT=`cat /etc/etcd/ssl/etcd.pem | base64 | tr -d <span class="hljs-string">'\n'</span>`<span class="hljs-built_in">export</span> ETCD_KEY=`cat /etc/etcd/ssl/etcd-key.pem | base64 | tr -d <span class="hljs-string">'\n'</span>`<span class="hljs-built_in">export</span> ETCD_CA=`cat /etc/etcd/ssl/etcd-root-ca.pem | base64 | tr -d <span class="hljs-string">'\n'</span>`sed -i <span class="hljs-string">"s@.*etcd-cert:.*@\ \ etcd-cert:\ <span class="hljs-variable">$&#123;ETCD_CERT&#125;</span>@gi"</span> calico.yamlsed -i <span class="hljs-string">"s@.*etcd-key:.*@\ \ etcd-key:\ <span class="hljs-variable">$&#123;ETCD_KEY&#125;</span>@gi"</span> calico.yamlsed -i <span class="hljs-string">"s@.*etcd-ca:.*@\ \ etcd-ca:\ <span class="hljs-variable">$&#123;ETCD_CA&#125;</span>@gi"</span> calico.yamlsed -i <span class="hljs-string">'s@.*etcd_ca:.*@\ \ etcd_ca:\ "/calico-secrets/etcd-ca"@gi'</span> calico.yamlsed -i <span class="hljs-string">'s@.*etcd_cert:.*@\ \ etcd_cert:\ "/calico-secrets/etcd-cert"@gi'</span> calico.yamlsed -i <span class="hljs-string">'s@.*etcd_key:.*@\ \ etcd_key:\ "/calico-secrets/etcd-key"@gi'</span> calico.yaml<span class="hljs-comment"># 注释掉 calico-node 部分(由 Systemd 接管)</span>sed -i <span class="hljs-string">'103,189s@.*@#&amp;@gi'</span> calico.yaml</code></pre><h4 id="5-2、创建-Systemd-文件"><a href="#5-2、创建-Systemd-文件" class="headerlink" title="5.2、创建 Systemd 文件"></a>5.2、创建 Systemd 文件</h4><p>上一步注释了 <code>calico.yaml</code> 中 Calico Node 相关内容，为了防止自动获取 IP 出现问题，将其移动到 Systemd，Systemd service 配置如下，<strong>每个节点都要安装 calico-node 的 Service</strong>，其他节点请自行修改 ip(被问我为啥是两个反引号 <code>\\</code>，自己试就知道了)</p><pre><code class="hljs sh">cat &gt; /usr/lib/systemd/system/calico-node.service &lt;&lt;EOF[Unit]Description=calico nodeAfter=docker.serviceRequires=docker.service[Service]User=rootPermissionsStartOnly=<span class="hljs-literal">true</span>ExecStart=/usr/bin/docker run   --net=host --privileged --name=calico-node \\                                -e ETCD_ENDPOINTS=https://10.10.1.5:2379,https://10.10.1.6:2379,https://10.10.1.7:2379 \\                                -e ETCD_CA_CERT_FILE=/etc/etcd/ssl/etcd-root-ca.pem \\                                -e ETCD_CERT_FILE=/etc/etcd/ssl/etcd.pem \\                                -e ETCD_KEY_FILE=/etc/etcd/ssl/etcd-key.pem \\                                -e NODENAME=docker1.node \\                                -e IP=10.10.1.5 \\                                -e IP6= \\                                -e AS= \\                                -e CALICO_IPV4POOL_CIDR=10.20.0.0/16 \\                                -e CALICO_IPV4POOL_IPIP=always \\                                -e CALICO_LIBNETWORK_ENABLED=<span class="hljs-literal">true</span> \\                                -e CALICO_NETWORKING_BACKEND=bird \\                                -e CALICO_DISABLE_FILE_LOGGING=<span class="hljs-literal">true</span> \\                                -e FELIX_IPV6SUPPORT=<span class="hljs-literal">false</span> \\                                -e FELIX_DEFAULTENDPOINTTOHOSTACTION=ACCEPT \\                                -e FELIX_LOGSEVERITYSCREEN=info \\                                -v /etc/etcd/ssl/etcd-root-ca.pem:/etc/etcd/ssl/etcd-root-ca.pem \\                                -v /etc/etcd/ssl/etcd.pem:/etc/etcd/ssl/etcd.pem \\                                -v /etc/etcd/ssl/etcd-key.pem:/etc/etcd/ssl/etcd-key.pem \\                                -v /var/run/calico:/var/run/calico \\                                -v /lib/modules:/lib/modules \\                                -v /run/docker/plugins:/run/docker/plugins \\                                -v /var/run/docker.sock:/var/run/docker.sock \\                                -v /var/<span class="hljs-built_in">log</span>/calico:/var/<span class="hljs-built_in">log</span>/calico \\                                quay.io/calico/node:v2.6.1ExecStop=/usr/bin/docker rm -f calico-nodeRestart=alwaysRestartSec=10[Install]WantedBy=multi-user.targetEOF</code></pre><h4 id="5-3、修改-kubelet-配置"><a href="#5-3、修改-kubelet-配置" class="headerlink" title="5.3、修改 kubelet 配置"></a>5.3、修改 kubelet 配置</h4><p>根据官方文档要求 <code>kubelet</code> 配置必须增加 <code>--network-plugin=cni</code> 选项，所以需要修改 kubelet 配置</p><pre><code class="hljs sh"><span class="hljs-comment">###</span><span class="hljs-comment"># kubernetes kubelet (minion) config</span><span class="hljs-comment"># The address for the info server to serve on (set to 0.0.0.0 or "" for all interfaces)</span>KUBELET_ADDRESS=<span class="hljs-string">"--address=10.10.1.5"</span><span class="hljs-comment"># The port for the info server to serve on</span><span class="hljs-comment"># KUBELET_PORT="--port=10250"</span><span class="hljs-comment"># You may leave this blank to use the actual hostname</span>KUBELET_HOSTNAME=<span class="hljs-string">"--hostname-override=docker1.node"</span><span class="hljs-comment"># location of the api-server</span><span class="hljs-comment"># KUBELET_API_SERVER=""</span><span class="hljs-comment"># Add your own!</span>KUBELET_ARGS=<span class="hljs-string">"--cgroup-driver=cgroupfs \</span><span class="hljs-string">              --network-plugin=cni \</span><span class="hljs-string">              --cluster-dns=10.254.0.2 \</span><span class="hljs-string">              --resolv-conf=/etc/resolv.conf \</span><span class="hljs-string">              --experimental-bootstrap-kubeconfig=/etc/kubernetes/bootstrap.kubeconfig \</span><span class="hljs-string">              --kubeconfig=/etc/kubernetes/kubelet.kubeconfig \</span><span class="hljs-string">              --fail-swap-on=false \</span><span class="hljs-string">              --cert-dir=/etc/kubernetes/ssl \</span><span class="hljs-string">              --cluster-domain=cluster.local. \</span><span class="hljs-string">              --hairpin-mode=promiscuous-bridge \</span><span class="hljs-string">              --serialize-image-pulls=false \</span><span class="hljs-string">              --pod-infra-container-image=gcr.io/google_containers/pause-amd64:3.0"</span></code></pre><p>然后重启即可</p><pre><code class="hljs sh">systemctl daemon-reloadsystemctl restart kubelet</code></pre><p>此时执行 <code>kubectl get node</code> 会看到 Node 为 <code>NotReady</code> 状态，属于正常情况</p><h4 id="5-4、创建-Calico-Daemonset"><a href="#5-4、创建-Calico-Daemonset" class="headerlink" title="5.4、创建 Calico Daemonset"></a>5.4、创建 Calico Daemonset</h4><pre><code class="hljs sh"><span class="hljs-comment"># 先创建 RBAC</span>kubectl apply -f https://docs.projectcalico.org/v2.6/getting-started/kubernetes/installation/rbac.yaml<span class="hljs-comment"># 再创建 Calico Daemonset</span>kubectl create -f calico.yaml</code></pre><h4 id="5-5、创建-Calico-Node"><a href="#5-5、创建-Calico-Node" class="headerlink" title="5.5、创建 Calico Node"></a>5.5、创建 Calico Node</h4><p>Calico Node 采用 Systemd 方式启动，在每个节点配置好 Systemd service后，<strong>每个节点修改对应的 <code>calico-node.service</code> 中的 IP 和节点名称，然后启动即可</strong></p><pre><code class="hljs sh">systemctl daemon-reloadsystemctl restart calico-nodesleep 5systemctl restart kubelet</code></pre><p>此时检查 Node 应该都处于 Ready 状态</p><p><img src="https://cdn.oss.link/markdown/agxp3.jpg" srcset="/img/loading.gif" alt="Node Ready"></p><p><strong>最后测试一下跨主机通讯</strong></p><pre><code class="hljs sh"><span class="hljs-comment"># 创建 deployment</span>cat &lt;&lt; EOF &gt;&gt; demo.deploy.ymlapiVersion: apps/v1beta2kind: Deploymentmetadata:  name: demo-deploymentspec:  replicas: 5  selector:    matchLabels:      app: demo  template:    metadata:      labels:        app: demo    spec:      containers:      - name: demo        image: mritd/demo        imagePullPolicy: IfNotPresent        ports:        - containerPort: 80EOFkubectl create -f demo.deploy.yml</code></pre><p><strong>进入其中一个 Pod，ping 另一个 Pod 的 IP 测试即可</strong></p><p><img src="https://cdn.oss.link/markdown/00krx.jpg" srcset="/img/loading.gif" alt="Test Calico"></p><h3 id="六、部署-DNS"><a href="#六、部署-DNS" class="headerlink" title="六、部署 DNS"></a>六、部署 DNS</h3><h4 id="6-1、部署集群-DNS"><a href="#6-1、部署集群-DNS" class="headerlink" title="6.1、部署集群 DNS"></a>6.1、部署集群 DNS</h4><p>DNS 组件部署非常简单，直接创建相应的 deployment 等即可；但是有一个事得说一嘴，Kubernets 一直在推那个 <code>Addon Manager</code> 的工具来管理 DNS 啥的，文档说的条条是道，就是不希望我们手动搞这些东西，防止意外修改云云… 但问题是关于那个 <code>Addon Manager</code> 咋用一句没提，虽然说里面就一个小脚本，看看也能懂；但是我还是选择手动 😌… 还有这个 DNS 配置文件好像又挪地方了，以前在 <code>contrib</code> 项目下的…</p><pre><code class="hljs sh"><span class="hljs-comment"># 获取文件</span>wget https://raw.githubusercontent.com/kubernetes/kubernetes/master/cluster/addons/dns/kube-dns.yaml.sedmv kube-dns.yaml.sed kube-dns.yaml<span class="hljs-comment"># 修改配置</span>sed -i <span class="hljs-string">'s/$DNS_DOMAIN/cluster.local/gi'</span> kube-dns.yamlsed -i <span class="hljs-string">'s/$DNS_SERVER_IP/10.254.0.2/gi'</span> kube-dns.yaml<span class="hljs-comment"># 创建</span>kubectl create -f kube-dns.yaml</code></pre><p>创建好以后如下所示</p><p><img src="https://cdn.oss.link/markdown/vg95n.jpg" srcset="/img/loading.gif" alt="DNS"></p><p>然后创建两组 Pod 和 Service，进入 Pod 中 curl 另一个 Service 名称看看是否能解析；同时还要测试一下外网能否解析</p><p><img src="https://cdn.oss.link/markdown/x185c.jpg" srcset="/img/loading.gif" alt="Test DNS1"></p><p>测试外网</p><p><img src="https://cdn.oss.link/markdown/3k9gz.jpg" srcset="/img/loading.gif" alt="Test DNS2"></p><h4 id="6-2、部署-DNS-自动扩容部署"><a href="#6-2、部署-DNS-自动扩容部署" class="headerlink" title="6.2、部署 DNS 自动扩容部署"></a>6.2、部署 DNS 自动扩容部署</h4><p>这个同样下载 yaml，然后创建一下即可，不需要修改任何配置</p><pre><code class="hljs sh">wget https://raw.githubusercontent.com/kubernetes/kubernetes/master/cluster/addons/dns-horizontal-autoscaler/dns-horizontal-autoscaler.yamlkubectl create -f dns-horizontal-autoscaler.yaml</code></pre><p>部署完成后如下</p><p><img src="https://cdn.oss.link/markdown/mid1u.jpg" srcset="/img/loading.gif" alt="DNS autoscaler"></p><p>自动扩容这里不做测试了，虚拟机吃不消了，详情自己参考 <a href="https://kubernetes.io/docs/tasks/administer-cluster/dns-horizontal-autoscaling/" target="_blank" rel="noopener">Autoscale the DNS Service in a Cluster</a></p><p><strong>kube-proxy ipvs 下一篇写，坑有点多，虽然搞定了，但是一篇写有点囫囵吞枣，后来想一想还是分开吧</strong></p>]]></content>
    
    
    <categories>
      
      <category>Kubernetes</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Linux</tag>
      
      <tag>Docker</tag>
      
      <tag>Kubernetes</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Alpine 3.6 OpenJDK 8 Bug</title>
    <link href="/2017/09/27/alpine-3.6-openjdk-8-bug/"/>
    <url>/2017/09/27/alpine-3.6-openjdk-8-bug/</url>
    
    <content type="html"><![CDATA[<blockquote><p>最近切换项目基础镜像踩到一个大坑，由于 alpine 基础镜像和 OpenJDK8 Bug 导致鼓捣了2天才解决，故记录一下这个问题</p></blockquote><h3 id="一、问题环境"><a href="#一、问题环境" class="headerlink" title="一、问题环境"></a>一、问题环境</h3><p>出现问题的基本环境如下</p><ul><li>OpneJDK 8u131</li><li>Alpine 3.6</li><li>Kaptcha (Java 验证码库)</li></ul><h3 id="二、问题描述"><a href="#二、问题描述" class="headerlink" title="二、问题描述"></a>二、问题描述</h3><p>出现问题表象为 <strong>Spring Boot 项目启动后，访问注册页(有验证码)时，验证码不显示，后台报错信息大意为缺失字体库，安装字体后会报错说 <code>libfontmanager.so: AWTFontDefaultChar: symbol not found</code></strong></p><h3 id="三、解决方案"><a href="#三、解决方案" class="headerlink" title="三、解决方案"></a>三、解决方案</h3><p>当出现字体找不到这种错误时，原因是 <strong>Alpine 太过精简，导致里面没有字体，只需要安装字体即可</strong>，在 Dockerfile 中添加如下命令即可:</p><pre><code class="hljs sh">apk add --update font-adobe-100dpi ttf-dejavu fontconfig</code></pre><p>当安装字体后，可能会出现如下错误:</p><pre><code class="hljs sh">Caused by: java.lang.UnsatisfiedLinkError: /usr/lib/jvm/java-1.8-openjdk/jre/lib/amd64/libfontmanager.so: Error relocating /usr/lib/jvm/java-1.8-openjdk/jre/lib/amd64/libfontmanager.so: AWTFontDefaultChar: symbol not found    at java.lang.ClassLoader<span class="hljs-variable">$NativeLibrary</span>.load(Native Method)    at java.lang.ClassLoader.loadLibrary0(ClassLoader.java:1941)    at java.lang.ClassLoader.loadLibrary(ClassLoader.java:1845)    at java.lang.Runtime.loadLibrary0(Runtime.java:870)    at java.lang.System.loadLibrary(System.java:1122)    at sun.font.FontManagerNativeLibrary<span class="hljs-variable">$1</span>.run(FontManagerNativeLibrary.java:61)    at java.security.AccessController.doPrivileged(Native Method)    at sun.font.FontManagerNativeLibrary.&lt;clinit&gt;(FontManagerNativeLibrary.java:32)    at sun.font.SunFontManager<span class="hljs-variable">$1</span>.run(SunFontManager.java:339)    at java.security.AccessController.doPrivileged(Native Method)    at sun.font.SunFontManager.&lt;clinit&gt;(SunFontManager.java:335)    at java.lang.Class.forName0(Native Method)    at java.lang.Class.forName(Class.java:348)    at sun.font.FontManagerFactory<span class="hljs-variable">$1</span>.run(FontManagerFactory.java:82)    at java.security.AccessController.doPrivileged(Native Method)    at sun.font.FontManagerFactory.getInstance(FontManagerFactory.java:74)    at java.awt.Font.getFont2D(Font.java:491)    at java.awt.Font.getFamily(Font.java:1220)    at java.awt.Font.getFamily_NoClientCode(Font.java:1194)    at java.awt.Font.getFamily(Font.java:1186)    at java.awt.Font.toString(Font.java:1683)    at hudson.util.ChartUtil.&lt;clinit&gt;(ChartUtil.java:260)    at hudson.WebAppMain.contextInitialized(WebAppMain.java:194)    ... 23 more</code></pre><p>Google 半天，最后找到了 <a href="https://bugs.alpinelinux.org/issues/7372" target="_blank" rel="noopener">Alpine 官方 Bug 列表</a>，在最后面做了回复，其中大意是: <strong>Alpine 3.6 版本的 Docker 镜像中安装的是 OpenJDK 8u131，这个版本有 BUG，并且在 3.6.3 的 OpenJDK 8.141.15 版本做了修复</strong>；从上面可知我们解决方案有两个:</p><ul><li>降级到 Alpine 3.5，其内的 OpneJDK 是 8u121 版本，没有这个 Bug</li><li>升级到 Alpine Edge，其内部 OpenJDK 版本为 8.144.01，已经修复了这个 Bug</li></ul><p>当然我选择浪一波，做了升级，最终基础镜像的 Dockerfile 如下</p><pre><code class="hljs sh">FROM alpine:edgeLABEL maintainer=<span class="hljs-string">"mritd &lt;mritd1234@gmail.com&gt;"</span>ENV JAVA_HOME /usr/lib/jvm/java-1.8-openjdkENV PATH <span class="hljs-variable">$PATH</span>:/usr/lib/jvm/java-1.8-openjdk/jre/bin:/usr/lib/jvm/java-1.8-openjdk/binENV JAVA_VERSION 8u144ENV JAVA_ALPINE_VERSION 8.144.01-r0RUN apk add --update bash curl tar wget ca-certificates unzip \        openjdk8=<span class="hljs-variable">$&#123;JAVA_ALPINE_VERSION&#125;</span> font-adobe-100dpi ttf-dejavu fontconfig \    &amp;&amp; rm -rf /var/cache/apk/* \CMD [<span class="hljs-string">"bash"</span>]</code></pre>]]></content>
    
    
    <categories>
      
      <category>Docker</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Java</tag>
      
      <tag>Linux</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Consul 集群搭建</title>
    <link href="/2017/09/21/set-up-ha-consul-cluster/"/>
    <url>/2017/09/21/set-up-ha-consul-cluster/</url>
    
    <content type="html"><![CDATA[<blockquote><p>不知道 Consul 用的人多还是少，最近有人问怎么搭建 Consul 集群，这里顺手记录一下吧</p></blockquote><h3 id="一、简介"><a href="#一、简介" class="headerlink" title="一、简介"></a>一、简介</h3><p>Consul 与 Etcd 一样，都属于分布式一致性数据库，其主要特性就是在分布式系统中出现意外情况如节点宕机的情况下保证数据的一致性；相对于 Etcd 来说，Consul 提供了更加实用的其他功能特性，如 DNS、健康检查、服务发现、多数据中心等，同时还有 web ui 界面，体验相对于更加友好</p><h3 id="二、环境准备"><a href="#二、环境准备" class="headerlink" title="二、环境准备"></a>二、环境准备</h3><p>同 Etcd 一样，Consul 最少也需要 3 台机器，这里测试实用 5 台机器进行部署集群，具体环境如下</p><table><thead><tr><th>节点</th><th>IP</th><th>Version</th></tr></thead><tbody><tr><td>server</td><td>192.168.1.11</td><td>v0.9.3</td></tr><tr><td>server</td><td>192.168.1.12</td><td>v0.9.3</td></tr><tr><td>server</td><td>192.168.1.13</td><td>v0.9.3</td></tr><tr><td>client</td><td>192.168.1.14</td><td>v0.9.3</td></tr><tr><td>client</td><td>192.168.1.15</td><td>v0.9.3</td></tr></tbody></table><p>其中 consul 采用 rpm 包的形式进行安装，这里并没有使用 docker 方式启动是因为个人习惯重要的数据存储服务交给 systemd管理；因为 docker 存在 docker daemon 的原因，如果用 docker 启动这种存储核心数据的组件，一但 daemon 出现问题那么所有容器都将出现问题；所以个人还是比较习惯将 etcd 和 consul 以二进制装在宿主机，由 systemd 直接管理。</p><h3 id="三、部署集群"><a href="#三、部署集群" class="headerlink" title="三、部署集群"></a>三、部署集群</h3><h4 id="3-1、Consul-集群模式"><a href="#3-1、Consul-集群模式" class="headerlink" title="3.1、Consul 集群模式"></a>3.1、Consul 集群模式</h4><p>Consul 集群与 Etcd 略有区别，<strong>Consul 在启动后分为两种模式:</strong></p><ul><li>Server 模式: 一个 Server 是一个有一组扩展功能的代理，这些功能包括参与 Raft 选举，维护集群状态，响应 RPC 查询，与其他数据中心交互 WAN gossip 和转发查询给 leader 或者远程数据中心。</li><li>Client 模式: 一个 Client 是一个转发所有 RPC 到 Server 的代理。这个 Client 是相对无状态的；Client 唯一执行的后台活动是加入 LAN gossip 池，这有一个最低的资源开销并且仅消耗少量的网络带宽。</li></ul><p><strong>其集群后如下所示:</strong></p><p><img src="https://cdn.oss.link/markdown/n4mdw.jpg" srcset="/img/loading.gif" alt="Consul Cluster"></p><h4 id="3-2、集群搭建"><a href="#3-2、集群搭建" class="headerlink" title="3.2、集群搭建"></a>3.2、集群搭建</h4><p>Consul 集群搭建时一般提供两种模式:</p><ul><li><strong>手动模式: 启动第一个节点后，此时此节点处于 bootstrap 模式，其节点手动执行加入</strong></li><li><strong>自动模式: 启动第一个节点后，在其他节点配置好尝试加入的目标节点，然后等待其自动加入(不需要人为命令加入)</strong></li></ul><p>这里采用自动加入模式，搭建过程如下:</p><p><strong>首先获取 Consul 的 rpm 包，鉴于官方并未提供 rpm 安装包，所以我自己造了一个轮子，打包脚本见 <a href="https://github.com/mritd/consul-rpm" target="_blank" rel="noopener">Github</a>，以下直接从我的 yum 源中安装</strong></p><pre><code class="hljs sh"><span class="hljs-comment"># 安装 yum 源</span>tee /etc/yum.repos.d/mritd.repo &lt;&lt; EOF[mritdrepo]name=Mritd Repositorybaseurl=https://yumrepo.b0.upaiyun.com/centos/7/x86_64enabled=1gpgcheck=1gpgkey=https://cdn.oss.link/keys/rpm.public.keyEOF<span class="hljs-comment"># 安装 Consul，请不要在大规模部署时使用此 yum 源，CDN 流量不多请手下留情，</span><span class="hljs-comment"># 如需大规模部署 请使用 yumdonwloader 工具下载 rpm 后手动分发安装</span>yum install -y consul</code></pre><p><strong>5 台机器安装好后修改其中三台为 Server 模式并启动</strong></p><pre><code class="hljs sh">vim /etc/consul/consul.json<span class="hljs-comment"># 配置如下</span>&#123;    <span class="hljs-string">"datacenter"</span>: <span class="hljs-string">"dc1"</span>,                // 数据中心名称    <span class="hljs-string">"data_dir"</span>: <span class="hljs-string">"/var/lib/consul"</span>,      // Server 节点数据目录    <span class="hljs-string">"log_level"</span>: <span class="hljs-string">"INFO"</span>,                // 日志级别    <span class="hljs-string">"node_name"</span>: <span class="hljs-string">"docker1.node"</span>,        // 当前节点名称    <span class="hljs-string">"server"</span>: <span class="hljs-literal">true</span>,                     // 是否为 Server 模式，<span class="hljs-literal">false</span> 为 Client 模式    <span class="hljs-string">"ui"</span>: <span class="hljs-literal">true</span>,                         // 是否开启 UI 访问    <span class="hljs-string">"bootstrap_expect"</span>: 1,              // 启动时期望的就绪节点，1 代表启动为 bootstrap 模式，等待其他节点加入    <span class="hljs-string">"bind_addr"</span>: <span class="hljs-string">"192.168.1.11"</span>,        // 绑定的 IP    <span class="hljs-string">"client_addr"</span>: <span class="hljs-string">"192.168.1.11"</span>,      // 同时作为 Client 接受请求的绑定 IP    <span class="hljs-string">"retry_join"</span>: [<span class="hljs-string">"192.168.1.12"</span>,<span class="hljs-string">"192.168.1.13"</span>],  // 尝试加入的其他节点    <span class="hljs-string">"retry_interval"</span>: <span class="hljs-string">"3s"</span>,             // 每次尝试间隔    <span class="hljs-string">"raft_protocol"</span>: 3,                 // Raft 协议版本    <span class="hljs-string">"enable_debug"</span>: <span class="hljs-literal">false</span>,              // 是否开启 Debug 模式    <span class="hljs-string">"rejoin_after_leave"</span>: <span class="hljs-literal">true</span>,         // 允许重新加入集群    <span class="hljs-string">"enable_syslog"</span>: <span class="hljs-literal">false</span>              // 是否开启 syslog&#125;</code></pre><p><strong>另外两个节点与以上配置大致相同，差别在于其他两个 Server 节点 <code>bootstrap_expect</code> 值为 2，即期望启动时已经有两个节点就绪；然后依次启动三个 Server 节点即可</strong></p><pre><code class="hljs sh">systemctl start consulsystemctl <span class="hljs-built_in">enable</span> consulsystemctl status consul</code></pre><p><strong>此时可访问任意一台 Server 节点的 UI 界面，地址为 <code>http://serverIP:8500</code>，截图如下</strong></p><p><img src="https://cdn.oss.link/markdown/t9cxf.jpg" srcset="/img/loading.gif" alt="Server Success"></p><p>接下来修改其他两个节点配置，使其作为 Client 加入到集群即可，<strong>注意的是当处于 Client 模式时，<code>bootstrap_expect</code> 必须为 0，即关闭状态；具体配置如下</strong></p><pre><code class="hljs json">&#123;    <span class="hljs-attr">"datacenter"</span>: <span class="hljs-string">"dc1"</span>,    <span class="hljs-attr">"data_dir"</span>: <span class="hljs-string">"/var/lib/consul"</span>,    <span class="hljs-attr">"log_level"</span>: <span class="hljs-string">"INFO"</span>,    <span class="hljs-attr">"node_name"</span>: <span class="hljs-string">"docker4.node"</span>,    <span class="hljs-attr">"server"</span>: <span class="hljs-literal">false</span>,    <span class="hljs-attr">"ui"</span>: <span class="hljs-literal">true</span>,    <span class="hljs-attr">"bootstrap_expect"</span>: <span class="hljs-number">0</span>,    <span class="hljs-attr">"bind_addr"</span>: <span class="hljs-string">"192.168.1.14"</span>,    <span class="hljs-attr">"client_addr"</span>: <span class="hljs-string">"192.168.1.14"</span>,    <span class="hljs-attr">"retry_join"</span>: [<span class="hljs-string">"192.168.1.11"</span>,<span class="hljs-string">"192.168.1.12"</span>,<span class="hljs-string">"192.168.1.13"</span>],    <span class="hljs-attr">"retry_interval"</span>: <span class="hljs-string">"3s"</span>,    <span class="hljs-attr">"raft_protocol"</span>: <span class="hljs-number">3</span>,    <span class="hljs-attr">"enable_debug"</span>: <span class="hljs-literal">false</span>,    <span class="hljs-attr">"rejoin_after_leave"</span>: <span class="hljs-literal">true</span>,    <span class="hljs-attr">"enable_syslog"</span>: <span class="hljs-literal">false</span>&#125;</code></pre><p>另外一个 Client 配置与以上相同，最终集群成功后如下所示</p><p><img src="https://cdn.oss.link/markdown/j1zrc.jpg" srcset="/img/loading.gif" alt="Cluster ok"></p><p><img src="https://cdn.oss.link/markdown/kq4cz.jpg" srcset="/img/loading.gif" alt="Command Line"></p><h3 id="四、其他说明"><a href="#四、其他说明" class="headerlink" title="四、其他说明"></a>四、其他说明</h3><p>关于 Consul 的其他各种参数说明，中文版可参考 <a href="http://www.10tiao.com/html/357/201705/2247485185/1.html" target="_blank" rel="noopener">Consul集群部署</a>；这个文章对大体上讲的基本很全了，但是随着版本变化，有些参数还是需要参考一下 <a href="https://www.consul.io/docs/agent/options.html" target="_blank" rel="noopener">官方配置文档</a></p>]]></content>
    
    
    <categories>
      
      <category>Docker</category>
      
      <category>Kubernetes</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Linux</tag>
      
      <tag>Docker</tag>
      
      <tag>Kubernetes</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>阿里云部署 Kubernetes</title>
    <link href="/2017/09/20/set-up-ha-kubernetes-cluster-on-aliyun-ecs/"/>
    <url>/2017/09/20/set-up-ha-kubernetes-cluster-on-aliyun-ecs/</url>
    
    <content type="html"><![CDATA[<blockquote><p>公司有点小需求，在阿里云上开了几台机器，然后部署了一个 Kubernetes 集群，以下记录一下阿里云踩坑问题，主要是网络组件的坑。</p></blockquote><h3 id="一、部署环境"><a href="#一、部署环境" class="headerlink" title="一、部署环境"></a>一、部署环境</h3><p>部署时开启了 4 台 ECS 实例，基本部署环境与裸机部署相似，其中区别是，阿里云网络采用 VPC 网络，不过以下流程适用于经典网络；以下为各个组件版本:</p><ul><li>OS CentOS</li><li>Kernel 4.4.88-1.el7.elrepo.x86_64</li><li>docker 1.13.1</li><li>Kubernetes 1.7.5</li><li>flannel v0.8.0-amd64</li></ul><p>flannel 采用 vxlan 模式，虽然性能不太好，但是兼容度高一点；在阿里云上 flannel 可以采用 vpc 方式，具体可参考 <a href="https://coreos.com/flannel/docs/latest/alicloud-vpc-backend.html" target="_blank" rel="noopener">官方文档</a>(这个文档中描述的方法应该更适合 CNM 方式，我用的是 CNI，所以没去折腾他)</p><h3 id="二、基本部署流程"><a href="#二、基本部署流程" class="headerlink" title="二、基本部署流程"></a>二、基本部署流程</h3><p>关于 Master HA 等基本部署流程可以参考 <a href="https://mritd.me/2017/07/21/set-up-kubernetes-ha-cluster-by-binary/" target="_blank" rel="noopener">手动档搭建 Kubernetes HA 集群</a> 这篇文章，在部署网络组件之前的流程是相同的，这里不再阐述</p><h3 id="三、Flannel-部署"><a href="#三、Flannel-部署" class="headerlink" title="三、Flannel 部署"></a>三、Flannel 部署</h3><p>关于 Flannel 部署，基本上有两种模式，一种是 vxlan，一种是采用 VPC，VPC 相关的部署上面已经提了，可以参考官方文档；以下说一下 Flannel 的 vxlan 部署方式:</p><h4 id="3-1、CNI-配置"><a href="#3-1、CNI-配置" class="headerlink" title="3.1、CNI 配置"></a>3.1、CNI 配置</h4><p>首先保证集群在不开启 CNI 插件的情况下所有 Node Ready 状态，然后修改 <code>/etc/kubernetes/kubelet</code> 配置文件，加入 CNI 支持( <code>--network-plugin</code> )，配置如下</p><pre><code class="hljs sh"><span class="hljs-comment"># kubernetes kubelet (minion) config</span><span class="hljs-comment"># The address for the info server to serve on (set to 0.0.0.0 or "" for all interfaces)</span>KUBELET_ADDRESS=<span class="hljs-string">"--address=192.168.1.77"</span><span class="hljs-comment"># The port for the info server to serve on</span><span class="hljs-comment"># KUBELET_PORT="--port=10250"</span><span class="hljs-comment"># You may leave this blank to use the actual hostname</span>KUBELET_HOSTNAME=<span class="hljs-string">"--hostname-override=docker77.node"</span><span class="hljs-comment"># location of the api-server</span><span class="hljs-comment"># KUBELET_API_SERVER=""</span><span class="hljs-comment"># Add your own!</span>KUBELET_ARGS=<span class="hljs-string">"--cgroup-driver=cgroupfs \</span><span class="hljs-string">              --cluster-dns=10.254.0.2 \</span><span class="hljs-string">              --network-plugin=cni \</span><span class="hljs-string">              --resolv-conf=/etc/resolv.conf \</span><span class="hljs-string">              --experimental-bootstrap-kubeconfig=/etc/kubernetes/bootstrap.kubeconfig \</span><span class="hljs-string">              --kubeconfig=/etc/kubernetes/kubelet.kubeconfig \</span><span class="hljs-string">              --require-kubeconfig \</span><span class="hljs-string">              --cert-dir=/etc/kubernetes/ssl \</span><span class="hljs-string">              --cluster-domain=cluster.local. \</span><span class="hljs-string">              --hairpin-mode promiscuous-bridge \</span><span class="hljs-string">              --serialize-image-pulls=false \</span><span class="hljs-string">              --pod-infra-container-image=gcr.io/google_containers/pause-amd64:3.0"</span></code></pre><h4 id="3-2、Cluster-CIDR-配置"><a href="#3-2、Cluster-CIDR-配置" class="headerlink" title="3.2、Cluster CIDR 配置"></a>3.2、Cluster CIDR 配置</h4><p><strong>在开启 CNI 时使用 Flannel，要设置 <code>--allocate-node-cidrs</code> 和 <code>--cluster-cidr</code> 以保证 Flannel 能正确进行 IP 分配，这两个配置需要加入到 <code>/etc/kubernetes/controller-manager</code> 配置中，完整配置如下</strong></p><pre><code class="hljs sh"><span class="hljs-comment">###</span><span class="hljs-comment"># The following values are used to configure the kubernetes controller-manager</span><span class="hljs-comment"># defaults from config and apiserver should be adequate</span><span class="hljs-comment"># Add your own!</span>KUBE_CONTROLLER_MANAGER_ARGS=<span class="hljs-string">"--address=192.168.1.77 \</span><span class="hljs-string">                              --allocate-node-cidrs=true \</span><span class="hljs-string">                              --cluster-cidr=10.244.0.0/16 \</span><span class="hljs-string">                              --service-cluster-ip-range=10.254.0.0/16 \</span><span class="hljs-string">                              --cluster-name=kubernetes \</span><span class="hljs-string">                              --cluster-signing-cert-file=/etc/kubernetes/ssl/k8s-root-ca.pem \</span><span class="hljs-string">                              --cluster-signing-key-file=/etc/kubernetes/ssl/k8s-root-ca-key.pem \</span><span class="hljs-string">                              --service-account-private-key-file=/etc/kubernetes/ssl/k8s-root-ca-key.pem \</span><span class="hljs-string">                              --root-ca-file=/etc/kubernetes/ssl/k8s-root-ca.pem \</span><span class="hljs-string">                              --leader-elect=true \</span><span class="hljs-string">                              --node-monitor-grace-period=40s \</span><span class="hljs-string">                              --node-monitor-period=5s \</span><span class="hljs-string">                              --pod-eviction-timeout=5m0s"</span></code></pre><h4 id="3-3、CNI-插件配置"><a href="#3-3、CNI-插件配置" class="headerlink" title="3.3、CNI 插件配置"></a>3.3、CNI 插件配置</h4><p>开启 CNI 后，kubelet 创建的 POD 则需要 CNI 插件支持，这里让我感觉奇怪的是 Flannel 的 yaml 中对于 <code>install-cni</code> 这个容器只进行了配置复制，没有做插件复制；所以我们需要手动安装 CNI 插件，CNI 插件最新版本请留意 <a href="https://github.com/containernetworking/plugins/releases" target="_blank" rel="noopener">Github</a>；安装过程如下:</p><pre><code class="hljs sh"><span class="hljs-comment"># 创建 CNI 目录</span>mkdir -p /opt/cni/bin<span class="hljs-comment"># 下载 CNI 插件</span>wget https://github.com/containernetworking/plugins/releases/download/v0.6.0/cni-plugins-amd64-v0.6.0.tgztar -zxvf cni-plugins-amd64-v0.6.0.tgz<span class="hljs-comment"># 移动 CNI 插件</span>mv bridge flannel host-local loopback /opt/cni/bin</code></pre><h4 id="3-4、安装-Flannel"><a href="#3-4、安装-Flannel" class="headerlink" title="3.4、安装 Flannel"></a>3.4、安装 Flannel</h4><p>当上面所有配置和 CNI 插件安装完成后，应当重启 kube-controller-manager 和 kubelet</p><pre><code class="hljs sh">systemctl daemon-reloadsystemctl restart kube-controller-manager kubelet</code></pre><p>然后安装 Flannel 并配置 RBAC 即可</p><pre><code class="hljs sh">kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.ymlkubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel-rbac.yml</code></pre><p><strong>其他部署如 dns 等与原流程相同，不在阐述</strong></p>]]></content>
    
    
    <categories>
      
      <category>Kubernetes</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Linux</tag>
      
      <tag>Docker</tag>
      
      <tag>Kubernetes</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>CI/CD Git Flow</title>
    <link href="/2017/09/05/git-flow-note/"/>
    <url>/2017/09/05/git-flow-note/</url>
    
    <content type="html"><![CDATA[<blockquote><p>由于 git 代码管理比较混乱，所以记录一下 Git Flow + GitLab 的整体工作流程</p></blockquote><h3 id="一、Git-Flow-简介"><a href="#一、Git-Flow-简介" class="headerlink" title="一、Git Flow 简介"></a>一、Git Flow 简介</h3><p>Git Flow 定义了一个围绕项目开发发布的严格 git 分支模型，用于管理多人协作的大型项目中实现高效的协作开发；Git Flow 分支模型最早起源于 <a href="http://nvie.com/about/" target="_blank" rel="noopener">Vincent Driessen</a> 的 <a href="http://nvie.com/posts/a-successful-git-branching-model/" target="_blank" rel="noopener">A successful Git branching model</a> 文章；随着时间发展，Git Flow 大致分为三种:</p><ul><li>Git Flow: 最原始的 Git Flow 分支模型</li><li>Github Flow: Git Flow 的简化版，专门配合持续发布</li><li>GitLab Flow: Git Flow 与 Github Flow 的结合版</li></ul><p>关于三种 Git Flow 区别详情可参考 <a href="http://www.ruanyifeng.com/blog/2015/12/git-workflow.html" target="_blank" rel="noopener">Git 工作流程</a></p><h3 id="二、-Git-Flow-流程"><a href="#二、-Git-Flow-流程" class="headerlink" title="二、 Git Flow 流程"></a>二、 Git Flow 流程</h3><p>Github Flow 和 GitLab Flow 对于持续发布支持比较好，但是原始版本的 Git Flow 对于传统的按照版本发布更加友好一些，所以以下主要说明以下 Git Flow 的工作流程；Git Flow 主要分支模型如下</p><p><img src="https://cdn.oss.link/markdown/80dio.jpg" srcset="/img/loading.gif" alt="git flow"></p><p>在整个分支模型中 <strong>存在两个长期分支: develop 和 master</strong>，其中 develop 分支为开发分支，master 为生产分支；<strong>master 代码始终保持随时可以部署到线上的状态；develop 分支用于合并最新提交的功能性代码</strong>；具体的分支定义如下</p><ul><li>master: 生产代码，始终保持可以直接部署生产的状态</li><li>develop: 开发分支，每次合并最新功能代码到此分支</li><li>feature: 新功能分支，所有新开发的功能将采用 <code>feature/xxxx</code> 形式命名分支</li><li>hotfixes: 紧急修复补丁分支，当新功能部署到了线上出现了严重 bug 需要紧急修复时，则创建 <code>hotfixes/xxxx</code> 形式命名的分支</li><li>release: 稳定版分支，当完成大版本变动后，应该创建 <code>release/xxxx</code> 分支</li></ul><p>在整个分支模型中，develop 分支为最上游分支，会不断有新的 feature 合并入 develop 分支，当功能开发达到完成所有版本需求时，则从 develop 分支创建 release 分支，release 后如没有发现其他问题，最终 release 会被合并到 master 分支以完成线上部署</p><h3 id="三、Git-Flow-工具"><a href="#三、Git-Flow-工具" class="headerlink" title="三、Git Flow 工具"></a>三、Git Flow 工具</h3><p>针对于 Git Flow，其手动操作 git 命令可能过于繁琐，所以后来有了 git-flow 工具；git-flow 是一个 git 扩展集，按 Vincent Driessen 的分支模型提供高层次的库操作；使用 git-flow 工具可以以更加简单的命令完成对 Vincent Driessen 分支模型的实践；<br>git-flow 安装以及使用具体请参考 <a href="https://danielkummer.github.io/git-flow-cheatsheet/index.zh_CN.html" target="_blank" rel="noopener">git-flow 备忘清单</a>，该文章详细描述了 git-flow 工具的使用方式</p><p>还有另一个工具是 <a href="https://github.com/tj/git-extras" target="_blank" rel="noopener">git-extras</a>，该工具没有 git-flow 那么简单化，不过其提供更加强大的命令支持</p><h3 id="四、Git-Commit-Message"><a href="#四、Git-Commit-Message" class="headerlink" title="四、Git Commit Message"></a>四、Git Commit Message</h3><p>在整个 Git Flow 中，commit message 也是必不可少的一部分；一个良好且统一的 commit message 有助于代码审计以及 review 等；目前使用最广泛的写法是 <a href="https://docs.google.com/document/d/1QrDFcIiPjSLDn3EL15IJygNPiHORgU1_OOAqWjiDU5Y/edit#heading=h.greljkmo14y0" target="_blank" rel="noopener">Angular 社区规范</a>，该规范大中 commit message 格式大致如下:</p><pre><code class="hljs sh">&lt;<span class="hljs-built_in">type</span>&gt;(&lt;scope&gt;): &lt;subject&gt;&lt;BLANK LINE&gt;&lt;body&gt;&lt;BLANK LINE&gt;&lt;footer&gt;</code></pre><p>总体格式大致分为 3 部分，首行主要 3 个组成部分:</p><ul><li>type: 本次提交类型</li><li>scope: 本次提交影响范围，一般标明影响版本号或者具体的范围如 <code>$browser, $compile, $rootScope, ngHref, ngClick, ngView, etc...</code></li><li>subject: 本次提交简短说明</li></ul><p>关于 type 提交类型，有如下几种值:</p><ul><li>feat：新功能(feature)</li><li>fix：修补 bug</li><li>docs：文档(documentation)</li><li>style： 格式(不影响代码运行的变动)</li><li>refactor：重构(即不是新增功能，也不是修改 bug 的代码变动)</li><li>test：增加测试</li><li>chore：构建过程或辅助工具的变动</li></ul><p>中间的 body 部分是对本次提交的详细描述信息，底部的 footer 部分一般分为两种情况:</p><ul><li>不兼容变动: 如果出现不兼容变动，则以 <code>BREAKING CHANGE:</code> 开头，后面跟上不兼容变动的具体描述和解决办法</li><li>关闭 issue: 如果该 commit 针对某个 issue，并且可以将其关闭，则可以在其中指定关闭的 issue，如 <code>Close #9527,#9528</code></li></ul><p>不过 footer 部分也有特殊情况，如回滚某次提交，则以 <code>revert:</code> 开头，后面紧跟 commit 信息和具体描述；还有时某些 commit 只是解决了 某个 issue 的一部分问题，这是可以使用 <code>refs ISSUE</code> 的方式来引用该 issue </p><h3 id="五、Git-Commit-Message-工具"><a href="#五、Git-Commit-Message-工具" class="headerlink" title="五、Git Commit Message 工具"></a>五、Git Commit Message 工具</h3><p>针对 Git 的 commit message 目前已经有了成熟的生成工具，比较有名的为 <a href="https://github.com/commitizen/cz-cli" target="_blank" rel="noopener">commitizen-cli</a> 工具，其采用 node.js 编写，执行 <code>git cz</code> 命令能够自动生成符合 Angular 社区规范的 commit message；不过由于其使用 node.js 编写，所以安装前需要安装 node.js，因此可能不适合其他非 node.js 的项目使用；这里推荐一个基于 shell 编写的 <a href="https://cimhealth.github.io/git-toolkit" target="_blank" rel="noopener">Git-toolkit</a>，安装此工具后执行 <code>git ci</code> 命令进行提交将会产生交互式生成 Angular git commit message 格式的提交说明，截图如下:</p><p><img src="https://cdn.oss.link/markdown/xnonb.jpg" srcset="/img/loading.gif" alt="git ci"></p><h3 id="六、GitLab-整合"><a href="#六、GitLab-整合" class="headerlink" title="六、GitLab 整合"></a>六、GitLab 整合</h3><p>以上 Git Flow 所有操作介绍的都是在本地操作，而正常我们在工作中都是基于 GitLab 搭建私有 Git 仓库来进行协同开发的，以下简述以下 Git Flow 配合 GitLab 的流程</p><h4 id="6-1、开发-features"><a href="#6-1、开发-features" class="headerlink" title="6.1、开发 features"></a>6.1、开发 features</h4><p>当开发一个新功能时流程如下:</p><ul><li>本地 <code>git flow feature start xxxx</code> 开启一个 feature 新分支</li><li><code>git flow feature publish xxxx</code> 将此分支推送到远端以便他人获取</li><li>完成开发后 GitLab 上向 <code>develop</code> 分支发起合并请求</li><li>CI sonar 等质量检测工具扫描，其他用户 review 代码</li><li>确认无误后 <code>master</code> 权限用户合并其到 <code>develop</code> 分支</li><li>部署到测试环境以便测试组测试</li><li>如果测试不通过，则继续基于此分支开发，直到该功能开发完成</li></ul><h4 id="6-2、创建-release"><a href="#6-2、创建-release" class="headerlink" title="6.2、创建 release"></a>6.2、创建 release</h4><p>当一定量的 feature 开发完成并合并到 develop 后，如所有 feature 都测试通过并满足版本需求，则可以创建 release 版本分支；release 分支流程如下</p><ul><li>本地 <code>git flow release start xxxx</code> 开启 release 分支</li><li><code>git flow release publish xxxx</code> 将其推送到远端以便他人获取</li><li>继续进行完整性测试，出现问题继续修复，直到 release 完全稳定</li><li>从 release 分支向 master、develop 分支分别发起合并请求</li><li>master 合并后创建对应的 release 标签，并部署生产环境</li><li>develop 合并 release 的后期修改</li></ul><h4 id="6-3、紧急修复"><a href="#6-3、紧急修复" class="headerlink" title="6.3、紧急修复"></a>6.3、紧急修复</h4><p>当 master 某个 tag 部署到生产环境后，也可能出现不符合预期的问题出现；此时应该基于 master 创建 hotfix 分支进行修复，流程如下</p><ul><li>本地 <code>git flow hotfix start xxxx</code> 创建紧急修复分支</li><li>修改代码后将其推送到远端，并像 master、develop 分支发起合并</li><li>develop 合并紧急修复补丁，如果必要最好再做一下测试</li><li>master 合并紧急修复补丁，创建紧急修复 tag，并部署生产环境</li></ul>]]></content>
    
    
    <categories>
      
      <category>CI/CD</category>
      
    </categories>
    
    
    <tags>
      
      <tag>CI/CD</tag>
      
      <tag>Git</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Calico 部署踩坑记录</title>
    <link href="/2017/07/31/calico-yml-bug/"/>
    <url>/2017/07/31/calico-yml-bug/</url>
    
    <content type="html"><![CDATA[<blockquote><p>自从上次在虚拟机中手动了部署了 Kubernetes 1.7.2 以后，自己在测试环境就来了一下，结果网络组件死活起不来，最后找到原因记录一下</p></blockquote><h3 id="一、Calico-部署注意事项"><a href="#一、Calico-部署注意事项" class="headerlink" title="一、Calico 部署注意事项"></a>一、Calico 部署注意事项</h3><p>在使用 Calico 前当然最好撸一下官方文档，地址在这里 <a href="http://docs.projectcalico.org/v2.3/getting-started/kubernetes/installation/" target="_blank" rel="noopener">Calico 官方文档</a>，其中部署前需要注意以下几点</p><ul><li><strong>官方文档中要求 <code>kubelet</code> 配置必须增加 <code>--network-plugin=cni</code> 选项</strong></li><li><strong><code>kube-proxy</code> 组件必须采用 <code>iptables</code> proxy mode 模式(1.2 以后是默认模式)</strong></li><li><strong><code>kubec-proxy</code> 组件不能采用 <code>--masquerade-all</code> 启动，因为会与 Calico policy 冲突</strong></li><li><strong><code>NetworkPolicy API</code> 只要需要 Kubernetes 1.3 以上</strong></li><li><strong>启用 RBAC 后需要设置对应的 RoleBinding，参考 <a href="http://docs.projectcalico.org/v2.3/getting-started/kubernetes/installation/hosted/" target="_blank" rel="noopener">官方文档 RBAC 部分</a></strong></li></ul><h3 id="二、Calico-官方部署方式"><a href="#二、Calico-官方部署方式" class="headerlink" title="二、Calico 官方部署方式"></a>二、Calico 官方部署方式</h3><p>在已经有了一个 Kubernetes 集群的情况下，官方部署方式描述的很简单，只需要改一改 yml 配置，然后 create 一下即可，具体描述见 <a href="http://docs.projectcalico.org/v2.3/getting-started/kubernetes/installation/hosted/" target="_blank" rel="noopener">官方文档</a></p><p>官方文档中大致给出了三种部署方案: </p><ul><li><strong>Standard Hosted Install:</strong> 修改 calico.yml etcd 相关配置，直接创建，证书配置等参考 <a href="https://mritd.me/2017/07/21/set-up-kubernetes-ha-cluster-by-binary/#%E5%85%AD%E9%83%A8%E7%BD%B2-calico" target="_blank" rel="noopener">手动部署 Kubernetes 文档</a></li><li><strong>Kubeadm Hosted Install:</strong> 根据 <code>1.6 or high</code> 和 <code>1.5</code> 区分两个 yml 配置，直接创建即可</li><li><strong>Kubernetes Datastore:</strong> 不使用 Etcd 存储数据，不推荐，这里也不做说明</li></ul><h3 id="三、Standard-Hosted-Install-的坑"><a href="#三、Standard-Hosted-Install-的坑" class="headerlink" title="三、Standard Hosted Install 的坑"></a>三、Standard Hosted Install 的坑</h3><p>当我从虚拟机中测试完全没问题以后，就在测试环境尝试创建 Calico 网络，结果出现的问题是<strong>某个(几个) Calico 节点无法启动，同时创建 deployment 后，执行 <code>route -n</code> 会发现每个 node 只有自己节点 Pod 的路由，正常每个 node 上会有所有 node 上 Pod 网段的路由，如下(正常情况)</strong></p><p><img src="https://cdn.oss.link/markdown/c44e7.jpg" srcset="/img/loading.gif" alt="calico route"></p><p>此时观察每个 node 上 Calico Pod 日志，会有提示 <strong>未知节点 xxxx</strong> 等错误日志，大体意思就是 <strong>未知的一个(几个)节点在进行 BGP 协议时被拒绝</strong>，偶尔某些 node 上还可能出现 <strong>IP 已经被占用</strong> 的神奇错误提示</p><p>后来经过翻查 <a href="http://docs.projectcalico.org/v2.3/getting-started/kubernetes/installation/integration" target="_blank" rel="noopener">Calico 自定义部署文档</a> 和 <a href="https://github.com/kubernetes-incubator/kubespray" target="_blank" rel="noopener">Kargo 项目源码</a> 发现了主要问题在于 <strong>官方文档中直接创建的 calico.yml 文件中，使用 DaemonSet 方式启动 calico-node，同时 calico-node 的 IP 设置和 NODENAME 设置均为空，此时 calico-node 会进行自动获取，网络复杂情况下获取会出现问题；比如 IP 拿到了 docker 网桥的 IP，NODENAME 获取不正确等，最终导致出现很奇怪的错误</strong></p><h3 id="四、解决方案"><a href="#四、解决方案" class="headerlink" title="四、解决方案"></a>四、解决方案</h3><p>一开始想到的解决方案很简单，直接照着 Kargo 抄，使用 Systemd 来启动 calico-node，然后在拆分过程中需要各种配置信息直接也根据 Kargo 的做法生成；当然鼓捣了 1/3 的时候就炸了，Kargo 是 ansible 批量部署的，有些变量找起来要人命；最后选择了一个折中(偷懒)的方案: <strong>使用官方的 calico.yml 创建相关组件，这样 ConfigMap、Etcd 配置、Calico policy 啥的直接创建好，然后把 DaemonSet 中 calico-node 容器单独搞出来，使用 Systemd 启动，这样就即方便又简单(我真特么机智)；最终操作如下:</strong></p><h4 id="4-1、首先修改-calico-yml"><a href="#4-1、首先修改-calico-yml" class="headerlink" title="4.1、首先修改 calico.yml"></a>4.1、首先修改 calico.yml</h4><p>在进行网络组件部署前，请确保集群已经满足 Calico 部署要求(本文第一部分)；然后获取 calico.yml，注释掉 DaemonSet 中 calico-node 部分，如下所示</p><pre><code class="hljs yml"><span class="hljs-comment"># Calico Version v2.3.0</span><span class="hljs-comment"># http://docs.projectcalico.org/v2.3/releases#v2.3.0</span><span class="hljs-comment"># This manifest includes the following component versions:</span><span class="hljs-comment">#   calico/node:v1.3.0</span><span class="hljs-comment">#   calico/cni:v1.9.1</span><span class="hljs-comment">#   calico/kube-policy-controller:v0.6.0</span><span class="hljs-comment"># This ConfigMap is used to configure a self-hosted Calico installation.</span><span class="hljs-attr">kind:</span> <span class="hljs-string">ConfigMap</span><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">v1</span><span class="hljs-attr">metadata:</span>  <span class="hljs-attr">name:</span> <span class="hljs-string">calico-config</span>  <span class="hljs-attr">namespace:</span> <span class="hljs-string">kube-system</span><span class="hljs-attr">data:</span>  <span class="hljs-comment"># Configure this with the location of your etcd cluster.</span>  <span class="hljs-attr">etcd_endpoints:</span> <span class="hljs-string">"https://192.168.1.11:2379,https://192.168.1.12:2379,https://192.168.1.13:2379"</span>  <span class="hljs-comment"># Configure the Calico backend to use.</span>  <span class="hljs-attr">calico_backend:</span> <span class="hljs-string">"bird"</span>  <span class="hljs-comment"># The CNI network configuration to install on each node.</span>  <span class="hljs-attr">cni_network_config:</span> <span class="hljs-string">|-</span>    <span class="hljs-string">&#123;</span>        <span class="hljs-attr">"name":</span> <span class="hljs-string">"k8s-pod-network"</span><span class="hljs-string">,</span>        <span class="hljs-attr">"cniVersion":</span> <span class="hljs-string">"0.1.0"</span><span class="hljs-string">,</span>        <span class="hljs-attr">"type":</span> <span class="hljs-string">"calico"</span><span class="hljs-string">,</span>        <span class="hljs-attr">"etcd_endpoints":</span> <span class="hljs-string">"__ETCD_ENDPOINTS__"</span><span class="hljs-string">,</span>        <span class="hljs-attr">"etcd_key_file":</span> <span class="hljs-string">"__ETCD_KEY_FILE__"</span><span class="hljs-string">,</span>        <span class="hljs-attr">"etcd_cert_file":</span> <span class="hljs-string">"__ETCD_CERT_FILE__"</span><span class="hljs-string">,</span>        <span class="hljs-attr">"etcd_ca_cert_file":</span> <span class="hljs-string">"__ETCD_CA_CERT_FILE__"</span><span class="hljs-string">,</span>        <span class="hljs-attr">"log_level":</span> <span class="hljs-string">"info"</span><span class="hljs-string">,</span>        <span class="hljs-attr">"ipam":</span> <span class="hljs-string">&#123;</span>            <span class="hljs-attr">"type":</span> <span class="hljs-string">"calico-ipam"</span>        <span class="hljs-string">&#125;,</span>        <span class="hljs-attr">"policy":</span> <span class="hljs-string">&#123;</span>            <span class="hljs-attr">"type":</span> <span class="hljs-string">"k8s"</span><span class="hljs-string">,</span>            <span class="hljs-attr">"k8s_api_root":</span> <span class="hljs-string">"https://__KUBERNETES_SERVICE_HOST__:__KUBERNETES_SERVICE_PORT__"</span><span class="hljs-string">,</span>            <span class="hljs-attr">"k8s_auth_token":</span> <span class="hljs-string">"__SERVICEACCOUNT_TOKEN__"</span>        <span class="hljs-string">&#125;,</span>        <span class="hljs-attr">"kubernetes":</span> <span class="hljs-string">&#123;</span>            <span class="hljs-attr">"kubeconfig":</span> <span class="hljs-string">"__KUBECONFIG_FILEPATH__"</span>        <span class="hljs-string">&#125;</span>    <span class="hljs-string">&#125;</span>  <span class="hljs-comment"># If you're using TLS enabled etcd uncomment the following.</span>  <span class="hljs-comment"># You must also populate the Secret below with these files.</span>  <span class="hljs-attr">etcd_ca:</span> <span class="hljs-string">"/calico-secrets/etcd-ca"</span>  <span class="hljs-attr">etcd_cert:</span> <span class="hljs-string">"/calico-secrets/etcd-cert"</span>  <span class="hljs-attr">etcd_key:</span> <span class="hljs-string">"/calico-secrets/etcd-key"</span><span class="hljs-meta">---</span><span class="hljs-comment"># The following contains k8s Secrets for use with a TLS enabled etcd cluster.</span><span class="hljs-comment"># For information on populating Secrets, see http://kubernetes.io/docs/user-guide/secrets/</span><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">v1</span><span class="hljs-attr">kind:</span> <span class="hljs-string">Secret</span><span class="hljs-attr">type:</span> <span class="hljs-string">Opaque</span><span class="hljs-attr">metadata:</span>  <span class="hljs-attr">name:</span> <span class="hljs-string">calico-etcd-secrets</span>  <span class="hljs-attr">namespace:</span> <span class="hljs-string">kube-system</span><span class="hljs-attr">data:</span>  <span class="hljs-comment"># Populate the following files with etcd TLS configuration if desired, but leave blank if</span>  <span class="hljs-comment"># not using TLS for etcd.</span>  <span class="hljs-comment"># This self-hosted install expects three files with the following names.  The values</span>  <span class="hljs-comment"># should be base64 encoded strings of the entire contents of each file.</span>  <span class="hljs-attr">etcd-key:</span> <span class="hljs-string">这块自己对</span> <span class="hljs-string">etcd</span> <span class="hljs-string">相关证书做</span> <span class="hljs-string">base64</span>  <span class="hljs-attr">etcd-cert:</span> <span class="hljs-string">这块自己对</span> <span class="hljs-string">etcd</span> <span class="hljs-string">相关证书做</span> <span class="hljs-string">base64</span>  <span class="hljs-attr">etcd-ca:</span> <span class="hljs-string">这块自己对</span> <span class="hljs-string">etcd</span> <span class="hljs-string">相关证书做</span> <span class="hljs-string">base64</span><span class="hljs-meta">---</span><span class="hljs-comment"># This manifest installs the calico/node container, as well</span><span class="hljs-comment"># as the Calico CNI plugins and network config on</span><span class="hljs-comment"># each master and worker node in a Kubernetes cluster.</span><span class="hljs-attr">kind:</span> <span class="hljs-string">DaemonSet</span><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">extensions/v1beta1</span><span class="hljs-attr">metadata:</span>  <span class="hljs-attr">name:</span> <span class="hljs-string">calico-node</span>  <span class="hljs-attr">namespace:</span> <span class="hljs-string">kube-system</span>  <span class="hljs-attr">labels:</span>    <span class="hljs-attr">k8s-app:</span> <span class="hljs-string">calico-node</span><span class="hljs-attr">spec:</span>  <span class="hljs-attr">selector:</span>    <span class="hljs-attr">matchLabels:</span>      <span class="hljs-attr">k8s-app:</span> <span class="hljs-string">calico-node</span>  <span class="hljs-attr">template:</span>    <span class="hljs-attr">metadata:</span>      <span class="hljs-attr">labels:</span>        <span class="hljs-attr">k8s-app:</span> <span class="hljs-string">calico-node</span>      <span class="hljs-attr">annotations:</span>        <span class="hljs-attr">scheduler.alpha.kubernetes.io/critical-pod:</span> <span class="hljs-string">''</span>        <span class="hljs-attr">scheduler.alpha.kubernetes.io/tolerations:</span> <span class="hljs-string">|</span>          <span class="hljs-string">[&#123;"key":</span> <span class="hljs-string">"dedicated"</span><span class="hljs-string">,</span> <span class="hljs-attr">"value":</span> <span class="hljs-string">"master"</span><span class="hljs-string">,</span> <span class="hljs-attr">"effect":</span> <span class="hljs-string">"NoSchedule"</span> <span class="hljs-string">&#125;,</span>           <span class="hljs-string">&#123;"key":"CriticalAddonsOnly",</span> <span class="hljs-string">"operator"</span><span class="hljs-string">:"Exists"&#125;]</span>    <span class="hljs-attr">spec:</span>      <span class="hljs-attr">hostNetwork:</span> <span class="hljs-literal">true</span>      <span class="hljs-attr">serviceAccountName:</span> <span class="hljs-string">calico-node</span>      <span class="hljs-attr">containers:</span>        <span class="hljs-comment"># Runs calico/node container on each Kubernetes node.  This</span>        <span class="hljs-comment"># container programs network policy and routes on each</span>        <span class="hljs-comment"># host.</span><span class="hljs-comment"># calico-node 注释掉，移动到 Systemd 中</span><span class="hljs-comment">#        - name: calico-node</span><span class="hljs-comment">#          image: quay.io/calico/node:v1.3.0</span><span class="hljs-comment">#          env:</span><span class="hljs-comment">#            # The location of the Calico etcd cluster.</span><span class="hljs-comment">#            - name: ETCD_ENDPOINTS</span><span class="hljs-comment">#              valueFrom:</span><span class="hljs-comment">#                configMapKeyRef:</span><span class="hljs-comment">#                  name: calico-config</span><span class="hljs-comment">#                  key: etcd_endpoints</span><span class="hljs-comment">#            # Choose the backend to use.</span><span class="hljs-comment">#            - name: CALICO_NETWORKING_BACKEND</span><span class="hljs-comment">#              valueFrom:</span><span class="hljs-comment">#                configMapKeyRef:</span><span class="hljs-comment">#                  name: calico-config</span><span class="hljs-comment">#                  key: calico_backend</span><span class="hljs-comment">#            # Disable file logging so `kubectl logs` works.</span><span class="hljs-comment">#            - name: CALICO_DISABLE_FILE_LOGGING</span><span class="hljs-comment">#              value: "true"</span><span class="hljs-comment">#            # Set Felix endpoint to host default action to ACCEPT.</span><span class="hljs-comment">#            - name: FELIX_DEFAULTENDPOINTTOHOSTACTION</span><span class="hljs-comment">#              value: "ACCEPT"</span><span class="hljs-comment">#            # Configure the IP Pool from which Pod IPs will be chosen.</span><span class="hljs-comment">#            - name: CALICO_IPV4POOL_CIDR</span><span class="hljs-comment">#              value: "10.254.64.0/18"</span><span class="hljs-comment">#            - name: CALICO_IPV4POOL_IPIP</span><span class="hljs-comment">#              value: "always"</span><span class="hljs-comment">#            # Disable IPv6 on Kubernetes.</span><span class="hljs-comment">#            - name: FELIX_IPV6SUPPORT</span><span class="hljs-comment">#              value: "false"</span><span class="hljs-comment">#            # Set Felix logging to "info"</span><span class="hljs-comment">#            - name: FELIX_LOGSEVERITYSCREEN</span><span class="hljs-comment">#              value: "info"</span><span class="hljs-comment">#            # Location of the CA certificate for etcd.</span><span class="hljs-comment">#            - name: ETCD_CA_CERT_FILE</span><span class="hljs-comment">#              valueFrom:</span><span class="hljs-comment">#                configMapKeyRef:</span><span class="hljs-comment">#                  name: calico-config</span><span class="hljs-comment">#                  key: etcd_ca</span><span class="hljs-comment">#            # Location of the client key for etcd.</span><span class="hljs-comment">#            - name: ETCD_KEY_FILE</span><span class="hljs-comment">#              valueFrom:</span><span class="hljs-comment">#                configMapKeyRef:</span><span class="hljs-comment">#                  name: calico-config</span><span class="hljs-comment">#                  key: etcd_key</span><span class="hljs-comment">#            # Location of the client certificate for etcd.</span><span class="hljs-comment">#            - name: ETCD_CERT_FILE</span><span class="hljs-comment">#              valueFrom:</span><span class="hljs-comment">#                configMapKeyRef:</span><span class="hljs-comment">#                  name: calico-config</span><span class="hljs-comment">#                  key: etcd_cert</span><span class="hljs-comment">#            # Auto-detect the BGP IP address.</span><span class="hljs-comment">#            - name: IP</span><span class="hljs-comment">#              value: ""</span><span class="hljs-comment">#          securityContext:</span><span class="hljs-comment">#            privileged: true</span><span class="hljs-comment">#          resources:</span><span class="hljs-comment">#            requests:</span><span class="hljs-comment">#              cpu: 250m</span><span class="hljs-comment">#          volumeMounts:</span><span class="hljs-comment">#            - mountPath: /lib/modules</span><span class="hljs-comment">#              name: lib-modules</span><span class="hljs-comment">#              readOnly: true</span><span class="hljs-comment">#            - mountPath: /var/run/calico</span><span class="hljs-comment">#              name: var-run-calico</span><span class="hljs-comment">#              readOnly: false</span><span class="hljs-comment">#            - mountPath: /calico-secrets</span><span class="hljs-comment">#              name: etcd-certs</span><span class="hljs-comment">#        # This container installs the Calico CNI binaries</span><span class="hljs-comment">#        # and CNI network config file on each node.</span>        <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">install-cni</span>          <span class="hljs-attr">image:</span> <span class="hljs-string">quay.io/calico/cni:v1.9.1</span>          <span class="hljs-attr">command:</span> <span class="hljs-string">["/install-cni.sh"]</span>          <span class="hljs-attr">env:</span>            <span class="hljs-comment"># The location of the Calico etcd cluster.</span>            <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">ETCD_ENDPOINTS</span>              <span class="hljs-attr">valueFrom:</span>                <span class="hljs-attr">configMapKeyRef:</span>                  <span class="hljs-attr">name:</span> <span class="hljs-string">calico-config</span>                  <span class="hljs-attr">key:</span> <span class="hljs-string">etcd_endpoints</span>            <span class="hljs-comment"># The CNI network config to install on each node.</span>            <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">CNI_NETWORK_CONFIG</span>              <span class="hljs-attr">valueFrom:</span>                <span class="hljs-attr">configMapKeyRef:</span>                  <span class="hljs-attr">name:</span> <span class="hljs-string">calico-config</span>                  <span class="hljs-attr">key:</span> <span class="hljs-string">cni_network_config</span>          <span class="hljs-attr">volumeMounts:</span>            <span class="hljs-bullet">-</span> <span class="hljs-attr">mountPath:</span> <span class="hljs-string">/host/opt/cni/bin</span>              <span class="hljs-attr">name:</span> <span class="hljs-string">cni-bin-dir</span>            <span class="hljs-bullet">-</span> <span class="hljs-attr">mountPath:</span> <span class="hljs-string">/host/etc/cni/net.d</span>              <span class="hljs-attr">name:</span> <span class="hljs-string">cni-net-dir</span>            <span class="hljs-bullet">-</span> <span class="hljs-attr">mountPath:</span> <span class="hljs-string">/calico-secrets</span>              <span class="hljs-attr">name:</span> <span class="hljs-string">etcd-certs</span>      <span class="hljs-attr">volumes:</span>        <span class="hljs-comment"># Used by calico/node.</span>        <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">lib-modules</span>          <span class="hljs-attr">hostPath:</span>            <span class="hljs-attr">path:</span> <span class="hljs-string">/lib/modules</span>        <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">var-run-calico</span>          <span class="hljs-attr">hostPath:</span>            <span class="hljs-attr">path:</span> <span class="hljs-string">/var/run/calico</span>        <span class="hljs-comment"># Used to install CNI.</span>        <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">cni-bin-dir</span>          <span class="hljs-attr">hostPath:</span>            <span class="hljs-attr">path:</span> <span class="hljs-string">/opt/cni/bin</span>        <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">cni-net-dir</span>          <span class="hljs-attr">hostPath:</span>            <span class="hljs-attr">path:</span> <span class="hljs-string">/etc/cni/net.d</span>        <span class="hljs-comment"># Mount in the etcd TLS secrets.</span>        <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">etcd-certs</span>          <span class="hljs-attr">secret:</span>            <span class="hljs-attr">secretName:</span> <span class="hljs-string">calico-etcd-secrets</span><span class="hljs-meta">---</span><span class="hljs-comment"># This manifest deploys the Calico policy controller on Kubernetes.</span><span class="hljs-comment"># See https://github.com/projectcalico/k8s-policy</span><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">extensions/v1beta1</span><span class="hljs-attr">kind:</span> <span class="hljs-string">Deployment</span><span class="hljs-attr">metadata:</span>  <span class="hljs-attr">name:</span> <span class="hljs-string">calico-policy-controller</span>  <span class="hljs-attr">namespace:</span> <span class="hljs-string">kube-system</span>  <span class="hljs-attr">labels:</span>    <span class="hljs-attr">k8s-app:</span> <span class="hljs-string">calico-policy</span>  <span class="hljs-attr">annotations:</span>    <span class="hljs-attr">scheduler.alpha.kubernetes.io/critical-pod:</span> <span class="hljs-string">''</span>    <span class="hljs-attr">scheduler.alpha.kubernetes.io/tolerations:</span> <span class="hljs-string">|</span>      <span class="hljs-string">[&#123;"key":</span> <span class="hljs-string">"dedicated"</span><span class="hljs-string">,</span> <span class="hljs-attr">"value":</span> <span class="hljs-string">"master"</span><span class="hljs-string">,</span> <span class="hljs-attr">"effect":</span> <span class="hljs-string">"NoSchedule"</span> <span class="hljs-string">&#125;,</span>       <span class="hljs-string">&#123;"key":"CriticalAddonsOnly",</span> <span class="hljs-string">"operator"</span><span class="hljs-string">:"Exists"&#125;]</span><span class="hljs-attr">spec:</span>  <span class="hljs-comment"># The policy controller can only have a single active instance.</span>  <span class="hljs-attr">replicas:</span> <span class="hljs-number">1</span>  <span class="hljs-attr">strategy:</span>    <span class="hljs-attr">type:</span> <span class="hljs-string">Recreate</span>  <span class="hljs-attr">template:</span>    <span class="hljs-attr">metadata:</span>      <span class="hljs-attr">name:</span> <span class="hljs-string">calico-policy-controller</span>      <span class="hljs-attr">namespace:</span> <span class="hljs-string">kube-system</span>      <span class="hljs-attr">labels:</span>        <span class="hljs-attr">k8s-app:</span> <span class="hljs-string">calico-policy</span>    <span class="hljs-attr">spec:</span>      <span class="hljs-comment"># The policy controller must run in the host network namespace so that</span>      <span class="hljs-comment"># it isn't governed by policy that would prevent it from working.</span>      <span class="hljs-attr">hostNetwork:</span> <span class="hljs-literal">true</span>      <span class="hljs-attr">serviceAccountName:</span> <span class="hljs-string">calico-policy-controller</span>      <span class="hljs-attr">containers:</span>        <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">calico-policy-controller</span>          <span class="hljs-attr">image:</span> <span class="hljs-string">quay.io/calico/kube-policy-controller:v0.6.0</span>          <span class="hljs-attr">env:</span>            <span class="hljs-comment"># The location of the Calico etcd cluster.</span>            <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">ETCD_ENDPOINTS</span>              <span class="hljs-attr">valueFrom:</span>                <span class="hljs-attr">configMapKeyRef:</span>                  <span class="hljs-attr">name:</span> <span class="hljs-string">calico-config</span>                  <span class="hljs-attr">key:</span> <span class="hljs-string">etcd_endpoints</span>            <span class="hljs-comment"># Location of the CA certificate for etcd.</span>            <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">ETCD_CA_CERT_FILE</span>              <span class="hljs-attr">valueFrom:</span>                <span class="hljs-attr">configMapKeyRef:</span>                  <span class="hljs-attr">name:</span> <span class="hljs-string">calico-config</span>                  <span class="hljs-attr">key:</span> <span class="hljs-string">etcd_ca</span>            <span class="hljs-comment"># Location of the client key for etcd.</span>            <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">ETCD_KEY_FILE</span>              <span class="hljs-attr">valueFrom:</span>                <span class="hljs-attr">configMapKeyRef:</span>                  <span class="hljs-attr">name:</span> <span class="hljs-string">calico-config</span>                  <span class="hljs-attr">key:</span> <span class="hljs-string">etcd_key</span>            <span class="hljs-comment"># Location of the client certificate for etcd.</span>            <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">ETCD_CERT_FILE</span>              <span class="hljs-attr">valueFrom:</span>                <span class="hljs-attr">configMapKeyRef:</span>                  <span class="hljs-attr">name:</span> <span class="hljs-string">calico-config</span>                  <span class="hljs-attr">key:</span> <span class="hljs-string">etcd_cert</span>            <span class="hljs-comment"># The location of the Kubernetes API.  Use the default Kubernetes</span>            <span class="hljs-comment"># service for API access.</span>            <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">K8S_API</span>              <span class="hljs-attr">value:</span> <span class="hljs-string">"https://kubernetes.default:443"</span>            <span class="hljs-comment"># Since we're running in the host namespace and might not have KubeDNS</span>            <span class="hljs-comment"># access, configure the container's /etc/hosts to resolve</span>            <span class="hljs-comment"># kubernetes.default to the correct service clusterIP.</span>            <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">CONFIGURE_ETC_HOSTS</span>              <span class="hljs-attr">value:</span> <span class="hljs-string">"true"</span>          <span class="hljs-attr">volumeMounts:</span>            <span class="hljs-comment"># Mount in the etcd TLS secrets.</span>            <span class="hljs-bullet">-</span> <span class="hljs-attr">mountPath:</span> <span class="hljs-string">/calico-secrets</span>              <span class="hljs-attr">name:</span> <span class="hljs-string">etcd-certs</span>      <span class="hljs-attr">volumes:</span>        <span class="hljs-comment"># Mount in the etcd TLS secrets.</span>        <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">etcd-certs</span>          <span class="hljs-attr">secret:</span>            <span class="hljs-attr">secretName:</span> <span class="hljs-string">calico-etcd-secrets</span><span class="hljs-meta">---</span><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">v1</span><span class="hljs-attr">kind:</span> <span class="hljs-string">ServiceAccount</span><span class="hljs-attr">metadata:</span>  <span class="hljs-attr">name:</span> <span class="hljs-string">calico-policy-controller</span>  <span class="hljs-attr">namespace:</span> <span class="hljs-string">kube-system</span><span class="hljs-meta">---</span><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">v1</span><span class="hljs-attr">kind:</span> <span class="hljs-string">ServiceAccount</span><span class="hljs-attr">metadata:</span>  <span class="hljs-attr">name:</span> <span class="hljs-string">calico-node</span>  <span class="hljs-attr">namespace:</span> <span class="hljs-string">kube-system</span></code></pre><p><strong>修改完成后直接 create 即可</strong></p><h4 id="4-2、增加-calico-node-Systemd-配置"><a href="#4-2、增加-calico-node-Systemd-配置" class="headerlink" title="4.2、增加 calico-node Systemd 配置"></a>4.2、增加 calico-node Systemd 配置</h4><p>最后写一个 service 文件(我放到了 <code>/etc/systemd/system/calico-node.service</code>)，使用 Systemd 启动即可；<strong>注意以下配置中 <code>IP</code>、<code>NODENAME</code> 是自己手动定义的，IP 为宿主机 IP，NODENAME 最好与 hostname 相同</strong></p><pre><code class="hljs sh">[Unit]Description=calico nodeAfter=docker.serviceRequires=docker.service[Service]User=rootPermissionsStartOnly=<span class="hljs-literal">true</span>ExecStart=/usr/bin/docker run   --net=host --privileged --name=calico-node \                                -e ETCD_ENDPOINTS=https://192.168.1.11:2379,https://192.168.1.12:2379,https://192.168.1.13:2379 \                                -e ETCD_CA_CERT_FILE=/etc/etcd/ssl/etcd-root-ca.pem \                                -e ETCD_CERT_FILE=/etc/etcd/ssl/etcd.pem \                                -e ETCD_KEY_FILE=/etc/etcd/ssl/etcd-key.pem \                                -e NODENAME=docker1.node \                                -e IP=192.168.1.11 \                                -e IP6= \                                -e AS= \                                -e CALICO_IPV4POOL_CIDR=10.20.0.0/16 \                                -e CALICO_IPV4POOL_IPIP=always \                                -e CALICO_LIBNETWORK_ENABLED=<span class="hljs-literal">true</span> \                                -e CALICO_NETWORKING_BACKEND=bird \                                -e CALICO_DISABLE_FILE_LOGGING=<span class="hljs-literal">true</span> \                                -e FELIX_IPV6SUPPORT=<span class="hljs-literal">false</span> \                                -e FELIX_DEFAULTENDPOINTTOHOSTACTION=ACCEPT \                                -e FELIX_LOGSEVERITYSCREEN=info \                                -v /etc/etcd/ssl/etcd-root-ca.pem:/etc/etcd/ssl/etcd-root-ca.pem \                                -v /etc/etcd/ssl/etcd.pem:/etc/etcd/ssl/etcd.pem \                                -v /etc/etcd/ssl/etcd-key.pem:/etc/etcd/ssl/etcd-key.pem \                                -v /var/run/calico:/var/run/calico \                                -v /lib/modules:/lib/modules \                                -v /run/docker/plugins:/run/docker/plugins \                                -v /var/run/docker.sock:/var/run/docker.sock \                                -v /var/<span class="hljs-built_in">log</span>/calico:/var/<span class="hljs-built_in">log</span>/calico \                                quay.io/calico/node:v1.3.0ExecStop=/usr/bin/docker rm -f calico-nodeRestart=alwaysRestartSec=10[Install]WantedBy=multi-user.target</code></pre>]]></content>
    
    
    <categories>
      
      <category>Kubernetes</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Kubernetes</tag>
      
      <tag>Calico</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>手动档搭建 Kubernetes HA 集群</title>
    <link href="/2017/07/21/set-up-kubernetes-ha-cluster-by-binary/"/>
    <url>/2017/07/21/set-up-kubernetes-ha-cluster-by-binary/</url>
    
    <content type="html"><![CDATA[<blockquote><p>以前一直用 Kargo(基于 ansible) 来搭建 Kubernetes 集群，最近发现 ansible 部署的时候有些东西有点 bug，而且 Kargo 对 rkt 等也做了适配，感觉问题已经有点复杂化了；在 2.2 release 没出来这个时候，准备自己纯手动挡部署一下，Master HA 直接抄 Kargo 的就行了，以下记录一下;<strong>本文以下部分所有用到的 rpm 、配置文件等全部已经上传到了 <a href="http://pan.baidu.com/s/1o8PZLKA" target="_blank" rel="noopener">百度云</a>  密码: x5v4</strong></p></blockquote><h3 id="一、环境准备"><a href="#一、环境准备" class="headerlink" title="一、环境准备"></a>一、环境准备</h3><blockquote><p>以下文章本着 <strong>多写代码少哔哔</strong> 的原则，会主要以实际操作为主，不会过多介绍每步细节动作，如果纯小白想要更详细的了解，可以参考 <a href="https://github.com/rootsongjc/kubernetes-handbook" target="_blank" rel="noopener">这里</a></p></blockquote><p><strong>环境总共 5 台虚拟机，2 个 master，3 个 etcd 节点，master 同时也作为 node 负载 pod，在分发证书等阶段将在另外一台主机上执行，该主机对集群内所有节点配置了 ssh 秘钥登录</strong></p><table><thead><tr><th>IP</th><th>节点</th></tr></thead><tbody><tr><td>192.168.1.11</td><td>master、node、etcd</td></tr><tr><td>192.168.1.12</td><td>master、node、etcd</td></tr><tr><td>192.168.1.13</td><td>master、node、etcd</td></tr><tr><td>192.168.1.14</td><td>node</td></tr><tr><td>192.168.1.15</td><td>node</td></tr></tbody></table><p>网络方案这里采用性能比较好的 Calico，集群开启 RBAC，RBAC 相关可参考 <a href="https://mritd.me/2017/07/17/kubernetes-rbac-chinese-translation/" target="_blank" rel="noopener">这里的胡乱翻译版本</a></p><h3 id="二、证书相关处理"><a href="#二、证书相关处理" class="headerlink" title="二、证书相关处理"></a>二、证书相关处理</h3><h4 id="2-1、证书说明"><a href="#2-1、证书说明" class="headerlink" title="2.1、证书说明"></a>2.1、证书说明</h4><p>由于 Etcd 和 Kubernetes 全部采用 TLS 通讯，所以先要生成 TLS 证书，<strong>证书生成工具采用 <a href="https://github.com/cloudflare/cfssl/releases" target="_blank" rel="noopener">cfssl</a>，具体使用方法这里不再详细阐述，生成证书时可在任一节点完成，这里在宿主机执行</strong>，证书列表如下</p><table><thead><tr><th>证书名称</th><th>配置文件</th><th>用途</th></tr></thead><tbody><tr><td>etcd-root-ca.pem</td><td>etcd-root-ca-csr.json</td><td>etcd 根 CA 证书</td></tr><tr><td>etcd.pem</td><td>etcd-gencert.json、etcd-csr.json</td><td>etcd 集群证书</td></tr><tr><td>k8s-root-ca.pem</td><td>k8s-root-ca-csr.json</td><td>k8s 根 CA 证书</td></tr><tr><td>kube-proxy.pem</td><td>k8s-gencert.json、kube-proxy-csr.json</td><td>kube-proxy 使用的证书</td></tr><tr><td>admin.pem</td><td>k8s-gencert.json、admin-csr.json</td><td>kubectl 使用的证书</td></tr><tr><td>kubernetes.pem</td><td>k8s-gencert.json、kubernetes-csr.json</td><td>kube-apiserver 使用的证书</td></tr></tbody></table><h4 id="2-2、CFSSL-工具安装"><a href="#2-2、CFSSL-工具安装" class="headerlink" title="2.2、CFSSL 工具安装"></a>2.2、CFSSL 工具安装</h4><p><strong>首先下载 cfssl，并给予可执行权限，然后扔到 PATH 目录下</strong></p><pre><code class="hljs sh">wget https://pkg.cfssl.org/R1.2/cfssl_linux-amd64wget https://pkg.cfssl.org/R1.2/cfssljson_linux-amd64chmod +x cfssl_linux-amd64 cfssljson_linux-amd64mv cfssl_linux-amd64 /usr/<span class="hljs-built_in">local</span>/bin/cfsslmv cfssljson_linux-amd64 /usr/<span class="hljs-built_in">local</span>/bin/cfssljson</code></pre><h4 id="2-3、生成-Etcd-证书"><a href="#2-3、生成-Etcd-证书" class="headerlink" title="2.3、生成 Etcd 证书"></a>2.3、生成 Etcd 证书</h4><p>Etcd 证书生成所需配置文件如下: </p><ul><li>etcd-root-ca-csr.json</li></ul><pre><code class="hljs json">&#123;  <span class="hljs-attr">"key"</span>: &#123;    <span class="hljs-attr">"algo"</span>: <span class="hljs-string">"rsa"</span>,    <span class="hljs-attr">"size"</span>: <span class="hljs-number">4096</span>  &#125;,  <span class="hljs-attr">"names"</span>: [    &#123;      <span class="hljs-attr">"O"</span>: <span class="hljs-string">"etcd"</span>,      <span class="hljs-attr">"OU"</span>: <span class="hljs-string">"etcd Security"</span>,      <span class="hljs-attr">"L"</span>: <span class="hljs-string">"Beijing"</span>,      <span class="hljs-attr">"ST"</span>: <span class="hljs-string">"Beijing"</span>,      <span class="hljs-attr">"C"</span>: <span class="hljs-string">"CN"</span>    &#125;  ],  <span class="hljs-attr">"CN"</span>: <span class="hljs-string">"etcd-root-ca"</span>&#125;</code></pre><ul><li>etcd-gencert.json</li></ul><pre><code class="hljs json">&#123;  <span class="hljs-attr">"signing"</span>: &#123;    <span class="hljs-attr">"default"</span>: &#123;        <span class="hljs-attr">"usages"</span>: [          <span class="hljs-string">"signing"</span>,          <span class="hljs-string">"key encipherment"</span>,          <span class="hljs-string">"server auth"</span>,          <span class="hljs-string">"client auth"</span>        ],        <span class="hljs-attr">"expiry"</span>: <span class="hljs-string">"87600h"</span>    &#125;  &#125;&#125;</code></pre><ul><li>etcd-csr.json</li></ul><pre><code class="hljs json">&#123;  <span class="hljs-attr">"key"</span>: &#123;    <span class="hljs-attr">"algo"</span>: <span class="hljs-string">"rsa"</span>,    <span class="hljs-attr">"size"</span>: <span class="hljs-number">4096</span>  &#125;,  <span class="hljs-attr">"names"</span>: [    &#123;      <span class="hljs-attr">"O"</span>: <span class="hljs-string">"etcd"</span>,      <span class="hljs-attr">"OU"</span>: <span class="hljs-string">"etcd Security"</span>,      <span class="hljs-attr">"L"</span>: <span class="hljs-string">"Beijing"</span>,      <span class="hljs-attr">"ST"</span>: <span class="hljs-string">"Beijing"</span>,      <span class="hljs-attr">"C"</span>: <span class="hljs-string">"CN"</span>    &#125;  ],  <span class="hljs-attr">"CN"</span>: <span class="hljs-string">"etcd"</span>,  <span class="hljs-attr">"hosts"</span>: [    <span class="hljs-string">"127.0.0.1"</span>,    <span class="hljs-string">"localhost"</span>,    <span class="hljs-string">"192.168.1.11"</span>,    <span class="hljs-string">"192.168.1.12"</span>,    <span class="hljs-string">"192.168.1.13"</span>,    <span class="hljs-string">"192.168.1.14"</span>,    <span class="hljs-string">"192.168.1.15"</span>  ]&#125;</code></pre><p>最后生成 Etcd 证书</p><pre><code class="hljs sh">cfssl gencert --initca=<span class="hljs-literal">true</span> etcd-root-ca-csr.json | cfssljson --bare etcd-root-cacfssl gencert --ca etcd-root-ca.pem --ca-key etcd-root-ca-key.pem --config etcd-gencert.json etcd-csr.json | cfssljson --bare etcd</code></pre><p>生成的证书列表如下</p><p><img src="https://cdn.oss.link/markdown/2x0ja.jpg" srcset="/img/loading.gif" alt="Etcd Certs"></p><h4 id="2-4、生成-Kubernetes-证书"><a href="#2-4、生成-Kubernetes-证书" class="headerlink" title="2.4、生成 Kubernetes 证书"></a>2.4、生成 Kubernetes 证书</h4><p>Kubernetes 证书生成所需配置文件如下:</p><ul><li>k8s-root-ca-csr.json</li></ul><pre><code class="hljs json">&#123;  <span class="hljs-attr">"CN"</span>: <span class="hljs-string">"kubernetes"</span>,  <span class="hljs-attr">"key"</span>: &#123;    <span class="hljs-attr">"algo"</span>: <span class="hljs-string">"rsa"</span>,    <span class="hljs-attr">"size"</span>: <span class="hljs-number">4096</span>  &#125;,  <span class="hljs-attr">"names"</span>: [    &#123;      <span class="hljs-attr">"C"</span>: <span class="hljs-string">"CN"</span>,      <span class="hljs-attr">"ST"</span>: <span class="hljs-string">"BeiJing"</span>,      <span class="hljs-attr">"L"</span>: <span class="hljs-string">"BeiJing"</span>,      <span class="hljs-attr">"O"</span>: <span class="hljs-string">"k8s"</span>,      <span class="hljs-attr">"OU"</span>: <span class="hljs-string">"System"</span>    &#125;  ]&#125;</code></pre><ul><li>k8s-gencert.json</li></ul><pre><code class="hljs json">&#123;  <span class="hljs-attr">"signing"</span>: &#123;    <span class="hljs-attr">"default"</span>: &#123;      <span class="hljs-attr">"expiry"</span>: <span class="hljs-string">"87600h"</span>    &#125;,    <span class="hljs-attr">"profiles"</span>: &#123;      <span class="hljs-attr">"kubernetes"</span>: &#123;        <span class="hljs-attr">"usages"</span>: [            <span class="hljs-string">"signing"</span>,            <span class="hljs-string">"key encipherment"</span>,            <span class="hljs-string">"server auth"</span>,            <span class="hljs-string">"client auth"</span>        ],        <span class="hljs-attr">"expiry"</span>: <span class="hljs-string">"87600h"</span>      &#125;    &#125;  &#125;&#125;</code></pre><ul><li>kubernetes-csr.json</li></ul><pre><code class="hljs json">&#123;    <span class="hljs-attr">"CN"</span>: <span class="hljs-string">"kubernetes"</span>,    <span class="hljs-attr">"hosts"</span>: [        <span class="hljs-string">"127.0.0.1"</span>,        <span class="hljs-string">"10.254.0.1"</span>,        <span class="hljs-string">"192.168.1.11"</span>,        <span class="hljs-string">"192.168.1.12"</span>,        <span class="hljs-string">"192.168.1.13"</span>,        <span class="hljs-string">"192.168.1.14"</span>,        <span class="hljs-string">"192.168.1.15"</span>,        <span class="hljs-string">"localhost"</span>,        <span class="hljs-string">"kubernetes"</span>,        <span class="hljs-string">"kubernetes.default"</span>,        <span class="hljs-string">"kubernetes.default.svc"</span>,        <span class="hljs-string">"kubernetes.default.svc.cluster"</span>,        <span class="hljs-string">"kubernetes.default.svc.cluster.local"</span>    ],    <span class="hljs-attr">"key"</span>: &#123;        <span class="hljs-attr">"algo"</span>: <span class="hljs-string">"rsa"</span>,        <span class="hljs-attr">"size"</span>: <span class="hljs-number">2048</span>    &#125;,    <span class="hljs-attr">"names"</span>: [        &#123;            <span class="hljs-attr">"C"</span>: <span class="hljs-string">"CN"</span>,            <span class="hljs-attr">"ST"</span>: <span class="hljs-string">"BeiJing"</span>,            <span class="hljs-attr">"L"</span>: <span class="hljs-string">"BeiJing"</span>,            <span class="hljs-attr">"O"</span>: <span class="hljs-string">"k8s"</span>,            <span class="hljs-attr">"OU"</span>: <span class="hljs-string">"System"</span>        &#125;    ]&#125;</code></pre><ul><li>kube-proxy-csr.json</li></ul><pre><code class="hljs json">&#123;  <span class="hljs-attr">"CN"</span>: <span class="hljs-string">"system:kube-proxy"</span>,  <span class="hljs-attr">"hosts"</span>: [],  <span class="hljs-attr">"key"</span>: &#123;    <span class="hljs-attr">"algo"</span>: <span class="hljs-string">"rsa"</span>,    <span class="hljs-attr">"size"</span>: <span class="hljs-number">2048</span>  &#125;,  <span class="hljs-attr">"names"</span>: [    &#123;      <span class="hljs-attr">"C"</span>: <span class="hljs-string">"CN"</span>,      <span class="hljs-attr">"ST"</span>: <span class="hljs-string">"BeiJing"</span>,      <span class="hljs-attr">"L"</span>: <span class="hljs-string">"BeiJing"</span>,      <span class="hljs-attr">"O"</span>: <span class="hljs-string">"k8s"</span>,      <span class="hljs-attr">"OU"</span>: <span class="hljs-string">"System"</span>    &#125;  ]&#125;</code></pre><ul><li>admin-csr.json</li></ul><pre><code class="hljs json">&#123;  <span class="hljs-attr">"CN"</span>: <span class="hljs-string">"admin"</span>,  <span class="hljs-attr">"hosts"</span>: [],  <span class="hljs-attr">"key"</span>: &#123;    <span class="hljs-attr">"algo"</span>: <span class="hljs-string">"rsa"</span>,    <span class="hljs-attr">"size"</span>: <span class="hljs-number">2048</span>  &#125;,  <span class="hljs-attr">"names"</span>: [    &#123;      <span class="hljs-attr">"C"</span>: <span class="hljs-string">"CN"</span>,      <span class="hljs-attr">"ST"</span>: <span class="hljs-string">"BeiJing"</span>,      <span class="hljs-attr">"L"</span>: <span class="hljs-string">"BeiJing"</span>,      <span class="hljs-attr">"O"</span>: <span class="hljs-string">"system:masters"</span>,      <span class="hljs-attr">"OU"</span>: <span class="hljs-string">"System"</span>    &#125;  ]&#125;</code></pre><p>生成 Kubernetes 证书</p><pre><code class="hljs sh">cfssl gencert --initca=<span class="hljs-literal">true</span> k8s-root-ca-csr.json | cfssljson --bare k8s-root-ca<span class="hljs-keyword">for</span> targetName <span class="hljs-keyword">in</span> kubernetes admin kube-proxy; <span class="hljs-keyword">do</span>    cfssl gencert --ca k8s-root-ca.pem --ca-key k8s-root-ca-key.pem --config k8s-gencert.json --profile kubernetes <span class="hljs-variable">$targetName</span>-csr.json | cfssljson --bare <span class="hljs-variable">$targetName</span><span class="hljs-keyword">done</span></code></pre><p>生成后证书列表如下</p><p><img src="https://cdn.oss.link/markdown/uj9q0.jpg" srcset="/img/loading.gif" alt="Kubernetes Certs"></p><h4 id="2-5、生成-token-及-kubeconfig"><a href="#2-5、生成-token-及-kubeconfig" class="headerlink" title="2.5、生成 token 及 kubeconfig"></a>2.5、生成 token 及 kubeconfig</h4><p>生成 token 如下</p><pre><code class="hljs sh"><span class="hljs-built_in">export</span> BOOTSTRAP_TOKEN=$(head -c 16 /dev/urandom | od -An -t x | tr -d <span class="hljs-string">' '</span>)cat &gt; token.csv &lt;&lt;EOF<span class="hljs-variable">$&#123;BOOTSTRAP_TOKEN&#125;</span>,kubelet-bootstrap,10001,<span class="hljs-string">"system:kubelet-bootstrap"</span>EOF</code></pre><p>创建 kubelet bootstrapping kubeconfig 配置(需要提前安装 kubectl 命令)，<strong>对于 node 节点，api server 地址为本地 nginx 监听的 127.0.0.1:6443，如果想把 master 也当做 node 使用，那么 master 上 api server 地址应该为 masterIP:6443，因为在 master 上没必要也无法启动 nginx 来监听 127.0.0.1:6443(6443 已经被 master 上的 api server 占用了)</strong></p><p><strong>所以以下配置只适合 node 节点，如果想把 master 也当做 node，那么需要重新生成下面的 kubeconfig 配置，并把 api server 地址修改为当前 master 的 api server 地址</strong></p><p><strong>没看懂上面这段话，照着下面的操作就行，看完下面的 Master HA 示意图就懂了</strong></p><pre><code class="hljs sh"><span class="hljs-comment"># Master 上该地址应为 https://MasterIP:6443</span><span class="hljs-built_in">export</span> KUBE_APISERVER=<span class="hljs-string">"https://127.0.0.1:6443"</span><span class="hljs-comment"># 设置集群参数</span>kubectl config <span class="hljs-built_in">set</span>-cluster kubernetes \  --certificate-authority=k8s-root-ca.pem \  --embed-certs=<span class="hljs-literal">true</span> \  --server=<span class="hljs-variable">$&#123;KUBE_APISERVER&#125;</span> \  --kubeconfig=bootstrap.kubeconfig<span class="hljs-comment"># 设置客户端认证参数</span>kubectl config <span class="hljs-built_in">set</span>-credentials kubelet-bootstrap \  --token=<span class="hljs-variable">$&#123;BOOTSTRAP_TOKEN&#125;</span> \  --kubeconfig=bootstrap.kubeconfig<span class="hljs-comment"># 设置上下文参数</span>kubectl config <span class="hljs-built_in">set</span>-context default \  --cluster=kubernetes \  --user=kubelet-bootstrap \  --kubeconfig=bootstrap.kubeconfig<span class="hljs-comment"># 设置默认上下文</span>kubectl config use-context default --kubeconfig=bootstrap.kubeconfig</code></pre><p>创建 kube-proxy kubeconfig 配置，同上面一样，如果想要把 master 当 node 使用，需要修改 api server</p><pre><code class="hljs sh"><span class="hljs-comment"># 设置集群参数</span>kubectl config <span class="hljs-built_in">set</span>-cluster kubernetes \  --certificate-authority=k8s-root-ca.pem \  --embed-certs=<span class="hljs-literal">true</span> \  --server=<span class="hljs-variable">$&#123;KUBE_APISERVER&#125;</span> \  --kubeconfig=kube-proxy.kubeconfig<span class="hljs-comment"># 设置客户端认证参数</span>kubectl config <span class="hljs-built_in">set</span>-credentials kube-proxy \  --client-certificate=kube-proxy.pem \  --client-key=kube-proxy-key.pem \  --embed-certs=<span class="hljs-literal">true</span> \  --kubeconfig=kube-proxy.kubeconfig<span class="hljs-comment"># 设置上下文参数</span>kubectl config <span class="hljs-built_in">set</span>-context default \  --cluster=kubernetes \  --user=kube-proxy \  --kubeconfig=kube-proxy.kubeconfig<span class="hljs-comment"># 设置默认上下文</span>kubectl config use-context default --kubeconfig=kube-proxy.kubeconfig</code></pre><h3 id="三、部署-HA-ETCD"><a href="#三、部署-HA-ETCD" class="headerlink" title="三、部署 HA ETCD"></a>三、部署 HA ETCD</h3><h4 id="3-1、安装-Etcd"><a href="#3-1、安装-Etcd" class="headerlink" title="3.1、安装 Etcd"></a>3.1、安装 Etcd</h4><p>ETCD 直接采用 rpm 安装，RPM 可以从 <a href="https://src.fedoraproject.org/cgit/rpms/etcd.git/" target="_blank" rel="noopener">Fedora 官方仓库</a> 获取 spec 文件自己 build，或者直接从 <a href="https://www.rpmfind.net/" target="_blank" rel="noopener">rpmFind 网站</a> 搜索</p><pre><code class="hljs sh"><span class="hljs-comment"># 下载 rpm 包</span>wget ftp://195.220.108.108/linux/fedora/linux/development/rawhide/Everything/x86_64/os/Packages/e/etcd-3.1.9-1.fc27.x86_64.rpm<span class="hljs-comment"># 分发并安装 rpm</span><span class="hljs-keyword">for</span> IP <span class="hljs-keyword">in</span> `seq 1 3`; <span class="hljs-keyword">do</span>    scp etcd-3.1.9-1.fc27.x86_64.rpm root@192.168.1.1<span class="hljs-variable">$IP</span>:~    ssh root@192.168.1.1<span class="hljs-variable">$IP</span> rpm -ivh etcd-3.1.9-1.fc27.x86_64.rpm<span class="hljs-keyword">done</span></code></pre><h4 id="3-2、分发证书"><a href="#3-2、分发证书" class="headerlink" title="3.2、分发证书"></a>3.2、分发证书</h4><pre><code class="hljs sh"><span class="hljs-keyword">for</span> IP <span class="hljs-keyword">in</span> `seq 1 3`;<span class="hljs-keyword">do</span>    ssh root@192.168.1.1<span class="hljs-variable">$IP</span> mkdir /etc/etcd/ssl    scp *.pem root@192.168.1.1<span class="hljs-variable">$IP</span>:/etc/etcd/ssl    ssh root@192.168.1.1<span class="hljs-variable">$IP</span> chown -R etcd:etcd /etc/etcd/ssl    ssh root@192.168.1.1<span class="hljs-variable">$IP</span> chmod -R 755 /etc/etcd<span class="hljs-keyword">done</span></code></pre><h4 id="3-3、修改配置"><a href="#3-3、修改配置" class="headerlink" title="3.3、修改配置"></a>3.3、修改配置</h4><p>rpm 安装好以后直接修改 <code>/etc/etcd/etcd.conf</code> 配置文件即可，其中单个节点配置如下(其他节点只是名字和 IP 不同)</p><pre><code class="hljs sh"><span class="hljs-comment"># [member]</span>ETCD_NAME=etcd1ETCD_DATA_DIR=<span class="hljs-string">"/var/lib/etcd/etcd1.etcd"</span>ETCD_WAL_DIR=<span class="hljs-string">"/var/lib/etcd/wal"</span>ETCD_SNAPSHOT_COUNT=<span class="hljs-string">"100"</span>ETCD_HEARTBEAT_INTERVAL=<span class="hljs-string">"100"</span>ETCD_ELECTION_TIMEOUT=<span class="hljs-string">"1000"</span>ETCD_LISTEN_PEER_URLS=<span class="hljs-string">"https://192.168.1.11:2380"</span>ETCD_LISTEN_CLIENT_URLS=<span class="hljs-string">"https://192.168.1.11:2379,http://127.0.0.1:2379"</span>ETCD_MAX_SNAPSHOTS=<span class="hljs-string">"5"</span>ETCD_MAX_WALS=<span class="hljs-string">"5"</span><span class="hljs-comment">#ETCD_CORS=""</span><span class="hljs-comment"># [cluster]</span>ETCD_INITIAL_ADVERTISE_PEER_URLS=<span class="hljs-string">"https://192.168.1.11:2380"</span><span class="hljs-comment"># if you use different ETCD_NAME (e.g. test), set ETCD_INITIAL_CLUSTER value for this name, i.e. "test=http://..."</span>ETCD_INITIAL_CLUSTER=<span class="hljs-string">"etcd1=https://192.168.1.11:2380,etcd2=https://192.168.1.12:2380,etcd3=https://192.168.1.13:2380"</span>ETCD_INITIAL_CLUSTER_STATE=<span class="hljs-string">"new"</span>ETCD_INITIAL_CLUSTER_TOKEN=<span class="hljs-string">"etcd-cluster"</span>ETCD_ADVERTISE_CLIENT_URLS=<span class="hljs-string">"https://192.168.1.11:2379"</span><span class="hljs-comment">#ETCD_DISCOVERY=""</span><span class="hljs-comment">#ETCD_DISCOVERY_SRV=""</span><span class="hljs-comment">#ETCD_DISCOVERY_FALLBACK="proxy"</span><span class="hljs-comment">#ETCD_DISCOVERY_PROXY=""</span><span class="hljs-comment">#ETCD_STRICT_RECONFIG_CHECK="false"</span><span class="hljs-comment">#ETCD_AUTO_COMPACTION_RETENTION="0"</span><span class="hljs-comment"># [proxy]</span><span class="hljs-comment">#ETCD_PROXY="off"</span><span class="hljs-comment">#ETCD_PROXY_FAILURE_WAIT="5000"</span><span class="hljs-comment">#ETCD_PROXY_REFRESH_INTERVAL="30000"</span><span class="hljs-comment">#ETCD_PROXY_DIAL_TIMEOUT="1000"</span><span class="hljs-comment">#ETCD_PROXY_WRITE_TIMEOUT="5000"</span><span class="hljs-comment">#ETCD_PROXY_READ_TIMEOUT="0"</span><span class="hljs-comment"># [security]</span>ETCD_CERT_FILE=<span class="hljs-string">"/etc/etcd/ssl/etcd.pem"</span>ETCD_KEY_FILE=<span class="hljs-string">"/etc/etcd/ssl/etcd-key.pem"</span>ETCD_CLIENT_CERT_AUTH=<span class="hljs-string">"true"</span>ETCD_TRUSTED_CA_FILE=<span class="hljs-string">"/etc/etcd/ssl/etcd-root-ca.pem"</span>ETCD_AUTO_TLS=<span class="hljs-string">"true"</span>ETCD_PEER_CERT_FILE=<span class="hljs-string">"/etc/etcd/ssl/etcd.pem"</span>ETCD_PEER_KEY_FILE=<span class="hljs-string">"/etc/etcd/ssl/etcd-key.pem"</span>ETCD_PEER_CLIENT_CERT_AUTH=<span class="hljs-string">"true"</span>ETCD_PEER_TRUSTED_CA_FILE=<span class="hljs-string">"/etc/etcd/ssl/etcd-root-ca.pem"</span>ETCD_PEER_AUTO_TLS=<span class="hljs-string">"true"</span><span class="hljs-comment"># [logging]</span><span class="hljs-comment">#ETCD_DEBUG="false"</span><span class="hljs-comment"># examples for -log-package-levels etcdserver=WARNING,security=DEBUG</span><span class="hljs-comment">#ETCD_LOG_PACKAGE_LEVELS=""</span></code></pre><h4 id="3-4、启动及验证"><a href="#3-4、启动及验证" class="headerlink" title="3.4、启动及验证"></a>3.4、启动及验证</h4><p>配置修改后在每个节点进行启动即可，<strong>注意，Etcd 各个节点间必须保证时钟同步，否则会造成启动失败等错误</strong></p><pre><code class="hljs sh">systemctl daemon-reloadsystemctl start etcdsystemctl <span class="hljs-built_in">enable</span> etcd</code></pre><p>启动成功后验证节点状态</p><pre><code class="hljs sh"><span class="hljs-built_in">export</span> ETCDCTL_API=3etcdctl --cacert=/etc/etcd/ssl/etcd-root-ca.pem --cert=/etc/etcd/ssl/etcd.pem --key=/etc/etcd/ssl/etcd-key.pem --endpoints=https://192.168.1.11:2379,https://192.168.1.12:2379,https://192.168.1.13:2379 endpoint health</code></pre><p>最后截图如下，警告可忽略</p><p><img src="https://cdn.oss.link/markdown/yr6k4.jpg" srcset="/img/loading.gif" alt="Etcd Healthy"></p><h3 id="四、部署-HA-Master"><a href="#四、部署-HA-Master" class="headerlink" title="四、部署 HA Master"></a>四、部署 HA Master</h3><h4 id="4-1、HA-Master-简述"><a href="#4-1、HA-Master-简述" class="headerlink" title="4.1、HA Master 简述"></a>4.1、HA Master 简述</h4><p>目前所谓的 Kubernetes HA 其实主要的就是 API Server 的 HA，master 上其他组件比如 controller-manager 等都是可以通过 Etcd 做选举；而 API Server 只是提供一个请求接收服务，所以对于 API Server 一般有两种方式做 HA；一种是对多个 API Server 做 vip，另一种使用 nginx 反向代理，本文采用 nginx 方式，以下为 HA 示意图</p><p><img src="https://cdn.oss.link/markdown/m2sug.jpg" srcset="/img/loading.gif" alt="master ha"></p><p><strong>master 之间除 api server 以外其他组件通过 etcd 选举，api server 默认不作处理；在每个 node 上启动一个 nginx，每个 nginx 反向代理所有 api server，node 上 kubelet、kube-proxy 连接本地的 nginx 代理端口，当 nginx 发现无法连接后端时会自动踢掉出问题的 api server，从而实现 api server 的 HA</strong></p><h4 id="4-2、部署前预处理"><a href="#4-2、部署前预处理" class="headerlink" title="4.2、部署前预处理"></a>4.2、部署前预处理</h4><p>一切以偷懒为主，所以我们仍然采用 rpm 的方式来安装 kubernetes 各个组件，关于 rpm 获取方式可以参考 <a href="https://mritd.me/2017/07/12/how-to-build-kubernetes-rpm/" target="_blank" rel="noopener">How to build Kubernetes RPM</a>，以下文章默认认为你已经搞定了 rpm</p><pre><code class="hljs sh"><span class="hljs-comment"># 分发 rpm</span><span class="hljs-keyword">for</span> IP <span class="hljs-keyword">in</span> `seq 1 3`; <span class="hljs-keyword">do</span>    scp kubernetes*.rpm root@192.168.1.1<span class="hljs-variable">$IP</span>:~;     ssh root@192.168.1.1<span class="hljs-variable">$IP</span> yum install -y conntrack-tools socat    ssh root@192.168.1.1<span class="hljs-variable">$IP</span> rpm -ivh kubernetes*.rpm<span class="hljs-keyword">done</span></code></pre><p>rpm 安装好以后还需要进行分发证书配置等</p><pre><code class="hljs sh"><span class="hljs-keyword">for</span> IP <span class="hljs-keyword">in</span> `seq 1 3`;<span class="hljs-keyword">do</span>    ssh root@192.168.1.1<span class="hljs-variable">$IP</span> mkdir /etc/kubernetes/ssl    scp *.pem root@192.168.1.1<span class="hljs-variable">$IP</span>:/etc/kubernetes/ssl    scp *.kubeconfig root@192.168.1.1<span class="hljs-variable">$IP</span>:/etc/kubernetes    scp token.csv root@192.168.1.1<span class="hljs-variable">$IP</span>:/etc/kubernetes    ssh root@192.168.1.1<span class="hljs-variable">$IP</span> chown -R kube:kube /etc/kubernetes/ssl<span class="hljs-keyword">done</span></code></pre><p><strong>最后由于 api server 会写入一些日志，所以先创建好相关目录，并做好授权，防止因为权限错误导致 api server 无法启动</strong></p><pre><code class="hljs sh"><span class="hljs-keyword">for</span> IP <span class="hljs-keyword">in</span> `seq 1 3`;<span class="hljs-keyword">do</span>    ssh root@192.168.1.1<span class="hljs-variable">$IP</span> mkdir /var/<span class="hljs-built_in">log</span>/kube-audit      ssh root@192.168.1.1<span class="hljs-variable">$IP</span> chown -R kube:kube /var/<span class="hljs-built_in">log</span>/kube-audit    ssh root@192.168.1.1<span class="hljs-variable">$IP</span> chmod -R 755 /var/<span class="hljs-built_in">log</span>/kube-audit<span class="hljs-keyword">done</span></code></pre><h4 id="4-3、修改-master-配置"><a href="#4-3、修改-master-配置" class="headerlink" title="4.3、修改 master 配置"></a>4.3、修改 master 配置</h4><p>rpm 安装好以后，默认会生成 <code>/etc/kubernetes</code> 目录，并且该目录中会有很多配置，其中 config 配置文件为通用配置，具体文件如下</p><pre><code class="hljs sh">➜  kubernetes tree.├── apiserver├── config├── controller-manager├── kubelet├── proxy└── scheduler0 directories, 6 files</code></pre><p><strong>master 需要编辑 <code>config</code>、<code>apiserver</code>、<code>controller-manager</code>、<code>scheduler</code>这四个文件，具体修改如下</strong></p><ul><li>config 通用配置</li></ul><pre><code class="hljs sh"><span class="hljs-comment">###</span><span class="hljs-comment"># kubernetes system config</span><span class="hljs-comment">#</span><span class="hljs-comment"># The following values are used to configure various aspects of all</span><span class="hljs-comment"># kubernetes services, including</span><span class="hljs-comment">#</span><span class="hljs-comment">#   kube-apiserver.service</span><span class="hljs-comment">#   kube-controller-manager.service</span><span class="hljs-comment">#   kube-scheduler.service</span><span class="hljs-comment">#   kubelet.service</span><span class="hljs-comment">#   kube-proxy.service</span><span class="hljs-comment"># logging to stderr means we get it in the systemd journal</span>KUBE_LOGTOSTDERR=<span class="hljs-string">"--logtostderr=true"</span><span class="hljs-comment"># journal message level, 0 is debug</span>KUBE_LOG_LEVEL=<span class="hljs-string">"--v=2"</span><span class="hljs-comment"># Should this cluster be allowed to run privileged docker containers</span>KUBE_ALLOW_PRIV=<span class="hljs-string">"--allow-privileged=true"</span><span class="hljs-comment"># How the controller-manager, scheduler, and proxy find the apiserver</span>KUBE_MASTER=<span class="hljs-string">"--master=http://127.0.0.1:8080"</span></code></pre><ul><li>apiserver 配置(其他节点只有 IP 不同)</li></ul><pre><code class="hljs sh"><span class="hljs-comment">###</span><span class="hljs-comment"># kubernetes system config</span><span class="hljs-comment">#</span><span class="hljs-comment"># The following values are used to configure the kube-apiserver</span><span class="hljs-comment">#</span><span class="hljs-comment"># The address on the local server to listen to.</span>KUBE_API_ADDRESS=<span class="hljs-string">"--advertise-address=192.168.1.11 --insecure-bind-address=127.0.0.1 --bind-address=192.168.1.11"</span><span class="hljs-comment"># The port on the local server to listen on.</span>KUBE_API_PORT=<span class="hljs-string">"--insecure-port=8080 --secure-port=6443"</span><span class="hljs-comment"># Port minions listen on</span><span class="hljs-comment"># KUBELET_PORT="--kubelet-port=10250"</span><span class="hljs-comment"># Comma separated list of nodes in the etcd cluster</span>KUBE_ETCD_SERVERS=<span class="hljs-string">"--etcd-servers=https://192.168.1.11:2379,https://192.168.1.12:2379,https://192.168.1.13:2379"</span><span class="hljs-comment"># Address range to use for services</span>KUBE_SERVICE_ADDRESSES=<span class="hljs-string">"--service-cluster-ip-range=10.254.0.0/16"</span><span class="hljs-comment"># default admission control policies</span>KUBE_ADMISSION_CONTROL=<span class="hljs-string">"--admission-control=NamespaceLifecycle,LimitRanger,SecurityContextDeny,ServiceAccount,ResourceQuota"</span><span class="hljs-comment"># Add your own!</span>KUBE_API_ARGS=<span class="hljs-string">"--authorization-mode=RBAC \</span><span class="hljs-string">               --runtime-config=rbac.authorization.k8s.io/v1beta1 \</span><span class="hljs-string">               --anonymous-auth=false \</span><span class="hljs-string">               --kubelet-https=true \</span><span class="hljs-string">               --experimental-bootstrap-token-auth \</span><span class="hljs-string">               --token-auth-file=/etc/kubernetes/token.csv \</span><span class="hljs-string">               --service-node-port-range=30000-50000 \</span><span class="hljs-string">               --tls-cert-file=/etc/kubernetes/ssl/kubernetes.pem \</span><span class="hljs-string">               --tls-private-key-file=/etc/kubernetes/ssl/kubernetes-key.pem \</span><span class="hljs-string">               --client-ca-file=/etc/kubernetes/ssl/k8s-root-ca.pem \</span><span class="hljs-string">               --service-account-key-file=/etc/kubernetes/ssl/k8s-root-ca.pem \</span><span class="hljs-string">               --etcd-quorum-read=true \</span><span class="hljs-string">               --storage-backend=etcd3 \</span><span class="hljs-string">               --etcd-cafile=/etc/etcd/ssl/etcd-root-ca.pem \</span><span class="hljs-string">               --etcd-certfile=/etc/etcd/ssl/etcd.pem \</span><span class="hljs-string">               --etcd-keyfile=/etc/etcd/ssl/etcd-key.pem \</span><span class="hljs-string">               --enable-swagger-ui=true \</span><span class="hljs-string">               --apiserver-count=3 \</span><span class="hljs-string">               --audit-log-maxage=30 \</span><span class="hljs-string">               --audit-log-maxbackup=3 \</span><span class="hljs-string">               --audit-log-maxsize=100 \</span><span class="hljs-string">               --audit-log-path=/var/log/kube-audit/audit.log \</span><span class="hljs-string">               --event-ttl=1h"</span></code></pre><ul><li>controller-manager 配置</li></ul><pre><code class="hljs sh"><span class="hljs-comment">###</span><span class="hljs-comment"># The following values are used to configure the kubernetes controller-manager</span><span class="hljs-comment"># defaults from config and apiserver should be adequate</span><span class="hljs-comment"># Add your own!</span>KUBE_CONTROLLER_MANAGER_ARGS=<span class="hljs-string">"--address=0.0.0.0 \</span><span class="hljs-string">                              --service-cluster-ip-range=10.254.0.0/16 \</span><span class="hljs-string">                              --cluster-name=kubernetes \</span><span class="hljs-string">                              --cluster-signing-cert-file=/etc/kubernetes/ssl/k8s-root-ca.pem \</span><span class="hljs-string">                              --cluster-signing-key-file=/etc/kubernetes/ssl/k8s-root-ca-key.pem \</span><span class="hljs-string">                              --service-account-private-key-file=/etc/kubernetes/ssl/k8s-root-ca-key.pem \</span><span class="hljs-string">                              --root-ca-file=/etc/kubernetes/ssl/k8s-root-ca.pem \</span><span class="hljs-string">                              --experimental-cluster-signing-duration=87600h0m0s \</span><span class="hljs-string">                              --leader-elect=true \</span><span class="hljs-string">                              --node-monitor-grace-period=40s \</span><span class="hljs-string">                              --node-monitor-period=5s \</span><span class="hljs-string">                              --pod-eviction-timeout=5m0s"</span></code></pre><ul><li>scheduler 配置</li></ul><pre><code class="hljs sh"><span class="hljs-comment">###</span><span class="hljs-comment"># kubernetes scheduler config</span><span class="hljs-comment"># default config should be adequate</span><span class="hljs-comment"># Add your own!</span>KUBE_SCHEDULER_ARGS=<span class="hljs-string">"--leader-elect=true --address=0.0.0.0"</span></code></pre><p>其他 master 节点配置相同，只需要修改以下 IP 地址即可，修改完成后启动 api server</p><pre><code class="hljs sh">systemctl daemon-reloadsystemctl start kube-apiserversystemctl start kube-controller-managersystemctl start kube-schedulersystemctl <span class="hljs-built_in">enable</span> kube-apiserversystemctl <span class="hljs-built_in">enable</span> kube-controller-managersystemctl <span class="hljs-built_in">enable</span> kube-scheduler</code></pre><p>各个节点启动成功后，验证组件状态(kubectl 在不做任何配置的情况下默认链接本地 8080 端口)如下，<strong>其中 etcd 全部为 Unhealthy 状态，并且提示 <code>remote error: tls: bad certificate</code> 这是个 bug，不影响实际使用，具体可参考 <a href="https://github.com/kubernetes/kubernetes/issues/29330" target="_blank" rel="noopener">issue</a></strong></p><p><img src="https://cdn.oss.link/markdown/0j7k2.jpg" srcset="/img/loading.gif" alt="api status"></p><h3 id="五、部署-Node"><a href="#五、部署-Node" class="headerlink" title="五、部署 Node"></a>五、部署 Node</h3><h4 id="5-1、部署前预处理"><a href="#5-1、部署前预处理" class="headerlink" title="5.1、部署前预处理"></a>5.1、部署前预处理</h4><p>部署前分发 rpm 以及证书、token 等配置</p><pre><code class="hljs sh"><span class="hljs-comment"># 分发 rpm</span><span class="hljs-keyword">for</span> IP <span class="hljs-keyword">in</span> `seq 4 5`;<span class="hljs-keyword">do</span>    scp kubernetes-node-1.6.7-1.el7.centos.x86_64.rpm kubernetes-client-1.6.7-1.el7.centos.x86_64.rpm root@192.168.1.1<span class="hljs-variable">$IP</span>:~;     ssh root@192.168.1.1<span class="hljs-variable">$IP</span> yum install -y conntrack-tools socat    ssh root@192.168.1.1<span class="hljs-variable">$IP</span> rpm -ivh kubernetes-node-1.6.7-1.el7.centos.x86_64.rpm kubernetes-client-1.6.7-1.el7.centos.x86_64.rpm<span class="hljs-keyword">done</span><span class="hljs-comment"># 分发证书等配置文件</span><span class="hljs-keyword">for</span> IP <span class="hljs-keyword">in</span> `seq 4 5`;<span class="hljs-keyword">do</span>    ssh root@192.168.1.1<span class="hljs-variable">$IP</span> mkdir /etc/kubernetes/ssl    scp *.pem root@192.168.1.1<span class="hljs-variable">$IP</span>:/etc/kubernetes/ssl    scp *.kubeconfig root@192.168.1.1<span class="hljs-variable">$IP</span>:/etc/kubernetes    scp token.csv root@192.168.1.1<span class="hljs-variable">$IP</span>:/etc/kubernetes    ssh root@192.168.1.1<span class="hljs-variable">$IP</span> chown -R kube:kube /etc/kubernetes/ssl<span class="hljs-keyword">done</span></code></pre><h4 id="5-2、修改-node-配置"><a href="#5-2、修改-node-配置" class="headerlink" title="5.2、修改 node 配置"></a>5.2、修改 node 配置</h4><p>node 节点上配置文件同样位于 <code>/etc/kubernetes</code> 目录，node 节点只需要修改 <code>config</code>、<code>kubelet</code>、<code>proxy</code> 这三个配置文件，修改如下</p><ul><li>config 通用配置</li></ul><p><strong>注意: config 配置文件(包括下面的 kubelet、proxy)中全部未 定义 API Server 地址，因为 kubelet 和 kube-proxy 组件启动时使用了 <code>--require-kubeconfig</code> 选项，该选项会使其从 <code>*.kubeconfig</code> 中读取 API Server 地址，而忽略配置文件中设置的；所以配置文件中设置的地址其实是无效的</strong></p><pre><code class="hljs sh"><span class="hljs-comment">###</span><span class="hljs-comment"># kubernetes system config</span><span class="hljs-comment">#</span><span class="hljs-comment"># The following values are used to configure various aspects of all</span><span class="hljs-comment"># kubernetes services, including</span><span class="hljs-comment">#</span><span class="hljs-comment">#   kube-apiserver.service</span><span class="hljs-comment">#   kube-controller-manager.service</span><span class="hljs-comment">#   kube-scheduler.service</span><span class="hljs-comment">#   kubelet.service</span><span class="hljs-comment">#   kube-proxy.service</span><span class="hljs-comment"># logging to stderr means we get it in the systemd journal</span>KUBE_LOGTOSTDERR=<span class="hljs-string">"--logtostderr=true"</span><span class="hljs-comment"># journal message level, 0 is debug</span>KUBE_LOG_LEVEL=<span class="hljs-string">"--v=2"</span><span class="hljs-comment"># Should this cluster be allowed to run privileged docker containers</span>KUBE_ALLOW_PRIV=<span class="hljs-string">"--allow-privileged=true"</span><span class="hljs-comment"># How the controller-manager, scheduler, and proxy find the apiserver</span><span class="hljs-comment"># KUBE_MASTER="--master=http://127.0.0.1:8080"</span></code></pre><ul><li>kubelet 配置</li></ul><pre><code class="hljs sh"><span class="hljs-comment">###</span><span class="hljs-comment"># kubernetes kubelet (minion) config</span><span class="hljs-comment"># The address for the info server to serve on (set to 0.0.0.0 or "" for all interfaces)</span>KUBELET_ADDRESS=<span class="hljs-string">"--address=192.168.1.14"</span><span class="hljs-comment"># The port for the info server to serve on</span><span class="hljs-comment"># KUBELET_PORT="--port=10250"</span><span class="hljs-comment"># You may leave this blank to use the actual hostname</span>KUBELET_HOSTNAME=<span class="hljs-string">"--hostname-override=docker4.node"</span><span class="hljs-comment"># location of the api-server</span><span class="hljs-comment"># KUBELET_API_SERVER=""</span><span class="hljs-comment"># Add your own!</span>KUBELET_ARGS=<span class="hljs-string">"--cgroup-driver=cgroupfs \</span><span class="hljs-string">              --cluster-dns=10.254.0.2 \</span><span class="hljs-string">              --resolv-conf=/etc/resolv.conf \</span><span class="hljs-string">              --experimental-bootstrap-kubeconfig=/etc/kubernetes/bootstrap.kubeconfig \</span><span class="hljs-string">              --kubeconfig=/etc/kubernetes/kubelet.kubeconfig \</span><span class="hljs-string">              --require-kubeconfig \</span><span class="hljs-string">              --cert-dir=/etc/kubernetes/ssl \</span><span class="hljs-string">              --cluster-domain=cluster.local. \</span><span class="hljs-string">              --hairpin-mode promiscuous-bridge \</span><span class="hljs-string">              --serialize-image-pulls=false \</span><span class="hljs-string">              --pod-infra-container-image=gcr.io/google_containers/pause-amd64:3.0"</span></code></pre><ul><li>proxy 配置</li></ul><pre><code class="hljs sh"><span class="hljs-comment">###</span><span class="hljs-comment"># kubernetes proxy config</span><span class="hljs-comment"># default config should be adequate</span><span class="hljs-comment"># Add your own!</span>KUBE_PROXY_ARGS=<span class="hljs-string">"--bind-address=192.168.1.14 \</span><span class="hljs-string">                 --hostname-override=docker4.node \</span><span class="hljs-string">                 --kubeconfig=/etc/kubernetes/kube-proxy.kubeconfig \</span><span class="hljs-string">                 --cluster-cidr=10.254.0.0/16"</span></code></pre><h4 id="5-3、创建-ClusterRoleBinding"><a href="#5-3、创建-ClusterRoleBinding" class="headerlink" title="5.3、创建 ClusterRoleBinding"></a>5.3、创建 ClusterRoleBinding</h4><p>由于 kubelet 采用了 <a href="https://kubernetes.io/docs/admin/kubelet-tls-bootstrapping/" target="_blank" rel="noopener">TLS Bootstrapping</a>，所有根绝 RBAC 控制策略，kubelet 使用的用户 <code>kubelet-bootstrap</code> 是不具备任何访问 API 权限的，这是需要预先在集群内创建 ClusterRoleBinding 授予其 <code>system:node-bootstrapper</code> Role</p><pre><code class="hljs sh"><span class="hljs-comment"># 在任意 master 执行即可</span>kubectl create clusterrolebinding kubelet-bootstrap \  --clusterrole=system:node-bootstrapper \  --user=kubelet-bootstrap</code></pre><h4 id="5-4、创建-nginx-代理"><a href="#5-4、创建-nginx-代理" class="headerlink" title="5.4、创建 nginx 代理"></a>5.4、创建 nginx 代理</h4><p><strong>根据上面描述的 master HA 架构，此时所有 node 应该连接本地的 nginx 代理，然后 nginx 来负载所有 api server；以下为 nginx 代理相关配置</strong></p><pre><code class="hljs sh"><span class="hljs-comment"># 创建配置目录</span>mkdir -p /etc/nginx<span class="hljs-comment"># 写入代理配置</span>cat &lt;&lt; EOF &gt;&gt; /etc/nginx/nginx.conferror_log stderr notice;worker_processes auto;events &#123;  multi_accept on;  use epoll;  worker_connections 1024;&#125;stream &#123;    upstream kube_apiserver &#123;        least_conn;        server 192.168.1.11:6443;        server 192.168.1.12:6443;        server 192.168.1.13:6443;    &#125;    server &#123;        listen        0.0.0.0:6443;        proxy_pass    kube_apiserver;        proxy_timeout 10m;        proxy_connect_timeout 1s;    &#125;&#125;EOF<span class="hljs-comment"># 更新权限</span>chmod +r /etc/nginx/nginx.conf</code></pre><p>为了保证 nginx 的可靠性，综合便捷性考虑，<strong>node 节点上的 nginx 使用 docker 启动，同时 使用 systemd 来守护，</strong> systemd 配置如下</p><pre><code class="hljs sh">cat &lt;&lt; EOF &gt;&gt; /etc/systemd/system/nginx-proxy.service[Unit]Description=kubernetes apiserver docker wrapperWants=docker.socketAfter=docker.service[Service]User=rootPermissionsStartOnly=<span class="hljs-literal">true</span>ExecStart=/usr/bin/docker run -p 127.0.0.1:6443:6443 \\                              -v /etc/nginx:/etc/nginx \\                              --name nginx-proxy \\                              --net=host \\                              --restart=on-failure:5 \\                              --memory=512M \\                              nginx:1.13.3-alpineExecStartPre=-/usr/bin/docker rm -f nginx-proxyExecStop=/usr/bin/docker stop nginx-proxyRestart=alwaysRestartSec=15sTimeoutStartSec=30s[Install]WantedBy=multi-user.targetEOF</code></pre><p><strong>最后启动 nginx，同时在每个 node 安装 kubectl，然后使用 kubectl 测试 api server 负载情况</strong></p><pre><code class="hljs sh">systemctl daemon-reloadsystemctl start nginx-proxysystemctl <span class="hljs-built_in">enable</span> nginx-proxy</code></pre><p>启动成功后如下</p><p><img src="https://cdn.oss.link/markdown/0shgz.jpg" srcset="/img/loading.gif" alt="nginx-proxy"></p><p>kubectl 测试联通性如下</p><p><img src="https://cdn.oss.link/markdown/maiz2.jpg" srcset="/img/loading.gif" alt="test nginx-proxy"></p><h4 id="5-5、添加-Node"><a href="#5-5、添加-Node" class="headerlink" title="5.5、添加 Node"></a>5.5、添加 Node</h4><p>一起准备就绪以后就可以启动 node 相关组件了</p><pre><code class="hljs sh">systemctl daemon-reloadsystemctl start kubeletsystemctl <span class="hljs-built_in">enable</span> kubelet</code></pre><p>由于采用了 <a href="https://kubernetes.io/docs/admin/kubelet-tls-bootstrapping/" target="_blank" rel="noopener">TLS Bootstrapping</a>，所以 kubelet 启动后不会立即加入集群，而是进行证书申请，从日志中可以看到如下输出</p><pre><code class="hljs sh">Jul 19 14:15:31 docker4.node kubelet[18213]: I0719 14:15:31.810914   18213 feature_gate.go:144] feature gates: map[]Jul 19 14:15:31 docker4.node kubelet[18213]: I0719 14:15:31.811025   18213 bootstrap.go:58] Using bootstrap kubeconfig to generate TLS client cert, key and kubeconfig file</code></pre><p><strong>此时只需要在 master 允许其证书申请即可</strong></p><pre><code class="hljs sh"><span class="hljs-comment"># 查看 csr</span>➜  kubectl get csrNAME        AGE       REQUESTOR           CONDITIONcsr-l9d25   2m        kubelet-bootstrap   Pending<span class="hljs-comment"># 签发证书</span>➜  kubectl certificate approve csr-l9d25certificatesigningrequest <span class="hljs-string">"csr-l9d25"</span> approved<span class="hljs-comment"># 查看 node</span>➜  kubectl get nodeNAME           STATUS    AGE       VERSIONdocker4.node   Ready     26s       v1.6.7</code></pre><p>最后再启动 kube-proxy 组件即可</p><pre><code class="hljs sh">systemctl start kube-proxysystemctl <span class="hljs-built_in">enable</span> kube-proxy</code></pre><h4 id="5-6、Master-开启-Pod-负载"><a href="#5-6、Master-开启-Pod-负载" class="headerlink" title="5.6、Master 开启 Pod 负载"></a>5.6、Master 开启 Pod 负载</h4><p>Master 上部署 Node 与单独 Node 部署大致相同，<strong>只需要修改 <code>bootstrap.kubeconfig</code>、<code>kube-proxy.kubeconfig</code> 中的 API Server 地址即可</strong></p><p><img src="https://cdn.oss.link/markdown/2gkyj.jpg" srcset="/img/loading.gif" alt="modify api server"></p><p>然后修改 <code>kubelet</code>、<code>proxy</code> 配置启动即可</p><pre><code class="hljs sh">systemctl daemon-reloadsystemctl start kubeletsystemctl <span class="hljs-built_in">enable</span> kubeletsystemctl start kube-proxysystemctl <span class="hljs-built_in">enable</span> kube-proxy</code></pre><p>最后在 master 签发一下相关证书</p><pre><code class="hljs sh">kubectl certificate approve csr-z090b</code></pre><p>整体部署完成后如下</p><p><img src="https://cdn.oss.link/markdown/fsddv.jpg" srcset="/img/loading.gif" alt="read"></p><h3 id="六、部署-Calico"><a href="#六、部署-Calico" class="headerlink" title="六、部署 Calico"></a>六、部署 Calico</h3><p>网路组件这里采用 Calico，Calico 目前部署也相对比较简单，只需要创建一下 yml 文件即可，具体可参考 <a href="http://docs.projectcalico.org/v2.3/getting-started/kubernetes/" target="_blank" rel="noopener">Calico 官方文档</a></p><p><strong>Cliaco 官方文档要求 kubelet 启动时要配置使用 cni 插件 <code>--network-plugin=cni</code>，同时 kube-proxy 不能使用 <code>--masquerade-all</code> 启动(会与 Calico policy 冲突)，所以需要修改所有 kubelet 和 proxy 配置文件，以下默认为这两项已经调整完毕，这里不做演示</strong></p><pre><code class="hljs sh"><span class="hljs-comment"># 获取相关 Cliaco.yml</span>wget http://docs.projectcalico.org/v2.3/getting-started/kubernetes/installation/hosted/calico.yaml<span class="hljs-comment"># 修改 Etcd 相关配置，以下列出主要修改部分(etcd 证书内容需要被 base64 转码)</span>sed -i <span class="hljs-string">'s@.*etcd_endpoints:.*@\ \ etcd_endpoints:\ \"https://192.168.1.11:2379,https://192.168.1.12:2379,https://192.168.1.13:2379\"@gi'</span> calico.yaml<span class="hljs-built_in">export</span> ETCD_CERT=`cat /etc/etcd/ssl/etcd.pem | base64 | tr -d <span class="hljs-string">'\n'</span>`<span class="hljs-built_in">export</span> ETCD_KEY=`cat /etc/etcd/ssl/etcd-key.pem | base64 | tr -d <span class="hljs-string">'\n'</span>`<span class="hljs-built_in">export</span> ETCD_CA=`cat /etc/etcd/ssl/etcd-root-ca.pem | base64 | tr -d <span class="hljs-string">'\n'</span>`sed -i <span class="hljs-string">"s@.*etcd-cert:.*@\ \ etcd-cert:\ <span class="hljs-variable">$&#123;ETCD_CERT&#125;</span>@gi"</span> calico.yamlsed -i <span class="hljs-string">"s@.*etcd-key:.*@\ \ etcd-key:\ <span class="hljs-variable">$&#123;ETCD_KEY&#125;</span>@gi"</span> calico.yamlsed -i <span class="hljs-string">"s@.*etcd-ca:.*@\ \ etcd-ca:\ <span class="hljs-variable">$&#123;ETCD_CA&#125;</span>@gi"</span> calico.yamlsed -i <span class="hljs-string">'s@.*etcd_ca:.*@\ \ etcd_ca:\ "/calico-secrets/etcd-ca"@gi'</span> calico.yamlsed -i <span class="hljs-string">'s@.*etcd_cert:.*@\ \ etcd_cert:\ "/calico-secrets/etcd-cert"@gi'</span> calico.yamlsed -i <span class="hljs-string">'s@.*etcd_key:.*@\ \ etcd_key:\ "/calico-secrets/etcd-key"@gi'</span> calico.yamlsed -i <span class="hljs-string">'s@192.168.0.0/16@10.254.64.0/18@gi'</span> calico.yaml</code></pre><p>执行部署操作，<strong>注意，在开启 RBAC 的情况下需要单独创建 ClusterRole 和 ClusterRoleBinding</strong></p><pre><code class="hljs sh">kubectl create -f calico.yamlkubectl apply -f http://docs.projectcalico.org/v2.3/getting-started/kubernetes/installation/rbac.yaml</code></pre><p>部署完成后如下 </p><p><img src="https://cdn.oss.link/markdown/ocqsf.jpg" srcset="/img/loading.gif" alt="caliaco"></p><p><strong>最后测试一下跨主机通讯</strong></p><pre><code class="hljs sh"><span class="hljs-comment"># 创建 deployment</span>cat &lt;&lt; EOF &gt;&gt; demo.deploy.ymlapiVersion: apps/v1beta1kind: Deploymentmetadata:  name: demo-deploymentspec:  replicas: 3  template:    metadata:      labels:        app: demo    spec:      containers:      - name: demo        image: mritd/demo        ports:        - containerPort: 80EOFkubectl create -f demo.deploy.yml</code></pre><p>exec 到一台主机 pod 内 ping 另一个不同 node 上的 pod 如下</p><p><img src="https://cdn.oss.link/markdown/phkgr.jpg" srcset="/img/loading.gif" alt="ping"></p><h3 id="七、部署-DNS"><a href="#七、部署-DNS" class="headerlink" title="七、部署 DNS"></a>七、部署 DNS</h3><h4 id="7-1、DNS-组件部署"><a href="#7-1、DNS-组件部署" class="headerlink" title="7.1、DNS 组件部署"></a>7.1、DNS 组件部署</h4><p>DNS 部署目前有两种方式，一种是纯手动，另一种是使用 <a href="https://github.com/kubernetes/kubernetes/blob/master/cluster/addons/addon-manager/README.md" target="_blank" rel="noopener">Addon-manager</a>，目前个人感觉 Addon-manager 有点繁琐，所以以下采取纯手动部署 DNS 组件</p><p>DNS 组件相关文件位于 <a href="https://github.com/kubernetes/kubernetes/blob/master/cluster/addons/dns/README.md" target="_blank" rel="noopener">kubernetes addons</a> 目录下，把相关文件下载下来然后稍作修改即可</p><pre><code class="hljs sh"><span class="hljs-comment"># 获取文件</span>mkdir dns &amp;&amp; <span class="hljs-built_in">cd</span> dnswget https://raw.githubusercontent.com/kubernetes/kubernetes/master/cluster/addons/dns/kubedns-cm.yamlwget https://raw.githubusercontent.com/kubernetes/kubernetes/master/cluster/addons/dns/kubedns-sa.yamlwget https://raw.githubusercontent.com/kubernetes/kubernetes/master/cluster/addons/dns/kubedns-svc.yaml.sedwget https://raw.githubusercontent.com/kubernetes/kubernetes/master/cluster/addons/dns/kubedns-controller.yaml.sedmv kubedns-controller.yaml.sed kubedns-controller.yamlmv kubedns-svc.yaml.sed kubedns-svc.yaml<span class="hljs-comment"># 修改配置</span>sed -i <span class="hljs-string">'s/$DNS_DOMAIN/cluster.local/gi'</span> kubedns-controller.yamlsed -i <span class="hljs-string">'s/$DNS_SERVER_IP/10.254.0.2/gi'</span> kubedns-svc.yaml<span class="hljs-comment"># 创建(我把所有 yml 放到的 dns 目录中)</span>kubectl create -f ../dns</code></pre><p><strong>接下来测试 DNS，</strong>测试方法创建两个 deployment 和 svc，通过在 pod 内通过 svc 域名方式访问另一个 deployment 下的 pod，相关测试的 deploy、svc 配置在这里不在展示，基本情况如下图所示</p><p><img src="https://cdn.oss.link/markdown/o94qb.jpg" srcset="/img/loading.gif" alt="deployment"></p><p><img src="https://cdn.oss.link/markdown/586mm.jpg" srcset="/img/loading.gif" alt="test dns"></p><h4 id="7-2、DNS-自动扩容部署"><a href="#7-2、DNS-自动扩容部署" class="headerlink" title="7.2、DNS 自动扩容部署"></a>7.2、DNS 自动扩容部署</h4><p>关于 DNS 自动扩容详细可参考 <a href="https://kubernetes.io/docs/tasks/administer-cluster/dns-horizontal-autoscaling/" target="_blank" rel="noopener">Autoscale the DNS Service in a Cluster</a>，以下直接操作</p><p>首先获取 Dns horizontal autoscaler 配置文件</p><pre><code class="hljs sh">wget https://raw.githubusercontent.com/kubernetes/kubernetes/master/cluster/addons/dns-horizontal-autoscaler/dns-horizontal-autoscaler-rbac.yamlwget https://raw.githubusercontent.com/kubernetes/kubernetes/master/cluster/addons/dns-horizontal-autoscaler/dns-horizontal-autoscaler.yaml</code></pre><p>然后直接 <code>kubectl create -f</code> 即可，<strong>DNS 自动扩容计算公式为 <code>replicas = max( ceil( cores * 1/coresPerReplica ) , ceil( nodes * 1/nodesPerReplica ) )</code>，如果想调整 DNS 数量(负载因子)，只需要调整 ConfigMap 中对应参数即可，具体计算细节参考上面的官方文档</strong></p><pre><code class="hljs sh"><span class="hljs-comment"># 编辑 Config Map</span>kubectl edit cm kube-dns-autoscaler --namespace=kube-system</code></pre>]]></content>
    
    
    <categories>
      
      <category>Kubernetes</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Linux</tag>
      
      <tag>Docker</tag>
      
      <tag>Kubernetes</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Kubernetes RBAC</title>
    <link href="/2017/07/17/kubernetes-rbac-chinese-translation/"/>
    <url>/2017/07/17/kubernetes-rbac-chinese-translation/</url>
    
    <content type="html"><![CDATA[<blockquote><p>基于角色的访问控制使用 <code>rbac.authorization.k8s.io</code> API 组来实现权限控制，RBAC 允许管理员通过 Kubernetes API 动态的配置权限策略。<strong>在 1.6 版本中 RBAC 还处于 Beat 阶段</strong>，如果想要开启 RBAC 授权模式需要在 apiserver 组件中指定 <code>--authorization-mode=RBAC</code> 选项。</p></blockquote><h3 id="一、API-Overview"><a href="#一、API-Overview" class="headerlink" title="一、API Overview"></a>一、API Overview</h3><p>本节介绍了 RBAC 的四个顶级类型，用户可以像与其他 Kubernetes API 资源一样通过 kubectl、API 调用方式与其交互；例如使用 <code>kubectl create -f (resource).yml</code> 命令创建资源对象，跟随本文档操作前最好先阅读引导部分。</p><h4 id="1-1、Role-and-ClusterRole"><a href="#1-1、Role-and-ClusterRole" class="headerlink" title="1.1、Role and ClusterRole"></a>1.1、Role and ClusterRole</h4><p>在 RBAC API 中，Role 表示一组规则权限，权限只会增加(累加权限)，不存在一个资源一开始就有很多权限而通过 RBAC 对其进行减少的操作；Role 可以定义在一个 namespace 中，如果想要跨 namespace 则可以创建 ClusterRole。</p><p><strong>Role 只能用于授予对单个命名空间中的资源访问权限，</strong> 以下是一个对默认命名空间中 Pods 具有访问权限的样例:</p><pre><code class="hljs yml"><span class="hljs-attr">kind:</span> <span class="hljs-string">Role</span><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">rbac.authorization.k8s.io/v1beta1</span><span class="hljs-attr">metadata:</span>  <span class="hljs-attr">namespace:</span> <span class="hljs-string">default</span>  <span class="hljs-attr">name:</span> <span class="hljs-string">pod-reader</span><span class="hljs-attr">rules:</span><span class="hljs-bullet">-</span> <span class="hljs-attr">apiGroups:</span> <span class="hljs-string">[""]</span> <span class="hljs-comment"># "" indicates the core API group</span>  <span class="hljs-attr">resources:</span> <span class="hljs-string">["pods"]</span>  <span class="hljs-attr">verbs:</span> <span class="hljs-string">["get",</span> <span class="hljs-string">"watch"</span><span class="hljs-string">,</span> <span class="hljs-string">"list"</span><span class="hljs-string">]</span></code></pre><p>ClusterRole 具有与 Role 相同的权限角色控制能力，不同的是 ClusterRole 是集群级别的，ClusterRole 可以用于:</p><ul><li>集群级别的资源控制(例如 node 访问权限)</li><li>非资源型 endpoints(例如 <code>/healthz</code> 访问)</li><li>所有命名空间资源控制(例如 pods)</li></ul><p>以下是 ClusterRole 授权某个特定命名空间或全部命名空间(取决于<a href="https://kubernetes.io/docs/admin/authorization/rbac/#rolebinding-and-clusterrolebinding" target="_blank" rel="noopener">绑定方式</a>)访问 secrets 的样例</p><pre><code class="hljs yml"><span class="hljs-attr">kind:</span> <span class="hljs-string">ClusterRole</span><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">rbac.authorization.k8s.io/v1beta1</span><span class="hljs-attr">metadata:</span>  <span class="hljs-comment"># "namespace" omitted since ClusterRoles are not namespaced</span>  <span class="hljs-attr">name:</span> <span class="hljs-string">secret-reader</span><span class="hljs-attr">rules:</span><span class="hljs-bullet">-</span> <span class="hljs-attr">apiGroups:</span> <span class="hljs-string">[""]</span>  <span class="hljs-attr">resources:</span> <span class="hljs-string">["secrets"]</span>  <span class="hljs-attr">verbs:</span> <span class="hljs-string">["get",</span> <span class="hljs-string">"watch"</span><span class="hljs-string">,</span> <span class="hljs-string">"list"</span><span class="hljs-string">]</span></code></pre><h4 id="1-2、RoleBinding-and-ClusterRoleBinding"><a href="#1-2、RoleBinding-and-ClusterRoleBinding" class="headerlink" title="1.2、RoleBinding and ClusterRoleBinding"></a>1.2、RoleBinding and ClusterRoleBinding</h4><p>RoloBinding 可以将角色中定义的权限授予用户或用户组，RoleBinding 包含一组权限列表(subjects)，权限列表中包含有不同形式的待授予权限资源类型(users, groups, or service accounts)；RoloBinding 同样包含对被 Bind 的 Role 引用；RoleBinding 适用于某个命名空间内授权，而 ClusterRoleBinding 适用于集群范围内的授权。</p><p>RoleBinding 可以在同一命名空间中引用对应的 Role，以下 RoleBinding 样例将 default 命名空间的 <code>pod-reader</code> Role 授予 jane 用户，此后 jane 用户在 default 命名空间中将具有 <code>pod-reader</code> 的权限</p><pre><code class="hljs yml"><span class="hljs-comment"># This role binding allows "jane" to read pods in the "default" namespace.</span><span class="hljs-attr">kind:</span> <span class="hljs-string">RoleBinding</span><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">rbac.authorization.k8s.io/v1beta1</span><span class="hljs-attr">metadata:</span>  <span class="hljs-attr">name:</span> <span class="hljs-string">read-pods</span>  <span class="hljs-attr">namespace:</span> <span class="hljs-string">default</span><span class="hljs-attr">subjects:</span><span class="hljs-bullet">-</span> <span class="hljs-attr">kind:</span> <span class="hljs-string">User</span>  <span class="hljs-attr">name:</span> <span class="hljs-string">jane</span>  <span class="hljs-attr">apiGroup:</span> <span class="hljs-string">rbac.authorization.k8s.io</span><span class="hljs-attr">roleRef:</span>  <span class="hljs-attr">kind:</span> <span class="hljs-string">Role</span>  <span class="hljs-attr">name:</span> <span class="hljs-string">pod-reader</span>  <span class="hljs-attr">apiGroup:</span> <span class="hljs-string">rbac.authorization.k8s.io</span></code></pre><p><strong>RoleBinding 同样可以引用 ClusterRole 来对当前 namespace 内用户、用户组或 ServiceAccount 进行授权，这种操作允许集群管理员在整个集群内定义一些通用的 ClusterRole，然后在不同的 namespace 中使用 RoleBinding 来引用</strong></p><p>例如，以下 RoleBinding 引用了一个 ClusterRole，这个 ClusterRole 具有整个集群内对 secrets 的访问权限；但是其授权用户 <code>dave</code> 只能访问 development 空间中的 secrets(因为 RoleBinding 定义在 development 命名空间)</p><pre><code class="hljs yml"><span class="hljs-comment"># This role binding allows "dave" to read secrets in the "development" namespace.</span><span class="hljs-attr">kind:</span> <span class="hljs-string">RoleBinding</span><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">rbac.authorization.k8s.io/v1beta1</span><span class="hljs-attr">metadata:</span>  <span class="hljs-attr">name:</span> <span class="hljs-string">read-secrets</span>  <span class="hljs-attr">namespace:</span> <span class="hljs-string">development</span> <span class="hljs-comment"># This only grants permissions within the "development" namespace.</span><span class="hljs-attr">subjects:</span><span class="hljs-bullet">-</span> <span class="hljs-attr">kind:</span> <span class="hljs-string">User</span>  <span class="hljs-attr">name:</span> <span class="hljs-string">dave</span>  <span class="hljs-attr">apiGroup:</span> <span class="hljs-string">rbac.authorization.k8s.io</span><span class="hljs-attr">roleRef:</span>  <span class="hljs-attr">kind:</span> <span class="hljs-string">ClusterRole</span>  <span class="hljs-attr">name:</span> <span class="hljs-string">secret-reader</span>  <span class="hljs-attr">apiGroup:</span> <span class="hljs-string">rbac.authorization.k8s.io</span></code></pre><p>最后，使用 ClusterRoleBinding 可以对整个集群中的所有命名空间资源权限进行授权；以下 ClusterRoleBinding 样例展示了授权 manager 组内所有用户在全部命名空间中对 secrets 进行访问</p><pre><code class="hljs yml"><span class="hljs-comment"># This cluster role binding allows anyone in the "manager" group to read secrets in any namespace.</span><span class="hljs-attr">kind:</span> <span class="hljs-string">ClusterRoleBinding</span><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">rbac.authorization.k8s.io/v1beta1</span><span class="hljs-attr">metadata:</span>  <span class="hljs-attr">name:</span> <span class="hljs-string">read-secrets-global</span><span class="hljs-attr">subjects:</span><span class="hljs-bullet">-</span> <span class="hljs-attr">kind:</span> <span class="hljs-string">Group</span>  <span class="hljs-attr">name:</span> <span class="hljs-string">manager</span>  <span class="hljs-attr">apiGroup:</span> <span class="hljs-string">rbac.authorization.k8s.io</span><span class="hljs-attr">roleRef:</span>  <span class="hljs-attr">kind:</span> <span class="hljs-string">ClusterRole</span>  <span class="hljs-attr">name:</span> <span class="hljs-string">secret-reader</span>  <span class="hljs-attr">apiGroup:</span> <span class="hljs-string">rbac.authorization.k8s.io</span></code></pre><h4 id="1-3、Referring-to-Resources"><a href="#1-3、Referring-to-Resources" class="headerlink" title="1.3、Referring to Resources"></a>1.3、Referring to Resources</h4><p>Kubernetes 集群内一些资源一般以其名称字符串来表示，这些字符串一般会在 API 的 URL 地址中出现；同时某些资源也会包含子资源，例如 logs 资源就属于 pods 的子资源，API 中 URL 样例如下</p><pre><code class="hljs sh">GET /api/v1/namespaces/&#123;namespace&#125;/pods/&#123;name&#125;/<span class="hljs-built_in">log</span></code></pre><p><strong>如果要在 RBAC 授权模型中控制这些子资源的访问权限，可以通过 <code>/</code> 分隔符来实现</strong>，以下是一个定义 pods 资资源 logs 访问权限的 Role 定义样例</p><pre><code class="hljs yml"><span class="hljs-attr">kind:</span> <span class="hljs-string">Role</span><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">rbac.authorization.k8s.io/v1beta1</span><span class="hljs-attr">metadata:</span>  <span class="hljs-attr">namespace:</span> <span class="hljs-string">default</span>  <span class="hljs-attr">name:</span> <span class="hljs-string">pod-and-pod-logs-reader</span><span class="hljs-attr">rules:</span><span class="hljs-bullet">-</span> <span class="hljs-attr">apiGroups:</span> <span class="hljs-string">[""]</span>  <span class="hljs-attr">resources:</span> <span class="hljs-string">["pods",</span> <span class="hljs-string">"pods/log"</span><span class="hljs-string">]</span>  <span class="hljs-attr">verbs:</span> <span class="hljs-string">["get",</span> <span class="hljs-string">"list"</span><span class="hljs-string">]</span></code></pre><p>具体的资源引用可以通过 <code>resourceNames</code> 来定义，当指定 <code>get</code>、<code>delete</code>、<code>update</code>、<code>patch</code> 四个动词时，可以控制对其目标资源的相应动作；以下为限制一个 subject 对名称为 my-configmap 的 configmap 只能具有 <code>get</code> 和 <code>update</code> 权限的样例</p><pre><code class="hljs yml"><span class="hljs-attr">kind:</span> <span class="hljs-string">Role</span><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">rbac.authorization.k8s.io/v1beta1</span><span class="hljs-attr">metadata:</span>  <span class="hljs-attr">namespace:</span> <span class="hljs-string">default</span>  <span class="hljs-attr">name:</span> <span class="hljs-string">configmap-updater</span><span class="hljs-attr">rules:</span><span class="hljs-bullet">-</span> <span class="hljs-attr">apiGroups:</span> <span class="hljs-string">[""]</span>  <span class="hljs-attr">resources:</span> <span class="hljs-string">["configmap"]</span>  <span class="hljs-attr">resourceNames:</span> <span class="hljs-string">["my-configmap"]</span>  <span class="hljs-attr">verbs:</span> <span class="hljs-string">["update",</span> <span class="hljs-string">"get"</span><span class="hljs-string">]</span></code></pre><p><strong>值得注意的是，当设定了 resourceNames 后，verbs 动词不能指定为 <code>list</code>、<code>watch</code>、<code>create</code> 和 <code>deletecollection</code>；因为这个具体的资源名称不在上面四个动词限定的请求 URL 地址中匹配到，最终会因为 URL 地址不匹配导致 Role 无法创建成功</strong></p><h5 id="1-3-1、Role-Examples"><a href="#1-3-1、Role-Examples" class="headerlink" title="1.3.1、Role Examples"></a>1.3.1、Role Examples</h5><p>以下样例只给出了 role 部分</p><p>在核心 API 组中允许读取 <code>pods</code> 资源</p><pre><code class="hljs yml"><span class="hljs-attr">rules:</span><span class="hljs-bullet">-</span> <span class="hljs-attr">apiGroups:</span> <span class="hljs-string">[""]</span>  <span class="hljs-attr">resources:</span> <span class="hljs-string">["pods"]</span>  <span class="hljs-attr">verbs:</span> <span class="hljs-string">["get",</span> <span class="hljs-string">"list"</span><span class="hljs-string">,</span> <span class="hljs-string">"watch"</span><span class="hljs-string">]</span></code></pre><p>在 <code>extensions</code> 和 <code>apps</code> API 组中允许读取/写入 <code>deployments</code></p><pre><code class="hljs yml"><span class="hljs-attr">rules:</span><span class="hljs-bullet">-</span> <span class="hljs-attr">apiGroups:</span> <span class="hljs-string">["extensions",</span> <span class="hljs-string">"apps"</span><span class="hljs-string">]</span>  <span class="hljs-attr">resources:</span> <span class="hljs-string">["deployments"]</span>  <span class="hljs-attr">verbs:</span> <span class="hljs-string">["get",</span> <span class="hljs-string">"list"</span><span class="hljs-string">,</span> <span class="hljs-string">"watch"</span><span class="hljs-string">,</span> <span class="hljs-string">"create"</span><span class="hljs-string">,</span> <span class="hljs-string">"update"</span><span class="hljs-string">,</span> <span class="hljs-string">"patch"</span><span class="hljs-string">,</span> <span class="hljs-string">"delete"</span><span class="hljs-string">]</span></code></pre><p>允许读取 <code>pods</code> 资源，允许读取/写入 <code>jobs</code> 资源</p><pre><code class="hljs yml"><span class="hljs-attr">rules:</span><span class="hljs-bullet">-</span> <span class="hljs-attr">apiGroups:</span> <span class="hljs-string">[""]</span>  <span class="hljs-attr">resources:</span> <span class="hljs-string">["pods"]</span>  <span class="hljs-attr">verbs:</span> <span class="hljs-string">["get",</span> <span class="hljs-string">"list"</span><span class="hljs-string">,</span> <span class="hljs-string">"watch"</span><span class="hljs-string">]</span><span class="hljs-bullet">-</span> <span class="hljs-attr">apiGroups:</span> <span class="hljs-string">["batch",</span> <span class="hljs-string">"extensions"</span><span class="hljs-string">]</span>  <span class="hljs-attr">resources:</span> <span class="hljs-string">["jobs"]</span>  <span class="hljs-attr">verbs:</span> <span class="hljs-string">["get",</span> <span class="hljs-string">"list"</span><span class="hljs-string">,</span> <span class="hljs-string">"watch"</span><span class="hljs-string">,</span> <span class="hljs-string">"create"</span><span class="hljs-string">,</span> <span class="hljs-string">"update"</span><span class="hljs-string">,</span> <span class="hljs-string">"patch"</span><span class="hljs-string">,</span> <span class="hljs-string">"delete"</span><span class="hljs-string">]</span></code></pre><p>允许读取名称为 <code>my-config</code> 的 ConfigMap(需要与 RoleBinding 绑定来限制某个特定命名空间和指定名字的 ConfigMap)</p><pre><code class="hljs yml"><span class="hljs-attr">rules:</span><span class="hljs-bullet">-</span> <span class="hljs-attr">apiGroups:</span> <span class="hljs-string">[""]</span>  <span class="hljs-attr">resources:</span> <span class="hljs-string">["configmaps"]</span>  <span class="hljs-attr">resourceNames:</span> <span class="hljs-string">["my-config"]</span>  <span class="hljs-attr">verbs:</span> <span class="hljs-string">["get"]</span></code></pre><p>允许在核心组中读取 <code>nodes</code> 资源( Node 是集群范围内的资源，需要使用 ClusterRole 并且与 ClusterRoleBinding 绑定才能进行限制)</p><pre><code class="hljs yml"><span class="hljs-attr">rules:</span><span class="hljs-bullet">-</span> <span class="hljs-attr">apiGroups:</span> <span class="hljs-string">[""]</span>  <span class="hljs-attr">resources:</span> <span class="hljs-string">["nodes"]</span>  <span class="hljs-attr">verbs:</span> <span class="hljs-string">["get",</span> <span class="hljs-string">"list"</span><span class="hljs-string">,</span> <span class="hljs-string">"watch"</span><span class="hljs-string">]</span></code></pre><p>允许对非资源型 endpoint <code>/healthz</code> 和其子路径 <code>/healthz/*</code> 进行 <code>GET</code> 和 <code>POST</code> 请求(同样需要使用 ClusterRole 和 ClusterRoleBinding 才能生效)</p><pre><code class="hljs yml"><span class="hljs-attr">rules:</span><span class="hljs-bullet">-</span> <span class="hljs-attr">nonResourceURLs:</span> <span class="hljs-string">["/healthz",</span> <span class="hljs-string">"/healthz/*"</span><span class="hljs-string">]</span> <span class="hljs-comment"># '*' in a nonResourceURL is a suffix glob match</span>  <span class="hljs-attr">verbs:</span> <span class="hljs-string">["get",</span> <span class="hljs-string">"post"</span><span class="hljs-string">]</span></code></pre><h4 id="1-4、Referring-to-Subjects"><a href="#1-4、Referring-to-Subjects" class="headerlink" title="1.4、Referring to Subjects"></a>1.4、Referring to Subjects</h4><p>RoleBinding 和 ClusterRoleBinding 可以将 Role 绑定到 Subjects；Subjects 可以是 groups、users 或者 service accounts。</p><p>Subjects 中 Users 使用字符串表示，它可以是一个普通的名字字符串，如 “alice”；也可以是 email 格式的邮箱地址，如 “<a href="mailto:bob@example.com">bob@example.com</a>“；甚至是一组字符串形式的数字 ID。Users 的格式必须满足集群管理员配置的<a href="https://kubernetes.io/docs/admin/authentication/" target="_blank" rel="noopener">验证模块</a>，RBAC 授权系统中没有对其做任何格式限定；<strong>但是 Users 的前缀 <code>system:</code> 是系统保留的，集群管理员应该确保普通用户不会使用这个前缀格式</strong></p><p>Kubernetes 的 Group 信息目前由 Authenticator 模块提供，Groups 书写格式与 Users 相同，都为一个字符串，并且没有特定的格式要求；<strong>同样 <code>system:</code> 前缀为系统保留</strong></p><p>具有 <code>system:serviceaccount:</code> 前缀的用户名和 <code>system:serviceaccounts:</code> 前缀的组为 <a href="https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/" target="_blank" rel="noopener">Service Accounts</a></p><h5 id="1-4-1、Role-Binding-Examples"><a href="#1-4-1、Role-Binding-Examples" class="headerlink" title="1.4.1、Role Binding Examples"></a>1.4.1、Role Binding Examples</h5><p>以下示例仅展示 RoleBinding 的 subjects 部分</p><p>指定一个名字为 <code>alice@example.com</code> 的用户</p><pre><code class="hljs yml"><span class="hljs-attr">subjects:</span><span class="hljs-bullet">-</span> <span class="hljs-attr">kind:</span> <span class="hljs-string">User</span>  <span class="hljs-attr">name:</span> <span class="hljs-string">"alice@example.com"</span>  <span class="hljs-attr">apiGroup:</span> <span class="hljs-string">rbac.authorization.k8s.io</span></code></pre><p>指定一个名字为 <code>frontend-admins</code> 的组</p><pre><code class="hljs yml"><span class="hljs-attr">subjects:</span><span class="hljs-bullet">-</span> <span class="hljs-attr">kind:</span> <span class="hljs-string">Group</span>  <span class="hljs-attr">name:</span> <span class="hljs-string">"frontend-admins"</span>  <span class="hljs-attr">apiGroup:</span> <span class="hljs-string">rbac.authorization.k8s.io</span></code></pre><p>指定 kube-system namespace 中默认的 Service Account</p><pre><code class="hljs yml"><span class="hljs-attr">subjects:</span><span class="hljs-bullet">-</span> <span class="hljs-attr">kind:</span> <span class="hljs-string">ServiceAccount</span>  <span class="hljs-attr">name:</span> <span class="hljs-string">default</span>  <span class="hljs-attr">namespace:</span> <span class="hljs-string">kube-system</span></code></pre><p>指定在 qa namespace 中全部的 Service Account</p><pre><code class="hljs yml"><span class="hljs-attr">subjects:</span><span class="hljs-bullet">-</span> <span class="hljs-attr">kind:</span> <span class="hljs-string">Group</span>  <span class="hljs-attr">name:</span> <span class="hljs-string">system:serviceaccounts:qa</span>  <span class="hljs-attr">apiGroup:</span> <span class="hljs-string">rbac.authorization.k8s.io</span></code></pre><p>指定全部 namspace 中的全部 Service Account</p><pre><code class="hljs yml"><span class="hljs-attr">subjects:</span><span class="hljs-bullet">-</span> <span class="hljs-attr">kind:</span> <span class="hljs-string">Group</span>  <span class="hljs-attr">name:</span> <span class="hljs-string">system:serviceaccounts</span>  <span class="hljs-attr">apiGroup:</span> <span class="hljs-string">rbac.authorization.k8s.io</span></code></pre><p>指定全部的 authenticated 用户(1.5+)</p><pre><code class="hljs yml"><span class="hljs-attr">subjects:</span><span class="hljs-bullet">-</span> <span class="hljs-attr">kind:</span> <span class="hljs-string">Group</span>  <span class="hljs-attr">name:</span> <span class="hljs-string">system:authenticated</span>  <span class="hljs-attr">apiGroup:</span> <span class="hljs-string">rbac.authorization.k8s.io</span></code></pre><p>指定全部的 unauthenticated 用户(1.5+)</p><pre><code class="hljs yml"><span class="hljs-attr">subjects:</span><span class="hljs-bullet">-</span> <span class="hljs-attr">kind:</span> <span class="hljs-string">Group</span>  <span class="hljs-attr">name:</span> <span class="hljs-string">system:unauthenticated</span>  <span class="hljs-attr">apiGroup:</span> <span class="hljs-string">rbac.authorization.k8s.io</span></code></pre><p>指定全部用户</p><pre><code class="hljs yml"><span class="hljs-attr">subjects:</span><span class="hljs-bullet">-</span> <span class="hljs-attr">kind:</span> <span class="hljs-string">Group</span>  <span class="hljs-attr">name:</span> <span class="hljs-string">system:authenticated</span>  <span class="hljs-attr">apiGroup:</span> <span class="hljs-string">rbac.authorization.k8s.io</span><span class="hljs-bullet">-</span> <span class="hljs-attr">kind:</span> <span class="hljs-string">Group</span>  <span class="hljs-attr">name:</span> <span class="hljs-string">system:unauthenticated</span>  <span class="hljs-attr">apiGroup:</span> <span class="hljs-string">rbac.authorization.k8s.io</span></code></pre><h3 id="二、Default-Roles-and-Role-Bindings"><a href="#二、Default-Roles-and-Role-Bindings" class="headerlink" title="二、Default Roles and Role Bindings"></a>二、Default Roles and Role Bindings</h3><p>集群创建后 API Server 默认会创建一些 ClusterRole 和 ClusterRoleBinding 对象；这些对象以 <code>system:</code> 为前缀，这表明这些资源对象由集群基础设施拥有；<strong>修改这些集群基础设施拥有的对象可能导致集群不可用。</strong> 一个简单的例子是 <code>system:node</code> ClusterRole，这个 ClusterRole 定义了 kubelet 的相关权限，如果该 ClusterRole 被修改可能导致 ClusterRole 不可用。</p><p><strong>所有的默认 ClusterRole 和 RoleBinding 都具有 <code>kubernetes.io/bootstrapping=rbac-defaults</code> lable</strong></p><h4 id="2-1、Auto-reconciliation"><a href="#2-1、Auto-reconciliation" class="headerlink" title="2.1、Auto-reconciliation"></a>2.1、Auto-reconciliation</h4><p>API Server 在每次启动后都会更新已经丢失的默认 ClusterRole 和 其绑定的相关 Subjects；这将允许集群自动修复因为意外更改导致的 RBAC 授权错误，同时能够使在升级集群后基础设施的 RBAC 授权得以自动更新。</p><p><strong>如果想要关闭 API Server 的自动修复功能，只需要将默认创建的 ClusterRole 和其 RoleBind 的 <code>rbac.authorization.kubernetes.io/autoupdate</code> 注解设置为 false 即可，这样做会有很大风险导致集群因为意外修改 RBAC 而无法工作</strong></p><p><strong>Auto-reconciliation 在 1.6+ 版本被默认启用(当 RBAC 授权被激活时)</strong></p><h4 id="2-2、Discovery-Roles"><a href="#2-2、Discovery-Roles" class="headerlink" title="2.2、Discovery Roles"></a>2.2、Discovery Roles</h4><table><thead><tr><th>Default ClusterRole</th><th>Default ClusterRoleBinding</th><th>Description</th></tr></thead><tbody><tr><td>system:basic-user</td><td>system:authenticated and system:unauthenticated groups</td><td>允许用户以只读的方式读取其基础信息</td></tr><tr><td>system:discovery</td><td>system:authenticated and system:unauthenticated groups</td><td>允许以只读的形式访问 发现和协商 API Level 所需的 API discovery endpoints</td></tr></tbody></table><h4 id="2-3、User-facing-Roles"><a href="#2-3、User-facing-Roles" class="headerlink" title="2.3、User-facing Roles"></a>2.3、User-facing Roles</h4><p>一些默认的 Role 并未以 <code>system:</code> 前缀开头，这表明这些默认的 Role 是面向用户级别的。这其中包括超级用户的一些 Role( <code>cluster-admin</code> )，和为面向集群范围授权的 RoleBinding( <code>cluster-status</code> )，以及在特定命名空间中授权的 RoleBinding( <code>admin</code>，<code>edit</code>，<code>view</code> )</p><table><thead><tr><th>Default ClusterRole</th><th>Default ClusterRoleBinding</th><th>Description</th></tr></thead><tbody><tr><td>cluster-admin</td><td>system:masters group</td><td>允许超级用户对集群内任意资源执行任何动作。当该 Role 绑定到 ClusterRoleBinding 时，将授予目标 subject 在任意 namespace 内对任何 resource 执行任何动作的权限；当绑定到 RoleBinding 时，将授予目标 subject 在当前 namespace 内对任意 resource 执行任何动作的权限，当然也包括 namespace 自己</td></tr><tr><td>admin</td><td>None</td><td>管理员权限，用于在单个 namespace 内授权；在与某个 RoleBinding 绑定后提供在单个 namesapce 中对资源的读写权限，包括在单个 namesapce 内创建 Role 和进行 RoleBinding 的权限。<strong>该 ClusterRole 不允许对资源配额和 namespace 本身进行修改</strong></td></tr><tr><td>edit</td><td>None</td><td>允许读写指定 namespace 中的大多数资源对象；<strong>该 ClusterRole 不允许查看或修改 Role 和 RoleBinding</strong></td></tr><tr><td>view</td><td>None</td><td>允许以只读方式访问特定 namespace 中的大多数资源对象；<strong>该 ClusterRole 不允许查看 Role 或 RoleBinding，同时不允许查看 secrets，因为他们会不断更新</strong></td></tr></tbody></table><h4 id="2-4、Core-Component-Roles"><a href="#2-4、Core-Component-Roles" class="headerlink" title="2.4、Core Component Roles"></a>2.4、Core Component Roles</h4><table><thead><tr><th>Default ClusterRole</th><th>Default ClusterRoleBinding</th><th>Description</th></tr></thead><tbody><tr><td>system:kube-scheduler</td><td>system:kube-scheduler user</td><td>允许访问 kube-scheduler 所需资源</td></tr><tr><td>system:kube-controller-manager</td><td>system:kube-controller-manager user</td><td>允许访问 kube-controller-manager 所需资源；<a href="https://kubernetes.io/docs/admin/authorization/rbac/#controller-roles" target="_blank" rel="noopener">该 ClusterRole</a> 包含每个控制循环所需要的权限</td></tr><tr><td>system:node</td><td>system:nodes group (deprecated in 1.7)</td><td>允许访问 kubelet 所需资源；包括对所有的 secrets 读访问权限和对所有 pod 的写权限；在 1.7 中更推荐使用 <a href="/docs/admin/authorization/node/">Node authorizer</a> 和 <a href="/docs/admin/admission-controllers#NodeRestriction">NodeRestriction admission plugin</a> 而非本 ClusterRole；Node authorizer 和 NodeRestriction admission plugin 可以授权当前 node 上运行的具体 pod 对 kubelet API 的访问权限，<strong>在 1.7 版本中，如果开启了 <code>Node authorization mode</code>，那么 <code>system:nodes</code> group将不会被创建和自动绑定</strong></td></tr><tr><td>system:node-proxier</td><td>system:kube-proxy user</td><td>允许访问 kube-proxy 所需资源</td></tr></tbody></table><h4 id="2-5、Other-Component-Roles"><a href="#2-5、Other-Component-Roles" class="headerlink" title="2.5、Other Component Roles"></a>2.5、Other Component Roles</h4><table><thead><tr><th>Default ClusterRole</th><th>Default ClusterRoleBinding</th><th>Description</th></tr></thead><tbody><tr><td>system:auth-delegator</td><td>None</td><td>允许委托认证和授权检查；此情况下通常由附加的 API Server 来进行统一认证和授权</td></tr><tr><td>system:heapster</td><td>None</td><td><a href="https://github.com/kubernetes/heapster" target="_blank" rel="noopener">Heapster</a> 组件相关权限</td></tr><tr><td>system:kube-aggregator</td><td>None</td><td><a href="https://github.com/kubernetes/kube-aggregator" target="_blank" rel="noopener">kube-aggregator</a> 相关权限</td></tr><tr><td>system:kube-dns</td><td>kube-dns service account in the kube-system namespace</td><td><a href="https://kubernetes.io/docs/admin/dns/" target="_blank" rel="noopener">kube-dns</a> 相关权限</td></tr><tr><td>system:node-bootstrapper</td><td>None</td><td>允许访问 <a href="https://kubernetes.io/docs/admin/kubelet-tls-bootstrapping/" target="_blank" rel="noopener">Kubelet TLS bootstrapping</a> 相关资源权限</td></tr><tr><td>system:node-problem-detector</td><td>None</td><td><a href="https://github.com/kubernetes/node-problem-detector" target="_blank" rel="noopener">node-problem-detector</a> 相关权限</td></tr><tr><td>system:persistent-volume-provisioner</td><td>Node</td><td>允许访问 <a href="https://kubernetes.io/docs/user-guide/persistent-volumes/#provisioner" target="_blank" rel="noopener">dynamic volume provisioners</a> 相关资源权限</td></tr></tbody></table><h4 id="2-6、Controller-Roles"><a href="#2-6、Controller-Roles" class="headerlink" title="2.6、Controller Roles"></a>2.6、Controller Roles</h4><p><a href="https://kubernetes.io/docs/admin/kube-controller-manager/" target="_blank" rel="noopener">Kubernetes controller manager</a> 运行着一些核心的 <code>control loops</code>，<strong>当使用 <code>--use-service-account-credentials</code> 参数启动时，每个 <code>control loop</code> 都会使用独立的 <code>Service Account</code> 启动；相应的 roles 会以 <code>system:controller</code> 前缀存在于每个 control loop 中；如果不指定该选项，那么 Kubernetes controller manager 将会使用自己的凭据来运行所有 <code>control loops</code>，此时必须保证 RBAC 授权模型中授予了其所有相关 Role，如下:</strong></p><ul><li>system:controller:attachdetach-controller</li><li>system:controller:certificate-controller</li><li>system:controller:cronjob-controller</li><li>system:controller:daemon-set-controller</li><li>system:controller:deployment-controller</li><li>system:controller:disruption-controller</li><li>system:controller:endpoint-controller</li><li>system:controller:generic-garbage-collector</li><li>system:controller:horizontal-pod-autoscaler</li><li>system:controller:job-controller</li><li>system:controller:namespace-controller</li><li>system:controller:node-controller</li><li>system:controller:persistent-volume-binder</li><li>system:controller:pod-garbage-collector</li><li>system:controller:replicaset-controller</li><li>system:controller:replication-controller</li><li>system:controller:resourcequota-controller</li><li>system:controller:route-controller</li><li>system:controller:service-account-controller</li><li>system:controller:service-controller</li><li>system:controller:statefulset-controller</li><li>system:controller:ttl-controller</li></ul><h3 id="三、Privilege-Escalation-Prevention-and-Bootstrapping"><a href="#三、Privilege-Escalation-Prevention-and-Bootstrapping" class="headerlink" title="三、Privilege Escalation Prevention and Bootstrapping"></a>三、Privilege Escalation Prevention and Bootstrapping</h3><p>RBAC API 会通过阻止用户编辑 Role 或 RoleBinding 来进行特权升级，RBAC 在 API 级别实现了这一机制，所以即使 RBAC authorizer 不被使用也适用。</p><p><strong>用户即使在对某个 Role 拥有全部权限的情况下也仅能在其作用范围内(ClusterRole -&gt; 集群范围内，Role -&gt; 当前 namespace 或 集群范围)对其进行 create 和 update 操作；</strong> 例如 “user-1” 用户不具有在集群范围内列出 secrets 的权限，那么他也无法在集群范围内创建具有该权限的 ClusterRole，也就是说想传递权限必须先获得该权限；想要允许用户 cretae/update Role 有两种方式:</p><ul><li>1、授予一个该用户期望 create/update 的 Role 或者 ClusterRole</li><li>2、授予一个包含该用户期望 create/update 的 Role 或者 ClusterRole 的 Role 或者 ClusterRole(有点绕…)；如果用户尝试 crate/update 一个其不拥有的 Role 或者 ClusterRole，则 API 会禁止</li></ul><p><strong>用户只有拥有了一个 RoleBind 引用的 Role 全部权限，或者被显示授予了对其具有 bind 的权限下，才能在其作用范围(范围同上)内对其进行 create/update 操作；</strong> 例如 “user-1” 在不具有列出集群内 secrets 权限的情况下，也不可能为具有该权限的 Role 创建 ClusterRoleBinding；如果想要用户具有 create/update ClusterRoleBinding 的权限有以下两种方式:</p><ul><li>1、授予一个该用户期望 create/update 的 RoleBinding 或者 ClusterRoleBinding 的 Role 或 ClusterRole 的 Role 或 ClusterRole(汉语专8)</li><li>2、通过其他方式授予一个该用户 期望 create/update 的 RoleBinding 或者 ClusterRoleBinding 的权限:<ul><li>2.1、授予一个包含用户期望 create/update 的 RoleBinding 或者 ClusterRoleBinding 的 Role 或 ClusterRole 的 Role 或 ClusterRole(我汉语10级)</li><li>2.2、明确的授予用户一个在对特定 Role 或 ClusterRole 进行 bind 的权限</li></ul></li></ul><p>以下样例中，ClusterRole 和 RoleBinding 将允许 “user-1” 用户具有授予其他用户在 “user-1-namespace” namespace 下具有 admin、edit 和 view roles 的权限</p><pre><code class="hljs yml"><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">rbac.authorization.k8s.io/v1beta1</span><span class="hljs-attr">kind:</span> <span class="hljs-string">ClusterRole</span><span class="hljs-attr">metadata:</span>  <span class="hljs-attr">name:</span> <span class="hljs-string">role-grantor</span><span class="hljs-attr">rules:</span><span class="hljs-bullet">-</span> <span class="hljs-attr">apiGroups:</span> <span class="hljs-string">["rbac.authorization.k8s.io"]</span>  <span class="hljs-attr">resources:</span> <span class="hljs-string">["rolebindings"]</span>  <span class="hljs-attr">verbs:</span> <span class="hljs-string">["create"]</span><span class="hljs-bullet">-</span> <span class="hljs-attr">apiGroups:</span> <span class="hljs-string">["rbac.authorization.k8s.io"]</span>  <span class="hljs-attr">resources:</span> <span class="hljs-string">["clusterroles"]</span>  <span class="hljs-attr">verbs:</span> <span class="hljs-string">["bind"]</span>  <span class="hljs-attr">resourceNames:</span> <span class="hljs-string">["admin","edit","view"]</span><span class="hljs-meta">---</span><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">rbac.authorization.k8s.io/v1beta1</span><span class="hljs-attr">kind:</span> <span class="hljs-string">RoleBinding</span><span class="hljs-attr">metadata:</span>  <span class="hljs-attr">name:</span> <span class="hljs-string">role-grantor-binding</span>  <span class="hljs-attr">namespace:</span> <span class="hljs-string">user-1-namespace</span><span class="hljs-attr">roleRef:</span>  <span class="hljs-attr">apiGroup:</span> <span class="hljs-string">rbac.authorization.k8s.io</span>  <span class="hljs-attr">kind:</span> <span class="hljs-string">ClusterRole</span>  <span class="hljs-attr">name:</span> <span class="hljs-string">role-grantor</span><span class="hljs-attr">subjects:</span><span class="hljs-bullet">-</span> <span class="hljs-attr">apiGroup:</span> <span class="hljs-string">rbac.authorization.k8s.io</span>  <span class="hljs-attr">kind:</span> <span class="hljs-string">User</span>  <span class="hljs-attr">name:</span> <span class="hljs-string">user-1</span></code></pre><p>当使用 bootstrapping 时，初始用户尚没有访问 API 的权限，此时想要授予他们一些尚未拥有的权限是不可能的，此时可以有两种解决方案:</p><ul><li>1、通过使用系统级的 <code>system:masters</code> 组从而通过默认绑定绑定到 <code>cluster-admin</code> 超级用户，这样就可以直接沟通 API Server</li><li>2、如果 API Server 开启了 <code>--insecure-port</code> 端口，那么可以通过此端口调用完成第一次授权动作</li></ul><h3 id="四、Command-line-Utilities"><a href="#四、Command-line-Utilities" class="headerlink" title="四、Command-line Utilities"></a>四、Command-line Utilities</h3><p>通过两个 <code>kubectl</code> 的子命令完成在特定命名空间或集群内的授权管理</p><h4 id="4-1、kubectl-create-rolebinding"><a href="#4-1、kubectl-create-rolebinding" class="headerlink" title="4.1、kubectl create rolebinding"></a>4.1、kubectl create rolebinding</h4><p>在特定 namespae 中创建 Role 或者 ClusterRole 的 RoleBinding 样例</p><p><strong>在 acme namespace 中授权用户 bob 具有 admin ClusterRole 的 RoleBinding</strong></p><pre><code class="hljs sh">kubectl create rolebinding bob-admin-binding --clusterrole=admin --user=bob --namespace=acme</code></pre><p><strong>在 acme namespace 中授权名称为 acme:myapp 的 service account 具有 view ClusterRole 的 RoleBinding</strong></p><pre><code class="hljs sh">kubectl create rolebinding myapp-view-binding --clusterrole=view --serviceaccount=acme:myapp --namespace=acme</code></pre><h4 id="4-2、kubectl-create-clusterrolebinding"><a href="#4-2、kubectl-create-clusterrolebinding" class="headerlink" title="4.2、kubectl create clusterrolebinding"></a>4.2、kubectl create clusterrolebinding</h4><p>在全部命名空间中创建 Role 或者 ClusterRole 的 ClusterRoleBinding 样例</p><p><strong>在整个集群内授权 “root” 用户具有 cluster-admin ClusterRole 的 ClusterRoleBinding</strong></p><pre><code class="hljs sh">kubectl create clusterrolebinding root-cluster-admin-binding --clusterrole=cluster-admin --user=root</code></pre><p><strong>在整个集群内授权 “kubelet” 用户具有 system:node ClusterRole 的 ClusterRoleBinding</strong></p><pre><code class="hljs sh">kubectl create clusterrolebinding kubelet-node-binding --clusterrole=system:node --user=kubelet</code></pre><p><strong>在 “acme” 命名空间中授权名称为 acme:myapp 的 service account 具有 view ClusterRole 的 ClusterRoleBinding</strong></p><pre><code class="hljs sh">kubectl create clusterrolebinding myapp-view-binding --clusterrole=view --serviceaccount=acme:myapp</code></pre><p>更详细使用请参考命令行帮助文档</p><h3 id="五、Service-Account-Permissions"><a href="#五、Service-Account-Permissions" class="headerlink" title="五、Service Account Permissions"></a>五、Service Account Permissions</h3><p>默认的 RBAC 权限策略仅向 control-plane 组件、nodes 和 controllers 进行授权，不包括 <code>kube-system</code> namespace 以外的 Service Account 进行授权(除了向已经被验证过的用户授予的 discovery 权限之外)</p><p>这允许你根据需要向特定的服务账户授予特定的权限；细粒度的权限角色绑定控制会更加安全，但是需要更大的精力来进行权限管理；更加宽松的权限角色绑定控制也许会给一些用户分配其不需要的权限，但是相对来说管理相对更加宽松</p><p>从最安全到最不安全的权限管理如下:</p><h4 id="5-1、为特定应用程序指定的服务账户授予特定的-Role-最佳实践"><a href="#5-1、为特定应用程序指定的服务账户授予特定的-Role-最佳实践" class="headerlink" title="5.1、为特定应用程序指定的服务账户授予特定的 Role(最佳实践)"></a>5.1、为特定应用程序指定的服务账户授予特定的 Role(最佳实践)</h4><p><strong>这种方式需要应用在 spec 中设置 serviceAccountName，同时这个 SserviceAccount 必须已经被创建(可以通过 API、manifest 文件或者 通过命令 <code>kubectl create serviceaccount</code> 等)</strong>。例如在 “my-namespace” namespace 下授予 “my-sa” ServiceAccount view ClusterRole 如下:</p><pre><code class="hljs sh">kubectl create rolebinding my-sa-view \  --clusterrole=view \  --serviceaccount=my-namespace:my-sa \  --namespace=my-namespace</code></pre><h4 id="5-2、为特定应用程序默认的服务账户授予特定的-Role"><a href="#5-2、为特定应用程序默认的服务账户授予特定的-Role" class="headerlink" title="5.2、为特定应用程序默认的服务账户授予特定的 Role"></a>5.2、为特定应用程序默认的服务账户授予特定的 Role</h4><p><strong>如果应用程序在 spec 中没有设置 serviceAccountName，那么将会使用 “default” ServiceAccount。</strong></p><p><strong>注意: 如果对 default ServiceAccount 进行 RoleBinding(授权)，那么在当前命名空间内所有没有指定 serviceAccountName 的 pod 都将获得该权限。</strong> 例如在 “my-namespace” namespace 下授予 “default” ServiceAccount view ClusterRole 如下:</p><pre><code class="hljs sh">kubectl create rolebinding default-view \  --clusterrole=view \  --serviceaccount=my-namespace:default \  --namespace=my-namespace</code></pre><p>目前大多数 <a href="https://kubernetes.io/docs/concepts/cluster-administration/addons/" target="_blank" rel="noopener">add-ons</a> 运行在 “kube-system” namespace 的 “default” ServiceAccount 下，如果想要 add-ons 使用超级用户的权限只需要对 “kube-system” namespace 下的 “default” ServiceAccount 授予超级用户权限即可，<strong>需要注意的是超级用户对 API secrets 具有读写权限，这将导致所有 add-ons 组件具有该权限</strong></p><pre><code class="hljs sh">kubectl create clusterrolebinding add-on-cluster-admin \  --clusterrole=cluster-admin \  --serviceaccount=kube-system:default</code></pre><h4 id="5-3、为特定命名空间的所有服务账户授权"><a href="#5-3、为特定命名空间的所有服务账户授权" class="headerlink" title="5.3、为特定命名空间的所有服务账户授权"></a>5.3、为特定命名空间的所有服务账户授权</h4><p>如果希望 namespace 中所有应用程序(无论属于哪个 ServiceAccount)都具有某一个 Role，则可以通过将该 Role 授予该 namespace 的 ServiceAccount 组来实现；例如授予 “my-namespace” namespace 下所有 ServiceAccount view ClusterRole 如下:</p><pre><code class="hljs sh">kubectl create rolebinding serviceaccounts-view \  --clusterrole=view \  --group=system:serviceaccounts:my-namespace \  --namespace=my-namespace</code></pre><h4 id="5-4、为集群范围内所有服务账户授权-不建议"><a href="#5-4、为集群范围内所有服务账户授权-不建议" class="headerlink" title="5.4、为集群范围内所有服务账户授权(不建议)"></a>5.4、为集群范围内所有服务账户授权(不建议)</h4><p>如果你懒得管理每个 namespace 的权限，那么可以将授权扩散到整个集群，将权限授予集群内每个 ServiceAccount；例如授予全部 namespace 中所有 ServiceAccount view ClusterRole:</p><pre><code class="hljs sh">kubectl create clusterrolebinding serviceaccounts-view \  --clusterrole=view \  --group=system:serviceaccounts</code></pre><h4 id="5-5、为集群范围内所有服务账户授予超级用户权限-no-zuo-no-die"><a href="#5-5、为集群范围内所有服务账户授予超级用户权限-no-zuo-no-die" class="headerlink" title="5.5、为集群范围内所有服务账户授予超级用户权限(no zuo no die)"></a>5.5、为集群范围内所有服务账户授予超级用户权限(no zuo no die)</h4><p>如果你根本不关心权限分配，那么可以向集群内所有 namespace 下所有 ServiceAccount 授予超级用户权限；<strong>注意: 这将允许具有读取权限的用户创建一个容器从而间接读取到超级用户凭据</strong></p><pre><code class="hljs sh">kubectl create clusterrolebinding serviceaccounts-cluster-admin \  --clusterrole=cluster-admin \  --group=system:serviceaccounts</code></pre><h3 id="六、Upgrading-from-1-5"><a href="#六、Upgrading-from-1-5" class="headerlink" title="六、Upgrading from 1.5"></a>六、Upgrading from 1.5</h3><p>在 Kubernetes 1.6 版本之前，许多部署使用了非常宽泛的 ABAC 授权策略，包括授予对所有服务帐户的完整API访问权限；默认的 RBAC 权限策略仅向 control-plane 组件、nodes 和 controllers 进行授权，不包括 <code>kube-system</code> namespace 以外的 Service Account 进行授权(除了向已经被验证过的用户授予的 discovery 权限之外)</p><p>这种方式虽然安全性更高，但是 RBAC 授权方式可能影响到已经存在的期望自动获得 API 权限的 workloads，以下有两种解决方案:</p><h4 id="6-1、Parallel-Authorizers"><a href="#6-1、Parallel-Authorizers" class="headerlink" title="6.1、Parallel Authorizers"></a>6.1、Parallel Authorizers</h4><p>并行授权策略允许同时运行 RBAC 和 ABAC，并且包含旧的 ABAC 授权策略</p><pre><code class="hljs sh">--authorization-mode=RBAC,ABAC --authorization-policy-file=mypolicy.jsonl</code></pre><p><strong>此时 RBAC 授权控制器将首先处理授权，如果请求被拒绝则转交给 ABAC 授权控制器处理；这种授权方式将会允许 RBAC 和 ABAC 同时处理授权请求，只要目标 Subjects 在 RBAC 或 ABAC 中任意一个授权器授权成功即可</strong></p><p>当日志级别设置为 2(–v=2) 或者更高时，可以在 API Server 日志中看到 RBAC 拒绝的日志(以 <code>RBAC DENY:</code> 开头)，你可以通过日志中该信息来确定哪些 Role 应该授予哪些 Subjects。一旦完成所有的授权处理，并且在日志中没有再出现 RBAC 授权拒绝的日志时，就可以删除掉 ABAC 授权</p><h4 id="6-2、Permissive-RBAC-Permissions"><a href="#6-2、Permissive-RBAC-Permissions" class="headerlink" title="6.2、Permissive RBAC Permissions"></a>6.2、Permissive RBAC Permissions</h4><p>您可以使用 RBAC RoleBinding 来复制一个允许的策略。</p><p><strong>注意: 以下策略允许所有服务帐户充当集群管理员。在容器中运行的任何应用程序都会自动接收服务帐户凭据，并可以针对 API 执行任何操作，包括查看和修改 secrets 权限；所以这种方法并不推荐。</strong></p><pre><code class="hljs sh">kubectl create clusterrolebinding permissive-binding \  --clusterrole=cluster-admin \  --user=admin \  --user=kubelet \  --group=system:serviceaccounts</code></pre>]]></content>
    
    
    <categories>
      
      <category>Kubernetes</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Linux</tag>
      
      <tag>Docker</tag>
      
      <tag>Kubernetes</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>How to build Kubernetes RPM</title>
    <link href="/2017/07/12/how-to-build-kubernetes-rpm/"/>
    <url>/2017/07/12/how-to-build-kubernetes-rpm/</url>
    
    <content type="html"><![CDATA[<blockquote><p>一直使用 Centos 运行 Kubernetes,有些时候基于二进制部署的情况下,手动复制二进制文件和创建 Systemd service 配置略显繁琐;最近找了一下 Kubernetes RPM 的 build 方式,以下记录一下 build 过程</p></blockquote><h3 id="一、RPM-build-方式选择"><a href="#一、RPM-build-方式选择" class="headerlink" title="一、RPM build 方式选择"></a>一、RPM build 方式选择</h3><p>目前我所知道的 build kubernetes RPM 的方式(测试过)总共 3 种,大致分为 2 类</p><ul><li>基于源码 build</li><li>基于已有 rpm 替换</li></ul><p>第一种方案的好处就是配置文件等能始终保持最新的,编译版本等不受限制;但是从源码 build 非常耗时,尤其是网络环境复杂的情况下,没有高配置国外服务器很难完成 build,而且要维护 build 所需 spec 文件等,自己维护这些未必能够尽善尽美;</p><p>第二种方式是创建速度快,build 方式简单可靠,但是由于是替换方式,所以 rpm 中的配置不一定能够即使更新,而且只能基于官方build 好以后的二进制文件进行替换,如果想要尝试 master 最新代码则无法实现</p><h3 id="二、基于源码-Build"><a href="#二、基于源码-Build" class="headerlink" title="二、基于源码 Build"></a>二、基于源码 Build</h3><p>对于 Centos RPM build 原理方式这里不再细说，基于源码 build 的关键就在于 spec 文件，我尝试过自己去写，后来对比一些开源项目的感觉 low 得很，所以以前一直采用一个国外哥们写的脚本 build(参见 <a href="https://github.com/mritd/kubernetes-rpm-builder" target="_blank" rel="noopener">这里</a>)；这个脚本不太好的地方是作者已经停止了维护；经过不懈努力，找到了 Fedora 系统的 rpm 仓库，鼓捣了一阵摸清了套路；以下主要以 Fedora 仓库为例进行 build</p><p><strong>以下 Build 在一台 Do 8核心 16G VPS 上进行，由于众所周知的原因，国内 Build 很费劲，一般国外 VPS 都是按小时收费，有个 2 块钱就够了</strong></p><h4 id="2-1、安装-build-所需依赖"><a href="#2-1、安装-build-所需依赖" class="headerlink" title="2.1、安装 build 所需依赖"></a>2.1、安装 build 所需依赖</h4><p><strong>由于 spec 文件中定义了依赖于 golang 这个包，所以如果不装的话会报错；事实上如果使用刚刚安装的这个 golang 去 build 还是会挂掉，因为实际编译要求 golang &gt; 1.7，直接 yum 装的是 1.6，故下面又使用 gvm 装了一个 1.8 的 golang，上面的 golang 安装只是为了通过 spec 检查</strong></p><pre><code class="hljs sh"><span class="hljs-comment"># EPEL</span>yum install epel-release -y<span class="hljs-comment"># update 系统组件</span>yum update -y &amp;&amp; yum upgrade -y<span class="hljs-comment"># 安装基本的编译依赖</span>yum install golang go-md2man go-bindata gcc bison git rpm-build vim -y<span class="hljs-comment"># 安装 gvm(用于 golang 版本管理)</span>bash &lt; &lt;(curl -s -S -L https://raw.githubusercontent.com/moovweb/gvm/master/binscripts/gvm-installer)<span class="hljs-built_in">source</span> /root/.gvm/scripts/gvm<span class="hljs-comment"># 安装 1.8 之前需要先安装 1.4</span>gvm install go1.4 -Bgvm use go1.4<span class="hljs-comment"># 使用 golang 1.8 版本 build</span>gvm install go1.8gvm use go1.8</code></pre><h4 id="2-2、克隆-build-仓库"><a href="#2-2、克隆-build-仓库" class="headerlink" title="2.2、克隆 build 仓库"></a>2.2、克隆 build 仓库</h4><p><strong>Fedora 官方 Kubernetes 仓库地址在 <a href="https://src.fedoraproject.org/cgit/rpms/kubernetes.git/" target="_blank" rel="noopener">这里</a>，如果有版本选择请自行区分</strong></p><pre><code class="hljs sh">git <span class="hljs-built_in">clone</span> https://src.fedoraproject.org/git/rpms/kubernetes.git</code></pre><h4 id="2-3、从-spec-获取所需文件"><a href="#2-3、从-spec-获取所需文件" class="headerlink" title="2.3、从 spec 获取所需文件"></a>2.3、从 spec 获取所需文件</h4><p>克隆好 build 仓库后首先查看 kubernetes.spec 文件，确定 build 所需文件，spec 文件如下</p><pre><code class="hljs sh"><span class="hljs-comment"># 省略...</span>%global provider                github%global provider_tld            com%global project                 kubernetes%global repo                    kubernetes<span class="hljs-comment"># https://github.com/kubernetes/kubernetes</span>%global provider_prefix         %&#123;provider&#125;.%&#123;provider_tld&#125;/%&#123;project&#125;/%&#123;repo&#125;%global import_path             k8s.io/kubernetes%global commit                  095136c3078ccf887b9034b7ce598a0a1faff769%global shortcommit              %(c=%&#123;commit&#125;; <span class="hljs-built_in">echo</span> <span class="hljs-variable">$&#123;c:0:7&#125;</span>)%global con_provider            github%global con_provider_tld        com%global con_project             kubernetes%global con_repo                contrib<span class="hljs-comment"># https://github.com/kubernetes/contrib</span>%global con_provider_prefix     %&#123;con_provider&#125;.%&#123;con_provider_tld&#125;/%&#123;con_project&#125;/%&#123;con_repo&#125;%global con_commit              0f5b210313371ff769da24d8264f5a7869c5a3f3%global con_shortcommit         %(c=%&#123;con_commit&#125;; <span class="hljs-built_in">echo</span> <span class="hljs-variable">$&#123;c:0:7&#125;</span>)%global kube_version            1.6.7%global kube_git_version        v%&#123;kube_version&#125;<span class="hljs-comment"># 省略...</span></code></pre><p><strong>从 spec 文件中可以看到 build 主要需要两个仓库的源码，一个是 kubernetes 主仓库，存放着主要的 build 源码；另一个是 contrib 仓库，存放着一些配置文件，如 systemd 配置等</strong></p><p><strong>接下来从 spec 文件的 source 段中可以解读到(source0、source1)最终所需的两个仓库压缩文件名为 <code>kubernetes-SHORTCOMMIT</code>、<code>contrib-SHORTCOMIT</code>，source 段如下</strong></p><pre><code class="hljs sh">Name:           kubernetesVersion:        %&#123;kube_version&#125;Release:        1%&#123;?dist&#125;Summary:        Container cluster managementLicense:        ASL 2.0URL:            https://%&#123;import_path&#125;ExclusiveArch:  x86_64 aarch64 ppc64le s390xSource0:        https://%&#123;provider_prefix&#125;/archive/%&#123;commit&#125;/%&#123;repo&#125;-%&#123;shortcommit&#125;.tar.gzSource1:        https://%&#123;con_provider_prefix&#125;/archive/%&#123;con_commit&#125;/%&#123;con_repo&#125;-%&#123;con_shortcommit&#125;.tar.gzSource3:        kubernetes-accounting.confSource4:        kubeadm.confSource33:       genmanpages.sh</code></pre><p><strong>我们准备 build 一个最新的 1.7.0 的 rpm，所以从 github 获取到 commitID 为 <code>d3ada0119e776222f11ec7945e6d860061339aad</code>，contrib 仓库同理，不过 contrib 一般直接取 master 即可 <code>7d344989fe6a3f11a6d84104b024a50960b021db</code>；接下来首要任务是替换 spec 中原有的 版本号和 commitID 如下</strong></p><pre><code class="hljs sh">%global kube_version            1.7.0%global con_commit              7d344989fe6a3f11a6d84104b024a50960b021db%global commit                  d3ada0119e776222f11ec7945e6d860061339aad</code></pre><h4 id="2-4、准备源码"><a href="#2-4、准备源码" class="headerlink" title="2.4、准备源码"></a>2.4、准备源码</h4><p>修改好文件以后，就可以下载源码文件了，源码下载不必去克隆 github 项目，直接从 spec 中给出的地址下载即可</p><pre><code class="hljs sh"><span class="hljs-built_in">cd</span> kuberneteswget https://github.com/kubernetes/kubernetes/archive/d3ada0119e776222f11ec7945e6d860061339aad/kubernetes-d3ada01.tar.gzwget https://github.com/kubernetes/contrib/archive/7d344989fe6a3f11a6d84104b024a50960b021db/contrib-7d34498.tar.gz</code></pre><h4 id="2-5、build-rpm"><a href="#2-5、build-rpm" class="headerlink" title="2.5、build rpm"></a>2.5、build rpm</h4><p>在正式开始 build 之前，还有一点需要注意的是 <strong>默认的 <code>kubernetes.spec</code> 文件中指定了该 rpm 依赖于 docker 这个包，在 CentOS 上可能我们会安装 docker-engine 或者 docker-ce，此时安装 kubernetes rpm 是无法安装的，因为他以来的包不存在，解决的办法就是编译之前删除 spec 文件中的 <code>Requires: docker</code> 即可</strong>，最后创建好 build 目录，并放置好源码文件开始 build 即可，当然 build 可以有不同选择</p><pre><code class="hljs sh"><span class="hljs-comment"># 由于我是 root 用户，所以目录位置在这</span><span class="hljs-comment"># 实际生产 强烈不推荐使用 root build(操作失误会损毁宿主机)</span><span class="hljs-comment"># 我的是一台临时 vps，所以无所谓了</span>mkdir -p /root/rpmbuild/SOURCES/mv ~/kubernetes/* /root/rpmbuild/SOURCES/<span class="hljs-built_in">cd</span> /root/rpmbuild/SOURCES/<span class="hljs-comment"># 执行 build</span>rpmbuild -ba kubernetes.spec</code></pre><p><strong>注意，由于我们选择的版本已经超出了仓库所支持的最大版本，所以有些 Patch 已经不再适用，如 spec 中的 <code>Patch12</code>、<code>Patch19</code> 会出错，所需要注释掉(%prep 段中也有一个)</strong></p><p><strong><code>rpmbuild 可选项有很多，常用的 3 个，可以根据自己实际需要进行 build:</code></strong></p><ul><li><code>-ba</code> : build 源码包+二进制包</li><li><code>-bb</code> : 只 build 二进制包</li><li><code>-bs</code> : 只 build 源码包</li></ul><p>最后 build 完成后如下</p><p><img src="https://cdn.oss.link/markdown/7tn2a.jpg" srcset="/img/loading.gif" alt="rpms"></p>]]></content>
    
    
    <categories>
      
      <category>Kubernetes</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Linux</tag>
      
      <tag>Docker</tag>
      
      <tag>Kubernetes</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Kubernetes 使用 Ceph 存储</title>
    <link href="/2017/06/03/use-ceph-storage-on-kubernetes/"/>
    <url>/2017/06/03/use-ceph-storage-on-kubernetes/</url>
    
    <content type="html"><![CDATA[<blockquote><p>本文主要记录一下 Kubernetes 使用 Ceph 存储的相关配置过程，Kubernetes 集群环境采用的 kargo 部署方式，并且所有组件以容器化运行</p></blockquote><h3 id="一、基础环境准备"><a href="#一、基础环境准备" class="headerlink" title="一、基础环境准备"></a>一、基础环境准备</h3><p>Kubernetes 集群总共有 5 台，部署方式为 kargo 容器化部署，<strong>采用 kargo 部署时确保配置中开启内核模块加载( <code>kubelet_load_modules: true</code> )</strong>；Kubernetes 版本为 1.6.4，Ceph 采用最新的稳定版 Jewel</p><table><thead><tr><th>节点</th><th>IP</th><th>部署</th></tr></thead><tbody><tr><td>docker1</td><td>192.168.1.11</td><td>master、monitor、osd</td></tr><tr><td>docker2</td><td>192.168.1.12</td><td>master、monitor、osd</td></tr><tr><td>docker3</td><td>192.168.1.13</td><td>node、monitor、osd</td></tr><tr><td>docker4</td><td>192.168.1.14</td><td>node、osd</td></tr><tr><td>docker5</td><td>192.168.1.15</td><td>node、osd</td></tr></tbody></table><h3 id="二、部署-Ceph-集群"><a href="#二、部署-Ceph-集群" class="headerlink" title="二、部署 Ceph 集群"></a>二、部署 Ceph 集群</h3><p>具体安装请参考 <a href="https://mritd.me/2017/05/27/ceph-note-1/" target="_blank" rel="noopener">Ceph 笔记(一)</a>、<a href="https://mritd.me/2017/05/30/ceph-note-2/" target="_blank" rel="noopener">Ceph 笔记(二)</a>，以下直接上命令</p><h4 id="2-1、部署集群"><a href="#2-1、部署集群" class="headerlink" title="2.1、部署集群"></a>2.1、部署集群</h4><pre><code class="hljs sh"><span class="hljs-comment"># 创建集群配置目录</span>mkdir ceph-cluster &amp;&amp; <span class="hljs-built_in">cd</span> ceph-cluster<span class="hljs-comment"># 创建 monitor-node</span>ceph-deploy new docker1 docker2 docker3<span class="hljs-comment"># 追加 OSD 副本数量</span><span class="hljs-built_in">echo</span> <span class="hljs-string">"osd pool default size = 5"</span> &gt;&gt; ceph.conf<span class="hljs-comment"># 安装 ceph</span>ceph-deploy install docker1 docker2 docker3 docker4 docker5<span class="hljs-comment"># init monitor node</span>ceph-deploy mon create-initial<span class="hljs-comment"># 初始化 ods</span>ceph-deploy osd prepare docker1:/dev/sda docker2:/dev/sda docker3:/dev/sda docker4:/dev/sda docker5:/dev/sda<span class="hljs-comment"># 激活 osd</span>ceph-deploy osd activate docker1:/dev/sda1:/dev/sda2 docker2:/dev/sda1:/dev/sda2 docker3:/dev/sda1:/dev/sda2 docker4:/dev/sda1:/dev/sda2 docker5:/dev/sda1:/dev/sda2<span class="hljs-comment"># 部署 ceph cli 工具和秘钥文件</span>ceph-deploy admin docker1 docker2 docker3 docker4 docker5<span class="hljs-comment"># 确保秘钥有读取权限</span>chmod +r /etc/ceph/ceph.client.admin.keyring<span class="hljs-comment"># 检测集群状态</span>ceph health</code></pre><h4 id="2-2、创建块设备"><a href="#2-2、创建块设备" class="headerlink" title="2.2、创建块设备"></a>2.2、创建块设备</h4><pre><code class="hljs sh"><span class="hljs-comment"># 创建存储池</span>rados mkpool data<span class="hljs-comment"># 创建 image</span>rbd create data --size 10240 -p data<span class="hljs-comment"># 关闭不支持特性</span>rbd feature <span class="hljs-built_in">disable</span> data exclusive-lock, object-map, fast-diff, deep-flatten -p data<span class="hljs-comment"># 映射(每个节点都要映射)</span>rbd map data --name client.admin -p data<span class="hljs-comment"># 格式化块设备(单节点即可)</span>mkfs.xfs /dev/rbd0</code></pre><h3 id="三、kubernetes-使用-Ceph"><a href="#三、kubernetes-使用-Ceph" class="headerlink" title="三、kubernetes 使用 Ceph"></a>三、kubernetes 使用 Ceph</h3><h4 id="3-1、PV-amp-PVC-方式"><a href="#3-1、PV-amp-PVC-方式" class="headerlink" title="3.1、PV &amp; PVC 方式"></a>3.1、PV &amp; PVC 方式</h4><p>传统的使用分布式存储的方案一般为 <code>PV &amp; PVC</code> 方式，也就是说管理员预先创建好相关 PV 和 PVC，然后对应的 deployment 或者 replication 挂载 PVC 来使用</p><p><strong>创建 Secret</strong></p><pre><code class="hljs sh"><span class="hljs-comment"># 获取管理 key 并进行 base64 编码</span>ceph auth get-key client.admin | base64<span class="hljs-comment"># 创建一个 secret 配置(key 为上条命令生成的)</span>cat &lt;&lt; EOF &gt;&gt; ceph-secret.ymlapiVersion: v1kind: Secretmetadata:  name: ceph-secretdata:  key: QVFDaWtERlpzODcwQWhBQTdxMWRGODBWOFZxMWNGNnZtNmJHVGc9PQo=EOFkubectl create -f ceph-secret.yml</code></pre><p><strong>创建 PV</strong></p><pre><code class="hljs sh"><span class="hljs-comment"># monitor 需要多个，pool 和 image 填写上面创建的</span>cat &lt;&lt; EOF &gt;&gt; test.pv.ymlapiVersion: v1kind: PersistentVolumemetadata:  name: <span class="hljs-built_in">test</span>-pvspec:  capacity:    storage: 2Gi  accessModes:    - ReadWriteOnce   rbd:    monitors:      - 192.168.1.11:6789      - 192.168.1.12:6789      - 192.168.1.13:6789    pool: data    image: data    user: admin    secretRef:      name: ceph-secret    fsType: xfs    readOnly: <span class="hljs-literal">false</span>  persistentVolumeReclaimPolicy: RecycleEOFkubectl create -f test.pv.yml</code></pre><p><strong>创建 PVC</strong></p><pre><code class="hljs sh">cat &lt;&lt; EOF &gt;&gt; test.pvc.ymlkind: PersistentVolumeClaimapiVersion: v1metadata:  name: <span class="hljs-built_in">test</span>-pvcspec:  accessModes:    - ReadWriteOnce  resources:    requests:      storage: 2GiEOFkubectl create -f test.pvc.yml</code></pre><p><strong>创建 Deployment并挂载</strong></p><pre><code class="hljs sh">cat &lt;&lt; EOF &gt;&gt; test.deploy.ymlapiVersion: apps/v1beta1kind: Deploymentmetadata:  name: demospec:  replicas: 3  template:    metadata:      labels:        app: demo    spec:      containers:      - name: demo        image: mritd/demo        ports:        - containerPort: 80        volumeMounts:          - mountPath: <span class="hljs-string">"/data"</span>            name: data      volumes:        - name: data          persistentVolumeClaim:            claimName: <span class="hljs-built_in">test</span>-pvcEOFkubectl create -f test.deploy.yml</code></pre><h4 id="3-2、StoragaClass-方式"><a href="#3-2、StoragaClass-方式" class="headerlink" title="3.2、StoragaClass 方式"></a>3.2、StoragaClass 方式</h4><p>在 1.4 以后，kubernetes 提供了一种更加方便的动态创建 PV 的方式；也就是说使用 StoragaClass 时无需预先创建固定大小的 PV，等待使用者创建 PVC 来使用；而是直接创建 PVC 即可分配使用</p><p><strong>创建系统级 Secret</strong></p><p><strong>注意: 由于 StorageClass 要求 Ceph 的 Secret type 必须为 <code>kubernetes.io/rbd</code>，所以上一步创建的 <code>ceph-secret</code> 需要先被删除，然后使用如下命令重新创建；此时的 key 并没有经过 base64</strong></p><pre><code class="hljs sh"><span class="hljs-comment"># 这个 secret type 必须为 kubernetes.io/rbd，否则会造成 PVC 无法使用</span>kubectl create secret generic ceph-secret --<span class="hljs-built_in">type</span>=<span class="hljs-string">"kubernetes.io/rbd"</span> --from-literal=key=<span class="hljs-string">'AQCikDFZs870AhAA7q1dF80V8Vq1cF6vm6bGTg=='</span> --namespace=kube-systemkubectl create secret generic ceph-secret --<span class="hljs-built_in">type</span>=<span class="hljs-string">"kubernetes.io/rbd"</span> --from-literal=key=<span class="hljs-string">'AQCikDFZs870AhAA7q1dF80V8Vq1cF6vm6bGTg=='</span> --namespace=default</code></pre><p><strong>创建 StorageClass</strong></p><pre><code class="hljs sh">cat &lt;&lt; EOF &gt;&gt; test.storageclass.ymlapiVersion: storage.k8s.io/v1kind: StorageClassmetadata:  name: <span class="hljs-built_in">test</span>-storageclassprovisioner: kubernetes.io/rbdparameters:  monitors: 192.168.1.11:6789,192.168.1.12:6789,192.168.1.13:6789  <span class="hljs-comment"># Ceph 客户端用户 ID(非 k8s 的)</span>  adminId: admin  adminSecretName: ceph-secret  adminSecretNamespace: kube-system  pool: data  userId: admin  userSecretName: ceph-secretEOFkubectl create -f test.storageclass.yml</code></pre><p><strong>关于上面的 adminId 等字段具体含义请参考这里 <a href="https://kubernetes.io/docs/concepts/storage/persistent-volumes/#ceph-rbd" target="_blank" rel="noopener">Ceph RBD</a></strong></p><p><strong>创建 PVC</strong></p><pre><code class="hljs sh">cat &lt;&lt; EOF &gt;&gt; test.sc.pvc.ymlkind: PersistentVolumeClaimapiVersion: v1metadata:  name: <span class="hljs-built_in">test</span>-sc-pvc  annotations:     volume.beta.kubernetes.io/storage-class: <span class="hljs-built_in">test</span>-storageclassspec:  accessModes:    - ReadWriteOnce   resources:    requests:      storage: 2GiEOFkubectl create -f test.sc.pvc.yml</code></pre><p><strong>创建 Deployment</strong></p><pre><code class="hljs sh">cat &lt;&lt; EOF &gt;&gt; test.sc.deploy.ymlapiVersion: apps/v1beta1kind: Deploymentmetadata:  name: demo-scspec:  replicas: 3  template:    metadata:      labels:        app: demo-sc    spec:      containers:      - name: demo-sc        image: mritd/demo        ports:        - containerPort: 80        volumeMounts:          - mountPath: <span class="hljs-string">"/data"</span>            name: data      volumes:        - name: data          persistentVolumeClaim:            claimName: <span class="hljs-built_in">test</span>-sc-pvcEOFkubectl create -f test.sc.deploy.yml</code></pre><p>到此完成，检测是否成功最简单的方式就是看相关 pod 是否正常运行</p>]]></content>
    
    
    <categories>
      
      <category>Kubernetes</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Linux</tag>
      
      <tag>Docker</tag>
      
      <tag>Kubernetes</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Ceph 笔记(二)</title>
    <link href="/2017/05/30/ceph-note-2/"/>
    <url>/2017/05/30/ceph-note-2/</url>
    
    <content type="html"><![CDATA[<blockquote><p>本篇文章主要简述了 Ceph 的存储对象名词解释及其含义，以及对 Ceph 集群内 CRUSH bucket 调整、PG/PGP 参数调整等设置；同时参考了一些书籍资料简单的概述一下 Ceph 集群硬件要求等</p></blockquote><h3 id="一、Ceph-组件及定义"><a href="#一、Ceph-组件及定义" class="headerlink" title="一、Ceph 组件及定义"></a>一、Ceph 组件及定义</h3><h4 id="1-1、对象"><a href="#1-1、对象" class="headerlink" title="1.1、对象"></a>1.1、对象</h4><p>对象是 Ceph 中最小的存储单元，对象是一个数据和一个元数据绑定的整体；元数据中存放了具体数据的相关属性描述信息等；Ceph 为每个对象生成一个集群内唯一的对象标识符，以保证对象在集群内的唯一性；在传统文件系统的存储中，单个文件的大小是有一定限制的，而 Ceph 中对象随着其元数据区增大可以变得非常巨大</p><h4 id="1-2、CRUSH"><a href="#1-2、CRUSH" class="headerlink" title="1.2、CRUSH"></a>1.2、CRUSH</h4><p>在传统的文件存储系统中，数据的元数据占据着极其重要的位置，每次系统中新增数据时，元数据首先被更新，然后才是实际的数据被写入；在较小的存储系统中(GB/TB)，这种将元数据存储在某个固定的存储节点或者磁盘阵列中的做法还可以满足需求；当数据量增大到 PB/ZB 级别时，元数据查找性能将会成为一个很大的瓶颈；同时元数据的统一存放还可能造成单点故障，即当元数据丢失后，实际数据将无法被找回；与传统文件存储系统不同的是，<strong>Ceph 使用可扩展散列下的受控复制(Controlled Replication Under Scalable Hashing,CRUSH)算法来精确地计算数据应该被写入哪里/从哪里读取；CRUSH按需计算元数据，而不是存储元数据，从而解决了传统文件存储系统的瓶颈</strong></p><h4 id="1-3、CRUSH-查找"><a href="#1-3、CRUSH-查找" class="headerlink" title="1.3、CRUSH 查找"></a>1.3、CRUSH 查找</h4><p>在 Ceph 中，元数据的计算和负载是分布式的，并且只有在需要时才会执行；元数据的计算过程称之为 CRUSH 查找，不同于其他分布式文件系统，Ceph 的 CRUSH 查找是由客户端使用自己的资源来完成的，从而去除了中心查找带来的性能以及单点故障问题；CRUSH 查找时，客户端先通过 monitor 获取集群 map 副本，然后从 map 副本中获取集群配置信息；然后通过对象信息、池ID等生成对象；接着通过对象和 PG 数散列后得到 Ceph 池中最终存放该对象的 PG；最终在通过 CRUSH 算法确定该 PG 所需存储的 OSD 位置，<strong>一旦确定了 OSD 位置，那么客户端将直接与 OSD 通讯完成数据读取与写入，这直接去除了中间环节，保证了性能的极大提升</strong></p><h4 id="1-4、CRUSH-层级结构"><a href="#1-4、CRUSH-层级结构" class="headerlink" title="1.4、CRUSH 层级结构"></a>1.4、CRUSH 层级结构</h4><p>在 Ceph 中，CRUSH 是完全支持各种基础设施和用户自定义的；CRUSH 设备列表中预先定义了一系列的设备，包括磁盘、节点、机架、行、开关、电源电路、房间、数据中心等等；这些组件称之为故障区(CRUSH bucket)，用户可以通过自己的配置把不同的 OSD 分布在不同区域；<strong>此后 Ceph 存储数据时根据 CRUSH bucket 结构，将会保证每份数据都会在所定义的物理组件之间完全隔离；</strong>比如我们定义了多个机架上的不同 OSD，那么 Ceph 存储时就会智能的将数据副本分散到多个机架之上，防止某个机架上机器全部跪了以后数据全部丢失的情况</p><h4 id="1-5、恢复和再平衡"><a href="#1-5、恢复和再平衡" class="headerlink" title="1.5、恢复和再平衡"></a>1.5、恢复和再平衡</h4><p>当故障区内任何组件出现故障时，Ceph 都会将其标记为 down 和 out 状态；然后默认情况下 Ceph 会等待 300秒之后进行数据恢复和再平衡，这个值可以通过在配置文件中的 <code>mon osd down out interval</code> 参数来调整</p><h4 id="1-6、PG"><a href="#1-6、PG" class="headerlink" title="1.6、PG"></a>1.6、PG</h4><p>PG 是一组对象集合体，根据 Ceph 的复制级别，每个PG 中的数据会被复制到多个 OSD 上，以保证其高可用状态</p><h4 id="1-7、Ceph-池"><a href="#1-7、Ceph-池" class="headerlink" title="1.7、Ceph 池"></a>1.7、Ceph 池</h4><p>Ceph 池是一个存储对象的逻辑分区，每一个池中都包含若干个 PG，进而实现将一定对象映射到集群内不同 OSD 中，<strong>池可以以复制方式或者纠错码方式创建，但不可同时使用这两种方式</strong></p><h3 id="二、Ceph-组件调整及操作"><a href="#二、Ceph-组件调整及操作" class="headerlink" title="二、Ceph 组件调整及操作"></a>二、Ceph 组件调整及操作</h3><h4 id="2-1、池操作"><a href="#2-1、池操作" class="headerlink" title="2.1、池操作"></a>2.1、池操作</h4><pre><code class="hljs sh"><span class="hljs-comment"># 创建池</span>rados mkpool <span class="hljs-built_in">test</span>-pool<span class="hljs-comment"># 列出池</span>rados lspools<span class="hljs-comment"># 复制池</span>rados cppool <span class="hljs-built_in">test</span>-pool cp-pool<span class="hljs-comment"># 删除池</span>rados rmpool <span class="hljs-built_in">test</span>-pool <span class="hljs-built_in">test</span>-pool --yes-i-really-really-mean-it</code></pre><h4 id="2-2、对象操作"><a href="#2-2、对象操作" class="headerlink" title="2.2、对象操作"></a>2.2、对象操作</h4><pre><code class="hljs sh"><span class="hljs-comment"># 将对象加入到池内</span>rados put testfile anaconda-ks.cfg -p <span class="hljs-built_in">test</span><span class="hljs-comment"># 列出池内对象</span>rados ls -p <span class="hljs-built_in">test</span><span class="hljs-comment"># 检查对象信息</span>ceph osd map <span class="hljs-built_in">test</span> testfile<span class="hljs-comment"># 删除对象</span>rados rm testfile -p <span class="hljs-built_in">test</span></code></pre><h4 id="2-3、修改-PG-和-PGP"><a href="#2-3、修改-PG-和-PGP" class="headerlink" title="2.3、修改 PG 和 PGP"></a>2.3、修改 PG 和 PGP</h4><p>计算 PG 数为 Ceph 企业级存储必不可少的的一部分，其中集群内 PG 计算公式如下</p><pre><code class="hljs sh">PG 总数 = (OSD 数 * 100) / 最大副本数</code></pre><p>对于单个池来讲，我们还应该为池设定 PG 数，其中池的 PG 数计算公式如下</p><pre><code class="hljs sh">PG 总数 = (OSD 数 * 100) / 最大副本数 / 池数</code></pre><p>PGP 是为了实现定位而设计的 PG，PGP 的值应该与 PG 数量保持一致；<strong>当池的 pg_num 增加的时候，池内所有 PG 都会一分为二，但是他们仍然保持着以前 OSD 的映射关系；当增加 pgp_num 的时候，Ceph 集群才会将 PG 进行 OSD 迁移，然后开始再平衡过程</strong></p><p>获取现有 PG 和 PGP 值可以通过如下命令</p><pre><code class="hljs sh">ceph osd pool get <span class="hljs-built_in">test</span> pg_numceph osd pool get <span class="hljs-built_in">test</span> pgp_num</code></pre><p>当计算好 PG 和 PGP 以后可以通过以下命令设置</p><pre><code class="hljs sh">ceph osd pool <span class="hljs-built_in">set</span> <span class="hljs-built_in">test</span> pgp_num 32ceph osd pool <span class="hljs-built_in">set</span> <span class="hljs-built_in">test</span> pgp_num 32</code></pre><p>同样在创建 pool 的时候也可以同步指定</p><pre><code class="hljs sh">ceph osd pool create POOLNAME PG PGP</code></pre><h4 id="2-4、pool-副本数调整"><a href="#2-4、pool-副本数调整" class="headerlink" title="2.4、pool 副本数调整"></a>2.4、pool 副本数调整</h4><p>默认情况，当创建一个新的 pool 时，向 pool 内存储的数据只会有 2 个副本，查看 pool 副本数可以通过如下命令</p><pre><code class="hljs sh">ceph osd dump | grep pool</code></pre><p>当我们需要修改默认副本数以使其满足高可靠性需求时，可以通过如下命令完成</p><pre><code class="hljs sh">ceph osd pool <span class="hljs-built_in">set</span> POOLNAME size NUM</code></pre><h4 id="2-5、定制机群布局"><a href="#2-5、定制机群布局" class="headerlink" title="2.5、定制机群布局"></a>2.5、定制机群布局</h4><p>上面已经讲述了 CRUSH bucket 的概念，通过以下相关命令，我们可以定制自己的集群布局，以使 Ceph 完成数据的容灾处理</p><pre><code class="hljs sh"><span class="hljs-comment"># 查看现有集群布局</span>ceph osd tree<span class="hljs-comment"># 添加机架</span>ceph osd crush add-bucket rack01 rackceph osd crush add-bucket rack02 rackceph osd crush add-bucket rack03 rack<span class="hljs-comment"># 移动主机到不同的机架(dockerX 为我的主机名)</span>ceph osd crush move docker1 rack=rack01ceph osd crush move docker2 rack=rack02ceph osd crush move docker3 rack=rack03<span class="hljs-comment"># 移动每个机架到默认的根下</span>ceph osd crush move rack01 root=defaultceph osd crush move rack02 root=defaultceph osd crush move rack03 root=default</code></pre><p>最终集群整体布局如下</p><pre><code class="hljs sh">➜  ~ ceph osd treeID WEIGHT  TYPE NAME            UP/DOWN REWEIGHT PRIMARY-AFFINITY-1 0.01469 root default-5 0.00490     rack rack01-2 0.00490         host docker1 0 0.00490             osd.0         up  1.00000          1.00000-6 0.00490     rack rack02-3 0.00490         host docker2 1 0.00490             osd.1         up  1.00000          1.00000-7 0.00490     rack rack03-4 0.00490         host docker3 2 0.00490             osd.2         up  1.00000          1.00000</code></pre><h3 id="三、Ceph-硬件配置"><a href="#三、Ceph-硬件配置" class="headerlink" title="三、Ceph 硬件配置"></a>三、Ceph 硬件配置</h3><p>硬件规划一般是一个企业级存储的必要工作，以下简述了 Ceph 的一般硬件需求</p><h4 id="3-1、监控需求"><a href="#3-1、监控需求" class="headerlink" title="3.1、监控需求"></a>3.1、监控需求</h4><p>Ceph monitor 通过维护整个集群的 map 从而完成集群的健康处理；但是 monitor 并不参与实际的数据存储，所以实际上 monitor 节点 CPU 占用、内存占用都比较少；一般单核 CPU 加几个 G 的内存即可满足需求；虽然 monitor 节点不参与实际存储工作，但是 monitor 的网卡至少应该是冗余的，因为一旦网络出现故障则集群健康会难以保证</p><h4 id="3-2、OSD-需求"><a href="#3-2、OSD-需求" class="headerlink" title="3.2、OSD 需求"></a>3.2、OSD 需求</h4><p>OSD 作为 Ceph 集群的主要存储设施，其会占用一定的 CPU 和内存资源；一般推荐做法是每个节点的每块硬盘作为一个 OSD；同时 OSD 还需要写入日志，所以应当为 OSD 集成日志留有充足的空间；在出现故障时，OSD 需求的资源可能会更多，所以 OSD 节点根据实际情况(每个 OSD 会有一个线程)应该分配更多的 CPU 和内存；固态硬盘也会增加 OSD 存取速度和恢复速度</p><h4 id="3-3、MDS-需求"><a href="#3-3、MDS-需求" class="headerlink" title="3.3、MDS 需求"></a>3.3、MDS 需求</h4><p>MDS 服务专门为 CephFS 存储元数据，所以相对于 monitor 和 OSD 节点，这个 MDS 节点的 CPU 需求会大得多，同时内存占用也是海量的，所以 MDS 一般会使用一个强劲的物理机单独搭建</p>]]></content>
    
    
    <categories>
      
      <category>Linux</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Linux</tag>
      
      <tag>Ceph</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Ceph 笔记(一)</title>
    <link href="/2017/05/27/ceph-note-1/"/>
    <url>/2017/05/27/ceph-note-1/</url>
    
    <content type="html"><![CDATA[<blockquote><p>Ceph 是一个符合POSIX、开源的分布式存储系统；其具备了极好的可靠性、统一性、鲁棒性；经过近几年的发展，Ceph 开辟了一个全新的数据存储途径。Ceph 具备了企业级存储的分布式、可大规模扩展、没有单点故障等特点，越来越受到人们青睐；以下记录了 Ceph 的相关学习笔记。</p></blockquote><h3 id="一、-Ceph-Quick-Start"><a href="#一、-Ceph-Quick-Start" class="headerlink" title="一、 Ceph Quick Start"></a>一、 Ceph Quick Start</h3><h4 id="1-1、安装前准备"><a href="#1-1、安装前准备" class="headerlink" title="1.1、安装前准备"></a>1.1、安装前准备</h4><blockquote><p>本文以 Centos 7 3.10 内核为基础环境，节点为 4 台 Vagrant 虚拟机；Ceph 版本为 Jewel.</p></blockquote><p>首先需要一台部署节点，这里使用的是宿主机；在部署节点上需要安装一些部署工具，如下</p><pre><code class="hljs sh"><span class="hljs-comment"># 安装 EPEL 源</span>yum install -y epel-release<span class="hljs-comment"># 添加 ceph 官方源</span>cat &lt;&lt; EOF &gt;&gt; /etc/yum.repos.d/ceph.repo[ceph-noarch]name=Ceph noarch packagesbaseurl=https://download.ceph.com/rpm-jewel/el7/noarchenabled=1gpgcheck=1<span class="hljs-built_in">type</span>=rpm-mdgpgkey=https://download.ceph.com/keys/release.ascEOF<span class="hljs-comment"># 安装部署工具</span>yum update -y &amp;&amp; yum install ceph-deploy -y</code></pre><p><strong>同时，ceph-deploy 工具需要使用 ssh 来自动化部署 Ceph 各个组件，因此需要保证部署节点能够免密码登录待部署节点；最后，待部署节点最好加入到部署节点的 hosts 中，方便使用域名(某些地方强制)连接管理</strong></p><h4 id="1-2、校对时钟"><a href="#1-2、校对时钟" class="headerlink" title="1.2、校对时钟"></a>1.2、校对时钟</h4><p>由于 Ceph 采用 Paxos 算法保证数据一致性，所以安装前需要先保证各个节点时钟同步</p><pre><code class="hljs sh"><span class="hljs-comment"># 安装 ntp 工具</span>yum install ntp ntpdate ntp-doc -y<span class="hljs-comment"># 校对系统时钟</span>ntpdate 0.cn.pool.ntp.org</code></pre><h4 id="1-3、创建集群配置"><a href="#1-3、创建集群配置" class="headerlink" title="1.3、创建集群配置"></a>1.3、创建集群配置</h4><p>ceph-deploy 工具部署集群前需要创建一些集群配置信息，其保存在 <code>ceph.conf</code> 文件中，这个文件未来将会被复制到每个节点的 <code>/etc/ceph/ceph.conf</code></p><pre><code class="hljs sh"><span class="hljs-comment"># 创建集群配置目录</span>mkdir ceph-cluster &amp;&amp; <span class="hljs-built_in">cd</span> ceph-cluster<span class="hljs-comment"># 创建 monitor-node</span>ceph-deploy new docker1<span class="hljs-comment"># 追加 OSD 副本数量(测试虚拟机总共有3台)</span><span class="hljs-built_in">echo</span> <span class="hljs-string">"osd pool default size = 3"</span> &gt;&gt; ceph.conf</code></pre><h4 id="1-4、创建集群"><a href="#1-4、创建集群" class="headerlink" title="1.4、创建集群"></a>1.4、创建集群</h4><p>创建集群使用 ceph-deploy 工具即可</p><pre><code class="hljs sql"><span class="hljs-comment"># 安装 ceph</span>ceph-deploy <span class="hljs-keyword">install</span> docker1 docker2 docker3<span class="hljs-comment"># 初始化 monitor node 和 秘钥文件</span>ceph-deploy mon <span class="hljs-keyword">create</span>-<span class="hljs-keyword">initial</span><span class="hljs-comment"># 在两个 osd 节点创建一个目录作为 osd 存储</span>mkdir /<span class="hljs-keyword">data</span>chown -R ceph:ceph /<span class="hljs-keyword">data</span><span class="hljs-comment"># 初始化 osd</span>ceph-deploy osd <span class="hljs-keyword">prepare</span> docker1:/<span class="hljs-keyword">data</span> docker2:/<span class="hljs-keyword">data</span> docker3:/<span class="hljs-keyword">data</span><span class="hljs-comment"># 激活 osd</span>ceph-deploy osd <span class="hljs-keyword">activate</span> docker1:/<span class="hljs-keyword">data</span> docker2:/<span class="hljs-keyword">data</span> docker3:/<span class="hljs-keyword">data</span><span class="hljs-comment"># 部署 ceph cli 工具和秘钥文件</span>ceph-deploy <span class="hljs-keyword">admin</span> docker1 docker2 docker3<span class="hljs-comment"># 确保秘钥有读取权限</span>chmod +r /etc/ceph/ceph.client.admin.keyring<span class="hljs-comment"># 检测集群状态</span>ceph health</code></pre><p>执行 <code>ceph health</code> 命令后应当返回 <code>HEALTH_OK</code>；如出现 <code>HEALTH_WARN clock skew detected on mon.docker2; Monitor clock skew detected</code>，说明时钟不同步，手动同步时钟稍等片刻后即可；其他错误可以通过如下命令重置集群重新部署</p><pre><code class="hljs sh">ceph-deploy purge &#123;ceph-node&#125; [&#123;ceph-node&#125;]ceph-deploy purgedata &#123;ceph-node&#125; [&#123;ceph-node&#125;]ceph-deploy forgetkeys</code></pre><p><strong>更多细节，如防火墙、SELinux配置等请参考 <a href="http://docs.ceph.com/docs/master/start/quick-start-preflight/#rhel-centos" target="_blank" rel="noopener">官方文档</a></strong></p><h4 id="1-5、其他组件创建"><a href="#1-5、其他组件创建" class="headerlink" title="1.5、其他组件创建"></a>1.5、其他组件创建</h4><pre><code class="hljs sh"><span class="hljs-comment"># 创建 MDS</span>ceph-deploy mds create docker1<span class="hljs-comment"># 创建 RGW</span>ceph-deploy rgw create docker1<span class="hljs-comment"># 增加 monitor</span><span class="hljs-built_in">echo</span> <span class="hljs-string">"public network = 192.168.1.0/24"</span> &gt;&gt; ceph.confceph-deploy --overwrite-conf mon create docker2 docker3<span class="hljs-comment"># 查看仲裁信息</span>ceph quorum_status --format json-pretty</code></pre><h3 id="二、Ceph-组件及测试"><a href="#二、Ceph-组件及测试" class="headerlink" title="二、Ceph 组件及测试"></a>二、Ceph 组件及测试</h3><h4 id="2-1、Ceph-架构图"><a href="#2-1、Ceph-架构图" class="headerlink" title="2.1、Ceph 架构图"></a>2.1、Ceph 架构图</h4><p>以下图片(摘自网络)展示了基本的 Ceph 架构</p><p><img src="https://cdn.oss.link/markdown/o8gct.jpg" srcset="/img/loading.gif" alt="ceph 架构"></p><ul><li>OSD: Ceph 实际存储数据单元被称为 OSD，OSD 可以使用某个物理机的目录、磁盘设备，甚至是 RAID 阵列；</li><li>MON: 在 OSD 之上则分布着多个 MON(monitor)，Ceph 集群内组件的状态信息等被维护成一个个 map，而 MON 则负责维护集群所有组件 map 信息，各个集群内组件心跳请求 MON 以确保其 map 保持最新状态；当集群发生故障时，Ceph 将采用 Paxos 算法保证数据一致性，这其中仲裁等主要由 MON 完成，所以 MON 节点建议最少为 3 个，并且为奇数以防止脑裂情况的发生；</li><li>MDS: Ceph 本身使用对象形式存储数据，而对于外部文件系统访问支持则提供了上层的 CephFS 接口；CephFS 作为文件系统接口则需要一些元数据，这些原数据就存放在 MDS 中；目前 Ceph 只支持单个 MDS 工作，<strong>但是可以通过设置 MDS 副本，以保证 MDS 的可靠性</strong></li><li>RADOS: RADOS 全称 Reliable Autonomic Distributed Object Store，即可靠分布式对象存储；其作为在整个 Ceph 集群核心基础设施，向外部提供基本的数据操作</li><li>librados: 为了支持私有云等程序调用，Ceph 提供了 C 实现的 API 库 librados，librados 可以支持主流编程语言直接调用，沟通 RADOS 完成数据存取等操作</li><li>RBD: RDB 个人理解是一个命令行工具，一般位于宿主机上，通过该工具可以直接跟 librados 交互，实现创建存储对象，格式化 Ceph 块设备等操作</li><li>RADOS GW: 从名字可以看出来，这个组件是一个代理网关，通过 RADOS GW 可以将 RADOS 响应转化为 HTTP 响应，同样可以将外部 HTTP 请求转化为 RADOS 调用；RADOS GW 主要提供了三大功能: <strong>兼容 S3 接口、兼容 Swift 接口、提供管理 RestFul API</strong></li></ul><p>下图(摘自网络)从应用角度描述了 Ceph 架构</p><p><img src="https://cdn.oss.link/markdown/fh5z8.jpg" srcset="/img/loading.gif" alt="APP Ceph 架构"></p><h4 id="2-2、对象存储测试"><a href="#2-2、对象存储测试" class="headerlink" title="2.2、对象存储测试"></a>2.2、对象存储测试</h4><p>此处直接上代码</p><pre><code class="hljs sh"><span class="hljs-comment"># 创建测试文件</span>dd <span class="hljs-keyword">if</span>=/dev/zero of=<span class="hljs-built_in">test</span> bs=1G count=1<span class="hljs-comment"># 创建对象存储池</span>rados mkpool data<span class="hljs-comment"># 放入对象</span>rados put <span class="hljs-built_in">test</span>-file <span class="hljs-built_in">test</span> --pool=data<span class="hljs-comment"># 检查存储池</span>rados -p data ls<span class="hljs-comment"># 检查对象信息</span>ceph osd map data <span class="hljs-built_in">test</span>-file<span class="hljs-comment"># 删除对象</span>rados -p data rm <span class="hljs-built_in">test</span>-file<span class="hljs-comment"># 删除存储池(存储池写两遍并且加上确认)</span>rados rmpool data data --yes-i-really-really-mean-it</code></pre><h4 id="2-3、块设备测试"><a href="#2-3、块设备测试" class="headerlink" title="2.3、块设备测试"></a>2.3、块设备测试</h4><p><strong>官方文档中提示，使用 rdb 的客户端不建议与 OSD 等节点在同一台机器上</strong></p><blockquote><p>You may use a virtual machine for your ceph-client node, but do not execute the following procedures on the same physical node as your Ceph Storage Cluster nodes (unless you use a VM). See FAQ for details.</p></blockquote><p>这里从第四台虚拟机上执行操作，首先安装所需客户端工具</p><pre><code class="hljs sh"><span class="hljs-comment"># 部署节点上 ceph-cluster 目录下执行</span>ceph-deploy install docker4ceph-deploy admin docker4</code></pre><p>然后创建块设备</p><pre><code class="hljs sh"><span class="hljs-comment"># 块设备单位为 MB</span>rbd create data --size 10240<span class="hljs-comment"># 映射块设备</span>map foo --name client.admin</code></pre><p><strong>在上面的 map 映射操作时，可能出现如下错误</strong></p><pre><code class="hljs sh">RBD image feature <span class="hljs-built_in">set</span> mismatch. You can <span class="hljs-built_in">disable</span> features unsupported by the kernel with <span class="hljs-string">"rbd feature disable"</span></code></pre><p>查看系统日志可以看到如下输出</p><pre><code class="hljs sh">➜  ~ dmesg | tail[-1127592253.530346] rbd: image data: image uses unsupported features: 0x38[-1127590337.563180] libceph: mon0 192.168.1.11:6789 session established[-1127590337.563741] libceph: client4200 fsid dd9fdfee-438a-47aa-be21-114372bc1f44</code></pre><p><strong>问题原因: 在 Ceph 高版本进行 map image 时，默认 Ceph 在创建 image(上文 data)时会增加许多 features，这些 features 需要内核支持，在 Centos7 的内核上支持有限，所以需要手动关掉一些 features</strong></p><p>首先使用 <code>rbd info data</code> 命令列出创建的 image 的 features</p><pre><code class="hljs sh">rbd image <span class="hljs-string">'data'</span>:        size 10240 MB <span class="hljs-keyword">in</span> 2560 objects        order 22 (4096 kB objects)        block_name_prefix: rbd_data.37c6238e1f29        format: 2        features: layering, exclusive-lock, object-map, fast-diff, deep-flatten        flags:</code></pre><p>在 features 中我们可以看到默认开启了很多:</p><ul><li>layering: 支持分层</li><li>striping: 支持条带化 v2</li><li>exclusive-lock: 支持独占锁</li><li>object-map: 支持对象映射(依赖 exclusive-lock)</li><li>fast-diff: 快速计算差异(依赖 object-map)</li><li>deep-flatten: 支持快照扁平化操作</li><li>journaling: 支持记录 IO 操作(依赖独占锁）</li></ul><p><strong>而实际上 Centos 7 的 3.10 内核只支持 layering… 所以我们要手动关闭一些 features，然后重新 map；如果想要一劳永逸，可以在 ceph.conf 中加入 <code>rbd_default_features = 1</code> 来设置默认 features(数值仅是 layering 对应的 bit 码所对应的整数值)。</strong></p><pre><code class="hljs sh"><span class="hljs-comment"># 关闭不支持特性</span>rbd feature <span class="hljs-built_in">disable</span> data exclusive-lock, object-map, fast-diff, deep-flatten<span class="hljs-comment"># 重新映射</span>rbd map data --name client.admin<span class="hljs-comment"># 成功后返回设备位置</span>/dev/rbd0</code></pre><p><strong>最后我们便可以格式化正常挂载这个设备了</strong></p><pre><code class="hljs sh">➜  ~ mkfs.xfs /dev/rbd0meta-data=/dev/rbd0              isize=512    agcount=17, agsize=162816 blks         =                       sectsz=512   attr=2, projid32bit=1         =                       crc=1        finobt=0, sparse=0data     =                       bsize=4096   blocks=2621440, imaxpct=25         =                       sunit=1024   swidth=1024 blksnaming   =version 2              bsize=4096   ascii-ci=0 ftype=1<span class="hljs-built_in">log</span>      =internal <span class="hljs-built_in">log</span>           bsize=4096   blocks=2560, version=2         =                       sectsz=512   sunit=8 blks, lazy-count=1realtime =none                   extsz=4096   blocks=0, rtextents=0➜  ~ mkdir test1➜  ~ mount /dev/rbd0 test1<span class="hljs-comment"># 写入测试</span>➜  ~ dd <span class="hljs-keyword">if</span>=/dev/zero of=test1/<span class="hljs-built_in">test</span>-file bs=1G count=1记录了1+0 的读入记录了1+0 的写出1073741824字节(1.1 GB)已复制，16.3689 秒，65.6 MB/秒➜  ~ ls test1<span class="hljs-built_in">test</span>-file</code></pre><h4 id="2-4、CephFS-测试"><a href="#2-4、CephFS-测试" class="headerlink" title="2.4、CephFS 测试"></a>2.4、CephFS 测试</h4><p>在测试 CephFS 之前需要先创建两个存储池和一个 fs，创建存储池要指定 PG 数量</p><pre><code class="hljs sh">ceph osd pool create cephfs_data 32ceph osd pool create cephfs_metadata 32ceph fs new testfs cephfs_metadata cephfs_data</code></pre><p><strong>PG 概念:</strong></p><blockquote><p>当 Ceph 集群接收到存储请求时，Ceph 会将其分散到各个 PG 中，PG 是一组对象的逻辑集合；根据 Ceph 存储池的复制级别，每个 PG的数据会被复制并分发到集群的多个 OSD 上；一般来说增加 PG 数量能降低 OSD 负载，一般每个 OSD 大约分配 50 ~ 100 PG，<strong>关于 PG 数量一般遵循以下公式</strong></p></blockquote><ul><li>集群 PG 总数 = (OSD 总数 * 100) / 数据最大副本数</li><li>单个存储池 PG 数 = (OSD 总数 * 100) / 数据最大副本数 /存储池数</li></ul><p><strong>注意，PG 最终结果应当为最接近以上计算公式的 2 的 N 次幂(向上取值)；如我的虚拟机环境每个存储池 PG 数 = <code>3(OSD) * 100 / 3(副本) / 5(大约 5 个存储池) = 20</code>，向上取 2 的 N 次幂 为 32</strong></p><p>挂载 CephFS 一般有两种方式，一种是使用内核驱动挂载，一种是使用 <code>ceph-fuse</code> 用户空间挂载；内核方式挂载需要提取 Ceph 管理 key，如下</p><pre><code class="hljs sh"><span class="hljs-comment"># key 在 ceph.conf 中</span><span class="hljs-built_in">echo</span> <span class="hljs-string">"AQB37CZZblBkDRAAUrIrRGsHj/NqdKmVlMQ7ww=="</span> &gt; ceph-key<span class="hljs-comment"># 创建目录挂载</span>mkdir test2mount -t ceph 192.168.1.11:6789:/ /root/test2 -o name=admin,secretfile=ceph-key<span class="hljs-comment"># 写入测试</span>➜  ~ dd <span class="hljs-keyword">if</span>=/dev/zero of=test2/testFs bs=1G count=1记录了1+0 的读入记录了1+0 的写出1073741824字节(1.1 GB)已复制，6.83251 秒，157 MB/秒</code></pre><p>使用 ceph-fuse 用户空间挂载方式比较简单，但需要先安装 <code>ceph-fuse</code> 软件包</p><pre><code class="hljs sh"><span class="hljs-comment"># 安装 ceph-fuse</span>yum install -y ceph-fuse<span class="hljs-comment"># 挂载</span>mkdir test3ceph-fuse -m 192.168.1.11:6789 test3<span class="hljs-comment"># 写入测试</span>➜  ~ dd <span class="hljs-keyword">if</span>=/dev/zero of=test3/testFs bs=1G count=1记录了1+0 的读入记录了1+0 的写出1073741824字节(1.1 GB)已复制，8.18417 秒，131 MB/秒</code></pre><h4 id="2-5、对象网关测试"><a href="#2-5、对象网关测试" class="headerlink" title="2.5、对象网关测试"></a>2.5、对象网关测试</h4><p>对象网关在 <strong>1.5、其他组件创建</strong> 部分已经做了创建(RGW)，此时直接访问 <code>http://ceph-node-ip:7480</code> 返回如下</p><pre><code class="hljs xml"><span class="hljs-tag">&lt;<span class="hljs-name">ListAllMyBucketsResult</span> <span class="hljs-attr">xmlns</span>=<span class="hljs-string">"http://s3.amazonaws.com/doc/2006-03-01/"</span>&gt;</span>    <span class="hljs-tag">&lt;<span class="hljs-name">Owner</span>&gt;</span>        <span class="hljs-tag">&lt;<span class="hljs-name">ID</span>&gt;</span>anonymous<span class="hljs-tag">&lt;/<span class="hljs-name">ID</span>&gt;</span>        <span class="hljs-tag">&lt;<span class="hljs-name">DisplayName</span>/&gt;</span>    <span class="hljs-tag">&lt;/<span class="hljs-name">Owner</span>&gt;</span>    <span class="hljs-tag">&lt;<span class="hljs-name">Buckets</span>/&gt;</span><span class="hljs-tag">&lt;/<span class="hljs-name">ListAllMyBucketsResult</span>&gt;</span></code></pre><p>这就说明网关已经 ok，由于手里没有能读写测试工具，这里暂不做过多说明</p><p><strong>本文主要参考 <a href="http://docs.ceph.com/docs/master/start/" target="_blank" rel="noopener">Ceph 官方文档 Quick Start</a> 部分，如有其它未说明到的细节可从官方文档获取</strong></p>]]></content>
    
    
    <categories>
      
      <category>Linux</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Linux</tag>
      
      <tag>Ceph</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Docker 分配宿主机网段 IP</title>
    <link href="/2017/05/12/docker-uses-the-host-network-segment-ip/"/>
    <url>/2017/05/12/docker-uses-the-host-network-segment-ip/</url>
    
    <content type="html"><![CDATA[<blockquote><p>工作需要临时启动一个 gitlab,无奈 gitlab 需要 ssh 的 22 端口;而使用传统网桥方式映射端口则 clone 等都需要输入端口号,很麻烦;22 端口宿主机又有 sshd 监听;研究了下 docker 网络,记录一下如何分配宿主机网段 IP</p></blockquote><h3 id="创建-macvlan-网络"><a href="#创建-macvlan-网络" class="headerlink" title="创建 macvlan 网络"></a>创建 macvlan 网络</h3><p>关于 Docker 网络模式这里不再细说;由于默认的网桥方式无法满足需要,所以需要创建一个 macvlan 网络</p><pre><code class="hljs sh">docker network create -d macvlan  --subnet=172.16.0.0/19 --gateway=172.16.0.1 -o parent=eth0 gitlab-net</code></pre><ul><li><code>--subnet</code>: 指定网段(宿主机)</li><li><code>--gateway</code>: 指定网关(宿主机)</li><li><code>parent</code>: 注定父网卡(宿主机)</li></ul><p>创建以后可以使用 <code>docker network ls</code> 查看</p><pre><code class="hljs sh">➜  ~  docker network lsNETWORK ID          NAME                    DRIVER              SCOPEa4a2980c9165        agent_default           bridge              <span class="hljs-built_in">local</span>               a0f29102b413        bridge                  bridge              <span class="hljs-built_in">local</span>               2f46dc70b763        gitlab-net              macvlan             <span class="hljs-built_in">local</span>               51bd6222530f        host                    host                <span class="hljs-built_in">local</span>               7a14a09c3cfc        none                    null                <span class="hljs-built_in">local</span></code></pre><h3 id="创建使用容器"><a href="#创建使用容器" class="headerlink" title="创建使用容器"></a>创建使用容器</h3><p>接下来创建容器指定网络即可</p><pre><code class="hljs sh">docker run --net=gitlab-net --ip=172.16.0.170  -dt --name <span class="hljs-built_in">test</span> centos:7</code></pre><p><strong><code>--net</code> 指定使用的网络,<code>--ip</code> 用于指定网段内 IP</strong>;启动后只需要在容器内启动程序测试即可</p><pre><code class="hljs sh"><span class="hljs-comment"># 启动一个 nginx</span>yum install nginxnginx</code></pre><p>启动后在局域网内能直接通过 IP:80 访问,而且宿主机 80 不受影响</p><h3 id="docker-compose-测试"><a href="#docker-compose-测试" class="headerlink" title="docker-compose 测试"></a>docker-compose 测试</h3><p>docker-compose 示例如下</p><pre><code class="hljs sh">version: <span class="hljs-string">'2'</span>services:  centos:    image: centos:7    restart: always    <span class="hljs-built_in">command</span>: /bin/bash -c <span class="hljs-string">"sleep 999999"</span>    networks:      app_net:        ipv4_address: 10.10.1.34networks:  app_net:    driver: macvlan    driver_opts:      parent: enp3s0    ipam:      config:      - subnet: 10.10.1.0/24        gateway: 10.10.1.2<span class="hljs-comment">#        ip_range: 10.25.87.32/28</span></code></pre>]]></content>
    
    
    <categories>
      
      <category>Docker</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Linux</tag>
      
      <tag>Docker</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Java 配合 mitmproxy HTTPS 抓包调试</title>
    <link href="/2017/03/25/java-capturing-https-packets-use-mitmproxy/"/>
    <url>/2017/03/25/java-capturing-https-packets-use-mitmproxy/</url>
    
    <content type="html"><![CDATA[<blockquote><p>今天对接接口，对方给的 Demo 和已有项目用的 HTTP 工具不是一个；后来出现人家的好使，我的死活不通的情况；无奈之下开始研究 Java 抓包，所以怕忘了记录一下……</p></blockquote><h3 id="一、mitmproxy-简介"><a href="#一、mitmproxy-简介" class="headerlink" title="一、mitmproxy 简介"></a>一、mitmproxy 简介</h3><p>mitmproxy 是一个命令行下的强大抓包工具，可以在命令行下抓取 HTTP(S) 数据包并加以分析；对于 HTTPS 抓包，首先要在本地添加 mitmproxy 的根证书，然后 mitmproxy 通过以下方式进行抓包：</p><p><img src="https://cdn.oss.link/markdown/x7lir.jpg" srcset="/img/loading.gif" alt="mitmproxy1"></p><ul><li>1、客户端发起一个到 mitmproxy 的连接，并且发出HTTP CONNECT 请求</li><li>2、mitmproxy作出响应(200)，模拟已经建立了CONNECT通信管道</li><li>3、客户端确信它正在和远端服务器会话，然后启动SSL连接。在SSL连接中指明了它正在连接的主机名(SNI)</li><li>4、mitmproxy连接服务器，然后使用客户端发出的SNI指示的主机名建立SSL连接</li><li>5、服务器以匹配的SSL证书作出响应，这个SSL证书里包含生成的拦截证书所必须的通用名(CN)和服务器备用名(SAN)</li><li>6、mitmproxy生成拦截证书，然后继续进行与第３步暂停的客户端SSL握手</li><li>7、客户端通过已经建立的SSL连接发送请求，</li><li>8、mitmproxy通过第４步建立的SSL连接传递这个请求给服务器</li></ul><h3 id="二、抓包配置"><a href="#二、抓包配置" class="headerlink" title="二、抓包配置"></a>二、抓包配置</h3><h4 id="2-1、安装-mitmproxy"><a href="#2-1、安装-mitmproxy" class="headerlink" title="2.1、安装 mitmproxy"></a>2.1、安装 mitmproxy</h4><p>mitmproxy 是由  python 编写的，所以直接通过 pip 即可安装，mac 下也可使用 brew 工具安装</p><pre><code class="hljs sh"><span class="hljs-comment"># mac</span>brew install mitmproxy<span class="hljs-comment"># Linux</span>pip install mitmproxy<span class="hljs-comment"># CentOS 安装时可能会出现 "致命错误：libxml/xmlversion.h：没有那个文件或目录"</span><span class="hljs-comment"># 需要安装如下软件包即可解决</span>yum install libxml2 libxml2-devel libxslt libxslt-devel -y</code></pre><h4 id="2-2、HTTPS-证书配置"><a href="#2-2、HTTPS-证书配置" class="headerlink" title="2.2、HTTPS 证书配置"></a>2.2、HTTPS 证书配置</h4><p>首先由于 HTTPS 的安全性，直接抓包是什么也看不到的；所以需要先在本地配置 mitmproxy 的根证书，使其能够解密 HTTPS 流量完成一个中间人的角色；证书下载方式需要先在本地启动 mitmproxy，然后通过设置本地连接代理到 mitmproxy 端口，访问 <code>mitm.it</code> 即可，具体可查看 <a href="https://mitmproxy.org/doc/certinstall.html" target="_blank" rel="noopener">官方文档</a></p><p><strong>首先启动 mitmproxy</strong></p><pre><code class="hljs sh">mitmproxy -p 4000 --no-mouse</code></pre><p><strong>浏览器通过设置代理访问 mitm.it</strong></p><p><img src="https://cdn.oss.link/markdown/unrnc.jpg" srcset="/img/loading.gif" alt="access"></p><p>选择对应平台并将其证书加入到系统信任根证书列表即可；对于 Java 程序来说可能有时候并不会生效，所以必须 <strong>修改 keystore</strong>，修改如下</p><pre><code class="hljs sh"><span class="hljs-comment"># Linux 一般在 JAVA_HOME/jre/lib/security/cacerts 下</span><span class="hljs-comment"># Mac 在 /Library/Java/JavaVirtualMachines/JAVA_HOME/Contents/Home/jre/lib/security/cacerts</span>sudo keytool -importcert -<span class="hljs-built_in">alias</span> mitmproxy -keystore /Library/Java/JavaVirtualMachines/jdk1.8.0_77.jdk/Contents/Home/jre/lib/security/cacerts -storepass changeit -trustcacerts -file ~/.mitmproxy/mitmproxy-ca-cert.pem</code></pre><h4 id="2-4、Java-抓包调试"><a href="#2-4、Java-抓包调试" class="headerlink" title="2.4、Java 抓包调试"></a>2.4、Java 抓包调试</h4><p>JVM 本身在启动时就可以设置代理参数，也可以通过代码层设置；以下为代码层设置代理方式</p><pre><code class="hljs sh">public void <span class="hljs-function"><span class="hljs-title">beforeTest</span></span>()&#123;    logger.info(<span class="hljs-string">"设置抓包代理......"</span>);    System.setProperty(<span class="hljs-string">"https.proxyHost"</span>, <span class="hljs-string">"127.0.0.1"</span>);    System.setProperty(<span class="hljs-string">"https.proxyPort"</span>, <span class="hljs-string">"4000"</span>);&#125;</code></pre><p><strong>然后保证在发送 HTTPS 请求之前此代码执行即可，以下为抓包示例</strong></p><p><img src="https://cdn.oss.link/markdown/kuzhd.jpg" srcset="/img/loading.gif" alt="zhuabao"></p><p>通过方向键+回车即可选择某个请求查看报文信息</p><p><img src="https://cdn.oss.link/markdown/vfifu.jpg" srcset="/img/loading.gif" alt="detail"></p><h3 id="三、Java-其他代理设置"><a href="#三、Java-其他代理设置" class="headerlink" title="三、Java 其他代理设置"></a>三、Java 其他代理设置</h3><p>Java 代理一般可以通过 2 种方式设置，一种是通过代码层，如下</p><pre><code class="hljs sh">// HTTP 代理，只能代理 HTTP 请求System.setProperty(<span class="hljs-string">"http.proxyHost"</span>, <span class="hljs-string">"127.0.0.1"</span>);System.setProperty(<span class="hljs-string">"http.proxyPort"</span>, <span class="hljs-string">"9876"</span>); // HTTPS 代理，只能代理 HTTPS 请求System.setProperty(<span class="hljs-string">"https.proxyHost"</span>, <span class="hljs-string">"127.0.0.1"</span>);System.setProperty(<span class="hljs-string">"https.proxyPort"</span>, <span class="hljs-string">"9876"</span>);// 同时支持代理 HTTP/HTTPS 请求System.setProperty(<span class="hljs-string">"proxyHost"</span>, <span class="hljs-string">"127.0.0.1"</span>);System.setProperty(<span class="hljs-string">"proxyPort"</span>, <span class="hljs-string">"9876"</span>); // SOCKS 代理，支持 HTTP 和 HTTPS 请求// 注意：如果设置了 SOCKS 代理就不要设 HTTP/HTTPS 代理System.setProperty(<span class="hljs-string">"socksProxyHost"</span>, <span class="hljs-string">"127.0.0.1"</span>);System.setProperty(<span class="hljs-string">"socksProxyPort"</span>, <span class="hljs-string">"1080"</span>);</code></pre><p>另一种还可以通过 JVM 启动参数设置</p><pre><code class="hljs sh">-DproxyHost=127.0.0.1 -DproxyPort=9876</code></pre><p>本文参考：</p><ul><li><a href="http://www.aneasystone.com/archives/2015/12/java-and-http-using-proxy.html" target="_blank" rel="noopener">Java 和 HTTP 的那些事</a></li><li><a href="http://blog.csdn.net/qq_30513483/article/details/53258637" target="_blank" rel="noopener">一步一步教你https抓包</a></li></ul>]]></content>
    
    
    <categories>
      
      <category>Java</category>
      
      <category>Linux</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Java</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>vim E492 Not an editor command Plugin xxxx</title>
    <link href="/2017/03/21/vim-e492-not-an-editor-command-plugin-xxxx/"/>
    <url>/2017/03/21/vim-e492-not-an-editor-command-plugin-xxxx/</url>
    
    <content type="html"><![CDATA[<blockquote><p>最近自用的 vim 装了不少插件，但是发现 <code>kubectl edit</code> 或者 <code>git merge</code> 时，调用 vim 总是会弹出各种错误，记录一下解决方法</p></blockquote><p><strong>出现这个错误一开始以为是 vim 没走 <code>.vimrc</code> 配置；后来翻了一堆资料，发现 <code>kubectl edit</code> 或者 <code>git merge</code> 后并非直接调用 vim，而是调用的 <code>/usr/bin/view</code>，那么看一下这个文件</strong></p><p><img src="https://cdn.oss.link/markdown/9c646.png" srcset="/img/loading.gif" alt="view"></p><p><strong>这东西就是链接到了 vi，只要把它链接到 vim 就完了</strong></p><p><img src="https://cdn.oss.link/markdown/f0c4e.png" srcset="/img/loading.gif" alt="relink view"></p>]]></content>
    
    
    <categories>
      
      <category>Linux</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Linux</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Btrfs 笔记</title>
    <link href="/2017/03/20/btrfs-note/"/>
    <url>/2017/03/20/btrfs-note/</url>
    
    <content type="html"><![CDATA[<blockquote><p>btrfs 是 Oracle 07 年基于 GPL 协议开源的 Linux 文件系统，其目的是替换传统的 Ext3、Ext4 系列文件系统；Ext 系列文件系统存在着诸多问题，比如反删除能力有限等；而 btrfs 在解决问题同时提供了更加强大的高级特性</p></blockquote><h3 id="一、Btrfs-特性"><a href="#一、Btrfs-特性" class="headerlink" title="一、Btrfs 特性"></a>一、Btrfs 特性</h3><p>btrfs 在文件系统级别支持写时复制(cow)机制，并且支持快照(增量快照)、支持对单个文件快照；同时支持单个超大文件、文件检查、内建 RAID；支持 B 树子卷(组合多个物理卷，多卷支持)等，具体如下</p><p><strong>btrfs 核心特性：</strong></p><ul><li>多物理卷支持：btrfs 可有多个物理卷组成(类似 LVM)；支持 RAID 以及联机 添加、删除、修改</li><li>写时复制更新机制(cow)：复制、更新、替换指针，而非传统意义上的覆盖</li><li>支持数据及元数据校验码：checksum 机制</li><li>支持创建子卷：sub_volume 机制，同时可多层创建</li><li>支持快照：基于 cow 实现快照，并且相对于 LVM 可以实快照的快照(增量快照)</li><li>支持透明压缩：后台自动压缩文件(消耗一定 CPU)，对前端程序透明</li></ul><h3 id="二、btrfs-常用命令"><a href="#二、btrfs-常用命令" class="headerlink" title="二、btrfs 常用命令"></a>二、btrfs 常用命令</h3><h4 id="2-1、创建文件系统"><a href="#2-1、创建文件系统" class="headerlink" title="2.1、创建文件系统"></a>2.1、创建文件系统</h4><p>同传统的 ext 系列文件系统一样，btrfs 文件系统格式化同样采用 <code>mkfs</code> 系列命令 <code>mkfs.btrfs</code>，其常用选项如下：</p><ul><li><code>-L</code> 指定卷标</li><li><code>-m</code> 指明元数据存放机制(RAID)</li><li><code>-d</code> 指明数据存放机制(RAID)</li><li><code>-O</code> 格式化时指定文件系统开启那些特性(不一定所有内核支持)，如果需要查看支持那些特性可使用 <code>mkfs.btrfs -O list-all</code></li></ul><h4 id="2-2、挂载-btrfs"><a href="#2-2、挂载-btrfs" class="headerlink" title="2.2、挂载 btrfs"></a>2.2、挂载 btrfs</h4><p>同 ext 系列一样，仍然使用 <code>mount</code> 命令，基本挂载如下：</p><pre><code class="hljs sh">mount -t brtfs DEVICE MOUNT_POINT</code></pre><p>在挂载时也可以直接开启文件系统一些特性，如<strong>透明压缩</strong></p><pre><code class="hljs sh">mount -t btrfs -o compress=&#123;lzo|zlib&#125; DEVICE MOUNT_POINT</code></pre><p>同时 btrfs 支持子卷，也可以单独挂载子卷</p><pre><code class="hljs sh">mount -t btrfs -o subvol=SUBVOL_NAME DEVICE</code></pre><h4 id="2-3、btrfs-相关命令"><a href="#2-3、btrfs-相关命令" class="headerlink" title="2.3、btrfs 相关命令"></a>2.3、btrfs 相关命令</h4><p>管理 btrfs 使用 <code>btrfs</code> 命令，该命令包含诸多子命令已完成不同的功能管理，常用命令如下</p><ul><li><strong>btrfs 文件系统属性查看：</strong> <code>btrfs filesystem show</code></li><li><strong>调整文件系统大小：</strong> <code>btrfs filesystem resize +10g MOUNT_POINT</code></li><li><strong>添加硬件设备：</strong> <code>btrfs filesystem add DEVICE MOUNT_POINT</code></li><li><strong>均衡文件负载：</strong> <code>btrfs blance status|start|pause|resume|cancel MOUNT_POINT</code></li><li><strong>移除物理卷(联机、自动移动)：</strong> <code>btrfs device delete DEVICE MOUNT_POINT</code></li><li><strong>动态调整数据存放机制：</strong> <code>btrfs balance start -dconvert=RAID MOUNT_POINT</code></li><li><strong>动态调整元数据存放机制：</strong> <code>btrfs balance start -mconvert=RAID MOUNT_POINT</code></li><li><strong>动态调整文件系统数据数据存放机制：</strong> <code>btrfs balance start -sconvert=RAID MOUNT_POINT</code></li><li><strong>创建子卷：</strong> <code>btrfs subvolume create MOUNT_POINT/DIR</code></li><li><strong>列出所有子卷：</strong> <code>btrfs subvolume list MOUNT_POINT</code></li><li><strong>显示子卷详细信息：</strong> <code>btrfs subvolume show MOUNT_POINT</code></li><li><strong>删除子卷：</strong> <code>btrfs subvolume delete MOUNT_POIN/DIR</code></li><li><strong>创建子卷快照(子卷快照必须存放与当前子卷的同一父卷中)：</strong> <code>btrfs subvolume snapshot SUBVOL PARVOL</code></li><li><strong>删除快照同删除子卷一样：</strong> <code>btrfs subvolume delete MOUNT_POIN/DIR</code></li></ul>]]></content>
    
    
    <categories>
      
      <category>Linux</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Linux</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>CentOS 7 配置 VNC Server</title>
    <link href="/2017/03/18/set-up-vnc-server-on-centos-7/"/>
    <url>/2017/03/18/set-up-vnc-server-on-centos-7/</url>
    
    <content type="html"><![CDATA[<blockquote><p>最近决定把小主机扔到客厅跟路由器放在一起(远程开机 666），因为本来就跑的是 Linux，平时图形化需求也不多；但是为了保险起见准备搞一个 VNC，以便必要时图形化上去，比如强制删除一些 Virtual Box 虚拟机等，记录一下安装过程</p></blockquote><h4 id="安装-VNC-Server"><a href="#安装-VNC-Server" class="headerlink" title="安装 VNC Server"></a>安装 VNC Server</h4><p>VNC Server 软件有很多，这里使用 <code>tigervnc-server</code></p><pre><code class="hljs sh">yum install epel-release -yyum makecacheyum install tigervnc-server -y</code></pre><h4 id="开机自启动"><a href="#开机自启动" class="headerlink" title="开机自启动"></a>开机自启动</h4><p>这地方踩了很多坑，网上不少帖子都是写的先测试 VNC Server，执行 <code>vncserver</code> 命令，然后云云；查设置开机自启动也是五花八门，大部分人的路子就是自己写一个 init 脚本，让系统开机时候执行它…… 从职业踩坑经验来看，这东西绝对有更有逼格的 Systemd 的打开方式；果不其然翻了一下 rpm 包 发现了一个 Systemd 模板 Service</p><pre><code class="hljs sh">[root@mritd ~]<span class="hljs-comment"># rpm -ql  tigervnc-server</span>/etc/sysconfig/vncservers/usr/bin/vncserver/usr/bin/x0vncserver/usr/lib/systemd/system/vncserver@.service/usr/share/man/man1/vncserver.1.gz/usr/share/man/man1/x0vncserver.1.gz</code></pre><p>由于好奇心作祟，先 <code>systemctl enable vncserver@:1.service</code> 了一下，后来发现起不来，所以 vim 看了一下原模板 Service，里面想写描述了如何设置开机启动</p><pre><code class="hljs sh"><span class="hljs-comment"># Quick HowTo:</span><span class="hljs-comment"># 1. Copy this file to /etc/systemd/system/vncserver@.service</span><span class="hljs-comment"># 2. Edit /etc/systemd/system/vncserver@.service, replacing &lt;USER&gt;</span><span class="hljs-comment">#    with the actual user name. Leave the remaining lines of the file unmodified</span><span class="hljs-comment">#    (ExecStart=/usr/sbin/runuser -l &lt;USER&gt; -c "/usr/bin/vncserver %i"</span><span class="hljs-comment">#     PIDFile=/home/&lt;USER&gt;/.vnc/%H%i.pid)</span><span class="hljs-comment"># 3. Run `systemctl daemon-reload`</span><span class="hljs-comment"># 4. Run `systemctl enable vncserver@:&lt;display&gt;.service`</span></code></pre><p>也就是说把这个模板文件 cp 到 <code>/etc/systemd/system/vncserver@.service</code> 然后替换用户名，执行两条命令，最后执行 <code>vncserver</code> 用于初始化密码和配置文件就行了</p><p>重装了一天系统，有点烦躁，听首歌安静一下…</p><div style="position:relative;height:0;padding-bottom:56.25%"><iframe src="https://www.youtube.com/embed/AsC0CN2eGkY?rel=0?ecver=2" width="640" height="360" frameborder="0" style="position:absolute;width:100%;height:100%;left:0" allowfullscreen></iframe></div>]]></content>
    
    
    <categories>
      
      <category>Linux</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Linux</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>kargo 集群扩展及细粒度配置</title>
    <link href="/2017/03/10/kargo-cluster-expansion-and-fine-grained-configuration/"/>
    <url>/2017/03/10/kargo-cluster-expansion-and-fine-grained-configuration/</url>
    
    <content type="html"><![CDATA[<blockquote><p>上一篇写了一下一下使用 kargo 快速部署 Kubernetes 高可用集群，但是一些细节部分不算完善，这里准备补一下，详细说明一下一些问题；比如后期如何扩展、一些配置如何自定义等</p></blockquote><h3 id="一、集群扩展"><a href="#一、集群扩展" class="headerlink" title="一、集群扩展"></a>一、集群扩展</h3><p>如果已经有了一个 kargo 搭建的集群，那么扩展其极其容易；只需要修改集群 <code>inventory</code> 配置，加入新节点重新运行命令价格参数即可，如下新增一个 node6 节点</p><pre><code class="hljs sh">vim inventory/inventory.cfg<span class="hljs-comment"># 在 Kubernetes node 组中加入新的 node6 节点</span>[all]node1    ansible_host=192.168.1.11 ip=192.168.1.11node2    ansible_host=192.168.1.12 ip=192.168.1.12node3    ansible_host=192.168.1.13 ip=192.168.1.13node4    ansible_host=192.168.1.14 ip=192.168.1.14node5    ansible_host=192.168.1.15 ip=192.168.1.15node6    ansible_host=192.168.1.16 ip=192.168.1.16[kube-master]node1node2node3node5[kube-node]node1node2node3node4node5node6[etcd]node1node2node3[k8s-cluster:children]kube-nodekube-master[calico-rr]</code></pre><p><strong>然后重新运行集群命令，注意增加 <code>--limit</code> 参数</strong></p><pre><code class="hljs sh">ansible-playbook -i inventory/inventory.cfg cluster.yml -b -v --private-key=~/.ssh/id_rsa --<span class="hljs-built_in">limit</span> node6</code></pre><p><strong>稍等片刻 node6 节点便加入现有集群，如果有多个节点加入，只需要以逗号分隔即可，如 <code>--limit node5,node6</code>；在此过程中只会操作新增的 node 节点，不会影响现有集群，可以实现动态集群扩容(master 也可以扩展)</strong></p><h3 id="二、kargo-细粒度控制"><a href="#二、kargo-细粒度控制" class="headerlink" title="二、kargo 细粒度控制"></a>二、kargo 细粒度控制</h3><p>对于 kargo 高度自动化的工具，可能有些东西我们已经预先处理好了，<strong>比如事先已经安装了 docker，而且 docker 配置了一些参数(日志驱动、存储驱动等)；这时候我们可能并不希望 kargo 再去处理，因为 kargo 会进行覆盖，可能导致一些问题</strong></p><p><strong>kargo 是基于 ansible 的，实际上也就是 ansible，只不过它帮我们写好了配置文件而已；按照 ansible 的规则，Play Book 首先执行 roles 目录下的 roles，在这些 roles 中定义了如何配置集群、如何初始化网络、怎么配置 docker 等等，所以只要我们去更改这些 roles 规则就可以实现一些功能的定制，roles 目录位置如下</strong></p><p><img src="https://cdn.oss.link/markdown/jmy7o.jpg" srcset="/img/loading.gif" alt="roles"></p><p>如果需要更改某些默认配置，那么只需要更改对应目录下的 role 即可，<strong>每个 role 子目录都是一个组件的配置过程(动作)，动作实际上就是不同的 task，所有的 task 定义在 <code>tasks/main.yml</code> 中，如果我们注释(删掉)了相关 task，那么也就关闭了 kargo 对应的处理；如下禁用了 kargo 安装 docker，但是允许 kargo 覆盖 docker service 文件</strong></p><p><img src="https://cdn.oss.link/markdown/vv2px.jpg" srcset="/img/loading.gif" alt="docker task"></p><p><strong>禁用掉 docker 仓库以及 docker 的安装动作</strong></p><pre><code class="hljs sh">vim roles/docker/tasks/main.yml---- name: gather os specific variables  include_vars: <span class="hljs-string">"&#123; &#123; item &#125; &#125;"</span>  with_first_found:    - files:      - <span class="hljs-string">"&#123; &#123; ansible_distribution|lower &#125; &#125;-&#123; &#123; ansible_distribution_version|lower|replace('/', '_') &#125; &#125;.yml"</span>      - <span class="hljs-string">"&#123; &#123; ansible_distribution|lower &#125; &#125;-&#123; &#123; ansible_distribution_release &#125; &#125;.yml"</span>      - <span class="hljs-string">"&#123; &#123; ansible_distribution|lower &#125; &#125;-&#123; &#123; ansible_distribution_major_version|lower|replace('/', '_') &#125; &#125;.yml"</span>      - <span class="hljs-string">"&#123; &#123; ansible_distribution|lower &#125; &#125;.yml"</span>      - <span class="hljs-string">"&#123; &#123; ansible_os_family|lower &#125; &#125;.yml"</span>      - defaults.yml      paths:      - ../vars      skip: <span class="hljs-literal">true</span>  tags: facts- include: set_facts_dns.yml  when: dns_mode != <span class="hljs-string">'none'</span> and resolvconf_mode == <span class="hljs-string">'docker_dns'</span>  tags: facts- name: check <span class="hljs-keyword">for</span> minimum kernel version  fail:    msg: &gt;          docker requires a minimum kernel version of          &#123; &#123; docker_kernel_min_version &#125; &#125; on          &#123; &#123; ansible_distribution &#125; &#125;-&#123; &#123; ansible_distribution_version &#125; &#125;  when: (not ansible_os_family <span class="hljs-keyword">in</span> [<span class="hljs-string">"CoreOS"</span>, <span class="hljs-string">"Container Linux by CoreOS"</span>]) and (ansible_kernel|version_compare(docker_kernel_min_version, <span class="hljs-string">"&lt;"</span>))  tags: facts<span class="hljs-comment"># 禁用 docker 仓库处理，因为默认 kargo 会写入国外 docker 源，我已经自己设置了清华大学的镜像源</span><span class="hljs-comment">#- name: ensure docker repository public key is installed</span><span class="hljs-comment">#  action: "&#123; &#123; docker_repo_key_info.pkg_key &#125; &#125;"</span><span class="hljs-comment">#  args:</span><span class="hljs-comment">#    id: "&#123; &#123;item&#125; &#125;"</span><span class="hljs-comment">#    keyserver: "&#123; &#123;docker_repo_key_info.keyserver&#125; &#125;"</span><span class="hljs-comment">#    state: present</span><span class="hljs-comment">#  register: keyserver_task_result</span><span class="hljs-comment">#  until: keyserver_task_result|success</span><span class="hljs-comment">#  retries: 4</span><span class="hljs-comment">#  delay: "&#123; &#123; retry_stagger | random + 3 &#125; &#125;"</span><span class="hljs-comment">#  with_items: "&#123; &#123; docker_repo_key_info.repo_keys &#125; &#125;"</span><span class="hljs-comment">#  when: not ansible_os_family in ["CoreOS", "Container Linux by CoreOS"]</span><span class="hljs-comment">#</span><span class="hljs-comment">#- name: ensure docker repository is enabled</span><span class="hljs-comment">#  action: "&#123; &#123; docker_repo_info.pkg_repo &#125; &#125;"</span><span class="hljs-comment">#  args:</span><span class="hljs-comment">#    repo: "&#123; &#123;item&#125; &#125;"</span><span class="hljs-comment">#    state: present</span><span class="hljs-comment">#  with_items: "&#123; &#123; docker_repo_info.repos &#125; &#125;"</span><span class="hljs-comment">#  when: (not ansible_os_family in ["CoreOS", "Container Linux by CoreOS"]) and (docker_repo_info.repos|length &gt; 0)</span><span class="hljs-comment">#</span><span class="hljs-comment">#- name: Configure docker repository on RedHat/CentOS</span><span class="hljs-comment">#  template:</span><span class="hljs-comment">#    src: "rh_docker.repo.j2"</span><span class="hljs-comment">#    dest: "/etc/yum.repos.d/docker.repo"</span><span class="hljs-comment">#  when: ansible_distribution in ["CentOS","RedHat"]</span><span class="hljs-comment">#</span><span class="hljs-comment"># 这步 kargo 会重新安装 docker，已经装好了，所以不需要再覆盖安装</span><span class="hljs-comment">#- name: ensure docker packages are installed</span><span class="hljs-comment">#  action: "&#123; &#123; docker_package_info.pkg_mgr &#125; &#125;"</span><span class="hljs-comment">#  args:</span><span class="hljs-comment">#    pkg: "&#123; &#123;item.name&#125; &#125;"</span><span class="hljs-comment">#    force: "&#123; &#123;item.force|default(omit)&#125; &#125;"</span><span class="hljs-comment">#    state: present</span><span class="hljs-comment">#  register: docker_task_result</span><span class="hljs-comment">#  until: docker_task_result|success</span><span class="hljs-comment">#  retries: 4</span><span class="hljs-comment">#  delay: "&#123; &#123; retry_stagger | random + 3 &#125; &#125;"</span><span class="hljs-comment">#  with_items: "&#123; &#123; docker_package_info.pkgs &#125; &#125;"</span><span class="hljs-comment">#  notify: restart docker</span><span class="hljs-comment">#  when: (not ansible_os_family in ["CoreOS", "Container Linux by CoreOS"]) and (docker_package_info.pkgs|length &gt; 0)</span><span class="hljs-comment">#</span><span class="hljs-comment"># 对于 docker 版本的检查个人感觉还是有点必要的</span>- name: check minimum docker version <span class="hljs-keyword">for</span> docker_dns mode. You need at least docker version &gt;= 1.12 <span class="hljs-keyword">for</span> resolvconf_mode=docker_dns  <span class="hljs-built_in">command</span>: <span class="hljs-string">"docker version -f '&#123; &#123; '&#123; &#123;' &#125; &#125;.Client.Version&#123; &#123; '&#125; &#125;' &#125; &#125;'"</span>  register: docker_version  failed_when: docker_version.stdout|version_compare(<span class="hljs-string">'1.12'</span>, <span class="hljs-string">'&lt;'</span>)  changed_when: <span class="hljs-literal">false</span>  when: dns_mode != <span class="hljs-string">'none'</span> and resolvconf_mode == <span class="hljs-string">'docker_dns'</span><span class="hljs-comment"># kargo 对 docker service 的配置会在此写入，我感觉还不错，所以留着了；但是注意的是它会把原来的覆盖掉</span>- name: Set docker systemd config  include: systemd.yml- name: ensure docker service is started and enabled  service:    name: <span class="hljs-string">"&#123; &#123; item &#125; &#125;"</span>    enabled: yes    state: started  with_items:    - docker</code></pre><p><strong>kargo 在进行各种任务(task)时可能会释放一些配置文件，比如 docker service 配置文件、kubernetes 配置文件等；这些文件一般位于 <code>roles/组件/templates</code> 目录，比如 docker 的 service 配置位于如下位置；我们可以更改，甚至直接换一个，把里面写死变成我们自己的</strong></p><p><img src="https://cdn.oss.link/markdown/f1f9g.jpg" srcset="/img/loading.gif" alt="docker service template"></p><h3 id="三、其他相关"><a href="#三、其他相关" class="headerlink" title="三、其他相关"></a>三、其他相关</h3><p><strong>以上只是介绍了自定义配置的大体思路，更深度的处理需要去玩转  ansible，如果玩明白了 ansible 那么基本上这个 kargo 就可以随便搞了；要写的差不多也就这么多了，感觉这东西比 kubeadm 要好的多，所有操作都是可视化的，没有莫名其妙的问题；其他的可以参考 <a href="http://ansible-tran.readthedocs.io/en/latest/" target="_blank" rel="noopener">ansible 中文文档</a>、<a href="https://github.com/kubernetes-incubator/kargo/blob/master/README.md" target="_blank" rel="noopener">kargo 官方文档</a></strong></p>]]></content>
    
    
    <categories>
      
      <category>Kubernetes</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Linux</tag>
      
      <tag>Docker</tag>
      
      <tag>Kubernetes</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Kubernetes Nginx Ingress 教程</title>
    <link href="/2017/03/04/how-to-use-nginx-ingress/"/>
    <url>/2017/03/04/how-to-use-nginx-ingress/</url>
    
    <content type="html"><![CDATA[<blockquote><p>最近发现好多人问 Ingress，同时一直也没去用 Nginx 的 Ingress，索性鼓捣了一把，发现跟原来确实有了点变化，在这里写篇文章记录一下</p></blockquote><h3 id="一、Ingress-介绍"><a href="#一、Ingress-介绍" class="headerlink" title="一、Ingress 介绍"></a>一、Ingress 介绍</h3><p>Kubernetes 暴露服务的方式目前只有三种：LoadBlancer Service、NodePort Service、Ingress；前两种估计都应该很熟悉，具体的可以参考下 <a href="https://mritd.me/2016/12/06/try-traefik-on-kubernetes/" target="_blank" rel="noopener">这篇文章</a>；下面详细的唠一下这个 Ingress</p><h4 id="1-1、Ingress-是个什么玩意"><a href="#1-1、Ingress-是个什么玩意" class="headerlink" title="1.1、Ingress 是个什么玩意"></a>1.1、Ingress 是个什么玩意</h4><p>可能从大致印象上 Ingress 就是能利用 Nginx、Haproxy 啥的负载均衡器暴露集群内服务的工具；那么问题来了，集群内服务想要暴露出去面临着几个问题：</p><h4 id="1-2、Pod-漂移问题"><a href="#1-2、Pod-漂移问题" class="headerlink" title="1.2、Pod 漂移问题"></a>1.2、Pod 漂移问题</h4><p>众所周知 Kubernetes 具有强大的副本控制能力，能保证在任意副本(Pod)挂掉时自动从其他机器启动一个新的，还可以动态扩容等，总之一句话，这个 Pod 可能在任何时刻出现在任何节点上，也可能在任何时刻死在任何节点上；那么自然随着 Pod 的创建和销毁，Pod IP 肯定会动态变化；<strong>那么如何把这个动态的 Pod IP 暴露出去？这里借助于 Kubernetes 的 Service 机制，Service 可以以标签的形式选定一组带有指定标签的 Pod，并监控和自动负载他们的 Pod IP，那么我们向外暴露只暴露 Service IP 就行了</strong>；这就是 NodePort 模式：即在每个节点上开起一个端口，然后转发到内部 Pod IP 上，如下图所示</p><p><img src="https://cdn.oss.link/markdown/5a1i4.jpg" srcset="/img/loading.gif" alt="NodePort"></p><h4 id="1-3、端口管理问题"><a href="#1-3、端口管理问题" class="headerlink" title="1.3、端口管理问题"></a>1.3、端口管理问题</h4><p>采用 NodePort 方式暴露服务面临一个坑爹的问题是，服务一旦多起来，NodePort 在每个节点上开启的端口会及其庞大，而且难以维护；这时候引出的思考问题是 <strong>“能不能使用 Nginx 啥的只监听一个端口，比如 80，然后按照域名向后转发？”</strong> 这思路很好，简单的实现就是使用 DaemonSet 在每个 node 上监听 80，然后写好规则，<strong>因为 Nginx 外面绑定了宿主机 80 端口(就像 NodePort)，本身又在集群内，那么向后直接转发到相应 Service IP 就行了</strong>，如下图所示</p><p><img src="https://cdn.oss.link/markdown/rrcuu.jpg" srcset="/img/loading.gif" alt="use nginx proxy"></p><h4 id="1-4、域名分配及动态更新问题"><a href="#1-4、域名分配及动态更新问题" class="headerlink" title="1.4、域名分配及动态更新问题"></a>1.4、域名分配及动态更新问题</h4><p>从上面的思路，采用 Nginx 似乎已经解决了问题，但是其实这里面有一个很大缺陷：<strong>每次有新服务加入怎么改 Nginx 配置？总不能手动改或者来个 Rolling Update 前端 Nginx Pod 吧？</strong>这时候 “伟大而又正直勇敢的” Ingress 登场，<strong>如果不算上面的 Nginx，Ingress 只有两大组件：Ingress Controller 和 Ingress</strong></p><p>Ingress 这个玩意，简单的理解就是 <strong>你原来要改 Nginx 配置，然后配置各种域名对应哪个 Service，现在把这个动作抽象出来，变成一个 Ingress 对象，你可以用 yml 创建，每次不要去改 Nginx 了，直接改 yml 然后创建/更新就行了</strong>；那么问题来了：”Nginx 咋整？”</p><p>Ingress Controller 这东西就是解决 “Nginx 咋整” 的；<strong>Ingress Controoler 通过与 Kubernetes API 交互，动态的去感知集群中 Ingress 规则变化，然后读取他，按照他自己模板生成一段 Nginx 配置，再写到 Nginx Pod 里，最后 reload 一下</strong>，工作流程如下图</p><p><img src="https://cdn.oss.link/markdown/e5fcy.jpg" srcset="/img/loading.gif" alt="Ingress"></p><p><strong>当然在实际应用中，最新版本 Kubernetes 已经将 Nginx 与 Ingress Controller 合并为一个组件，所以 Nginx 无需单独部署，只需要部署 Ingress Controller 即可</strong></p><h3 id="二、怼一个-Nginx-Ingress"><a href="#二、怼一个-Nginx-Ingress" class="headerlink" title="二、怼一个 Nginx Ingress"></a>二、怼一个 Nginx Ingress</h3><p>上面啰嗦了那么多，只是为了讲明白 Ingress 的各种理论概念，下面实际部署很简单</p><h4 id="2-1、部署默认后端"><a href="#2-1、部署默认后端" class="headerlink" title="2.1、部署默认后端"></a>2.1、部署默认后端</h4><p>我们知道 <strong>前端的 Nginx 最终要负载到后端 service 上，那么如果访问不存在的域名咋整？</strong>官方给出的建议是部署一个 <strong>默认后端</strong>，对于未知请求全部负载到这个默认后端上；这个后端啥也不干，就是返回 404，部署如下</p><pre><code class="hljs sh">➜  ~ kubectl create -f default-backend.yamldeployment <span class="hljs-string">"default-http-backend"</span> createdservice <span class="hljs-string">"default-http-backend"</span> created</code></pre><p>这个 <code>default-backend.yaml</code> 文件可以在 <a href="https://github.com/kubernetes/ingress/blob/master/examples/deployment/nginx/default-backend.yaml" target="_blank" rel="noopener">官方 Ingress 仓库</a> 找到，由于篇幅限制这里不贴了，仓库位置如下</p><p><img src="https://cdn.oss.link/markdown/1ct6w.jpg" srcset="/img/loading.gif" alt="default-backend"></p><h4 id="2-2、部署-Ingress-Controller"><a href="#2-2、部署-Ingress-Controller" class="headerlink" title="2.2、部署 Ingress Controller"></a>2.2、部署 Ingress Controller</h4><p>部署完了后端就得把最重要的组件 Nginx+Ingres Controller(官方统一称为 Ingress Controller) 部署上</p><pre><code class="hljs sh">➜  ~ kubectl create -f nginx-ingress-controller.yamldaemonset <span class="hljs-string">"nginx-ingress-lb"</span> created</code></pre><p><strong>注意：官方的 Ingress Controller 有个坑，至少我看了 DaemonSet 方式部署的有这个问题：没有绑定到宿主机 80 端口，也就是说前端 Nginx 没有监听宿主机 80 端口(这还玩个卵啊)；所以需要把配置搞下来自己加一下 <code>hostNetwork</code></strong>，截图如下</p><p><img src="https://cdn.oss.link/markdown/n1fsc.jpg" srcset="/img/loading.gif" alt="add hostNetwork"></p><p>同样配置文件自己找一下，地址 <a href="https://github.com/kubernetes/ingress/blob/master/examples/daemonset/nginx/nginx-ingress-daemonset.yaml" target="_blank" rel="noopener">点这里</a>，仓库截图如下</p><p><img src="https://cdn.oss.link/markdown/jirhn.jpg" srcset="/img/loading.gif" alt="Ingress Controller"></p><p><strong>当然它支持以 deamonset 的方式部署，这里用的就是(个人喜欢而已)，所以你发现我上面截图是 deployment，但是链接给的却是 daemonset，因为我截图截错了…..</strong></p><h4 id="2-3、部署-Ingress"><a href="#2-3、部署-Ingress" class="headerlink" title="2.3、部署 Ingress"></a>2.3、部署 Ingress</h4><p><strong>这个可就厉害了，这个部署完就能装逼了</strong></p><p><img src="https://cdn.oss.link/markdown/v450z.jpg" srcset="/img/loading.gif" alt="daitouzhaungbi"><br><img src="https://cdn.oss.link/markdown/b1kz2.jpg" srcset="/img/loading.gif" alt="zhanxianjishu"></p><p><strong>咳咳，回到正题，从上面可以知道 Ingress 就是个规则，指定哪个域名转发到哪个 Service，所以说首先我们得有个 Service，当然 Service 去哪找这里就不管了；这里默认为已经有了两个可用的 Service，以下以 Dashboard 和 kibana 为例</strong></p><p><strong>先写一个 Ingress 文件，语法格式啥的请参考 <a href="https://kubernetes.io/docs/user-guide/ingress" target="_blank" rel="noopener">官方文档</a>，由于我的 Dashboard 和 Kibana 都在 kube-system 这个命名空间，所以要指定 namespace</strong>，写之前 Service 分布如下</p><p><img src="https://cdn.oss.link/markdown/vtg8f.jpg" srcset="/img/loading.gif" alt="All Service"></p><pre><code class="hljs sh">vim dashboard-kibana-ingress.ymlapiVersion: extensions/v1beta1kind: Ingressmetadata:  name: dashboard-kibana-ingress  namespace: kube-systemspec:  rules:  - host: dashboard.mritd.me    http:      paths:      - backend:          serviceName: kubernetes-dashboard          servicePort: 80  - host: kibana.mritd.me    http:      paths:      - backend:          serviceName: kibana-logging          servicePort: 5601</code></pre><p><strong>装逼成功截图如下</strong></p><p><img src="https://cdn.oss.link/markdown/pyhdy.jpg" srcset="/img/loading.gif" alt="Dashboard"></p><p><img src="https://cdn.oss.link/markdown/p3qli.jpg" srcset="/img/loading.gif" alt="Kibana"></p><h3 id="三、部署-Ingress-TLS"><a href="#三、部署-Ingress-TLS" class="headerlink" title="三、部署 Ingress TLS"></a>三、部署 Ingress TLS</h3><p>上面已经搞定了 Ingress，下面就顺便把 TLS 怼上；官方给出的样例很简单，大致步骤就两步：<strong>创建一个含有证书的 secret、在 Ingress 开启证书</strong>；但是我不得不喷一下，文档就提那么一嘴，大坑一堆，比如多域名配置，还有下面这文档特么的是逗我玩呢？</p><p><img src="https://cdn.oss.link/markdown/t3n1j.jpg" srcset="/img/loading.gif" alt="douniwan"></p><h4 id="3-1、创建证书"><a href="#3-1、创建证书" class="headerlink" title="3.1、创建证书"></a>3.1、创建证书</h4><p>首先第一步当然要有个证书，由于我这个 Ingress 有两个服务域名，所以证书要支持两个域名；生成证书命令如下：</p><pre><code class="hljs sh"><span class="hljs-comment"># 生成 CA 自签证书</span>mkdir cert &amp;&amp; <span class="hljs-built_in">cd</span> certopenssl genrsa -out ca-key.pem 2048openssl req -x509 -new -nodes -key ca-key.pem -days 10000 -out ca.pem -subj <span class="hljs-string">"/CN=kube-ca"</span><span class="hljs-comment"># 编辑 openssl 配置</span>cp /etc/pki/tls/openssl.cnf .vim openssl.cnf<span class="hljs-comment"># 主要修改如下</span>[req]req_extensions = v3_req <span class="hljs-comment"># 这行默认注释关着的 把注释删掉</span><span class="hljs-comment"># 下面配置是新增的</span>[ v3_req ]basicConstraints = CA:FALSEkeyUsage = nonRepudiation, digitalSignature, keyEnciphermentsubjectAltName = @alt_names[alt_names]DNS.1 = dashboard.mritd.meDNS.2 = kibana.mritd.me<span class="hljs-comment"># 生成证书</span>openssl genrsa -out ingress-key.pem 2048openssl req -new -key ingress-key.pem -out ingress.csr -subj <span class="hljs-string">"/CN=kube-ingress"</span> -config openssl.cnfopenssl x509 -req -<span class="hljs-keyword">in</span> ingress.csr -CA ca.pem -CAkey ca-key.pem -CAcreateserial -out ingress.pem -days 365 -extensions v3_req -extfile openssl.cnf</code></pre><h4 id="3-2、创建-secret"><a href="#3-2、创建-secret" class="headerlink" title="3.2、创建 secret"></a>3.2、创建 secret</h4><p>创建好证书以后，需要将证书内容放到 secret 中，secret 中全部内容需要 base64 编码，然后注意去掉换行符(变成一行)；以下是我的 secret 样例(上一步中 ingress.pem 是证书，ingress-key.pem 是证书的 key)</p><pre><code class="hljs sh">vim ingress-secret.ymlapiVersion: v1data:  tls.crt: LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSUM5akNDQWQ2Z0F3SUJBZ0lKQU5TR2dNNnYvSVd5TUEwR0NTcUdTSWIzRFFFQkJRVUFNQkl4RURBT0JnTlYKQkFNTUIydDFZbVV0WTJFd0hoY05NVGN3TXpBME1USTBPRFF5V2hjTk1UZ3dNekEwTVRJME9EUXlXakFYTVJVdwpFd1lEVlFRRERBeHJkV0psTFdsdVozSmxjM013Z2dFaU1BMEdDU3FHU0liM0RRRUJBUVVBQTRJQkR3QXdnZ0VLCkFvSUJBUUM2dkNZRFhGSFpQOHI5Zk5jZXlkV015VVlELzAwQ2xnS0M2WjNpYWZ0QlRDK005TmcrQzloUjhJUE4KWW00cjZOMkw1MmNkcmZvQnBHZXovQVRIT0NJYUhJdlp1K1ZaTzNMZjcxZEVLR09nV21LMTliSVAzaGpSeDZhWQpIeGhEVWNab3ZzYWY1UWJHRnUydEF4L2doMTFMdXpTZWJkT0Y1dUMrWHBhTGVzWWdQUjhFS0cxS0VoRXBLMDFGCmc4MjhUU1g2TXVnVVZmWHZ1OUJRUXExVWw0Q2VMOXhQdVB5T3lMSktzbzNGOEFNUHFlaS9USWpsQVFSdmRLeFYKVUMzMnBtTHRlUFVBb2thNDRPdElmR3BIOTZybmFsMW0rMXp6YkdTemRFSEFaL2k1ZEZDNXJOaUthRmJnL2NBRwppalhlQ01xeGpzT3JLMEM4MDg4a0tjenJZK0JmQWdNQkFBR2pTakJJTUM0R0ExVWRFUVFuTUNXQ0VtUmhjMmhpCmIyRnlaQzV0Y21sMFpDNXRaWUlQYTJsaVlXNWhMbTF5YVhSa0xtMWxNQWtHQTFVZEV3UUNNQUF3Q3dZRFZSMFAKQkFRREFnWGdNQTBHQ1NxR1NJYjNEUUVCQlFVQUE0SUJBUUNFN1ByRzh6MytyaGJESC8yNGJOeW5OUUNyYVM4NwphODJUUDNxMmsxUUJ1T0doS1pwR1N3SVRhWjNUY0pKMkQ2ZlRxbWJDUzlVeDF2ckYxMWhGTWg4MU9GMkF2MU4vCm5hSU12YlY5cVhYNG16eGNROHNjakVHZ285bnlDSVpuTFM5K2NXejhrOWQ1UHVaejE1TXg4T3g3OWJWVFpkZ0sKaEhCMGJ5UGgvdG9hMkNidnBmWUR4djRBdHlrSVRhSlFzekhnWHZnNXdwSjlySzlxZHd1RHA5T3JTNk03dmNOaQpseWxDTk52T3dNQ0h3emlyc01nQ1FRcVRVamtuNllLWmVsZVY0Mk1yazREVTlVWFFjZ2dEb1FKZEM0aWNwN0sxCkRPTDJURjFVUGN0ODFpNWt4NGYwcUw1aE1sNGhtK1BZRyt2MGIrMjZjOVlud3ROd24xdmMyZVZHCi0tLS0tRU5EIENFUlRJRklDQVRFLS0tLS0K  tls.key: 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kind: Secretmetadata:  name: ingress-secret  namespace: kube-system<span class="hljs-built_in">type</span>: Opaque</code></pre><p><strong>创建完成后 create 一下就可</strong></p><pre><code class="hljs sh">➜  ~ kubectl create -f ingress-secret.ymlsecret <span class="hljs-string">"ingress-secret"</span> created</code></pre><p><strong>其实这个配置比如证书转码啥的没必要手动去做，可以直接使用下面的命令创建，这里写这么多只是为了把步骤写清晰</strong></p><pre><code class="hljs sh">kubectl create secret tls ingress-secret --key cert/ingress-key.pem --cert cert/ingress.pem</code></pre><h4 id="3-3、重新部署-Ingress"><a href="#3-3、重新部署-Ingress" class="headerlink" title="3.3、重新部署 Ingress"></a>3.3、重新部署 Ingress</h4><p>生成完成后需要在 Ingress 中开启 TLS，Ingress 修改后如下</p><pre><code class="hljs sh">apiVersion: extensions/v1beta1kind: Ingressmetadata:  name: dashboard-kibana-ingress  namespace: kube-systemspec:  tls:  - hosts:    - dashboard.mritd.me    - kibana.mritd.me    secretName: ingress-secret  rules:  - host: dashboard.mritd.me    http:      paths:      - backend:          serviceName: kubernetes-dashboard          servicePort: 80  - host: kibana.mritd.me    http:      paths:      - backend:          serviceName: kibana-logging          servicePort: 5601</code></pre><p><strong>注意：一个 Ingress 只能使用一个 secret(secretName 段只能有一个)，也就是说只能用一个证书，更直白的说就是如果你在一个 Ingress 中配置了多个域名，那么使用 TLS 的话必须保证证书支持该 Ingress 下所有域名；并且这个 <code>secretName</code> 一定要放在上面域名列表最后位置，否则会报错 <code>did not find expected key</code> 无法创建；同时上面的 <code>hosts</code> 段下域名必须跟下面的 <code>rules</code> 中完全匹配</strong></p><p><strong>更需要注意一点：之所以这里单独开一段就是因为有大坑；Kubernetes Ingress 默认情况下，当你不配置证书时，会默认给你一个 TLS 证书的，也就是说你 Ingress 中配置错了，比如写了2个 <code>secretName</code>、或者 <code>hosts</code> 段中缺了某个域名，那么对于写了多个 <code>secretName</code> 的情况，所有域名全会走默认证书；对于 <code>hosts</code> 缺了某个域名的情况，缺失的域名将会走默认证书，部署时一定要验证一下证书，不能 “有了就行”；更新 Ingress 证书可能需要等一段时间才会生效</strong></p><p>最后重新部署一下即可</p><pre><code class="hljs sh">➜  ~ kubectl delete -f dashboard-kibana-ingress.ymlingress <span class="hljs-string">"dashboard-kibana-ingress"</span> deleted➜  ~ kubectl create -f dashboard-kibana-ingress.ymlingress <span class="hljs-string">"dashboard-kibana-ingress"</span> created</code></pre><p><strong>注意：部署 TLS 后 80 端口会自动重定向到 443</strong>，最终访问截图如下</p><p><img src="https://cdn.oss.link/markdown/6o0pj.jpg" srcset="/img/loading.gif" alt="Ingress TLS"></p><p><img src="https://cdn.oss.link/markdown/2ch1k.jpg" srcset="/img/loading.gif" alt="Ingress TLS Certificate"></p><p><strong>历时 5 个小时鼓捣，到此结束</strong></p>]]></content>
    
    
    <categories>
      
      <category>Kubernetes</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Linux</tag>
      
      <tag>Docker</tag>
      
      <tag>Kubernetes</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>快速部署 kubernetes 高可用集群</title>
    <link href="/2017/03/03/set-up-kubernetes-ha-cluster-by-kargo/"/>
    <url>/2017/03/03/set-up-kubernetes-ha-cluster-by-kargo/</url>
    
    <content type="html"><![CDATA[<blockquote><p>鼓捣 kubernetes 好长一段时间了，kubernetes 1.5 后新增了 kubeadm 工具用于快速部署 kubernetes 集群，不过该工具尚未稳定，无法自动部署高可用集群，而且还存在一些 BUG，所以生产环境还无法使用；本文基于 kargo 工具实现一键部署 kubernetes 容器化(可选) 高可用集群</p></blockquote><h3 id="一、基本环境准备"><a href="#一、基本环境准备" class="headerlink" title="一、基本环境准备"></a>一、基本环境准备</h3><p>本文基本环境如下:</p><p><strong>五台虚拟机，基于 vagrant 启动(穷)，vagrant 配置文件参考 <a href="https://github.com/mritd/config/tree/master/vagrant" target="_blank" rel="noopener">这里</a></strong></p><table><thead><tr><th>主机地址</th><th>节点角色</th></tr></thead><tbody><tr><td>192.168.1.11</td><td>master</td></tr><tr><td>192.168.1.12</td><td>master</td></tr><tr><td>192.168.1.13</td><td>node</td></tr><tr><td>192.168.1.14</td><td>node</td></tr><tr><td>192.168.1.15</td><td>node</td></tr></tbody></table><p>同时保证部署机器对集群内节点拥有 root 免密登录权限，<strong>由于墙的原因，部署所需镜像已经全部打包到百度云，点击 <a href="https://pan.baidu.com/s/1jHMvMn0" target="_blank" rel="noopener">这里</a> 下载，然后进行 load 即可；注意: 直接使用我的 <code>vagrant</code> 文件时，请删除我在 <code>init.sh</code> 脚本里对 docker 设置的本地代理，直接使用可能导致 docker 无法 pull 任何镜像；vagrant 可能需要执行 <code>vagrant plugin install vagrant-hosts</code> 安装插件以支持自动设置 host；如果自己采用其他虚拟机请保证单台虚拟机最低 1.5G 内存，否则会导致安装失败，别问我怎么知道的</strong></p><p><strong>最新更新：经过测试，请使用 pip 安装 ansible，保证 <code>ansible &gt;= 2.2.1</code> &amp;&amp; <code>jinja2 &gt;= 2.8,&lt; 2.9</code>；否则可能出现安装时校验失败问题</strong></p><h3 id="二、搭建集群"><a href="#二、搭建集群" class="headerlink" title="二、搭建集群"></a>二、搭建集群</h3><h4 id="2-1、获取源码"><a href="#2-1、获取源码" class="headerlink" title="2.1、获取源码"></a>2.1、获取源码</h4><p><strong>kargo 是基于 ansible 的 Playbooks 的，其官方推荐的 kargo-cli 目前只适用于各种云平台部署安装，所以我们需要手动使用 Playbooks 部署，当然第一步先把源码搞下来</strong></p><pre><code class="hljs sh">git <span class="hljs-built_in">clone</span> https://github.com/kubernetes-incubator/kargo.git</code></pre><h4 id="2-2、安装-ansible"><a href="#2-2、安装-ansible" class="headerlink" title="2.2、安装 ansible"></a>2.2、安装 ansible</h4><p>既然 kargo 是基于 ansible 的(实际上就是 Playbooks)，那么自然要先安装 ansible，同时下面配置生成会用到 python3，所以也一并安装</p><pre><code class="hljs sh"><span class="hljs-comment"># 安装 python 及 epel</span>yum install -y epel-release python-pip python34 python34-pip<span class="hljs-comment"># 安装 ansible(必须先安装 epel 源再安装 ansible)</span>yum install -y ansible</code></pre><h4 id="2-3、编辑配置文件"><a href="#2-3、编辑配置文件" class="headerlink" title="2.3、编辑配置文件"></a>2.3、编辑配置文件</h4><p><strong>注意：以下配置段中，所有双大括号 <code>{ {</code> 、<code>} }</code>，中间全部加了空格，因为双大括号会跟主题模板引擎产生冲突，默认应该是没有的，请自行 vim 替换</strong></p><p>首先根据自己需要更改 kargo 的配置，配置文件位于 <code>inventory/group_vars/k8s-cluster.yml</code>，<strong>最新稳定版本版本(2.1.0) 配置文件还未更名，全部在 <code>inventory/group_vars/all.yml</code> 中，这里采用最新版本的原因是…借着写博客我也看看更新了啥(偷笑…)</strong></p><pre><code class="hljs sh">vim inventory/group_vars/k8s-cluster.yml<span class="hljs-comment"># 配置文件如下</span><span class="hljs-comment"># 启动集群的基础系统</span>bootstrap_os: centos<span class="hljs-comment"># etcd 数据存放位置</span>etcd_data_dir: /var/lib/etcd<span class="hljs-comment"># 二进制文件将要安装的位置</span>bin_dir: /usr/<span class="hljs-built_in">local</span>/bin<span class="hljs-comment"># Kubernetes 配置文件存放目录以及命名空间</span>kube_config_dir: /etc/kuberneteskube_script_dir: <span class="hljs-string">"&#123; &#123; bin_dir &#125; &#125;/kubernetes-scripts"</span>kube_manifest_dir: <span class="hljs-string">"&#123; &#123; kube_config_dir &#125; &#125;/manifests"</span>system_namespace: kube-system<span class="hljs-comment"># 日志存放位置</span>kube_log_dir: <span class="hljs-string">"/var/log/kubernetes"</span><span class="hljs-comment"># kubernetes 证书存放位置</span>kube_cert_dir: <span class="hljs-string">"&#123; &#123; kube_config_dir &#125; &#125;/ssl"</span><span class="hljs-comment"># token存放位置</span>kube_token_dir: <span class="hljs-string">"&#123; &#123; kube_config_dir &#125; &#125;/tokens"</span><span class="hljs-comment"># basic auth 认证文件存放位置</span>kube_users_dir: <span class="hljs-string">"&#123; &#123; kube_config_dir &#125; &#125;/users"</span><span class="hljs-comment"># 关闭匿名授权</span>kube_api_anonymous_auth: <span class="hljs-literal">false</span><span class="hljs-comment">## 使用的 kubernetes 版本</span>kube_version: v1.5.3<span class="hljs-comment"># 安装过程中缓存文件下载位置(最少 1G)</span>local_release_dir: <span class="hljs-string">"/tmp/releases"</span><span class="hljs-comment"># 重试次数，比如下载失败等情况</span>retry_stagger: 5<span class="hljs-comment"># 证书组</span>kube_cert_group: kube-cert<span class="hljs-comment"># 集群日志等级</span>kube_log_level: 2<span class="hljs-comment"># HTTP 下 api server 密码及用户</span>kube_api_pwd: <span class="hljs-string">"changeme"</span>kube_users:  kube:    pass: <span class="hljs-string">"&#123; &#123;kube_api_pwd&#125; &#125;"</span>    role: admin  root:    pass: <span class="hljs-string">"&#123; &#123;kube_api_pwd&#125; &#125;"</span>    role: admin<span class="hljs-comment"># 网络 CNI 组件 (calico, weave or flannel)</span>kube_network_plugin: calico<span class="hljs-comment"># 服务地址分配</span>kube_service_addresses: 10.233.0.0/18<span class="hljs-comment"># pod 地址分配</span>kube_pods_subnet: 10.233.64.0/18<span class="hljs-comment"># 网络节点大小分配</span>kube_network_node_prefix: 24<span class="hljs-comment"># api server 监听地址及端口</span>kube_apiserver_ip: <span class="hljs-string">"&#123; &#123; kube_service_addresses|ipaddr('net')|ipaddr(1)|ipaddr('address') &#125; &#125;"</span>kube_apiserver_port: 6443 <span class="hljs-comment"># (https)</span>kube_apiserver_insecure_port: 8080 <span class="hljs-comment"># (http)</span><span class="hljs-comment"># 默认 dns 后缀</span>cluster_name: cluster.local<span class="hljs-comment"># Subdomains of DNS domain to be resolved via /etc/resolv.conf for hostnet pods</span>ndots: 2<span class="hljs-comment"># DNS 组件 dnsmasq_kubedns/kubedns</span>dns_mode: dnsmasq_kubedns<span class="hljs-comment"># Can be docker_dns, host_resolvconf or none</span>resolvconf_mode: docker_dns<span class="hljs-comment"># 部署 netchecker 来检测 DNS 和 HTTP 状态</span>deploy_netchecker: <span class="hljs-literal">false</span><span class="hljs-comment"># skydns service IP 配置</span>skydns_server: <span class="hljs-string">"&#123; &#123; kube_service_addresses|ipaddr('net')|ipaddr(3)|ipaddr('address') &#125; &#125;"</span>dns_server: <span class="hljs-string">"&#123; &#123; kube_service_addresses|ipaddr('net')|ipaddr(2)|ipaddr('address') &#125; &#125;"</span>dns_domain: <span class="hljs-string">"&#123; &#123; cluster_name &#125; &#125;"</span><span class="hljs-comment"># docker 存储目录</span>docker_daemon_graph: <span class="hljs-string">"/var/lib/docker"</span><span class="hljs-comment"># docker 的额外配置参数，默认会在 /etc/systemd/system/docker.service.d/ 创建相关配置，如果节点已经安装了 docker，并且做了自己的配置，比如启用了 devicemapper ，那么要更改这里，并把自己的 devicemapper 参数加到这里，因为 kargo 会复写 systemd service 文件，会导致自己在 service 中配置的参数被清空，最后 docker 将无法启动</span><span class="hljs-comment">## A string of extra options to pass to the docker daemon.</span><span class="hljs-comment">## This string should be exactly as you wish it to appear.</span><span class="hljs-comment">## An obvious use case is allowing insecure-registry access</span><span class="hljs-comment">## to self hosted registries like so:</span>docker_options: <span class="hljs-string">"--insecure-registry=&#123; &#123; kube_service_addresses &#125; &#125; --graph=&#123; &#123; docker_daemon_graph &#125; &#125; --iptables=false"</span>docker_bin_dir: <span class="hljs-string">"/usr/bin"</span><span class="hljs-comment"># 组件部署方式</span><span class="hljs-comment"># Settings for containerized control plane (etcd/kubelet/secrets)</span>etcd_deployment_type: dockerkubelet_deployment_type: dockercert_management: scriptvault_deployment_type: docker<span class="hljs-comment"># K8s image pull policy (imagePullPolicy)</span>k8s_image_pull_policy: IfNotPresent<span class="hljs-comment"># Monitoring apps for k8s</span>efk_enabled: <span class="hljs-literal">false</span></code></pre><h4 id="2-4、生成集群配置"><a href="#2-4、生成集群配置" class="headerlink" title="2.4、生成集群配置"></a>2.4、生成集群配置</h4><p>配置完基本集群参数后，还需要生成一个集群配置文件，用于指定需要在哪几台服务器安装，和指定 master、node 节点分布，以及 etcd 集群等安装在那几台机器上</p><pre><code class="hljs sh"><span class="hljs-comment"># 定义集群 IP</span>IPS=(192.168.1.11 192.168.1.12 192.168.1.13 192.168.1.14 192.168.1.15)<span class="hljs-comment"># 生成配置</span><span class="hljs-built_in">cd</span> kargoCONFIG_FILE=inventory/inventory.cfg python3 contrib/inventory_builder/inventory.py <span class="hljs-variable">$&#123;IPS[@]&#125;</span></code></pre><p>生成的配置如下，已经很简单了，怎么改动相信猜也能猜到</p><pre><code class="hljs sh">vim inventory/inventory.cfg[all]node1    ansible_host=192.168.1.11 ip=192.168.1.11node2    ansible_host=192.168.1.12 ip=192.168.1.12node3    ansible_host=192.168.1.13 ip=192.168.1.13node4    ansible_host=192.168.1.14 ip=192.168.1.14node5    ansible_host=192.168.1.15 ip=192.168.1.15[kube-master]node1node2[kube-node]node1node2node3node4node5[etcd]node1node2node3[k8s-cluster:children]kube-nodekube-master[calico-rr]</code></pre><h4 id="2-5、一键部署"><a href="#2-5、一键部署" class="headerlink" title="2.5、一键部署"></a>2.5、一键部署</h4><p>首先启动 vagrant 虚拟机，<strong>不过注意的是本文提供的 vagrant 文件默认安装了 docker，并配置了 devicemapper 和docker 代理，所以使用时上面的 docker 参数需要替换成自己的，因为默认 kargo 会覆盖 docker 的 service 文件；会导致我已经配置完的 docker devicemapper 参数失效，所以要把自己配置的参数加到配置文件中，如下</strong></p><pre><code class="hljs sh">docker_options: <span class="hljs-string">"--insecure-registry=&#123; &#123; kube_service_addresses &#125; &#125; --graph=&#123; &#123; docker_daemon_graph &#125; &#125; --iptables=false --storage-driver=devicemapper --storage-opt=dm.thinpooldev=/dev/mapper/docker-thinpool --storage-opt dm.use_deferred_removal=true --storage-opt=dm.use_deferred_deletion=true"</span></code></pre><p><strong>这个 vagrant 配置文件自动设置了主机名、host、ssh 密钥，实际生产环境仍需自己处理</strong></p><p><strong>每个虚拟机需要自己登陆并生成 ssh-key(ssh-keygen)，因为 ansible 会用到</strong></p><pre><code class="hljs sh"><span class="hljs-comment"># 启动虚拟机</span><span class="hljs-built_in">cd</span> vagrant &amp;&amp; vagrant up<span class="hljs-comment"># 走你(没梯子先 load 好镜像)</span><span class="hljs-built_in">cd</span> kargo <span class="hljs-comment"># 私钥指定的是每个虚拟机 ssh 目录下的私钥</span>ansible-playbook -i inventory/inventory.cfg cluster.yml -b -v --private-key=~/.ssh/id_rsa</code></pre><p>部署成功后截图如下</p><p><img src="https://cdn.oss.link/markdown/ksgtg.jpg" srcset="/img/loading.gif" alt="deploy success"></p><p>相关 pod 启动情况如下 </p><p><img src="https://cdn.oss.link/markdown/c7zaz.jpg" srcset="/img/loading.gif" alt="deploy pods"></p><p><strong>最后附上我部署是的 kargo 配置</strong></p><pre><code class="hljs sh"><span class="hljs-comment"># 启动集群的基础系统</span>bootstrap_os: centos<span class="hljs-comment"># etcd 数据存放位置</span>etcd_data_dir: /var/lib/etcd<span class="hljs-comment"># 二进制文件将要安装的位置</span>bin_dir: /usr/<span class="hljs-built_in">local</span>/bin<span class="hljs-comment"># Kubernetes 配置文件存放目录以及命名空间</span>kube_config_dir: /etc/kuberneteskube_script_dir: <span class="hljs-string">"&#123; &#123; bin_dir &#125; &#125;/kubernetes-scripts"</span>kube_manifest_dir: <span class="hljs-string">"&#123; &#123; kube_config_dir &#125; &#125;/manifests"</span>system_namespace: kube-system<span class="hljs-comment"># 日志存放位置</span>kube_log_dir: <span class="hljs-string">"/var/log/kubernetes"</span><span class="hljs-comment"># kubernetes 证书存放位置</span>kube_cert_dir: <span class="hljs-string">"&#123; &#123; kube_config_dir &#125; &#125;/ssl"</span><span class="hljs-comment"># token存放位置</span>kube_token_dir: <span class="hljs-string">"&#123; &#123; kube_config_dir &#125; &#125;/tokens"</span><span class="hljs-comment"># basic auth 认证文件存放位置</span>kube_users_dir: <span class="hljs-string">"&#123; &#123; kube_config_dir &#125; &#125;/users"</span><span class="hljs-comment"># 关闭匿名授权</span>kube_api_anonymous_auth: <span class="hljs-literal">false</span><span class="hljs-comment">## 使用的 kubernetes 版本</span>kube_version: v1.5.3<span class="hljs-comment"># 安装过程中缓存文件下载位置(最少 1G)</span>local_release_dir: <span class="hljs-string">"/tmp/releases"</span><span class="hljs-comment"># 重试次数，比如下载失败等情况</span>retry_stagger: 5<span class="hljs-comment"># 证书组</span>kube_cert_group: kube-cert<span class="hljs-comment"># 集群日志等级</span>kube_log_level: 2<span class="hljs-comment"># HTTP 下 api server 密码及用户</span>kube_api_pwd: <span class="hljs-string">"test123"</span>kube_users:  kube:    pass: <span class="hljs-string">"&#123; &#123;kube_api_pwd&#125; &#125;"</span>    role: admin  root:    pass: <span class="hljs-string">"&#123; &#123;kube_api_pwd&#125; &#125;"</span>    role: admin<span class="hljs-comment"># 网络 CNI 组件 (calico, weave or flannel)</span>kube_network_plugin: calico<span class="hljs-comment"># 服务地址分配</span>kube_service_addresses: 10.233.0.0/18<span class="hljs-comment"># pod 地址分配</span>kube_pods_subnet: 10.233.64.0/18<span class="hljs-comment"># 网络节点大小分配</span>kube_network_node_prefix: 24<span class="hljs-comment"># api server 监听地址及端口</span>kube_apiserver_ip: <span class="hljs-string">"&#123; &#123; kube_service_addresses|ipaddr('net')|ipaddr(1)|ipaddr('address') &#125; &#125;"</span>kube_apiserver_port: 6443 <span class="hljs-comment"># (https)</span>kube_apiserver_insecure_port: 8080 <span class="hljs-comment"># (http)</span><span class="hljs-comment"># 默认 dns 后缀</span>cluster_name: cluster.local<span class="hljs-comment"># Subdomains of DNS domain to be resolved via /etc/resolv.conf for hostnet pods</span>ndots: 2<span class="hljs-comment"># DNS 组件 dnsmasq_kubedns/kubedns</span>dns_mode: dnsmasq_kubedns<span class="hljs-comment"># Can be docker_dns, host_resolvconf or none</span>resolvconf_mode: docker_dns<span class="hljs-comment"># 部署 netchecker 来检测 DNS 和 HTTP 状态</span>deploy_netchecker: <span class="hljs-literal">true</span> <span class="hljs-comment"># skydns service IP 配置</span>skydns_server: <span class="hljs-string">"&#123; &#123; kube_service_addresses|ipaddr('net')|ipaddr(3)|ipaddr('address') &#125; &#125;"</span>dns_server: <span class="hljs-string">"&#123; &#123; kube_service_addresses|ipaddr('net')|ipaddr(2)|ipaddr('address') &#125; &#125;"</span>dns_domain: <span class="hljs-string">"&#123; &#123; cluster_name &#125; &#125;"</span><span class="hljs-comment"># docker 存储目录</span>docker_daemon_graph: <span class="hljs-string">"/var/lib/docker"</span><span class="hljs-comment"># docker 的额外配置参数，默认会在 /etc/systemd/system/docker.service.d/ 创建相关配置，如果节点已经安装了 docker，并且做了自己的配置，比如启用的 device mapper ，那么要删除/更改这里，防止冲突导致 docker 无法启动</span><span class="hljs-comment">## A string of extra options to pass to the docker daemon.</span><span class="hljs-comment">## This string should be exactly as you wish it to appear.</span><span class="hljs-comment">## An obvious use case is allowing insecure-registry access</span><span class="hljs-comment">## to self hosted registries like so:</span>docker_options: <span class="hljs-string">"--insecure-registry=&#123; &#123; kube_service_addresses &#125; &#125; --graph=&#123; &#123; docker_daemon_graph &#125; &#125; --iptables=false --storage-driver=devicemapper --storage-opt=dm.thinpooldev=/dev/mapper/docker-thinpool --storage-opt dm.use_deferred_removal=true --storage-opt=dm.use_deferred_deletion=true"</span>docker_bin_dir: <span class="hljs-string">"/usr/bin"</span><span class="hljs-comment"># 组件部署方式</span><span class="hljs-comment"># Settings for containerized control plane (etcd/kubelet/secrets)</span>etcd_deployment_type: dockerkubelet_deployment_type: dockercert_management: scriptvault_deployment_type: docker<span class="hljs-comment"># K8s image pull policy (imagePullPolicy)</span>k8s_image_pull_policy: IfNotPresent<span class="hljs-comment"># Monitoring apps for k8s</span>efk_enabled: <span class="hljs-literal">true</span></code></pre>]]></content>
    
    
    <categories>
      
      <category>Kubernetes</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Linux</tag>
      
      <tag>Docker</tag>
      
      <tag>Kubernetes</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Vagrant 使用</title>
    <link href="/2017/03/01/how-to-use-vagrant/"/>
    <url>/2017/03/01/how-to-use-vagrant/</url>
    
    <content type="html"><![CDATA[<blockquote><p>Vagrant 是一个开源的 基于 ruby 的开源虚拟机管理工具；最近在鼓捣 kubernetes ，常常需要做集群部署测试，由于比较穷 😂😂😂；所以日常测试全部是自己开虚拟机；每次使用 VirtualBox开5个虚拟机很烦，而且为了保证环境干净不受其他因素影响，所以每次测试都是新开…..每次都会有种 WTF 的感觉，所以研究了一下 Vagrant 这个工具，发现很好用，一下记录一下简单的使用</p></blockquote><h3 id="一、Vagrant-介绍"><a href="#一、Vagrant-介绍" class="headerlink" title="一、Vagrant 介绍"></a>一、Vagrant 介绍</h3><p>上面已经简单的说了一下 Vagrant，Vagrant 定位为一个虚拟机管理工具；它能够以脚本化的方式启动、停止、和和删除虚拟机，当然这些手动也没费劲；更重要的是它能够自己定义网络分配、初始化执行的脚本、添加硬盘等各种复杂的动作；最重要的是 Vagrant 提供了类似于 docker image 的 box；Vagrant Box 就是一个完整的虚拟机分发包，可以自己制作也可以从网络下载；并且 Vagrant 开源特性使得各路大神开发了很多 Vagrant 插件方便我们使用，基于以上这些特点，我们可以实现:</p><ul><li>一个脚本定义好虚拟机的数量</li><li>一个脚本定义好虚拟机初始化工作，比如装 docker</li><li>一个脚本完成多台虚拟机网络配置</li><li>一条命令启动、停止、删除多个虚拟机</li><li>更多玩法自行摸索…..</li></ul><h3 id="二、Vagrant-使用"><a href="#二、Vagrant-使用" class="headerlink" title="二、Vagrant 使用"></a>二、Vagrant 使用</h3><h4 id="2-1、Vagrant-安装"><a href="#2-1、Vagrant-安装" class="headerlink" title="2.1、Vagrant 安装"></a>2.1、Vagrant 安装</h4><p>Vagrant 安装极其简单，目前官方已经打包好了各个平台的安装包文件，地址访问 <a href="https://www.vagrantup.com/downloads.html" target="_blank" rel="noopener">Vagrant 官方下载地址</a>；截图如下</p><p><img src="https://cdn.oss.link/markdown/m46fa.jpg" srcset="/img/loading.gif" alt="vagrant download"></p><p>以下为 CentOS 上的安装命令</p><pre><code class="hljs sh">wget https://releases.hashicorp.com/vagrant/1.9.2/vagrant_1.9.2_x86_64.rpmrpm -ivh vagrant_1.9.2_x86_64.rpm</code></pre><h4 id="2-2、Vagrant-Box-下载"><a href="#2-2、Vagrant-Box-下载" class="headerlink" title="2.2、Vagrant Box 下载"></a>2.2、Vagrant Box 下载</h4><p>装虚拟机大家都不陌生，首先应该搞个系统镜像；同样 Vagrant 也需要先搞一个 Vagrant Box，Vagrant Box 是一个已经预装好操作系统的虚拟机打包文件；根据不同系统可以选择不同的 Vagrant Box，官方维护了一个 Vagrant Box 仓库，地址 <a href="https://atlas.hashicorp.com/boxes/search" target="_blank" rel="noopener">点这里</a>，截图如下</p><p><img src="https://cdn.oss.link/markdown/2duz7.jpg" srcset="/img/loading.gif" alt="vagrant boxes"></p><p>点击对应的系统后可以看到如下界面</p><p><img src="https://cdn.oss.link/markdown/kiohr.jpg" srcset="/img/loading.gif" alt="box detail"></p><p>该页面罗列出了使用不同虚拟机时应当使用扥添加明令；当然执行这些命令后 vagrant 将会从网络下载这个 box 文件并添加到本地 box 仓库；<strong>不过众所周知的原因，这个下载速度会让你怀疑人生，所有简单的办法是执行以下这条命令，然后会显示 box 的实际下载地址；拿到地址以后用梯子先把文件下载下来，然后使用 vagrant 导入也可以(centos7 本地已经有了一下以 ubuntu 为例)</strong></p><p><img src="https://cdn.oss.link/markdown/p36th.jpg" srcset="/img/loading.gif" alt="box download url"><br>下载后使用 <code>vagrant box add xxxx.box</code> 即可将 box 导入到本地仓库</p><h4 id="2-3、启动一个虚拟机"><a href="#2-3、启动一个虚拟机" class="headerlink" title="2.3、启动一个虚拟机"></a>2.3、启动一个虚拟机</h4><p>万事俱备只差东风，在上一步执行 <code>vagrant init ubuntu/trusty64; vagrant up --provider virtualbox</code> 命令获取 box 下载地址时，已经在当前目录下生成了一个 Vagrantfile 文件，这个文件其实就是虚拟机配置文件，具体下面再说；box 导入以后先启动一下再说，执行 <code>vagrnat up</code> 即可</p><p>其他几个常用命令如下</p><ul><li><code>vagrant box [list|add|remove]</code> 查看添加删除 box 等</li><li><code>vagrant up</code> 启动虚拟机</li><li><code>vagrant halt</code> 关闭虚拟机</li><li><code>vagrant init</code> 初始化一个指定系统的 Vagrantfile 文件</li><li><code>vagrant destroy</code> 删除虚拟机</li><li><code>vagrant ssh</code> ssh 到虚拟机里</li></ul><p><strong>特别说明一下 ssh 这个命令，一般默认的规范是 <code>vagrant ssh VM_NAME</code> 后，会以 vagrant 用户身份登录到目标虚拟机，如果当前目录的 Vagrantfile 中只有一个虚拟机那么无需指定虚拟机名称(init 后默认就是)；虚拟机内(box 封装时)vagrant这个用户拥有全局免密码 sudo 权限；root 用户一般密码为 vagrant</strong></p><h3 id="三、Vagrantfile"><a href="#三、Vagrantfile" class="headerlink" title="三、Vagrantfile"></a>三、Vagrantfile</h3><blockquote><p>我发现基本国内所有的 Vagrant 教程都是简单的提了一嘴那几个常用命令；包括我上面也写了点，估计可能到这已经被喷了(“妈的那几个命令老子 help 一下就出来了，一看一猜就知道啥意思 用得着你讲？”)；个人觉得 Vagrant 最复杂的是这个配置文件，以下直接上一个目前仓库里的做示例，仓库地址 <a href="https://github.com/mritd/config/tree/master/vagrant" target="_blank" rel="noopener">戳这里</a></p></blockquote><p><strong>直接贴 Vagrantfile，以下配置在进行 <code>vagrant up</code> 之前可能需要使用 <code>vagrant plugin install vagrant-host</code> 插件，以支持自动在各节点之间添加 host</strong></p><pre><code class="hljs sh">Vagrant.configure(<span class="hljs-string">"2"</span>) <span class="hljs-keyword">do</span> |config|    <span class="hljs-comment"># 定义虚拟机数量</span>    vms = Array(1..5)    <span class="hljs-comment"># 数据盘存放目录</span>    <span class="hljs-variable">$data_base_dir</span> = <span class="hljs-string">"/data/vm/disk"</span>    vms.each <span class="hljs-keyword">do</span> |i|        config.vm.define <span class="hljs-string">"docker#&#123;i&#125;"</span> <span class="hljs-keyword">do</span> |docker|            <span class="hljs-comment"># 设置虚拟机的Box</span>            docker.vm.box = <span class="hljs-string">"centos/7"</span>            <span class="hljs-comment"># 不检查 box 更新</span>            docker.vm.box_check_update = <span class="hljs-literal">false</span>             <span class="hljs-comment"># 设置虚拟机的主机名</span>            docker.vm.hostname=<span class="hljs-string">"docker#&#123;i&#125;.node"</span>            <span class="hljs-comment"># 设置虚拟机的IP (wlp2s0 为桥接本机的网卡)</span>            docker.vm.network <span class="hljs-string">"public_network"</span>, ip: <span class="hljs-string">"192.168.1.1#&#123;i&#125;"</span>, bridge: <span class="hljs-string">"wlp2s0"</span>            <span class="hljs-comment"># 设置主机与虚拟机的共享目录</span>            <span class="hljs-comment">#docker.vm.synced_folder "~/Desktop/share", "/home/vagrant/share"</span>            <span class="hljs-comment"># VirtaulBox相关配置</span>            docker.vm.provider <span class="hljs-string">"virtualbox"</span> <span class="hljs-keyword">do</span> |v|                <span class="hljs-comment"># 设置虚拟机的名称</span>                v.name = <span class="hljs-string">"docker#&#123;i&#125;"</span>                <span class="hljs-comment"># 设置虚拟机的内存大小  </span>                v.memory = 1536                 <span class="hljs-comment"># 设置虚拟机的CPU个数</span>                v.cpus = 1                <span class="hljs-comment"># 增加磁盘</span>                docker_disk = <span class="hljs-string">"#<span class="hljs-variable">$data_base_dir</span>/docker-disk#&#123;i&#125;.vdi"</span>                data_disk = <span class="hljs-string">"#<span class="hljs-variable">$data_base_dir</span>/data-disk#&#123;i&#125;.vdi"</span>                <span class="hljs-comment"># 判断虚拟机启动后</span>                <span class="hljs-keyword">if</span> ARGV[0] == <span class="hljs-string">"up"</span>                    <span class="hljs-comment"># 如果两个文件都不存在 则创建 SATA 控制器(这里调用的是 Virtual Box 的命令)</span>                    <span class="hljs-keyword">if</span> ! File.exist?(docker_disk) &amp;&amp; ! File.exist?(data_disk)                        v.customize [                            <span class="hljs-string">'storagectl'</span>, :id,                            <span class="hljs-string">'--name'</span>, <span class="hljs-string">'SATA Controller'</span>,                            <span class="hljs-string">'--add'</span>, <span class="hljs-string">'sata'</span>,                            <span class="hljs-string">'--portcount'</span>, <span class="hljs-string">'5'</span>,                            <span class="hljs-string">'--controller'</span>, <span class="hljs-string">'IntelAhci'</span>,                            <span class="hljs-string">'--bootable'</span>, <span class="hljs-string">'on'</span>                        ]                    end                    <span class="hljs-comment"># 创建磁盘文件</span>                    <span class="hljs-keyword">if</span> ! File.exist?(docker_disk)                        v.customize [                            <span class="hljs-string">'createhd'</span>,                             <span class="hljs-string">'--filename'</span>, docker_disk,                             <span class="hljs-string">'--format'</span>, <span class="hljs-string">'VDI'</span>,                             <span class="hljs-string">'--size'</span>, 10 * 1024 <span class="hljs-comment"># 10 GB</span>                        ]                     end                    <span class="hljs-keyword">if</span> ! File.exist?(data_disk)                        v.customize [                            <span class="hljs-string">'createhd'</span>,                             <span class="hljs-string">'--filename'</span>, data_disk,                             <span class="hljs-string">'--format'</span>, <span class="hljs-string">'VDI'</span>,                             <span class="hljs-string">'--size'</span>, 10 * 1024 <span class="hljs-comment"># 10 GB</span>                        ]                     end                    <span class="hljs-comment"># 连接到 SATA 控制器</span>                    v.customize [                        <span class="hljs-string">'storageattach'</span>, :id,                         <span class="hljs-string">'--storagectl'</span>, <span class="hljs-string">'SATA Controller'</span>,                         <span class="hljs-string">'--port'</span>, 1, <span class="hljs-string">'--device'</span>, 0,                         <span class="hljs-string">'--type'</span>, <span class="hljs-string">'hdd'</span>, <span class="hljs-string">'--medium'</span>,                         docker_disk                    ]                    v.customize [                        <span class="hljs-string">'storageattach'</span>, :id,                         <span class="hljs-string">'--storagectl'</span>, <span class="hljs-string">'SATA Controller'</span>,                         <span class="hljs-string">'--port'</span>, 2, <span class="hljs-string">'--device'</span>, 0,                         <span class="hljs-string">'--type'</span>, <span class="hljs-string">'hdd'</span>, <span class="hljs-string">'--medium'</span>,                         data_disk                    ]                end            end            <span class="hljs-comment"># 增加各节点 host 配置</span>            config.vm.provision :hosts <span class="hljs-keyword">do</span> |provisioner|                vms.each <span class="hljs-keyword">do</span> |x|                    provisioner.add_host <span class="hljs-string">"192.168.1.1#&#123;x&#125;"</span>, [<span class="hljs-string">"docker#&#123;x&#125;.node"</span>]                end            end            <span class="hljs-comment"># 自定义执行脚本</span>            docker.vm.provision <span class="hljs-string">"shell"</span>, path: <span class="hljs-string">"init.sh"</span>            <span class="hljs-comment"># 每次开机后重启 network 和 ssh，解决公网网卡不启动问题 </span>            docker.vm.provision <span class="hljs-string">"shell"</span>, run: <span class="hljs-string">"always"</span>, inline: &lt;&lt;-SHELL                systemctl restart network                systemctl restart sshd                <span class="hljs-built_in">echo</span> -e <span class="hljs-string">"\033[32mvirtual machine docker#&#123;i&#125; init success!\033[0m"</span>            SHELL        end    endend</code></pre><p>以上基本都加了注释，所以大致应该很清晰，至于第一行那个 <code>Vagrant.configure(&quot;2&quot;)</code> 代表调用第二版 API，不能改动，其他的可参考注释同时综合仓库中的其他配置文件即可</p><p><strong>Vagrantfile 实质上就是一个 ruby 文件，可以自己在里面定义变量等，可以在里面按照 ruby 的语法进行各种复杂的操作；具体 ruby 语法可以参考相关文档学习一下</strong></p>]]></content>
    
    
    <categories>
      
      <category>Linux</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Linux</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>如何下载 Kubernetes 镜像和 rpm</title>
    <link href="/2017/02/27/how-to-download-kubernetes-images-and-rpm/"/>
    <url>/2017/02/27/how-to-download-kubernetes-images-and-rpm/</url>
    
    <content type="html"><![CDATA[<blockquote><p>随着 kubernetes 容器化部署逐渐推进，gcr.io 镜像、kubernetes rpm 下载由于 “伟大的” 墙的原因成为阻碍玩 kubernetes 第一道屏障，以下记录了个人维护的 yum 仓库和 gcr.io 反代仓库使用</p></blockquote><h3 id="一、yum-源"><a href="#一、yum-源" class="headerlink" title="一、yum 源"></a>一、yum 源</h3><p>目前个人维护了一个 kubernetes 的 yum 源，目前 yum 源包含 rpm 如下</p><table><thead><tr><th>rpm 包</th><th>版本</th></tr></thead><tbody><tr><td>etcd</td><td>3.1.0-1.x86_64</td></tr><tr><td>flannel</td><td>0.7.0-1.x86_64</td></tr><tr><td>kubernetes</td><td>1.5.3-1.x86_64</td></tr><tr><td>kubeadm</td><td>1.6.0-0.alpha.0.2074.a092d8e0f95f52.x86_64</td></tr><tr><td>kubectl</td><td>1.5.3-0.x86_64</td></tr><tr><td>kubelet</td><td>1.5.3-0.x86_64</td></tr><tr><td>kubernetes-cni</td><td>0.3.0.1-0.07a8a2.x86_64</td></tr></tbody></table><p>使用方法如下</p><pre><code class="hljs sh"><span class="hljs-comment"># 添加 yum 源</span>tee /etc/yum.repos.d/mritd.repo &lt;&lt; EOF[mritd]name=Mritd Repositorybaseurl=https://yum.mritd.me/centos/7/x86_64enabled=1gpgcheck=1gpgkey=https://cdn.oss.link/keys/rpm.public.keyEOF<span class="hljs-comment"># 刷新cache</span>yum makecache<span class="hljs-comment"># 安装 yum-utils</span>yum install -y yum-utils socat <span class="hljs-comment"># 下载 rpm 到本地</span>yumdownloader kubelet kubectl kubernetes-cni kubeadm<span class="hljs-comment"># 安装 rpm</span>rpm -ivh kube*.rpm</code></pre><p><strong>所有关于 yum 源地址变更等都将在 <a href="https://yum.mritd.me" target="_blank" rel="noopener">https://yum.mritd.me</a> 页面公告，如出现不能使用请访问此页面查看相关原因</strong>；<strong>如果实在下载过慢可以将 <code>yum.mritd.me</code> 替换成 <code>yumrepo.b0.upaiyun.com</code>，此域名 yum 源在 CDN 上，由于流量有限，请使用 yumdownloader 工具下载到本地分发安装，谢谢</strong></p><h3 id="二、kubernetes-镜像"><a href="#二、kubernetes-镜像" class="headerlink" title="二、kubernetes 镜像"></a>二、kubernetes 镜像</h3><p>关于 kubernetes 镜像下载，一般有三种方式：</p><ul><li>直接从国外服务器 pull 然后 save 出来传到本地</li><li>通过第三方仓库做中转，如 Docker hub</li><li>在本地/国外能访问的服务器通过官方 registry 加代理反代 gcr.io</li></ul><p><strong>个人在国外服务器上维护了一个 gcr.io 的反代仓库，使用方式如下</strong></p><pre><code class="hljs sh">docker pull gcr.mritd.me/google_containers/kube-apiserver-amd64:v1.5.3</code></pre><p>如果对于 gcr.mritd.me 访问过慢可参考 <a href="https://mritd.me/2017/02/09/gcr.io-registy-proxy/" target="_blank" rel="noopener">gcr.io 仓库代理</a> 使用带有梯子的本地私服，如果使用 Docker Hub 等中转可参考 <a href="https://mritd.me/2016/10/29/set-up-kubernetes-cluster-by-kubeadm/#22%E9%95%9C%E5%83%8F%E4%BB%8E%E5%93%AA%E6%9D%A5" target="_blank" rel="noopener">kubeadm 搭建 kubernetes 集群</a></p>]]></content>
    
    
    <categories>
      
      <category>Kubernetes</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Linux</tag>
      
      <tag>Docker</tag>
      
      <tag>Kubernetes</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Jekyll + Travis CI 自动化部署博客</title>
    <link href="/2017/02/25/jekyll-blog-+-travis-ci-auto-deploy/"/>
    <url>/2017/02/25/jekyll-blog-+-travis-ci-auto-deploy/</url>
    
    <content type="html"><![CDATA[<blockquote><p>由于 Github 访问过慢，所以博客一直放在自己的服务器上托管；博客采用了 Jekyll 生成静态展点，最近鼓捣了一下完成了 Travis CI 自动化部署，顺便在此记录下</p></blockquote><h3 id="一、原部署方式"><a href="#一、原部署方式" class="headerlink" title="一、原部署方式"></a>一、原部署方式</h3><h4 id="1-1、原部署流程"><a href="#1-1、原部署流程" class="headerlink" title="1.1、原部署流程"></a>1.1、原部署流程</h4><p>由于博客访问量不大，同时 jekyll 启动后会开启 http 服务器，还能自动监听文件变化实现刷新；所以以前的方式就是打了一个 docker 镜像，然后镜像每隔 15 分钟拉取 Github 仓库，实现定时更新，基本流程如下</p><ul><li>本地写博客 Markdown 文件 commit 到 Github</li><li>服务器上将 jekyll 打成 docker 镜像启动</li><li>服务器镜像内使用 crond 每隔 15 分钟拉取最新代码</li><li>jekyll 获得最新代码后自动刷新</li></ul><p>整体 “架构” 如下</p><p><img src="https://cdn.oss.link/markdown/44qmr.jpg" srcset="/img/loading.gif" alt="老架构"></p><h4 id="1-2、存在问题"><a href="#1-2、存在问题" class="headerlink" title="1.2、存在问题"></a>1.2、存在问题</h4><p>按照以前的方式其实存在一个很大问题就是部署不及时，每次写完文章实际上都是自己 ssh 到服务器手动 pull 一下，感觉很繁琐；另一个大问题(这也不能算 bug) jekyll 的 rss 插件默认生成的 rss 引用地址为 <code>jekyll server -H x.x.x.x</code> 的监听地址，而容器化启动 jekyll 监听地址必然是 <code>0.0.0.0</code>；后果就是 feed.xml 无法访问，如下所示(这里监听的是 localhost)</p><p><img src="https://cdn.oss.link/markdown/fq9im.jpg" srcset="/img/loading.gif" alt="feed error"></p><h4 id="1-3、新部署思路"><a href="#1-3、新部署思路" class="headerlink" title="1.3、新部署思路"></a>1.3、新部署思路</h4><p>从问题角度上来说，每次手动 pull 虽然有点烦，但是并不是大问题；而 rss 订阅由于网友反馈，再加个人强迫症感觉确实是个大毛病；随着试验发现，<strong>在进行 <code>jekyll build</code> 时生成的 feed.xml 中的引用地址会正确读取 _config.yml 中的网址</strong>；而 <code>jekyll server</code> 命令实际上也是先 build，然后生成静态文件到 <code>_site</code> 目录，最后搞个 http 服务器发布出去</p><p>基于以上试验可以得到一个简单的解决方案：不使用 <code>jekyll server</code> 启动，先 build 生成正确的 feed.xml 等静态文件，然后自己搞个 nginx 把它发布出去</p><h3 id="二、Travis-CI-自动化部署"><a href="#二、Travis-CI-自动化部署" class="headerlink" title="二、Travis CI 自动化部署"></a>二、Travis CI 自动化部署</h3><h4 id="2-1、任务拆分"><a href="#2-1、任务拆分" class="headerlink" title="2.1、任务拆分"></a>2.1、任务拆分</h4><p>从上面的结论上基本要实现自动化部署需要以下步骤：</p><ul><li>Github commit 后要能自动 build</li><li>生成的 _site 目录文件能实时更新到容器</li></ul><h4 id="2-2、Travis-CI-配置"><a href="#2-2、Travis-CI-配置" class="headerlink" title="2.2、Travis CI 配置"></a>2.2、Travis CI 配置</h4><h5 id="2-2-1、基本配置"><a href="#2-2-1、基本配置" class="headerlink" title="2.2.1、基本配置"></a>2.2.1、基本配置</h5><p>对于自动 build，好在 Travis CI 对于开源项目完全免费，并且能自动感知到 Github 的 commit；所以自动 build 生成 静态文件这个过程就由 Travis CI 完成，以下为配置过程</p><p>首先注册好 Travis CI 账号，然后点击最左侧 <code>+</code> 按钮添加项目</p><p><img src="https://cdn.oss.link/markdown/7axvx.jpg" srcset="/img/loading.gif" alt="add repo"></p><p>在想要使用 Travis CI 的项目上开启 build</p><p><img src="https://cdn.oss.link/markdown/ouod9.jpg" srcset="/img/loading.gif" alt="open"></p><p>点击设置按钮设置一下项目</p><p><img src="https://cdn.oss.link/markdown/p1cad.jpg" srcset="/img/loading.gif" alt="set options"></p><h5 id="2-2-2、-travis-yml-配置"><a href="#2-2-2、-travis-yml-配置" class="headerlink" title="2.2.2、.travis.yml 配置"></a>2.2.2、.travis.yml 配置</h5><p>当项目内存在 <code>.travis.yml</code> 文件时，Travis CI 会按照其定义完成自动 build 过程，所以开启了上述配置以后还要在项目下创建 <code>.travis.yml</code> 配置文件，配置文件定义如下</p><pre><code class="hljs sh">language: rubyrvm:  - 2.3.3before_install:  - openssl aes-256-cbc -K <span class="hljs-variable">$encrypted_ecabfac08d8e_key</span> -iv <span class="hljs-variable">$encrypted_ecabfac08d8e_iv</span>    -<span class="hljs-keyword">in</span> id_rsa.enc -out ~/.ssh/id_rsa -d  - chmod 600 ~/.ssh/id_rsa  - <span class="hljs-built_in">echo</span> -e <span class="hljs-string">"Host mritd.me\n\tStrictHostKeyChecking no\n"</span> &gt;&gt; ~/.ssh/configscript:  - bundle install  - bundle <span class="hljs-built_in">exec</span> jekyll buildafter_success:  - git <span class="hljs-built_in">clone</span> https://github.com/mritd/mritd.me.git  - <span class="hljs-built_in">cd</span> mritd.me &amp;&amp; rm -rf * &amp;&amp; cp -r ../_site/* .  - git config user.name <span class="hljs-string">"mritd"</span>  - git config user.email <span class="hljs-string">"mritd1234@gmail.com"</span>  - git add --all .  - git commit -m <span class="hljs-string">"Travis CI Auto Builder"</span>  - git push --force https://<span class="hljs-variable">$DEPLOY_TOKEN</span>@github.com/mritd/mritd.me.git master  - ssh root@mritd.me <span class="hljs-string">"docker restart mritd_jekyll_1"</span>branches:  only:  - masterenv:  global:  - NOKOGIRI_USE_SYSTEM_LIBRARIES=<span class="hljs-literal">true</span></code></pre><p>其中 <code>language</code>声明使用的语言，<code>rvm</code> 是 ruby 的管理工具，并定义了 ruby 版本；<code>before_install</code> 定义了执行前的预处理动作，上面增加了一个密钥用于远程登录服务器；<code>script</code> 段真正定义了编译过程的命令，<code>after_success</code> 定义了如何 build 后如何处理发布物，<code>branches</code> 指定有哪些分支变动后触发 CI build，env 是一些环境变量，上面添加了一个变量(根据 jekyll 官方文档)用于加速 jekyll 编译(有些配置不理解往下看)</p><p><strong>具体 <code>.travis.yml</code> 配置请参考 <a href="https://docs.travis-ci.com/" target="_blank" rel="noopener">官方文档</a></strong></p><h4 id="2-3、静态文件的自动更新"><a href="#2-3、静态文件的自动更新" class="headerlink" title="2.3、静态文件的自动更新"></a>2.3、静态文件的自动更新</h4><p>Travis CI 在完成 build 后会在 <code>_site</code> 目录生成博客的静态文件，而如如何将这些静态文件发送到服务器完成更新是个待解决的问题</p><h5 id="2-3-1、解决思路"><a href="#2-3-1、解决思路" class="headerlink" title="2.3.1、解决思路"></a>2.3.1、解决思路</h5><p>将 Travis CI 生成的静态文件推送到 Github，博客仍然 docker 化部署，采用 <code>nginx</code> + <code>静态文件</code> 方式；每次容器启动后都要从 Github pull 最新的静态文件，流程如下</p><ul><li>本地提交博客 Markdown 文件 到 Github</li><li>Github 触发 Travis CI 执行自动编译</li><li>Travis CI 编译后 push 静态文件到 Github</li><li>Travis CI 通知服务器重启博客容器</li><li>容器重启拉取最新静态文件完成更新</li></ul><p>流程图如下</p><p><img src="https://cdn.oss.link/markdown/8tro9.jpg" srcset="/img/loading.gif" alt="new auto deploy"></p><h5 id="2-3-2、实现方法"><a href="#2-3-2、实现方法" class="headerlink" title="2.3.2、实现方法"></a>2.3.2、实现方法</h5><p>其实主要问题是从上面第三步开始：</p><p>Travis CI push 静态文件到 Github 通过 Github 的 token<br>实现授权，代码如下</p><pre><code class="hljs sh">after_success:  - git <span class="hljs-built_in">clone</span> https://github.com/mritd/mritd.me.git  - <span class="hljs-built_in">cd</span> mritd.me &amp;&amp; rm -rf * &amp;&amp; cp -r ../_site/* .  - git config user.name <span class="hljs-string">"mritd"</span>  - git config user.email <span class="hljs-string">"mritd1234@gmail.com"</span>  - git add --all .  - git commit -m <span class="hljs-string">"Travis CI Auto Builder"</span>  - git push --force https://<span class="hljs-variable">$DEPLOY_TOKEN</span>@github.com/mritd/mritd.me.git master</code></pre><p><code>$DEPLOY_TOKEN</code> 是从 Github 授权得到的，然后给于相应权限即可</p><p><img src="https://cdn.oss.link/markdown/pco7k.jpg" srcset="/img/loading.gif" alt="Github token"></p><p><strong>关于代码中 <code>$DEPLOY_TOKEN</code> 这种重要的密码类变量，Travis CI 在每个项目下提供了设置环境变量功能，如下图</strong></p><p><img src="https://cdn.oss.link/markdown/7zmj2.jpg" srcset="/img/loading.gif" alt="setting"></p><p><img src="https://cdn.oss.link/markdown/0b91x.jpg" srcset="/img/loading.gif" alt="add env"></p><p><strong>设置后可在 <code>.travis.yml</code> 中直接引用，不过注意一定要关闭 <code>Display value in build log</code> 功能，否则 CI log 中会显示 <code>export XXXX=XXXX</code> 这种 log 从而暴露重要密码(公有项目的 log 别人可以查看的)；如果开启了那么尽快找到相应 build 并删除 log 日志，如下</strong></p><p><img src="https://cdn.oss.link/markdown/kal69.jpg" srcset="/img/loading.gif" alt="delete log"></p><p>在成功 push 了静态文件以后，就要实现服务器的自动更新，自动更新很简单，只需要写个脚本让容器启动后自动 pull 即可，这里不再阐述；下面说一下怎么通知服务器重启容器，这里的思路很简单，<strong>让 CI ssh 上去执行一些 docker 命令即可</strong>；但是有个很大问题是 <strong>SSH 密码怎么整？</strong></p><p><strong>Travis CI 提供了存放加密文件的方式，文档见 <a href="https://docs.travis-ci.com/user/encrypting-files/" target="_blank" rel="noopener">这里</a></strong></p><p>简单的说就是将你的 ssh 私钥加密以后扔进去即可，按照稳当的步骤很简单：</p><ul><li>先安装 ruby 环境，然后用 gem 装 travis (gem install travis)</li><li>然后登陆 travis (travis login)</li><li>登陆后加密文件 (travis encrypt-file xxxx)，注意要在 <code>.travis.yml</code> 同级目录下执行，待加密文件可在任意位置</li><li>travis 会在 <code>.travis.yml</code> 写入 <code>before_install</code> 段解密还原回该文件，如果是 ssh 密钥的话参考上面的配置再改一下权限即可</li></ul><p>最后一个小问题是可能会有主机信任问题，因为 CI 服务器第一次连接我的服务器会出现 ssh 主机验证，官方给出的做法是添加 addons 配置</p><pre><code class="hljs sh">addons:  ssh_known_hosts: mritd.me</code></pre><h3 id="三、一些想法"><a href="#三、一些想法" class="headerlink" title="三、一些想法"></a>三、一些想法</h3><p>这只是一个小博客，所以服务中断以下无所谓；如果大型部署肯定不会直接重启容器，至少应该是 k8s 滚动升级等措施；与服务器通讯也不应该采用 ssh 方式，虽然 Travis CI 做了加密，但是总感觉不那么稳妥，最好应该写个程序开放一个 REST 接口，并做好授权，必要的话需要开启一次性认证令牌那种方式，其他的后续接着优化 ( :</p>]]></content>
    
    
    <categories>
      
      <category>Linux</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Linux</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Java CAS 理解</title>
    <link href="/2017/02/06/java-cas/"/>
    <url>/2017/02/06/java-cas/</url>
    
    <content type="html"><![CDATA[<blockquote><p>CAS(Compare and Swap) 是利用底层硬件平台特性，实现原子性操作的算法，Java 1.5 以后 JUC(java.util.concurrent) 实现主要以此为基础；找了不少资料以下记录一下个人对于 CAS 的理解(部分资料 copy 的)</p></blockquote><h3 id="一、CAS-简述"><a href="#一、CAS-简述" class="headerlink" title="一、CAS 简述"></a>一、CAS 简述</h3><p>从最基础的 Java 中的 <code>i++</code> 操作来说，<code>i++</code> 并非原子操作，实质上相当于先读取 i 值，然后在内存中创建缓存变量保存 ++ 后结果，最后写会变量 i；而在这期间 i 变量都可能被其他线程读或写，从而造成线程安全性问题</p><p>CAS 算法大致原理是：在对变量进行计算之前(如 ++ 操作)，首先读取原变量值，称为 <code>旧的预期值 A</code>，然后在更新之前再获取当前内存中的值，称为 <code>当前内存值 V</code>，如果 <code>A==V</code> 则说明变量从未被其他线程修改过，此时将会写入新值 B，如果 <code>A!=V</code> 则说明变量已经被其他线程修改过，当前线程应当什么也不做；</p><h3 id="二、-CAS-原理"><a href="#二、-CAS-原理" class="headerlink" title="二、 CAS 原理"></a>二、 CAS 原理</h3><h4 id="2-1、openjdk-中-CAS-实现"><a href="#2-1、openjdk-中-CAS-实现" class="headerlink" title="2.1、openjdk 中 CAS 实现"></a>2.1、openjdk 中 CAS 实现</h4><p>翻了一下 AtomicInteger 的源码，发现其实质上都会调用到 Unsafe 类中的方法，而 Unsafe 中大部分方法是 native 的，也就是说实质使用 JNI 上调用了 C 来沟通底层硬件完成 CAS；具体调用源码(openjdk)为 <code>unsafe.cpp</code>、<code>atomic.cpp</code>、<code>atomicwindowsx86.inline.hpp</code>；以下是一部分代码片段(不懂 C…)</p><pre><code class="hljs c"><span class="hljs-comment">// Adding a lock prefix to an instruction on MP machine</span><span class="hljs-comment">// VC++ doesn't like the lock prefix to be on a single line</span><span class="hljs-comment">// so we can't insert a label after the lock prefix.</span><span class="hljs-comment">// By emitting a lock prefix, we can define a label after it.</span><span class="hljs-meta">#<span class="hljs-meta-keyword">define</span> LOCK_IF_MP(mp) __asm cmp mp, 0  \</span>                       __asm je L0      \                       __asm _emit <span class="hljs-number">0xF0</span> \                       __asm L0:<span class="hljs-function"><span class="hljs-keyword">inline</span> jlong    <span class="hljs-title">Atomic::cmpxchg</span>    <span class="hljs-params">(jlong    exchange_value, <span class="hljs-keyword">volatile</span> jlong*    dest, jlong    compare_value)</span> </span>&#123;  <span class="hljs-keyword">return</span> (*os::atomic_cmpxchg_long_func)(exchange_value, dest, compare_value);&#125;</code></pre><blockquote><p>下面开始抄的 （：…. 看 C 完全懵逼</p></blockquote><p>如上面源代码所示，程序会根据当前处理器的类型来决定是否为 cmpxchg 指令添加lock前缀；如果程序跑在多核处理器上，就为 cmpxchg 指令加上 lock 前缀 (lock cmpxchg)；反之，如果程序跑在单核处理器上，就省略 lock 前缀 (单处理器自身会维护单处理器内的顺序一致性，不需要 lock 前缀提供的内存屏障效果)</p><h4 id="2-2、intel-lock-前缀说明"><a href="#2-2、intel-lock-前缀说明" class="headerlink" title="2.2、intel lock 前缀说明"></a>2.2、intel lock 前缀说明</h4><p>intel 的手册对lock前缀的说明如下:</p><ul><li>确保对内存的读-改-写操作原子执行。在Pentium及Pentium之前的处理器中，带有lock前缀的指令在执行期间会锁住总线，使得其他处理器暂时无法通过总线访问内存。很显然，这会带来昂贵的开销。从Pentium 4，Intel Xeon及P6处理器开始，intel在原有总线锁的基础上做了一个很有意义的优化：如果要访问的内存区域（area of memory）在lock前缀指令执行期间已经在处理器内部的缓存中被锁定（即包含该内存区域的缓存行当前处于独占或以修改状态），并且该内存区域被完全包含在单个缓存行（cache line）中，那么处理器将直接执行该指令。由于在指令执行期间该缓存行会一直被锁定，其它处理器无法读/写该指令要访问的内存区域，因此能保证指令执行的原子性。这个操作过程叫做缓存锁定（cache locking），缓存锁定将大大降低lock前缀指令的执行开销，但是当多处理器之间的竞争程度很高或者指令访问的内存地址未对齐时，仍然会锁住总线。</li><li>禁止该指令与之前和之后的读和写指令重排序。</li><li>把写缓冲区中的所有数据刷新到内存中</li></ul><h4 id="2-3、cpu-锁"><a href="#2-3、cpu-锁" class="headerlink" title="2.3、cpu 锁"></a>2.3、cpu 锁</h4><p>关于CPU的锁有如下3种:</p><ul><li>处理器自动保证基本内存操作的原子性</li></ul><p>首先处理器会自动保证基本的内存操作的原子性。处理器保证从系统内存当中读取或者写入一个字节是原子的，意思是当一个处理器读取一个字节时，其他处理器不能访问这个字节的内存地址。奔腾6和最新的处理器能自动保证单处理器对同一个缓存行里进行16/32/64位的操作是原子的，但是复杂的内存操作处理器不能自动保证其原子性，比如跨总线宽度，跨多个缓存行，跨页表的访问。但是处理器提供总线锁定和缓存锁定两个机制来保证复杂内存操作的原子性。</p><ul><li>使用总线锁保证原子性</li></ul><p>第一个机制是通过总线锁保证原子性。如果多个处理器同时对共享变量进行读改写（i++就是经典的读改写操作）操作，那么共享变量就会被多个处理器同时进行操作，这样读改写操作就不是原子的，操作完之后共享变量的值会和期望的不一致，举个例子：如果i=1,我们进行两次i++操作，我们期望的结果是3，但是有可能结果是2；</p><p>原因是有可能多个处理器同时从各自的缓存中读取变量i，分别进行加一操作，然后分别写入系统内存当中。那么想要保证读改写共享变量的操作是原子的，就必须保证CPU1读改写共享变量的时候，CPU2不能操作缓存了该共享变量内存地址的缓存。</p><p>处理器使用总线锁就是来解决这个问题的。所谓总线锁就是使用处理器提供的一个LOCK＃信号，当一个处理器在总线上输出此信号时，其他处理器的请求将被阻塞住,那么该处理器可以独占使用共享内存。</p><ul><li>使用缓存锁保证原子性</li></ul><p>第二个机制是通过缓存锁定保证原子性。在同一时刻我们只需保证对某个内存地址的操作是原子性即可，但总线锁定把CPU和内存之间通信锁住了，这使得锁定期间，其他处理器不能操作其他内存地址的数据，所以总线锁定的开销比较大，最近的处理器在某些场合下使用缓存锁定代替总线锁定来进行优化。</p><p>频繁使用的内存会缓存在处理器的L1，L2和L3高速缓存里，那么原子操作就可以直接在处理器内部缓存中进行，并不需要声明总线锁，在奔腾6和最近的处理器中可以使用“缓存锁定”的方式来实现复杂的原子性。所谓“缓存锁定”就是如果缓存在处理器缓存行中内存区域在LOCK操作期间被锁定，当它执行锁操作回写内存时，处理器不在总线上声言LOCK＃信号，而是修改内部的内存地址，并允许它的缓存一致性机制来保证操作的原子性，因为缓存一致性机制会阻止同时修改被两个以上处理器缓存的内存区域数据，当其他处理器回写已被锁定的缓存行的数据时会起缓存行无效，在例1中，当CPU1修改缓存行中的i时使用缓存锁定，那么CPU2就不能同时缓存了i的缓存行。</p><p>但是有两种情况下处理器不会使用缓存锁定。第一种情况是：当操作的数据不能被缓存在处理器内部，或操作的数据跨多个缓存行（cache line），则处理器会调用总线锁定。第二种情况是：有些处理器不支持缓存锁定。对于Inter486和奔腾处理器,就算锁定的内存区域在处理器的缓存行中也会调用总线锁定。</p><p>以上两个机制我们可以通过Inter处理器提供了很多LOCK前缀的指令来实现。比如位测试和修改指令BTS，BTR，BTC，交换指令XADD，CMPXCHG和其他一些操作数和逻辑指令，比如ADD（加），OR（或）等，被这些指令操作的内存区域就会加锁，导致其他处理器不能同时访问它。</p><h3 id="三、CAS-缺点"><a href="#三、CAS-缺点" class="headerlink" title="三、CAS 缺点"></a>三、CAS 缺点</h3><h4 id="3-1、ABA-问题"><a href="#3-1、ABA-问题" class="headerlink" title="3.1、ABA 问题"></a>3.1、ABA 问题</h4><p>由于 CAS 设计机制就是获取某两个时刻(初始预期值和当前内存值)变量值，并进行比较更新，所以说如果在获取初始预期值和当前内存值这段时间间隔内，变量值由 A 变为 B 再变为 A，那么对于 CAS 来说是不可感知的，但实际上变量已经发生了变化；解决办法是在每次获取时加版本号，并且每次更新对版本号 +1，这样当发生 ABA 问题时通过版本号可以得知变量被改动过</p><p>JDK 1.5 以后的 AtomicStampedReference 类就提供了此种能力，其中的 compareAndSet 方法就是 <strong>首先检查当前引用是否等于预期引用，并且当前标志是否等于预期标志，如果全部相等，则以原子方式将该引用和该标志的值设置为给定的更新值。</strong></p><h4 id="3-2、循环时间长开销大"><a href="#3-2、循环时间长开销大" class="headerlink" title="3.2、循环时间长开销大"></a>3.2、循环时间长开销大</h4><p>所谓循环时间长开销大问题就是当 CAS 判定变量被修改了以后则放弃本次修改，<strong>但往往为了保证数据正确性该计算会以循环的方式再次发起 CAS，如果多次 CAS 判定失败，则会产生大量的时间消耗和性能浪费</strong>；如果JVM能支持处理器提供的pause指令那么效率会有一定的提升，pause指令有两个作用，第一它可以延迟流水线执行指令（de-pipeline）,使CPU不会消耗过多的执行资源，延迟的时间取决于具体实现的版本，在一些处理器上延迟时间是零。第二它可以避免在退出循环的时候因内存顺序冲突（memory order violation）而引起CPU流水线被清空（CPU pipeline flush），从而提高CPU的执行效率。</p><h4 id="3-3、只能保证一个共享变量的原子操作"><a href="#3-3、只能保证一个共享变量的原子操作" class="headerlink" title="3.3、只能保证一个共享变量的原子操作"></a>3.3、只能保证一个共享变量的原子操作</h4><p>CAS 只对单个共享变量有效，当操作涉及跨多个共享变量时 CAS 无效；<strong>从 JDK 1.5开始提供了 AtomicReference 类来保证引用对象之间的原子性，你可以把多个变量放在一个对象里来进行 CAS 操作</strong></p><h3 id="四、JUC-实现"><a href="#四、JUC-实现" class="headerlink" title="四、JUC 实现"></a>四、JUC 实现</h3><p>由于 Java 的 CAS 同时具有 volatile 读和volatile写的内存语义，因此 Java 线程之间的通信现在有了下面四种方式：</p><ul><li>A线程写 volatile 变量，随后B线程读这个 volatile 变量</li><li>A线程写 volatile 变量，随后B线程用 CAS 更新这个 volatile 变量</li><li>A线程用 CAS 更新一个 volatile 变量，随后B线程用 CAS 更新这个 volatile 变量</li><li>A线程用 CAS 更新一个 volatile 变量，随后B线程读这个 volatile 变量</li></ul><p>Java 的 CAS 会使用现代处理器上提供的高效机器级别原子指令，这些原子指令以原子方式对内存执行读-改-写操作，这是在多处理器中实现同步的关键（从本质上来说，能够支持原子性读-改-写指令的计算机器，是顺序计算图灵机的异步等价机器，因此任何现代的多处理器都会去支持某种能对内存执行原子性读-改-写操作的原子指令）。同时，volatile 变量的读/写和 CAS 可以实现线程之间的通信。把这些特性整合在一起，就形成了整个 concurrent 包得以实现的基石。如果我们仔细分析 concurrent 包的源代码实现，会发现一个通用化的实现模式：</p><ul><li>首先，声明共享变量为 volatile；</li><li>然后，使用 CAS 的原子条件更新来实现线程之间的同步；</li><li>同时，配合以 volatile 的读/写和 CAS 所具有的 volatile 读和写的内存语义来实现线程之间的通信。</li></ul><p>AQS，非阻塞数据结构和原子变量类 ( <code>java.util.concurrent.atomic</code> 包中的类)，这些 concurrent 包中的基础类都是使用这种模式来实现的，而 concurrent 包中的高层类又是依赖于这些基础类来实现的。从整体来看，concurrent 包的实现示意图如下：</p><p><img src="https://cdn.oss.link/markdown/l72qr.jpg" srcset="/img/loading.gif" alt="concurrent"></p><h3 id="五、JDK9-改变"><a href="#五、JDK9-改变" class="headerlink" title="五、JDK9 改变"></a>五、JDK9 改变</h3><p>随着 JDK9 即将发布，CAS 相关主要类 <code>Unsafe</code> 有些变动，以下变动主要由 <code>Mikael Vidstedt</code> 提交，更新内容如下</p><ul><li>避免代码重复，<code>sun.misc.Unsafe</code> 将全部实现委托给 <code>jdk.internal.misc.Unsafe</code>，这意味着java虚拟机(特别是 <code>unsafe.cpp</code> ）不再需要关心s.m.Unsafe的实现。</li><li><code>s.m.Unsafe</code> 的委托方法通常会被内联，但是为了避免性能下降的风险，仍然添加了 <code>@ForceInline</code> 注解</li><li>更新文档，指明用户应该确保Unsafe类的参数正确</li><li>参数检查从Unsage.cpp移入java，简化本地代码以及允许JIT进一步优化</li><li>放松了特定参数的检查，比方说最近引入的 <code>U.copySwapMemory</code> 没有检查空指针。具体原因可以参考 <code>j.i.m.U.checkPointer</code> 的文档。除了 <code>U.copySwapMemory</code>，现在Unsafe类方法也都没有对参数执行NULL检查</li><li>在 <code>U.copySwapMemory</code> 类的基础上，对 <code>j.i.m.U.copyMemory</code> 增加了一个测试案例。请随时提醒我合并过来（本该如此）</li></ul><p>在 Mikael Vidstedt 看来，Usage 类的清理算是 “相当激进” 了，值得注意的地方有：</p><ul><li>Unsafe_方法以及 <code>unsafe.cpp</code> 中的其他本地方法被申明为静态方法</li><li>新增 <code>unsafe.hpp</code> 代码文件，文件中移入VM其他组件的一些方法。移除部分 <code>extern</code> 函数声明（不要过度使用extern）</li><li>对于不怎么用到的UNSAFE_LEAF，移除警告性质的注释（没有必要，只是个VM_LEAF）</li><li>一些简单的leaf方法使用UNSAFE_LEAF</li><li>UNSAFE_ENTRY/UNSAFE_END代码块新增大括号，帮助自动缩进</li><li>移除未使用的Unsafe_&lt;…&gt;##140形式的函数和宏</li><li>更新宏参数，与unsafe.cpp的宏定义保持一致</li><li>更换带断言的参数检查，正如前面提及，这些检查移入了 <code>j.i.m.Unsafe</code>，移除所有 <code>s.m.Unsafe</code> 相关的代码</li></ul><p>本文参考自 <a href="http://zl198751.iteye.com/blog/1848575" target="_blank" rel="noopener">JAVA CAS原理深度分析</a>、<a href="http://zkread.com/article/780437.html" target="_blank" rel="noopener">Java 9发布在即，Oracle OpenJDK着手优化Unsafe类</a></p>]]></content>
    
    
    <categories>
      
      <category>Java</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Java</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Logical Volume Manager 笔记</title>
    <link href="/2017/01/27/logical-volume-manager-note/"/>
    <url>/2017/01/27/logical-volume-manager-note/</url>
    
    <content type="html"><![CDATA[<blockquote><p>Logical Volume Manager 简称 LVM，LVM 是一种可用在Linux内核的逻辑分卷管理器；可用于管理磁盘驱动器或其他类似的大容量存储设备；LVM 依赖于 内核 device mapper 机制，可以实现动态伸缩逻辑卷大小，而屏蔽底层硬件变化，为后期磁盘扩展提供便利</p></blockquote><h3 id="一、LVM-简介"><a href="#一、LVM-简介" class="headerlink" title="一、LVM 简介"></a>一、LVM 简介</h3><p>LVM 技术依据底层内核的 device mapper 机制，聚合底层硬件存储设备存储空间，在上层抽象出可扩展的逻辑分区；LVM 主要术语(磁盘术语)如下</p><ul><li><code>PV</code> : 底层物理存储设备，如 <code>/dev/sda</code></li><li><code>VG</code> : 卷组，意将多个 <code>PV</code> 组合后的抽象存储介质统称</li><li><code>PE</code> : <code>VG</code> 将 <code>PV</code> 聚合后，需向上层提供存储能力，而 <code>PE</code> 即为 <code>VG</code> 中最小的存储单位，<strong>一般默认为 4MB</strong></li><li><code>LV</code> : 在 <code>VG</code> 之上划分一定量的存储空间，形成逻辑分区，即为 <code>LV</code></li><li><code>LE</code> : <code>LV</code> 从 <code>VG</code> 之上划分，即 <code>LV</code> 实质上有 <code>VG</code> 上最小存储单位 <code>PE</code> 构成；但是 <code>PE</code> 构成 <code>LV</code> 之后，又称作 <code>LE</code></li></ul><p><strong>以上各术语(定义)之间关系如下图所示</strong></p><p><img src="https://cdn.oss.link/markdown/mbzrv.jpg" srcset="/img/loading.gif" alt="PV_VG_LV"></p><h3 id="二、LVM-相关命令"><a href="#二、LVM-相关命令" class="headerlink" title="二、LVM 相关命令"></a>二、LVM 相关命令</h3><h4 id="2-1、PV-管理工具"><a href="#2-1、PV-管理工具" class="headerlink" title="2.1、PV 管理工具"></a>2.1、PV 管理工具</h4><ul><li>pvs : 简要显示 pv 信息</li><li>pvdisplay : 详细显示 pv 信息</li><li>pvcreate : 创建 pv</li><li>pvmove : 移动 pv (再删除 vg 前必须移动有数据的 pv)</li><li>pvremove : 删除 pv</li></ul><h4 id="2-2、VG-管理命令"><a href="#2-2、VG-管理命令" class="headerlink" title="2.2、VG 管理命令"></a>2.2、VG 管理命令</h4><ul><li>vgs : 简要显示 vg 信息</li><li>vgdisplay : 详细显示 vg 信息</li><li>vgcreate : 创建 vg</li><li>vgrename : 重命名 vg</li><li>vgremove : 删除 vg</li><li>vgscan : 扫描 vg</li><li>vgextend : 扩展 vg</li><li>vgsplit : 切割 vg (将 vg 中 pv 移动到已存在 vg 中)</li><li>vgreduce : 缩减 vg (删除 vg 内指定 pv)</li></ul><h4 id="2-3、LV-管理命令"><a href="#2-3、LV-管理命令" class="headerlink" title="2.3、LV 管理命令"></a>2.3、LV 管理命令</h4><ul><li>lvs : 简要显示 lv 信息</li><li>lvdisplay : 详细显示 lv 信息</li><li>lvcreate : 创建逻辑卷</li><li>lvextend : 扩展逻辑卷</li><li>lvreduce : 缩减逻辑分区</li><li>lvremove : 删除逻辑卷</li></ul><h5 id="2-3-1、lvcreate"><a href="#2-3-1、lvcreate" class="headerlink" title="2.3.1、lvcreate"></a>2.3.1、lvcreate</h5><p>lvcreate 用于从 vg 上创建 lv 逻辑分区，基本命令格式如下 :</p><pre><code class="hljs sh">lvcreate -L <span class="hljs-comment">#[mMgGtT] -n NAME VolumeGroup</span></code></pre><p>其常用选项如下 : </p><ul><li>-L | –size : 指定要创建的 lv 大小，采用 <code>+5G</code> 这种方式</li><li>-l | –extents : 指定大小范围，类似于 fdisk 分区时选择盘区范围</li><li>-s | –snapshot : 创建一个快照卷，创建快照卷时后面必须跟快照卷名称</li><li>p : 权限(r、rw)</li></ul><h5 id="2-3-2、lvextend"><a href="#2-3-2、lvextend" class="headerlink" title="2.3.2、lvextend"></a>2.3.2、lvextend</h5><p>lvextend 支持在线扩展扩展逻辑分区大小，命令格式如下 : </p><pre><code class="hljs sh">lvextend -L [+]<span class="hljs-comment">#[mMgGtT] /dev/VG_NAME/LV_NAME</span></code></pre><p><strong>注意 -L 选项后 + 可以省略，但是省略后代表总容量，如当前 LV 2G，需扩展 3G，则不写 + 需要输入 5G</strong></p><p><strong>lv 扩展后并不能立即体现在 df 等命令上，因为虽然逻辑卷已经扩展，但是文件系统尚未扩展，对于 ext 系列的文件系统可采用 <code>resize2fs /dev/VG_NAME/LV_NAME</code> 的方式刷新文件系统，其他分区格式需采用其他工具</strong></p><h5 id="2-3-3、lvreduce"><a href="#2-3-3、lvreduce" class="headerlink" title="2.3.3、lvreduce"></a>2.3.3、lvreduce</h5><p><strong>在缩减 lv 时，必须先卸载文件系统，然后缩减文件系统，并且缩减文件系统后要强制做文件系统检测，以防止发生损坏，操作命令及顺序如下 :</strong></p><ul><li>umount /dev/VG_NAME/LV_NAME</li><li>e2fsck -f /dev/VG_NAME/LV_NAME</li><li>resize2fs /dev/VG_NAME/LV_NAME #[mMgGtT]</li><li>lvreduce [-]#[mMgGtT] /dev/VG_NAME/LV_NAME</li><li>mount /dev/VG_NAME/LV_NAME DIR</li></ul>]]></content>
    
    
    <categories>
      
      <category>Linux</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Linux</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>利用 frp 进行内网穿透</title>
    <link href="/2017/01/21/use-frp-for-internal-network-wear/"/>
    <url>/2017/01/21/use-frp-for-internal-network-wear/</url>
    
    <content type="html"><![CDATA[<blockquote><p>最近新入了一台小主机，家里还有个树莓派，索性想通过小主机跑点东西，然后通过外网访问家里的设备；不过鉴于大天朝内网环境，没有公网 IP 并且多层路由的情况下只能选择使用内网穿透方案，以下记录了一下使用 frp 进行内网穿透的过程</p></blockquote><h3 id="一、内网穿透原理"><a href="#一、内网穿透原理" class="headerlink" title="一、内网穿透原理"></a>一、内网穿透原理</h3><p>简单地说，内网穿透依赖于 NAT 原理，根据 NAT 设备不同大致可分为以下 4 大类(前3种NAT类型可统称为cone类型)：</p><ul><li>全克隆(Full Cone)：NAT 把所有来自相同内部 IP 地址和端口的请求映射到相同的外部 IP 地址和端口上，任何一个外部主机均可通过该映射反向发送 IP 包到该内部主机</li><li>限制性克隆(Restricted Cone)：NAT 把所有来自相同内部 IP 地址和端口的请求映射到相同的外部 IP 地址和端口；但是，只有当内部主机先给 IP 地址为 X 的外部主机发送 IP 包时，该外部主机才能向该内部主机发送 IP 包</li><li>端口限制性克隆(Port Restricted Cone)：端口限制性克隆与限制性克隆类似，只是多了端口号的限制，即只有内部主机先向 IP 地址为 X，端口号为 P 的外部主机发送1个 IP 包,该外部主机才能够把源端口号为 P 的 IP 包发送给该内部主机</li><li>对称式NAT(Symmetric NAT)：这种类型的 NAT 与上述3种类型的不同，在于当同一内部主机使用相同的端口与不同地址的外部主机进行通信时， NAT 对该内部主机的映射会有所不同；对称式 NAT 不保证所有会话中的私有地址和公开 IP 之间绑定的一致性；相反，它为每个新的会话分配一个新的端口号；导致此种 NAT 根本没法穿透</li></ul><p>内网穿透的作用就是利用以上规则，创建一条从外部服务器到内部设备的 “隧道”，具体的 NAT 原理等可参考 <a href="http://www.cnblogs.com/eyye/archive/2012/10/23/2734807.html" target="_blank" rel="noopener">内网打洞</a>、<a href="http://blog.csdn.net/hzhsan/article/details/45038265" target="_blank" rel="noopener">网络地址转换NAT原理</a></p><h3 id="二、环境准备"><a href="#二、环境准备" class="headerlink" title="二、环境准备"></a>二、环境准备</h3><p>实际上根据以上 NAT 规则，基本上大部分家用设备和运营商上级路由等都在前三种规则之中，所以只需要借助成熟的内网穿透工具即可，以下为本次穿透环境</p><ul><li>最新版本 frp</li><li>一台公网 VPS 服务器</li><li>内网一台服务器，最好 Linux 系统</li></ul><h3 id="三、服务端搭建"><a href="#三、服务端搭建" class="headerlink" title="三、服务端搭建"></a>三、服务端搭建</h3><p>服务器作为公网访问唯一的固定地址，即作为 server 端；内网客户端作为 client 端，会主动向 server 端创建连接，此时再从 server 端反向发送数据即可实现内网穿透</p><pre><code class="hljs sh"><span class="hljs-comment"># 下载 frp 并解压</span>wget https://github.com/fatedier/frp/releases/download/v0.9.3/frp_0.9.3_linux_amd64.tar.gztar -zxvf frp_0.9.3_linux_amd64.tar.gz<span class="hljs-built_in">cd</span> frp_0.9.3_linux_amd64</code></pre><p>编辑 <code>frps.ini</code> 如下</p><pre><code class="hljs sh"><span class="hljs-comment"># 通用配置段</span>[common]<span class="hljs-comment"># frp 监听地址</span>bind_addr = 0.0.0.0bind_port = 7000<span class="hljs-comment"># 如果需要代理 web(http) 服务，则开启此端口</span>vhost_http_port = 4080vhost_https_port = 4443<span class="hljs-comment"># frp 控制面板</span>dashboard_port = 7500dashboard_user = admindashboard_pwd = admin<span class="hljs-comment"># 默认日志输出位置(这里输出到标准输出)</span>log_file = /dev/stdout<span class="hljs-comment"># 日志级别，支持: debug, info, warn, error</span>log_level = infolog_max_days = 3<span class="hljs-comment"># 是否开启特权模式(特权模式下，客户端更改配置无需更新服务端)</span>privilege_mode = <span class="hljs-literal">true</span><span class="hljs-comment"># 授权 token 建议随机生成</span>privilege_token = HE7qTtW8Lg83UDKY<span class="hljs-comment"># 特权模式下允许分配的端口(避免端口滥用)</span>privilege_allow_ports = 4000-50000<span class="hljs-comment"># 心跳检测超时</span><span class="hljs-comment"># heartbeat_timeout = 30</span><span class="hljs-comment"># 后端连接池最大连接数量</span>max_pool_count = 100<span class="hljs-comment"># 口令超时时间</span>authentication_timeout = 900<span class="hljs-comment"># 子域名(特权模式需下将 *.domain.com 解析到公网服务器)</span>subdomain_host = domain.com<span class="hljs-comment"># 开启 ssh 穿透(可通过外网链接内网 ssh)</span>[ssh]<span class="hljs-built_in">type</span> = tcpauth_token = M4P2xsH6RuUkbP9dbind_addr = 0.0.0.0listen_port = 6000<span class="hljs-comment"># 开启 dns 查询穿透(个人用不上)</span><span class="hljs-comment">#[dns]</span><span class="hljs-comment">#type = udp</span><span class="hljs-comment">#auth_token = M4P2xsH6RuUkbP9d</span><span class="hljs-comment">#bind_addr = 0.0.0.0</span><span class="hljs-comment">#listen_port = 5353</span></code></pre><p><strong>其他具体配置说明请参考frp <a href="https://github.com/fatedier/frp/blob/master/README_zh.md" target="_blank" rel="noopener"> README</a> 文档</strong></p><p>设置完成后执行 <code>./frps -c frps.ini</code> 启动即可</p><h3 id="四、客户端配置"><a href="#四、客户端配置" class="headerlink" title="四、客户端配置"></a>四、客户端配置</h3><p>客户端作为发起链接的主动方，只需要正确配置服务器地址，以及要映射客户端的哪些服务端口等即可</p><pre><code class="hljs sh"><span class="hljs-comment"># 下载 frp 并解压</span>wget https://github.com/fatedier/frp/releases/download/v0.9.3/frp_0.9.3_linux_amd64.tar.gztar -zxvf frp_0.9.3_linux_amd64.tar.gz<span class="hljs-built_in">cd</span> frp_0.9.3_linux_amd64</code></pre><p>编辑 <code>frpc.ini</code> 文件</p><pre><code class="hljs sh"><span class="hljs-comment"># 通用配置</span>[common]<span class="hljs-comment"># 服务端地址及端口</span>server_addr = domain.comserver_port = 7000log_file = /dev/stdoutlog_level = infolog_max_days = 3<span class="hljs-comment"># 授权 token，必须与服务端保持一致方可实现映射</span>auth_token = ouRRXE4tk69oNZ6f<span class="hljs-comment"># 特权模式 token，同样要与服务端一致</span>privilege_token = VfJiyhDVJ38t7Qu6<span class="hljs-comment"># 心跳检测</span><span class="hljs-comment"># heartbeat_interval = 10</span><span class="hljs-comment"># heartbeat_timeout = 30</span><span class="hljs-comment"># 将本地 ssh 映射到服务器</span>[ssh]<span class="hljs-built_in">type</span> = tcplocal_ip = 127.0.0.1local_port = 22<span class="hljs-comment"># 是否开启加密(流量加密，应对防火墙)</span>use_encryption = <span class="hljs-literal">true</span><span class="hljs-comment"># 是否开启压缩</span>use_gzip = <span class="hljs-literal">true</span><span class="hljs-comment"># dns 用不到</span><span class="hljs-comment">#[dns]</span><span class="hljs-comment">#type = udp</span><span class="hljs-comment">#local_ip = 114.114.114.114</span><span class="hljs-comment">#local_port = 53</span><span class="hljs-comment"># 发布本地 web 服务</span>[web01]<span class="hljs-built_in">type</span> = httplocal_ip = 127.0.0.1local_port = 8000<span class="hljs-comment"># 是否启用特权模式(特权模式下服务端无需配置)</span>privilege_mode = <span class="hljs-literal">true</span>use_encryption = <span class="hljs-literal">true</span>use_gzip = <span class="hljs-literal">true</span><span class="hljs-comment"># 连接数量</span>pool_count = 20<span class="hljs-comment"># 是否开启密码访问</span><span class="hljs-comment">#http_user = admin</span><span class="hljs-comment">#http_pwd = admin</span><span class="hljs-comment"># 子域名，当服务端开启特权模式，并且将 "*.domain.com" 解析到服务端 IP后，</span><span class="hljs-comment"># 客户端在选项(privilege_mode)中声明当前映射为特权模式时，服务器端就会</span><span class="hljs-comment"># 给于一个 "subdomain.domain.com" 映射，此示例将在服务端开一个</span><span class="hljs-comment"># "http://test.domain.com/:4080" 的服务映射到内网 8000 端口上</span>subdomain = <span class="hljs-built_in">test</span></code></pre><p>最后使用 <code>./frpc -c frpc.ini</code> 启动即可</p><h3 id="五、测试"><a href="#五、测试" class="headerlink" title="五、测试"></a>五、测试</h3><p>服务端和客户端同时开启完成后，即可访问 <code>http://domain.com:7500</code> 进入 frp 控制面板，如下</p><p><img src="https://cdn.oss.link/markdown/1d8pq.jpg" srcset="/img/loading.gif" alt="dashboard"></p><p>此时通过 <code>ssh root@domain.com -p 6000</code> 即可连接到内网服务器，通过访问 <code>http://test.domain.com:4080</code> 即可访问内网发布的位于 <code>8000</code> 端口服务</p><h3 id="六、Systemd-管理"><a href="#六、Systemd-管理" class="headerlink" title="六、Systemd 管理"></a>六、Systemd 管理</h3><p>在较新的 Linux 系统中一经采用 Systemd 作为系统服务管理工具，以下为服务端作为服务方式运行的方式</p><pre><code class="hljs sh"><span class="hljs-comment"># 复制文件</span>cp frps /usr/<span class="hljs-built_in">local</span>/bin/frpsmkdir /etc/frpcp frps.ini /etc/frp/frps.ini<span class="hljs-comment"># 编写 frp service 文件，以 centos7 为例</span>vim /usr/lib/systemd/system/frps.service<span class="hljs-comment"># 内容如下</span>[Unit]Description=frpsAfter=network.target[Service]TimeoutStartSec=30ExecStart=/usr/<span class="hljs-built_in">local</span>/bin/frps -c /etc/frp/frps.iniExecStop=/bin/<span class="hljs-built_in">kill</span> <span class="hljs-variable">$MAINPID</span>[Install]WantedBy=multi-user.target<span class="hljs-comment"># 启动 frp 并设置开机启动</span>systemctl <span class="hljs-built_in">enable</span> frpssystemctl start frpssystemctl status frps</code></pre><p>客户端与此类似，这里不再重复编写，更多详细设置(如代理 https 等)请参考 frp 官方文档</p>]]></content>
    
    
    <categories>
      
      <category>Linux</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Linux</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Git 使用 socks5 代理</title>
    <link href="/2017/01/12/git-uses-the-socks5-proxy/"/>
    <url>/2017/01/12/git-uses-the-socks5-proxy/</url>
    
    <content type="html"><![CDATA[<blockquote><p>最近伟大的墙又开始搞事情，导致 gayhub 访问奇慢，没办法研究一下 socks5 代理 git，效果还不错</p></blockquote><h3 id="一、Mac-amp-Ubuntu-下代理-git"><a href="#一、Mac-amp-Ubuntu-下代理-git" class="headerlink" title="一、Mac &amp; Ubuntu 下代理 git"></a>一、Mac &amp; Ubuntu 下代理 git</h3><p>git 目前支持 4 种协议: <code>https</code>、<code>ssh</code>、<code>git</code>、本地文件；其中 <code>git</code>协议与 <code>ssh</code> 协议及其类似，暂不清楚底层实现，不过目前发现只需要成功代理 <code>ssh</code> 协议就可以实现代理 <code>git</code>；不清楚两者有什么基情，根据官方描述，<code>git</code> 协议传输非常快，验证基于 <code>ssh</code> 协议，详见 <a href="https://git-scm.com/book/zh/v2/%E6%9C%8D%E5%8A%A1%E5%99%A8%E4%B8%8A%E7%9A%84-Git-%E5%8D%8F%E8%AE%AE" target="_blank" rel="noopener">服务器上的 Git - 协议</a></p><p>代理 <code>ssh</code> 协议在 Mac 和 Ubuntu 上可以使用 <code>netcat-openbsd</code> 包中的 <code>nc</code> 命令，这里由于梯子工具问题，所以仅讨论如何使用 <code>nc</code> 代理 <code>ssh</code> 协议到 <code>socks5</code> 上</p><p>Mac 默认就有 <code>nc</code> 命令， Ubuntu 新版本也有，如果较老版本可使用 <code>apt-get install -y netcat-openbsd</code> 安装</p><h4 id="1-1、创建代理命令工具"><a href="#1-1、创建代理命令工具" class="headerlink" title="1.1、创建代理命令工具"></a>1.1、创建代理命令工具</h4><p>首先创建一个代理脚本即可，<code>socks5</code> 地址根据需要更改</p><pre><code class="hljs sh">tee /usr/<span class="hljs-built_in">local</span>/bin/proxy-wrapper &lt;&lt;EOF<span class="hljs-meta">#!/bin/bash</span>nc -x127.0.0.1:1080 -X5 \$*EOFchmod +x /usr/<span class="hljs-built_in">local</span>/bin/proxy-wrapper</code></pre><h4 id="1-2、增加-ssh-配置"><a href="#1-2、增加-ssh-配置" class="headerlink" title="1.2、增加 ssh 配置"></a>1.2、增加 ssh 配置</h4><p>代理 <code>git</code> 协议只需要代理 <code>ssh</code> 即可，其中 <code>Host</code> 后可以跟多个想要被代理的域名，由于代理的是 <code>ssh</code> 协议，所以 <strong>使用 <code>ssh</code> 连接服务器也会根据域名选择是否走代理</strong></p><pre><code class="hljs sh">tee ~/.ssh/config &lt;&lt;EOFHost github github.com mritd.me<span class="hljs-comment">#Hostname github.com</span><span class="hljs-comment">#User git</span>ProxyCommand /usr/<span class="hljs-built_in">local</span>/bin/proxy-wrapper <span class="hljs-string">'%h %p'</span>EOF</code></pre><h4 id="1-3、测试"><a href="#1-3、测试" class="headerlink" title="1.3、测试"></a>1.3、测试</h4><p>配置好以后，保证你得 <code>socks5</code> 代理无问题的情况下，使用 <code>git clone git@github.com:xxxxx/xxxxx.git</code> 克隆一个项目即可验证是否成功</p><h3 id="二、CentOS-下代理-git"><a href="#二、CentOS-下代理-git" class="headerlink" title="二、CentOS 下代理 git"></a>二、CentOS 下代理 git</h3><p>默认的 CentOS 下是没有 <code>netcat-openbsd</code> 的，CentOS 下的 netcat 并非 openbsd 版本，所以会出现 <code>nc: invalid option -- &#39;X&#39;</code> 错误；so，用不了 <code>nc</code> 了，不过 Linux 下还有另一款软件可以实现代理 <code>ssh</code> 协议到 <code>socks5</code></p><h4 id="2-1、安装-connect-proxy"><a href="#2-1、安装-connect-proxy" class="headerlink" title="2.1、安装 connect-proxy"></a>2.1、安装 connect-proxy</h4><p>没有 <code>netcat-openbsd</code> 可以安装 <code>connect-proxy</code></p><pre><code class="hljs sh">yum install connect-proxy -y</code></pre><h4 id="2-2、创建代理脚本"><a href="#2-2、创建代理脚本" class="headerlink" title="2.2、创建代理脚本"></a>2.2、创建代理脚本</h4><p>同上面一样，也最好搞一个脚本</p><pre><code class="hljs sh">tee /usr/<span class="hljs-built_in">local</span>/bin/proxy-wrapper &lt;&lt;EOF<span class="hljs-meta">#!/bin/bash</span>connect-proxy -S 192.168.1.120:1083 $*EOFchmod +x /usr/<span class="hljs-built_in">local</span>/bin/proxy-wrapper</code></pre><h4 id="2-3、增加-ssh-配置"><a href="#2-3、增加-ssh-配置" class="headerlink" title="2.3、增加 ssh 配置"></a>2.3、增加 ssh 配置</h4><p>ssh 配置同上面一样</p><pre><code class="hljs sh">tee ~/.ssh/config &lt;&lt;EOFHost github github.com mritd.me<span class="hljs-comment">#Hostname github.com</span><span class="hljs-comment">#User git</span>ProxyCommand /usr/<span class="hljs-built_in">local</span>/bin/proxy-wrapper <span class="hljs-string">'%h %p'</span>EOF</code></pre><h4 id="2-3、测试"><a href="#2-3、测试" class="headerlink" title="2.3、测试"></a>2.3、测试</h4><p>测试掠过……</p><h3 id="三、其他"><a href="#三、其他" class="headerlink" title="三、其他"></a>三、其他</h3><p>对于 <code>https</code> 协议的代理可以参考 <a href="https://mritd.me/2016/07/22/Linux-%E5%91%BD%E4%BB%A4%E8%A1%8C%E4%B8%8B%E4%BD%BF%E7%94%A8-Shadowsocks-%E4%BB%A3%E7%90%86/" target="_blank" rel="noopener">Linux 命令行下使用 Shadowsocks 代理</a></p>]]></content>
    
    
    <categories>
      
      <category>Linux</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Linux</tag>
      
      <tag>Git</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>使用 Nexus 搭建 Docker 仓库</title>
    <link href="/2017/01/08/set-up-docker-registry-by-nexus/"/>
    <url>/2017/01/08/set-up-docker-registry-by-nexus/</url>
    
    <content type="html"><![CDATA[<blockquote><p>nexus 最初用于搭建 maven 私服，提供企业级 maven jar 包管理等功能；2.x 后续支持了 npm、rpm 等包管理；最新版本 3.x 开始支持 Docker 仓库，以下为使用 neuxs 撸一个 docker 仓库的教程</p></blockquote><h3 id="一、环境准备"><a href="#一、环境准备" class="headerlink" title="一、环境准备"></a>一、环境准备</h3><p>初始环境如下</p><ul><li>Centos 7 x86_64</li><li>OpenJDK 8</li><li>Nexus 3.2.0-01</li></ul><p>安装 OpenJDK 命令如下</p><pre><code class="hljs sh">yum install java-1.8.0-openjdk -y</code></pre><p>安装完成后验证是否安装成功</p><pre><code class="hljs sh">➜  ~ java -versionopenjdk version <span class="hljs-string">"1.8.0_111"</span>OpenJDK Runtime Environment (build 1.8.0_111-b15)OpenJDK 64-Bit Server VM (build 25.111-b15, mixed mode)</code></pre><p>下载 neuxs3 安装包并解压</p><pre><code class="hljs sh">wget --no-check-certificate https://download.sonatype.com/nexus/3/nexus-3.2.0-01-unix.tar.gztar -zxvf nexus-3.2.0-01-unix.tar.gz</code></pre><h3 id="二、安装-nexus"><a href="#二、安装-nexus" class="headerlink" title="二、安装 nexus"></a>二、安装 nexus</h3><p>首先将 nexus 移动到任意位置</p><pre><code class="hljs sh">mv nexus-3.2.0-01 /usr/<span class="hljs-built_in">local</span></code></pre><p>创建 nexus 用户</p><pre><code class="hljs sh">adduser -r -s /sbin/nologin -d /data/nexus-data nexus</code></pre><p><strong>默认 nexus 运行后会在同级目录下创建一个 <code>sonatype-work</code> 工作目录，并将其数据保存在此目录中，所以为了数据持久化先手动创建并设置其数据存储位置</strong></p><pre><code class="hljs sh"><span class="hljs-comment"># 创建基本目录结构</span>mkdir -p /usr/<span class="hljs-built_in">local</span>/sonatype-work<span class="hljs-comment"># 创建建数据目录</span>mkdir -p /data/nexus-data/&#123;etc,<span class="hljs-built_in">log</span>,tmp&#125;<span class="hljs-comment"># 将数据目录软连接到工作目录</span>ln -s /data/nexus-data /usr/<span class="hljs-built_in">local</span>/sonatype-work/nexus3<span class="hljs-comment"># 更新所有目录权限</span>chmod -R 755 /usr/<span class="hljs-built_in">local</span>/&#123;sonatype-work,nexus-3.2.0-01&#125; /data/nexus-datachown -R nexus:nexus /usr/<span class="hljs-built_in">local</span>/&#123;sonatype-work,nexus-3.2.0-01&#125; /data/nexus-data</code></pre><p>最后启动 nexus 访问 8081 端口即可</p><pre><code class="hljs sh"><span class="hljs-comment"># 以前台方式运行</span>sudo -u nexus /usr/<span class="hljs-built_in">local</span>/nexus-3.2.0-01/bin/nexus run<span class="hljs-comment"># 后台运行</span>sudo -u nexus /usr/<span class="hljs-built_in">local</span>/nexus-3.2.0-01/bin/nexus start</code></pre><p>默认账户 <code>admin</code> 密码 <code>admin123</code>，登录如下</p><p><img src="https://cdn.oss.link/markdown/sb9dw.jpg" srcset="/img/loading.gif" alt="nexus_homepage"></p><h3 id="三、创建-docker-仓库"><a href="#三、创建-docker-仓库" class="headerlink" title="三、创建 docker 仓库"></a>三、创建 docker 仓库</h3><p>在设置 <code>Repositories</code> 选项卡中中选择 <code>Create repository</code></p><p><img src="https://cdn.oss.link/markdown/m7m53.jpg" srcset="/img/loading.gif" alt="Create repository"></p><p>仓库类型有很多，docker 相关总共有三种类型，其秉承 maven 私服的概念</p><p><img src="https://cdn.oss.link/markdown/pm0r8.jpg" srcset="/img/loading.gif" alt="repository type"></p><ul><li>hosted: 本地存储，即同 docker 官方仓库一样提供本地私服功能</li><li>proxy: 提供代理其他仓库的类型，如 docker 中央仓库</li><li>group: 组类型，实质作用是组合多个仓库为一个地址</li></ul><h4 id="3-1、创建一个私服"><a href="#3-1、创建一个私服" class="headerlink" title="3.1、创建一个私服"></a>3.1、创建一个私服</h4><p>选择 <code>hosted</code> 类型仓库，然后输入一个仓库名，<strong>并勾选 HTTP 选项，端口任意即可(下面截图失误，不补了)</strong></p><p><img src="https://cdn.oss.link/markdown/972cl.jpg" srcset="/img/loading.gif" alt="create hosted repository"></p><h4 id="3-2、测试私服"><a href="#3-2、测试私服" class="headerlink" title="3.2、测试私服"></a>3.2、测试私服</h4><p>创建好以后更改 docker 参数，测试即可</p><pre><code class="hljs sh"><span class="hljs-comment"># 增加非安全仓库</span>vim /usr/lib/systemd/system/docker.service<span class="hljs-comment"># 在 ExecStart 后面增加(这里改了 host，上面端口用的 8800)</span>--insecure-registry registry.com:8800<span class="hljs-comment"># 重启 docker</span>systemctl daemon-reloadsystemctl restart docker</code></pre><p>测试 push 和 pull 镜像</p><pre><code class="hljs sh">➜  ~ docker tag mritd/alpine registry.com:8800/alpine➜  ~ docker push registry.com:8800/alpineThe push refers to a repository [registry.com:8800/alpine]754684812d65: Pushed60ab55d3379d: Pushedlatest: digest: sha256:28f397aca53eb3e8ea1627f4af9c262fca7db17f0c6db492b53adc7bca7d0f91 size: 739➜  ~ docker rmi registry.com:8800/alpineUntagged: registry.com:8800/alpine:latestUntagged: registry.com:8800/alpine@sha256:28f397aca53eb3e8ea1627f4af9c262fca7db17f0c6db492b53adc7bca7d0f91➜  ~ docker rmi mritd/alpineUntagged: mritd/alpine:latestUntagged: mritd/alpine@sha256:28f397aca53eb3e8ea1627f4af9c262fca7db17f0c6db492b53adc7bca7d0f91Deleted: sha256:090c790ee6f28f495d92d5be43641573b0d1b5502b35f7662d88cdbf8d548afdDeleted: sha256:378e2b887fcdffcbd113a7cf6f97e9f8a58851b0a205b31a93acdb887912850d➜  ~ docker pull registry.com:8800/alpineUsing default tag: latestlatest: Pulling from alpine0a8490d0dfd3: Already exists8fb018fb4173: Pull completeDigest: sha256:28f397aca53eb3e8ea1627f4af9c262fca7db17f0c6db492b53adc7bca7d0f91Status: Downloaded newer image <span class="hljs-keyword">for</span> registry.com:8800/alpine:latest</code></pre><h4 id="3-2、创建代理仓库"><a href="#3-2、创建代理仓库" class="headerlink" title="3.2、创建代理仓库"></a>3.2、创建代理仓库</h4><p>代理仓库参考官方文档 <a href="http://books.sonatype.com/nexus-book/reference3/docker.html#docker-introduction" target="_blank" rel="noopener">点这里</a>，本人不才….没成功，有爱探索的可以尝试一下，如果成功可以探讨一下…..个人怀疑是 index 有问题</p><p> 根据官方文档的这段提示</p><blockquote><p>Just to recap, in order to configure a proxy for Docker Hub you configure the Remote Storage URL to <a href="https://registry-1.docker.io" target="_blank" rel="noopener">https://registry-1.docker.io</a>, enable Docker V1 API support and for the choice of Docker Index select the Use Docker Hub option.</p></blockquote><p>创建仓库类型选择 <code>proxy</code>，Remote storage 填写 <code>https://registry-1.docker.io</code>，Docker index 选择 <code>Use Docker Hub</code>，然后从 代理仓库地址 pull 就可以，但是本人百试不成功，截图如下</p><p><img src="https://cdn.oss.link/markdown/q350r.jpg" srcset="/img/loading.gif" alt="proxy registry"></p><h4 id="3-4、创建-group-仓库"><a href="#3-4、创建-group-仓库" class="headerlink" title="3.4、创建 group 仓库"></a>3.4、创建 group 仓库</h4><p>group 不提供具体存储服务，其主要作用就是类似一个前端反代，可以把多个仓库(比如 hosted 私服和 proxy)组合成一个地址提供访问，创建方法基本相同，主要是添加多个 hosted 或者 proxy 类型的其他仓库即可，这里不再详细阐述，截图如下</p><p><img src="https://cdn.oss.link/markdown/2q1qv.jpg" srcset="/img/loading.gif" alt="group registry"></p><h3 id="四、其他相关"><a href="#四、其他相关" class="headerlink" title="四、其他相关"></a>四、其他相关</h3><p>由于 nexus 在 maven jar 管理方面已经是很成熟的产品，增加了 docker 等支持以后基本思想没有太大变化，所以关于其他仓库配置这里不再提及，具体可以参考<a href="http://books.sonatype.com/nexus-book/reference3/index.html" target="_blank" rel="noopener">官方文档</a>；2.x 可以通过图形界面上传 jar，3.x 目前只能通过 maven deploy 插件实现，可以参考<a href="https://maven.apache.org/guides/mini/guide-3rd-party-jars-remote.html" target="_blank" rel="noopener">这里</a></p>]]></content>
    
    
    <categories>
      
      <category>Docker</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Linux</tag>
      
      <tag>Docker</tag>
      
      <tag>Nexus</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>从 WWDC16 ATS 说起</title>
    <link href="/2016/12/30/configure-nginx-https-under-centos-to-support-ios-ats/"/>
    <url>/2016/12/30/configure-nginx-https-under-centos-to-support-ios-ats/</url>
    
    <content type="html"><![CDATA[<blockquote><p>WWDC16 苹果正式发出公告，2017年1月1号后所有 IOS 应用需要使用 HTTPS 配置，并且 HTTPS 还得符合 ATS 要求，以下记录一下 CentoS 7 下配置 nginx HTTPS 并满足 ATS 过程</p></blockquote><h3 id="一、Nginx-HTTPS-最佳实践"><a href="#一、Nginx-HTTPS-最佳实践" class="headerlink" title="一、Nginx HTTPS 最佳实践"></a>一、Nginx HTTPS 最佳实践</h3><p>随着 HTTPS 呼声越来越高，web 站点 HTTPS 化必不可免；而使用 Nginx 作为前端反向代理服务器配置 HTTPS 时有很多复杂参数，这里采取偷懒办法，直接采用 mozilla 给出的最佳实践参数(如果自己玩的非常溜可以自己自定义)；mozilla 给出了一个生成 HTTPS 配置的 web 页面，基本上给出的 HTTPS 配置已经是很好的最佳实践了，地址 –&gt; <a href="https://mozilla.github.io/server-side-tls/ssl-config-generator/" target="_blank" rel="noopener">Generate Mozilla Security Recommended Web Server Configuration Files</a></p><p>服务器选择 Nginx，输入对应 Nginx 版本号和 OpenSSL 版本号；<strong>注意：为了支持 HTTP2，OpenSSL版本必须大于等于 1.0.2</strong>，截图如下</p><p><img src="https://cdn.oss.link/markdown/z5umu.jpg" srcset="/img/loading.gif" alt="config"></p><h3 id="二、升级-OpenSSL"><a href="#二、升级-OpenSSL" class="headerlink" title="二、升级 OpenSSL"></a>二、升级 OpenSSL</h3><h4 id="2-1、基础准备"><a href="#2-1、基础准备" class="headerlink" title="2.1、基础准备"></a>2.1、基础准备</h4><p>准备好配置参数以后，需要升级 CentOS 7 默认的 openssl(默认最新版本 1.0.1e)，升级时最好打上 cloudflare 提供的用于支持对移动端比较友好的 chacha20 加密算法补丁，具体过程如下</p><pre><code class="hljs sh"><span class="hljs-comment"># 安装编译依赖</span>yum install gcc glibc glibc-devel make pcre \        pcre-devel zlib zlib-devel kernel-devel \        curl gnupg libxslt libxslt-devel gd-devel \        geoip-devel perl-devel perl-ExtUtils-Embed \        lua lua-devel patch -y        <span class="hljs-comment"># 下载 openssl 源码</span>wget https://www.openssl.org/<span class="hljs-built_in">source</span>/openssl-1.0.2j.tar.gz<span class="hljs-comment"># 下载 chacha20 补丁</span>wget https://raw.githubusercontent.com/cloudflare/sslconfig/master/patches/openssl__chacha20_poly1305_draft_and_rfc_ossl102j.patch<span class="hljs-comment"># 解压源码</span>tar -zxvf openssl-1.0.2j.tar.gz<span class="hljs-comment"># 打补丁</span>mv openssl__chacha20_poly1305_draft_and_rfc_ossl102j.patch openssl-1.0.2j<span class="hljs-built_in">cd</span> openssl-1.0.2j &amp;&amp; patch -p1 &lt; openssl__chacha20_poly1305_draft_and_rfc_ossl102j.patch</code></pre><h4 id="2-2、编译安装"><a href="#2-2、编译安装" class="headerlink" title="2.2、编译安装"></a>2.2、编译安装</h4><p>打过补丁以后就可以安装并进行替换了</p><pre><code class="hljs sh"><span class="hljs-comment"># 编译并安装</span>./config shared zlib-dynamicmake &amp;&amp; make install<span class="hljs-comment"># 备份原来的 openssl 以防不测</span>mv /usr/bin/openssl  /usr/bin/openssl.oldmv /usr/include/openssl  /usr/include/openssl.old<span class="hljs-comment"># 使用软连接方式替换</span>ln -s /usr/<span class="hljs-built_in">local</span>/ssl/bin/openssl  /usr/bin/opensslln -s /usr/<span class="hljs-built_in">local</span>/ssl/include/openssl  /usr/include/openssl<span class="hljs-comment"># libssl.so 不同操作系统位置不同，建议先 find 一下，然后挨个替换</span>ln -s /usr/<span class="hljs-built_in">local</span>/ssl/lib/libssl.so /usr/lib/libssl.soln -s /usr/<span class="hljs-built_in">local</span>/ssl/lib/libssl.so /usr/<span class="hljs-built_in">local</span>/lib64/libssl.so<span class="hljs-comment"># 刷新 共享库缓存</span><span class="hljs-built_in">echo</span> <span class="hljs-string">"/usr/local/ssl/lib"</span> &gt;&gt; /etc/ld.so.confldconfig -v<span class="hljs-comment"># 最后验证一下 如果都显示为 1.0.2j 表明成功</span>openssl versionstrings /usr/<span class="hljs-built_in">local</span>/lib64/libssl.so |grep OpenSSL</code></pre><h3 id="三、编译安装-Nginx"><a href="#三、编译安装-Nginx" class="headerlink" title="三、编译安装 Nginx"></a>三、编译安装 Nginx</h3><p>编译 Nginx 参数有很多，具体的可以自行更改，以下参考官方参数并且安装了 <a href="https://github.com/openresty/lua-nginx-module" target="_blank" rel="noopener">lua-nginx</a>、<a href="https://github.com/openresty/headers-more" target="_blank" rel="noopener">headers-more</a>、<a href="https://github.com/yaoweibin/nginx_upstream_check_module" target="_blank" rel="noopener">upstream_check</a>、<a href="https://github.com/simpl/ngx_devel_kit" target="_blank" rel="noopener">ngx_devel_kit</a></p><pre><code class="hljs sh"><span class="hljs-comment"># 定义版本</span><span class="hljs-built_in">export</span> NGINX_VERSION=<span class="hljs-string">"1.11.6"</span><span class="hljs-built_in">export</span> NGINX_LUA_MODULE_VERSION=<span class="hljs-string">"0.10.7"</span><span class="hljs-built_in">export</span> OPENSSL_VERSION=<span class="hljs-string">"1.0.1t"</span><span class="hljs-built_in">export</span> HEADERS_MORE_VERSION=<span class="hljs-string">"0.32"</span><span class="hljs-built_in">export</span> UPSTREAM_CHECK_VERSION=<span class="hljs-string">"0.3.0"</span><span class="hljs-built_in">export</span> DEVEL_KIT_VERSION=<span class="hljs-string">"0.3.0"</span><span class="hljs-built_in">export</span> LUAJIT_VERSION=<span class="hljs-string">"2.0.4"</span><span class="hljs-built_in">export</span> LUAJIT_MAIN_VERSION=<span class="hljs-string">"2.0"</span><span class="hljs-built_in">export</span> LUAJIT_LIB=<span class="hljs-string">"/usr/local/lib"</span><span class="hljs-built_in">export</span> LUAJIT_INC=<span class="hljs-string">"/usr/local/include/luajit-<span class="hljs-variable">$LUAJIT_MAIN_VERSION</span>"</span><span class="hljs-comment"># 下载相关源码</span>wget http://nginx.org/download/nginx-<span class="hljs-variable">$&#123;NGINX_VERSION&#125;</span>.tar.gzwget https://github.com/openresty/lua-nginx-module/archive/v<span class="hljs-variable">$&#123;NGINX_LUA_MODULE_VERSION&#125;</span>.tar.gz -O lua-nginx-module-v<span class="hljs-variable">$&#123;NGINX_LUA_MODULE_VERSION&#125;</span>.tar.gzwget https://github.com/openresty/headers-more-nginx-module/archive/v<span class="hljs-variable">$&#123;HEADERS_MORE_VERSION&#125;</span>.tar.gzwget https://github.com/yaoweibin/nginx_upstream_check_module/archive/v<span class="hljs-variable">$&#123;UPSTREAM_CHECK_VERSION&#125;</span>.tar.gzwget https://github.com/simpl/ngx_devel_kit/archive/v<span class="hljs-variable">$&#123;DEVEL_KIT_VERSION&#125;</span>.tar.gz -O ngx_devel_kit-v<span class="hljs-variable">$&#123;DEVEL_KIT_VERSION&#125;</span>.tar.gzwget http://luajit.org/download/LuaJIT-<span class="hljs-variable">$LUAJIT_VERSION</span>.tar.gz<span class="hljs-comment"># 解压</span><span class="hljs-keyword">for</span> tgzName <span class="hljs-keyword">in</span> `ls *.tar.gz`;<span class="hljs-keyword">do</span>    tar -zxvf <span class="hljs-variable">$tgzName</span><span class="hljs-keyword">done</span><span class="hljs-comment"># 编译并安装(上面下载是在 /usr/src 下进行的)</span>CONFIG_ARGS=<span class="hljs-string">"\</span><span class="hljs-string">    --prefix=<span class="hljs-variable">$&#123;PREFIX:-/usr/local/nginx&#125;</span> \</span><span class="hljs-string">    --pid-path=/var/run/nginx.pid \</span><span class="hljs-string">    --lock-path=/var/run/nginx.lock \</span><span class="hljs-string">    --with-http_ssl_module \</span><span class="hljs-string">    --with-http_realip_module \</span><span class="hljs-string">    --with-http_addition_module \</span><span class="hljs-string">    --with-http_sub_module \</span><span class="hljs-string">    --with-http_dav_module \</span><span class="hljs-string">    --with-http_flv_module \</span><span class="hljs-string">    --with-http_mp4_module \</span><span class="hljs-string">    --with-http_gunzip_module \</span><span class="hljs-string">    --with-http_gzip_static_module \</span><span class="hljs-string">    --with-http_random_index_module \</span><span class="hljs-string">    --with-http_secure_link_module \</span><span class="hljs-string">    --with-http_stub_status_module \</span><span class="hljs-string">    --with-http_auth_request_module \</span><span class="hljs-string">    --with-http_xslt_module=dynamic \</span><span class="hljs-string">    --with-http_image_filter_module=dynamic \</span><span class="hljs-string">    --with-http_geoip_module=dynamic \</span><span class="hljs-string">    --with-http_perl_module=dynamic \</span><span class="hljs-string">    --with-threads \</span><span class="hljs-string">    --with-stream \</span><span class="hljs-string">    --with-stream_ssl_module \</span><span class="hljs-string">    --with-stream_ssl_preread_module \</span><span class="hljs-string">    --with-stream_realip_module \</span><span class="hljs-string">    --with-stream_geoip_module=dynamic \</span><span class="hljs-string">    --with-http_slice_module \</span><span class="hljs-string">    --with-mail \</span><span class="hljs-string">    --with-mail_ssl_module \</span><span class="hljs-string">    --with-file-aio \</span><span class="hljs-string">    --with-http_v2_module \</span><span class="hljs-string">    --with-openssl=/usr/src/openssl-<span class="hljs-variable">$&#123;OPENSSL_VERSION&#125;</span> \</span><span class="hljs-string">    --add-module=/usr/src/headers-more-nginx-module-<span class="hljs-variable">$&#123;HEADERS_MORE_VERSION&#125;</span> \</span><span class="hljs-string">    --add-module=/usr/src/nginx_upstream_check_module-<span class="hljs-variable">$&#123;UPSTREAM_CHECK_VERSION&#125;</span> \</span><span class="hljs-string">    --add-module=/usr/src/ngx_devel_kit-<span class="hljs-variable">$&#123;DEVEL_KIT_VERSION&#125;</span> \</span><span class="hljs-string">    --add-module=/usr/src/lua-nginx-module-<span class="hljs-variable">$&#123;NGINX_LUA_MODULE_VERSION&#125;</span> \</span><span class="hljs-string">    --http-client-body-temp-path=/tmp/client_body_temp \</span><span class="hljs-string">    --http-proxy-temp-path=/tmp/proxy_temp \</span><span class="hljs-string">    --http-fastcgi-temp-path=/tmp/fastcgi_temp \</span><span class="hljs-string">    --http-uwsgi-temp-path=/tmp/uwsgi_temp \</span><span class="hljs-string">    --http-scgi-temp-path=/tmp/scgi_temp \</span><span class="hljs-string">    "</span><span class="hljs-comment"># 先安装 lua</span><span class="hljs-built_in">cd</span> /usr/src/LuaJIT-<span class="hljs-variable">$LUAJIT_VERSION</span>make -j$(getconf _NPROCESSORS_ONLN)make install<span class="hljs-comment"># 安装 nginx</span><span class="hljs-built_in">cd</span> /usr/src/nginx-<span class="hljs-variable">$NGINX_VERSION</span>./configure <span class="hljs-variable">$CONFIG_ARGS</span> --with-debugmake -j$(getconf _NPROCESSORS_ONLN)make install</code></pre><h3 id="四、配置-HTTPS"><a href="#四、配置-HTTPS" class="headerlink" title="四、配置 HTTPS"></a>四、配置 HTTPS</h3><p>主配置参考步骤一的 HTTPS 最佳实践，以下只做简要说明</p><p>前向保密 <code>dhparam.pem</code> 文件通过 <code>openssl dhparam 4096 &gt; dhparam.pem</code> 生成</p><p><code>ssl_trusted_certificate</code> 需要 CA 根证书，请根据具体证书 CA 自行下载</p><h3 id="五、验证-ATS"><a href="#五、验证-ATS" class="headerlink" title="五、验证 ATS"></a>五、验证 ATS</h3><pre><code class="hljs sh"><span class="hljs-comment"># 验证命令 如果 grep 到 FAIL 则说明配置不通过，</span><span class="hljs-comment"># 需重新检查配置，否则则证明 ATS 通过</span>nscurl --ats-diagnostics --verbose https://mritd.me | grep FAIL</code></pre>]]></content>
    
    
    <categories>
      
      <category>Linux</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Linux</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Traefik-kubernetes 初试</title>
    <link href="/2016/12/06/try-traefik-on-kubernetes/"/>
    <url>/2016/12/06/try-traefik-on-kubernetes/</url>
    
    <content type="html"><![CDATA[<blockquote><p>traefik 是一个前端负载均衡器，对于微服务架构尤其是 kubernetes 等编排工具具有良好的支持；同 nginx 等相比，traefik 能够自动感知后端容器变化，从而实现自动服务发现；今天小试了一下，在此记录一下使用过程</p></blockquote><h3 id="一、Kubernetes-服务暴露介绍"><a href="#一、Kubernetes-服务暴露介绍" class="headerlink" title="一、Kubernetes 服务暴露介绍"></a>一、Kubernetes 服务暴露介绍</h3><p>从 kubernetes 1.2 版本开始，kubernetes提供了 Ingress 对象来实现对外暴露服务；到目前为止 kubernetes 总共有三种暴露服务的方式:</p><ul><li>LoadBlancer Service</li><li>NodePort Service</li><li>Ingress</li></ul><h4 id="1-1、LoadBlancer-Service"><a href="#1-1、LoadBlancer-Service" class="headerlink" title="1.1、LoadBlancer Service"></a>1.1、LoadBlancer Service</h4><p>LoadBlancer Service 是 kubernetes 深度结合云平台的一个组件；当使用 LoadBlancer Service 暴露服务时，实际上是通过<strong>向底层云平台申请创建一个负载均衡器</strong>来向外暴露服务；目前 LoadBlancer Service 支持的云平台已经相对完善，比如国外的 GCE、DigitalOcean，国内的 阿里云，私有云 Openstack 等等，由于 LoadBlancer Service 深度结合了云平台，所以只能在一些云平台上来使用</p><h4 id="1-2、NodePort-Service"><a href="#1-2、NodePort-Service" class="headerlink" title="1.2、NodePort Service"></a>1.2、NodePort Service</h4><p>NodePort Service 顾名思义，实质上就是通过在集群的每个 node 上暴露一个端口，然后将这个端口映射到某个具体的 service 来实现的，虽然每个 node 的端口有很多(0~65535)，但是由于安全性和易用性(服务多了就乱了，还有端口冲突问题)实际使用可能并不多</p><h4 id="1-3、Ingress"><a href="#1-3、Ingress" class="headerlink" title="1.3、Ingress"></a>1.3、Ingress</h4><p>Ingress 这个东西是 1.2 后才出现的，通过 Ingress 用户可以实现使用 nginx 等开源的反向代理负载均衡器实现对外暴露服务，以下详细说一下 Ingress，毕竟 traefik 用的就是 Ingress</p><p><strong>使用 Ingress 时一般会有三个组件:</strong></p><ul><li>反向代理负载均衡器</li><li>Ingress Controller</li><li>Ingress</li></ul><h5 id="1-3-1、反向代理负载均衡器"><a href="#1-3-1、反向代理负载均衡器" class="headerlink" title="1.3.1、反向代理负载均衡器"></a>1.3.1、反向代理负载均衡器</h5><p>反向代理负载均衡器很简单，说白了就是 nginx、apache 什么的；在集群中反向代理负载均衡器可以自由部署，可以使用 Replication Controller、Deployment、DaemonSet 等等，不过个人喜欢以 DaemonSet 的方式部署，感觉比较方便</p><h5 id="1-3-2、Ingress-Controller"><a href="#1-3-2、Ingress-Controller" class="headerlink" title="1.3.2、Ingress Controller"></a>1.3.2、Ingress Controller</h5><p>Ingress Controller 实质上可以理解为是个监视器，Ingress Controller 通过不断地跟 kubernetes API 打交道，实时的感知后端 service、pod 等变化，比如新增和减少 pod，service 增加与减少等；当得到这些变化信息后，Ingress Controller 再结合下文的 Ingress 生成配置，然后更新反向代理负载均衡器，并刷新其配置，达到服务发现的作用</p><h5 id="1-3-3、Ingress"><a href="#1-3-3、Ingress" class="headerlink" title="1.3.3、Ingress"></a>1.3.3、Ingress</h5><p>Ingress 简单理解就是个规则定义；比如说某个域名对应某个 service，即当某个域名的请求进来时转发给某个 service;这个规则将与 Ingress Controller 结合，然后 Ingress Controller 将其动态写入到负载均衡器配置中，从而实现整体的服务发现和负载均衡</p><p><strong>有点懵逼，那就看图</strong></p><p><img src="https://cdn.oss.link/markdown/qflqj.jpg" srcset="/img/loading.gif" alt="Ingress"></p><p><strong>从上图中可以很清晰的看到，实际上请求进来还是被负载均衡器拦截，比如 nginx，然后 Ingress Controller 通过跟 Ingress 交互得知某个域名对应哪个 service，再通过跟 kubernetes API 交互得知 service 地址等信息；综合以后生成配置文件实时写入负载均衡器，然后负载均衡器 reload 该规则便可实现服务发现，即动态映射</strong></p><p><strong>了解了以上内容以后，这也就很好的说明了我为什么喜欢把负载均衡器部署为 Daemon Set；因为无论如何请求首先是被负载均衡器拦截的，所以在每个 node 上都部署一下，同时 hostport 方式监听 80 端口；那么就解决了其他方式部署不确定 负载均衡器在哪的问题，同时访问每个 node 的 80 都能正确解析请求；如果前端再 放个 nginx 就又实现了一层负载均衡</strong></p><h3 id="二、Traefik-使用"><a href="#二、Traefik-使用" class="headerlink" title="二、Traefik 使用"></a>二、Traefik 使用</h3><p>由于微服务架构以及 Docker 技术和 kubernetes 编排工具最近几年才开始逐渐流行，所以一开始的反向代理服务器比如 nginx、apache 并未提供其支持，毕竟他们也不是先知；所以才会出现 Ingress Controller 这种东西来做 kubernetes 和前端负载均衡器如 nginx 之间做衔接；<strong>即 Ingress Controller 的存在就是为了能跟 kubernetes 交互，又能写 nginx 配置，还能 reload 它，这是一种折中方案</strong>；而最近开始出现的 traefik 天生就是提供了对 kubernetes 的支持，<strong>也就是说 traefik 本身就能跟 kubernetes API 交互，感知后端变化，因此可以得知: 在使用 traefik 时，Ingress Controller 已经无卵用了，所以整体架构如下</strong></p><p><img src="https://cdn.oss.link/markdown/pot7r.jpg" srcset="/img/loading.gif" alt="traefik"></p><h4 id="2-1、部署-Traefik"><a href="#2-1、部署-Traefik" class="headerlink" title="2.1、部署 Traefik"></a>2.1、部署 Traefik</h4><p>已经从大体上搞懂了 Ingress 和 traefik，那么部署起来就很简单</p><h5 id="2-1-1、部署-Daemon-Set"><a href="#2-1-1、部署-Daemon-Set" class="headerlink" title="2.1.1、部署 Daemon Set"></a>2.1.1、部署 Daemon Set</h5><p><strong>首先以 Daemon Set 的方式在每个 node 上启动一个 traefik，并使用 hostPort 的方式让其监听每个 node 的 80 端口(有没有感觉这就是个 NodePort? 不过区别就是这个 Port 后面有负载均衡器 –&gt;手动微笑)</strong></p><pre><code class="hljs sh">kubectl create -f traefik.ds.yaml<span class="hljs-comment"># Daemon set 文件如下</span>apiVersion: extensions/v1beta1kind: DaemonSetmetadata:  name: traefik-ingress-lb  namespace: kube-system  labels:    k8s-app: traefik-ingress-lbspec:  template:    metadata:      labels:        k8s-app: traefik-ingress-lb        name: traefik-ingress-lb    spec:      terminationGracePeriodSeconds: 60      hostNetwork: <span class="hljs-literal">true</span>      restartPolicy: Always      containers:      - image: traefik        name: traefik-ingress-lb        resources:          limits:            cpu: 200m            memory: 30Mi          requests:            cpu: 100m            memory: 20Mi        ports:        - name: http          containerPort: 80          hostPort: 80        - name: admin          containerPort: 8580        args:        - --web        - --web.address=:8580        - --kubernetes</code></pre><p><strong>其中 traefik 监听 node 的 80 和 8580 端口，80 提供正常服务，8580 是其自带的 UI 界面，原本默认是 8080，因为环境里端口冲突了，所以这里临时改一下</strong></p><h5 id="2-1-2、部署-Ingress"><a href="#2-1-2、部署-Ingress" class="headerlink" title="2.1.2、部署 Ingress"></a>2.1.2、部署 Ingress</h5><p>从上面的长篇大论已经得知了 Ingress Controller 是无需部署的，所以直接部署 Ingress 即可</p><pre><code class="hljs sh">kubectl create -f traefik.ing.yaml<span class="hljs-comment"># Ingress 文件如下</span>apiVersion: extensions/v1beta1kind: Ingressmetadata:  name: traefik-ingressspec:  rules:  - host: traefik.www.test.com    http:      paths:      - path: /        backend:          serviceName: <span class="hljs-built_in">test</span>-www          servicePort: 8080  - host: traefik.api.test.com    http:      paths:      - path: /        backend:          serviceName: <span class="hljs-built_in">test</span>-api          servicePort: 8080</code></pre><p><strong>实际上事先集群中已经存在了相应的名为 test-www 和 test-api 的 service，对应的 service 后端也有很多 pod；所以这里就不在具体写部署实际业务容器(test-www、test-api)的过程了，各位测试时，只需要把这个 test 的 service 替换成自己业务的 service 即可</strong></p><h5 id="2-1-3、部署-Traefik-UI"><a href="#2-1-3、部署-Traefik-UI" class="headerlink" title="2.1.3、部署 Traefik UI"></a>2.1.3、部署 Traefik UI</h5><p>traefik 本身还提供了一套 UI 供我们使用，其同样以 Ingress 方式暴露，只需要创建一下即可</p><pre><code class="hljs sh">kubectl create -f ui.yaml<span class="hljs-comment"># ui yaml 如下</span>---apiVersion: v1kind: Servicemetadata:  name: traefik-web-ui  namespace: kube-systemspec:  selector:    k8s-app: traefik-ingress-lb  ports:  - name: web    port: 80    targetPort: 8580---apiVersion: extensions/v1beta1kind: Ingressmetadata:  name: traefik-web-ui  namespace: kube-systemspec:  rules:  - host: traefik-ui.local    http:      paths:      - path: /        backend:          serviceName: traefik-web-ui          servicePort: web</code></pre><h5 id="2-1-4、访问测试"><a href="#2-1-4、访问测试" class="headerlink" title="2.1.4、访问测试"></a>2.1.4、访问测试</h5><p>都创建无误以后，只需要将待测试的域名解析到任意一台 node 上即可，页面就不截图了，截图就暴露了…..下面来两张 ui 的</p><p><img src="https://cdn.oss.link/markdown/i32ab.jpg" srcset="/img/loading.gif" alt="traefik ui"></p><p><img src="https://cdn.oss.link/markdown/1qtmb.jpg" srcset="/img/loading.gif" alt="traefik ui health"></p><h4 id="2-2、健康检查"><a href="#2-2、健康检查" class="headerlink" title="2.2、健康检查"></a>2.2、健康检查</h4><p>关于健康检查，测试可以使用 kubernetes 的 Liveness Probe 实现，如果 Liveness Probe检查失败，则 traefik 会自动移除该 pod，以下是一个 示例</p><p><strong>test 的 deployment，健康检查方式是 <code>cat /tmp/health</code>，容器启动 2 分钟后会删掉这个文件，模拟健康检查失败</strong></p><pre><code class="hljs sh">apiVersion: v1kind: DeploymentapiVersion: extensions/v1beta1metadata:  name: <span class="hljs-built_in">test</span>  namespace: default  labels:    <span class="hljs-built_in">test</span>: alpinespec:  replicas: 1  selector:    matchLabels:      <span class="hljs-built_in">test</span>: alpine  template:    metadata:      labels:        <span class="hljs-built_in">test</span>: alpine        name: <span class="hljs-built_in">test</span>    spec:      containers:      - image: mritd/alpine:3.4        name: alpine        resources:          limits:            cpu: 200m            memory: 30Mi          requests:            cpu: 100m            memory: 20Mi        ports:        - name: http          containerPort: 80        args:        <span class="hljs-built_in">command</span>:        - <span class="hljs-string">"bash"</span>        - <span class="hljs-string">"-c"</span>        - <span class="hljs-string">"echo ok &gt; /tmp/health;sleep 120;rm -f /tmp/health"</span>        livenessProbe:          <span class="hljs-built_in">exec</span>:            <span class="hljs-built_in">command</span>:            - cat            - /tmp/health          initialDelaySeconds: 20</code></pre><p><strong>test 的 service</strong></p><pre><code class="hljs sh">apiVersion: v1kind: Servicemetadata:  name: <span class="hljs-built_in">test</span>   labels:    name: <span class="hljs-built_in">test</span>spec:  ports:  - port: 8123    targetPort: 80  selector:    name: <span class="hljs-built_in">test</span></code></pre><p><strong>test 的 Ingress</strong></p><pre><code class="hljs sh">apiVersion: extensions/v1beta1kind: Ingressmetadata:  name: <span class="hljs-built_in">test</span>spec:  rules:  - host: test.com    http:      paths:      - path: /        backend:          serviceName: <span class="hljs-built_in">test</span>          servicePort: 8123</code></pre><p><strong>全部创建好以后，进入 traefik ui 界面，可以观察到每隔 2 分钟健康检查失败后，kubernetes 重建 pod，同时 traefik 会从后端列表中移除这个 pod</strong></p><p><strong>其他更多玩法请参考 <a href="https://docs.traefik.io/" target="_blank" rel="noopener">官方文档</a>(我发现他居然支持 Let’s Entrypt，个人博客福音啊)，如有错误欢迎指正</strong></p>]]></content>
    
    
    <categories>
      
      <category>Kubernetes</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Linux</tag>
      
      <tag>Docker</tag>
      
      <tag>Kubernetes</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>LVS 笔记</title>
    <link href="/2016/12/05/lvs-note/"/>
    <url>/2016/12/05/lvs-note/</url>
    
    <content type="html"><![CDATA[<blockquote><p>LVS(Linux Virtual Server) 意为Linux虚拟服务器，是针对高可伸缩、高可用网络服务要求基于 IP 层和内容请求分发的负载均衡调度解决方案</p></blockquote><h3 id="一、LVS-负载均衡技术"><a href="#一、LVS-负载均衡技术" class="headerlink" title="一、LVS 负载均衡技术"></a>一、LVS 负载均衡技术</h3><h4 id="1-1、Virtual-Server-via-Network-Address-Translation-VS-NAT"><a href="#1-1、Virtual-Server-via-Network-Address-Translation-VS-NAT" class="headerlink" title="1.1、Virtual Server via Network Address Translation(VS/NAT)"></a>1.1、Virtual Server via Network Address Translation(VS/NAT)</h4><p>此种负载均衡技术实质上通过重写请求和响应报文的目标地址及源地址实现网络地址转换的；在请求时，前端负载均衡器首先根据特定算法选择使用哪个后端真实服务器，然后改写请求报文的目标地址再将请求报文转发给后端的真实服务器；后端服务器返回时，前端负载均衡器再次改写响应报文的源地址，并完成响应转发；<strong>由于请求响应报文都需要通过前端负载均衡器进行地址重写，所以此种负载均衡技术对前端负载均衡器性能要求较高，业务量大的情况下前端负载均衡器可能成为瓶颈</strong></p><h4 id="1-2、Virtual-Server-via-IP-Tunneling-VS-TUN"><a href="#1-2、Virtual-Server-via-IP-Tunneling-VS-TUN" class="headerlink" title="1.2、Virtual Server via IP Tunneling(VS/TUN)"></a>1.2、Virtual Server via IP Tunneling(VS/TUN)</h4><p>TUN 模式采用 NAT 技术，在请求时，前端负载均衡器通过 <strong>IP 隧道</strong> 直接转发给后端特定的的真实服务器(根据指定算法)；响应时后端真实服务器直接返回报文给客户端；<strong>由于响应报文不需要再经过前端负载均衡器，所以极大地提高了吞吐量，但是使用此种模式后端服务器必须支持 IP 隧道协议</strong></p><h4 id="1-3、Virtual-Server-via-Direct-Routing-VS-DR"><a href="#1-3、Virtual-Server-via-Direct-Routing-VS-DR" class="headerlink" title="1.3、Virtual Server via Direct Routing(VS/DR)"></a>1.3、Virtual Server via Direct Routing(VS/DR)</h4><p>相对于 VS/TUN 方式，由于 TUN 方式仍需要打开 IP 隧道，所以在请求时仍有很大开销；而此种模式通过直接改写请求报文的 MAC 地址实现，MAC 地址改写后直接根据指定算法转发到后端服务器，后端服务器响应时也直接返回给客户端而不经过前端负载均衡器，所以此种模式吞吐量会更大；<strong>虽然没有了 IP 隧道开销，但是此种模式要求前端负载均衡器必须与后端真实服务器必须在同一网段下，而由于同一网段下机器数量有限，所以也限制了应用范围</strong></p><h3 id="二、负载均衡调度算法"><a href="#二、负载均衡调度算法" class="headerlink" title="二、负载均衡调度算法"></a>二、负载均衡调度算法</h3><h4 id="2-1、轮询算法-RR"><a href="#2-1、轮询算法-RR" class="headerlink" title="2.1、轮询算法(RR)"></a>2.1、轮询算法(RR)</h4><p>轮询(Round Robin) 算法简称 RR，负载均衡器通过轮询调度算法将外部请求轮流分配到集群的各个后端服务器上，对待后端服务器属于 “无差别攻击”，所以也会忽略后端服务器的实际负载等</p><h4 id="2-2、加权轮询算法-WRR"><a href="#2-2、加权轮询算法-WRR" class="headerlink" title="2.2、加权轮询算法(WRR)"></a>2.2、加权轮询算法(WRR)</h4><p>加权轮询(Weighted Round Robin) 根据真实服务器的不同处理能力(自动询问)来处理请求分发，轮询时处理能力强的服务器会得到更多的请求分发</p><h4 id="2-3、最少连接算法-LC"><a href="#2-3、最少连接算法-LC" class="headerlink" title="2.3、最少连接算法(LC)"></a>2.3、最少连接算法(LC)</h4><p>使用最少链接(Least Connections)时，前端负载均衡器会自动选择后端真实服务器中连接数最少的服务器进行请求分发，<strong>如果后端服务器性能相近，则能够很好地负载均衡，否则可能造成性能问题</strong></p><h4 id="2-4、加权最少连接算法"><a href="#2-4、加权最少连接算法" class="headerlink" title="2.4、加权最少连接算法"></a>2.4、加权最少连接算法</h4><p>加权最少连接算法算法(Weighted Least Connections)即在 LC 基础上增加服务器性能权重，通过自动询问后端服务器性能和连接情况综合分发请求</p><h4 id="2-5、基于局部性最少链接算法-LBLC"><a href="#2-5、基于局部性最少链接算法-LBLC" class="headerlink" title="2.5、基于局部性最少链接算法(LBLC)"></a>2.5、基于局部性最少链接算法(LBLC)</h4><p>基于局部性最少连接算法(Locality-Based Least Connections)类似 IP 亲和技术加上最少链接算法；请求分发时，前端负载均衡器根据请求目标 IP 查找该请求目标 IP 最近使用的服务器，如果该服务器没有超载则将请求分发给该服务器；否则采用最小连接算法选择一台服务器分发请求</p><h4 id="2-6、带复制的基于局部性最少连接算法-LBLCR"><a href="#2-6、带复制的基于局部性最少连接算法-LBLCR" class="headerlink" title="2.6、带复制的基于局部性最少连接算法(LBLCR)"></a>2.6、带复制的基于局部性最少连接算法(LBLCR)</h4><p>带复制的基于局部性最少连接算法(Locality-Based Least Connections with Replications) 也是针对 IP 的负载均衡技术；与 LBLC 相比，该算法维护一个从目标 IP 到服务器组的映射关系；分发请求时，根据目标 IP 选择对应的服务器组，然后按照最小链接算法选择一台服务器进行请求分发，如果选中的服务器已经超载，则使用最小链接算法从服务器组外选择一台服务器进行请求分发，同时将该服务器加入到目标 IP 对应的服务器组中</p><h4 id="2-7、目标地址散列算法-DH"><a href="#2-7、目标地址散列算法-DH" class="headerlink" title="2.7、目标地址散列算法(DH)"></a>2.7、目标地址散列算法(DH)</h4><p>目标地址散列算法(Destination Hashing)使用请求的目标 IP 地址作为散列键，然后从静态分配的散列表中找出对应的真实服务器进行请求分发，如果该服务器超载，则返回空</p><h4 id="2-8、源地址散列算法-SH"><a href="#2-8、源地址散列算法-SH" class="headerlink" title="2.8、源地址散列算法(SH)"></a>2.8、源地址散列算法(SH)</h4><p>同 DH 类似，只不过散列键换成请求源 IP 地址而已</p><h3 id="三、LVS-负载均衡配置"><a href="#三、LVS-负载均衡配置" class="headerlink" title="三、LVS 负载均衡配置"></a>三、LVS 负载均衡配置</h3><h4 id="3-1、软件安装"><a href="#3-1、软件安装" class="headerlink" title="3.1、软件安装"></a>3.1、软件安装</h4><pre><code class="hljs sh">yum install ipvsadm</code></pre><p>安装后总共3个可执行文件，如下</p><table><thead><tr><th>命令</th><th>作用</th></tr></thead><tbody><tr><td><code>ipvsadm</code></td><td>LVS 主程序，负责 RS 天剑、删除和修改</td></tr><tr><td><code>ipvsadm-save</code></td><td>备份 LVS 配置</td></tr><tr><td><code>ipvsadm-restore</code></td><td>恢复 LVS 配置</td></tr></tbody></table><p><strong><code>ipvsadm</code> 常用参数如下</strong></p><table><thead><tr><th>参数</th><th>说明</th></tr></thead><tbody><tr><td><code>-A</code></td><td>在内核虚拟服务器列表中添加一条新的虚拟服务器记录</td></tr><tr><td><code>-E</code></td><td>编辑在内核虚拟服务器列表中的一条虚拟服务器记录</td></tr><tr><td><code>-D</code></td><td>删除内和虚拟服务器列表中的一条虚拟服务器记录</td></tr><tr><td><code>-C</code></td><td>清除内核虚拟服务器列表所有记录</td></tr><tr><td><code>-R</code></td><td>恢复虚拟服务器规则</td></tr><tr><td><code>-S</code></td><td>保存虚拟服务器规则，输出为 <code>-R</code> 可读的格式</td></tr><tr><td><code>-a</code></td><td>在内核虚拟服务器列表中添加一条真实服务器记录</td></tr><tr><td><code>-e</code></td><td>编辑内核虚拟服务器中一条真实服务器记录</td></tr><tr><td><code>-d</code></td><td>删除内核虚拟服务器列表中一条真实服务器记录</td></tr><tr><td><code>-L</code>、<code>-l</code></td><td>显示内核虚拟服务器列表</td></tr><tr><td><code>-Z</code></td><td>内核虚拟服务器列表计数器清零(清除链接数量等)</td></tr><tr><td><code>-set</code></td><td>- tcp tcpfin udp 设置连接超时值</td></tr><tr><td><code>--start-daemon</code></td><td>启动同步守护进程</td></tr><tr><td><code>--stop-daemon</code></td><td>停止同步守护进程</td></tr><tr><td><code>-daemon</code></td><td>显示同步守护进程状态</td></tr><tr><td><code>-h</code></td><td>帮助信息</td></tr><tr><td><code>-t</code></td><td>声明该虚拟服务器提供的是 TCP 服务(用于添加时)</td></tr><tr><td><code>-u</code></td><td>声明该虚拟服务器提供的是 UDP 服务(用于添加时)</td></tr><tr><td><code>-f</code></td><td>声明是经过 <code>iptables</code> 标记过的服务类型</td></tr><tr><td><code>-s</code></td><td>使用的负载均衡调度算法(rr、wrr、lc、wlc、lblc、lblcr、dh、sh、sed、nq)</td></tr><tr><td><code>-p</code></td><td>声明提供持久服务</td></tr><tr><td><code>-r</code></td><td>声明是一台真是的服务器</td></tr><tr><td><code>-g</code></td><td>指定 LVS 工作模式为直接路由模式</td></tr><tr><td><code>-i</code></td><td>指定 LVS 工作模式为隧道模式</td></tr><tr><td><code>-m</code></td><td>指定 LVS 工作模式为 NAT 模式</td></tr><tr><td><code>-w</code></td><td>设置真实服务器的权重值</td></tr><tr><td><code>-c</code></td><td>显示 LVS 目前的连接数</td></tr><tr><td><code>-timeout</code></td><td>显示 tcp tcpfin udp 的超时时间</td></tr><tr><td><code>--stats</code></td><td>显示统计信息</td></tr><tr><td><code>--rate</code></td><td>显示速率信息</td></tr><tr><td><code>--sort</code></td><td>对虚拟服务器和真实服务器排序输出</td></tr><tr><td><code>-n</code></td><td>输出 IP 地址和端口的数字形式</td></tr></tbody></table><h4 id="3-2、基于-NAT-模式"><a href="#3-2、基于-NAT-模式" class="headerlink" title="3.2、基于 NAT 模式"></a>3.2、基于 NAT 模式</h4><p>服务器列表</p><table><thead><tr><th>IP</th><th>节点</th></tr></thead><tbody><tr><td>192.168.1.100</td><td>前端负载均衡器</td></tr><tr><td>192.168.1.150</td><td>虚拟 IP</td></tr><tr><td>192.168.1.104</td><td>真实服务器</td></tr><tr><td>192.168.1.105</td><td>真实服务器</td></tr></tbody></table><h5 id="3-2-1、前端负载均衡器配置"><a href="#3-2-1、前端负载均衡器配置" class="headerlink" title="3.2.1、前端负载均衡器配置"></a>3.2.1、前端负载均衡器配置</h5><pre><code class="hljs sh"><span class="hljs-comment"># 安装 ipvsadm</span>yum install ipvsadm -y<span class="hljs-comment"># 开启 ip_forward</span><span class="hljs-built_in">echo</span> 1 &gt; /proc/sys/net/ipv4/ip_forward<span class="hljs-comment"># 安装 LVS 服务</span>ipvsadm -A -t 192.168.1.150:80<span class="hljs-comment"># 增加 realserver</span>ipvsadm -a -t 192.168.1.150:80 -r 192.168.1.104:80 -m -w 1ipvsadm -a -t 192.168.1.150:80 -r 192.168.1.105:80 -m -w 1</code></pre><h5 id="3-2-2、后端真实服务器配置"><a href="#3-2-2、后端真实服务器配置" class="headerlink" title="3.2.2、后端真实服务器配置"></a>3.2.2、后端真实服务器配置</h5><pre><code class="hljs sh"><span class="hljs-comment"># 关闭 ip_forward</span><span class="hljs-built_in">echo</span> 0 &gt; /proc/sys/net/ipv4/ip_forward<span class="hljs-comment"># 设置 VIP</span><span class="hljs-comment"># 复制一个网卡配置，在主网卡上绑定一个虚拟网卡</span>cp /etc/sysconfig/network-scripts/ifcfg-enp0s3 /etc/sysconfig/network-scripts/ifcfg-enp0s3:1<span class="hljs-comment"># 编辑虚拟网卡，修改其 IP 为 192.168.1.150，</span><span class="hljs-comment"># 注意重新生成 uuid 并更新 DEVICE 设备名称为 enp0s3:1</span>vim /etc/sysconfig/network-scripts/ifcfg-enp0s3:1<span class="hljs-comment"># 处理 arp 广播问题</span><span class="hljs-built_in">echo</span> 1 &gt; /proc/sys/net/ipv4/conf/enp0s3/arp_ignore<span class="hljs-built_in">echo</span> 2 &gt; /proc/sys/net/ipv4/conf/enp0s3/arp_announce<span class="hljs-built_in">echo</span> 1 &gt; /proc/sys/net/ipv4/conf/all/arp_ignore<span class="hljs-built_in">echo</span> 2 &gt; /proc/sys/net/ipv4/conf/all/arp_announce<span class="hljs-comment"># 设置路由</span>route add -host 192.168.1.150 dev enp0s3</code></pre><h5 id="3-2-3、测试-LVS"><a href="#3-2-3、测试-LVS" class="headerlink" title="3.2.3、测试 LVS"></a>3.2.3、测试 LVS</h5><p>在两台后端服务器上分别安装 nginx，并更改 index 页面，打印当前服务器 IP，最后在前端负载均衡器上测试访问虚拟 IP，此时前端负载均衡器将会自动根据算法负载到后端；此例中可以看到不断请求虚拟 IP 时，实际后端在做轮训</p><p><img src="https://cdn.oss.link/markdown/qjo23.jpg" srcset="/img/loading.gif" alt="test lvs1"></p><h4 id="3-3、基于-DR-模式"><a href="#3-3、基于-DR-模式" class="headerlink" title="3.3、基于 DR 模式"></a>3.3、基于 DR 模式</h4><h5 id="3-3-1、前端负载均衡器配置"><a href="#3-3-1、前端负载均衡器配置" class="headerlink" title="3.3.1、前端负载均衡器配置"></a>3.3.1、前端负载均衡器配置</h5><p>服务器列表同上面的相同，ipvsadm 安装掠过</p><pre><code class="hljs sh"><span class="hljs-comment"># 先清除上次的配置</span>ipvsadm -C<span class="hljs-comment"># 开启 ip_forward(实际上面已经操作过)</span><span class="hljs-built_in">echo</span> 1 &gt; /proc/sys/net/ipv4/ip_forward<span class="hljs-comment"># 安装 LVS 服务</span>ipvsadm -A -t 192.168.1.150:80<span class="hljs-comment"># 增加两台 realserver 使用 DR 模式</span>ipvsadm -a -t 192.168.1.150:80 -r 192.168.1.104:80 -g -w 1ipvsadm -a -t 192.168.1.150:80 -r 192.168.1.105:80 -g -w 1</code></pre><h5 id="1-3-2、后端真实服务器配置"><a href="#1-3-2、后端真实服务器配置" class="headerlink" title="1.3.2、后端真实服务器配置"></a>1.3.2、后端真实服务器配置</h5><p>后端配置同上，测试掠过</p><h4 id="3-4、基于-TUN-模式"><a href="#3-4、基于-TUN-模式" class="headerlink" title="3.4、基于 TUN 模式"></a>3.4、基于 TUN 模式</h4><p><strong>从上面两个配置可以看到，不同模式实际上只是修改前端负载均衡器创建后端真实服务器的命令参数不通而已(<code>-m</code>、<code>-g</code>、<code>-i</code>)；唯一要注意的是不同模式对后端服务器要求可能不通，比如 DR 要求前端负载均衡器与后端真实服务器在同一网段，TUN 要求后端真实服务器支持 IP 隧道协议等，所以 TUN 模式在此也不在演示；关于上面修改内核参数的做法最好在 <code>sysctl.conf</code> 中修改保证永久生效</strong></p>]]></content>
    
    
    <categories>
      
      <category>Linux</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Linux</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Kubernetes 网络搭建-Calico</title>
    <link href="/2016/12/01/set-up-kubernetes-cluster-by-calico/"/>
    <url>/2016/12/01/set-up-kubernetes-cluster-by-calico/</url>
    
    <content type="html"><![CDATA[<blockquote><p>接上一篇，大早上试下 Calico，从目前的各种评论上来看 Calico 的性能要更好些，不过由于是纯三层的解决方案，某些用到二层的应用可能无法使用，不过目前还没遇到过，个人理解这种情况应该不多</p></blockquote><h3 id="一、环境准备"><a href="#一、环境准备" class="headerlink" title="一、环境准备"></a>一、环境准备</h3><p>首先有个 kubernetes 集群，集群网络处于未部署状态，集群信息如下</p><table><thead><tr><th>IP地址</th><th>节点</th></tr></thead><tbody><tr><td>192.168.1.101</td><td>master</td></tr><tr><td>192.168.1.102</td><td>node,etcd(单点)</td></tr><tr><td>192.168.1.103</td><td>node</td></tr></tbody></table><h3 id="二、开搞"><a href="#二、开搞" class="headerlink" title="二、开搞"></a>二、开搞</h3><p>至于 kubernetes 集群创建实在不想啰嗦，具体参考上一篇博客</p><p>Calico 官方提供了很好的文档支持，<a href="http://docs.projectcalico.org/v1.6/getting-started/kubernetes/" target="_blank" rel="noopener">在这里</a> 基本能找到所有的参考教程，以下直接照着官方文档来</p><p>首先把 Calico 的 yaml 下载下来，这里采用官方文档 kubernetes 页面的 yaml，<strong>非 kubeadm 的</strong>，kubeadm 页面的 yaml 里面 多了创建 etcd 集群信息啥的，没什么卵用</p><pre><code class="hljs sh">wget http://docs.projectcalico.org/v1.6/getting-started/kubernetes/installation/hosted/calico.yaml</code></pre><p>编辑 <code>calico.yaml</code>，修改 etcd 地址</p><pre><code class="hljs sh">vim calico.yaml<span class="hljs-comment"># 将 etcd_endpoints 修改掉即可</span>etcd_endpoints: <span class="hljs-string">"http://192.168.1.102:2379"</span></code></pre><p>然后创建网络</p><pre><code class="hljs sh">kubectl create -f calico.yaml</code></pre><p>创建完成后如下</p><p><img src="https://cdn.oss.link/markdown/ub8yg.jpg" srcset="/img/loading.gif" alt="Calico"></p><p>节点测试如下</p><p><img src="https://cdn.oss.link/markdown/p7zlt.jpg" srcset="/img/loading.gif" alt="all node"></p><p><img src="https://cdn.oss.link/markdown/ybdw5.jpg" srcset="/img/loading.gif" alt="node2"></p><p><img src="https://cdn.oss.link/markdown/3qm8t.jpg" srcset="/img/loading.gif" alt="node3"></p><p><strong>更细节的性能体现等可参考 <a href="http://blog.dataman-inc.com/shurenyun-docker-133/" target="_blank" rel="noopener">将Docker网络方案进行到底</a></strong></p>]]></content>
    
    
    <categories>
      
      <category>Kubernetes</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Linux</tag>
      
      <tag>Docker</tag>
      
      <tag>Kubernetes</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Kubernetes 网络搭建-flannel</title>
    <link href="/2016/11/30/set-up-kubernetes-cluster-by-flannel/"/>
    <url>/2016/11/30/set-up-kubernetes-cluster-by-flannel/</url>
    
    <content type="html"><![CDATA[<blockquote><p>一直用 weave，本篇记录一下 kubernetes 使用 flannel 作为网络组件，flannel 以 pod 方式部署</p></blockquote><h3 id="一、环境准备"><a href="#一、环境准备" class="headerlink" title="一、环境准备"></a>一、环境准备</h3><p>首先有个 kubernetes 集群，集群网络处于未部署状态，集群信息如下</p><table><thead><tr><th>IP地址</th><th>节点</th></tr></thead><tbody><tr><td>192.168.1.101</td><td>master</td></tr><tr><td>192.168.1.102</td><td>node,etcd(单点)</td></tr><tr><td>192.168.1.103</td><td>node</td></tr></tbody></table><h3 id="二、开搞"><a href="#二、开搞" class="headerlink" title="二、开搞"></a>二、开搞</h3><h4 id="2-1、创建-kubernetes-集群"><a href="#2-1、创建-kubernetes-集群" class="headerlink" title="2.1、创建 kubernetes 集群"></a>2.1、创建 kubernetes 集群</h4><p>具体各种注意细节这里不再阐述，请参考本博客其他文章，<strong>唯一要注意一点是创建集群(init)时要增加 <code>--pod-network-cidr 10.244.0.0/16</code> 参数；</strong>网段根据需要自己指定，如果不使用 <code>--pod-network-cidr</code>  参数，则 flannel pod 启动后会出现 <code>failed to register network: failed to acquire lease: node &quot;xxxxxx&quot; pod cidr not assigned</code> 错误，以下为部分样例命令</p><pre><code class="hljs sh"><span class="hljs-comment"># 安装 rpm</span>tee /etc/yum.repos.d/mritd.repo &lt;&lt; EOF[mritdrepo]name=Mritd Repositorybaseurl=https://rpm.mritd.me/centos/7/x86_64enabled=1gpgcheck=1gpgkey=https://cdn.oss.link/keys/rpm.public.keyEOFyum install -y kubelet kubectl kubernetes-cni kubeadm<span class="hljs-comment"># 处理 hostname</span><span class="hljs-built_in">echo</span> <span class="hljs-string">"192-168-1-101.master"</span> &gt; /etc/hostname<span class="hljs-built_in">echo</span> <span class="hljs-string">"127.0.0.1   192-168-1-101.master"</span> &gt;&gt; /etc/hostssysctl kernel.hostname=<span class="hljs-string">"192-168-1-101.master"</span><span class="hljs-comment"># load 镜像</span>images=(kube-proxy-amd64:v1.4.6 kube-discovery-amd64:1.0 kubedns-amd64:1.7 kube-scheduler-amd64:v1.4.6 kube-controller-manager-amd64:v1.4.6 kube-apiserver-amd64:v1.4.6 etcd-amd64:2.2.5 kube-dnsmasq-amd64:1.3 exechealthz-amd64:1.1 pause-amd64:3.0 kubernetes-dashboard-amd64:v1.4.1)<span class="hljs-keyword">for</span> imageName <span class="hljs-keyword">in</span> <span class="hljs-variable">$&#123;images[@]&#125;</span> ; <span class="hljs-keyword">do</span>  docker pull mritd/<span class="hljs-variable">$imageName</span>  docker tag mritd/<span class="hljs-variable">$imageName</span> gcr.io/google_containers/<span class="hljs-variable">$imageName</span>  docker rmi mritd/<span class="hljs-variable">$imageName</span><span class="hljs-keyword">done</span><span class="hljs-comment"># 其他的什么 dns、etcd 搞完了直接初始化</span>kubeadm init --api-advertise-addresses 192.168.1.101 --external-etcd-endpoints http://192.168.1.102:2379 --use-kubernetes-version v1.4.6 --pod-network-cidr 10.244.0.0/16</code></pre><h4 id="2-2、创建-flannel-网络"><a href="#2-2、创建-flannel-网络" class="headerlink" title="2.2、创建 flannel 网络"></a>2.2、创建 flannel 网络</h4><p>前面如果都设置好创建网络很简单，跟 weave 一样</p><pre><code class="hljs sh">kubectl create -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml</code></pre><p>有兴趣的可以把 yml 搞下来看下，由于他的镜像托管在 <code>quay.io</code>，所以没有墙的问题，也可以提前 load 进来；对于 yml 上面的 <code>ConfigMap</code> 中的 ip 段最好与 <code>--pod-network-cidr</code> 一致(不一致没测试，想作死自己试吧)，然后稍等片刻网络便创建成功，截图如下</p><p><img src="https://cdn.oss.link/markdown/fh723.jpg" srcset="/img/loading.gif" alt="flannel"></p><h4 id="2-3、网络测试"><a href="#2-3、网络测试" class="headerlink" title="2.3、网络测试"></a>2.3、网络测试</h4><p>由于环境有限(virtualbox 虚拟机)，所以暂时只测试一下网络互通是否有问题，关于性能啥的由于本人对网络部分也一直是个短板，需要大神们自己来了，如果可以给篇测试报告我也看看 <code>:)</code></p><p><strong>rc 如下</strong></p><pre><code class="hljs sh">apiVersion: v1kind: ReplicationControllermetadata:  name: alpine  labels:    name: alpinespec:  replicas: 2  selector:    name: alpine  template:    metadata:      labels:        name: alpine    spec:      containers:        - image: mritd/alpine:3.4          imagePullPolicy: Always          name: alpine          <span class="hljs-built_in">command</span>:             - <span class="hljs-string">"bash"</span>             - <span class="hljs-string">"-c"</span>            - <span class="hljs-string">"while true;do echo test;done"</span>          ports:            - containerPort: 8080              name: alpine</code></pre><p><strong>去两个主机上分别进入容器，然后互 ping 集群 IP 可以 ping 通</strong>；图2 ping 错了，不重新截图了，谅解</p><p><img src="https://cdn.oss.link/markdown/x4i0j.jpg" srcset="/img/loading.gif" alt="cluster ip"></p><p><img src="https://cdn.oss.link/markdown/v24ju.jpg" srcset="/img/loading.gif" alt="node2 ping"></p><p><img src="https://cdn.oss.link/markdown/iukrh.jpg" srcset="/img/loading.gif" alt="node3 ping"></p><p><strong>本文只是简单搭建，其他更高级的性能测试交给各位玩网络的大神吧</strong></p>]]></content>
    
    
    <categories>
      
      <category>Kubernetes</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Linux</tag>
      
      <tag>Docker</tag>
      
      <tag>Kubernetes</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>kubeadm 续坑篇</title>
    <link href="/2016/11/21/kubeadm-other-problems/"/>
    <url>/2016/11/21/kubeadm-other-problems/</url>
    
    <content type="html"><![CDATA[<blockquote><p>断断续续鼓捣 kubeadm 搭建集群已经很长时间了，目前 kubeadm 已经进入了 beat 阶段，各项功能相对稳定，但是继上篇 <a href="https://mritd.me/2016/10/29/set-up-kubernetes-cluster-by-kubeadm/" target="_blank" rel="noopener">kubeadm 搭建 kubernetes 集群</a> 之后还是踩了许多坑，在此记录一下</p></blockquote><h3 id="一、etcd-单点问题"><a href="#一、etcd-单点问题" class="headerlink" title="一、etcd 单点问题"></a>一、etcd 单点问题</h3><p>默认 kubeadm 创建的集群会在内部启动一个单点的 etcd，当然大部分情况下 etcd 还是很稳定的，<strong>但是一但 etcd 由于某种原因挂掉，这个问题会非常严重，会导致整个集群不可用</strong>。具体原因是 etcd 存储着 kubernetes 各种元数据信息；包括 <code>kubectl get pod</code> 等等基础命令实际上全部是调用 RESTful API 从 etcd 中获取的信息；<strong>所以一但 etcd 挂掉以后，基本等同于 <code>kubectl</code> 命令不可用，此时将变为 ‘瞎子’，集群各节点也会因无法从 etcd 获取数据而出现无法调度，最终挂掉</strong>。</p><p><strong>解决办法是在使用 kubeadm 创建集群时使用 <code>--external-etcd-endpoints</code> 参数指定外部 etcd 集群，此时 kubeadm 将不会在内部创建 etcd，转而使用外部我们指定的 etcd 集群，如果外部 etcd 集群配置了 SSL 加密，那么还需要配合 <code>--external-etcd-cafile</code>、<code>--external-etcd-certfile</code>、<code>--external-etcd-keyfile</code> 三个参数指定 etcd 的 CA证书、CA签发的使用证书和私钥文件，命令如下</strong></p><pre><code class="hljs sh"><span class="hljs-comment"># 非 SSL</span>kubeadm init --external-etcd-endpoints http://192.168.1.100:2379<span class="hljs-comment"># etcd SSL</span>kubeadm init --external-etcd-endpoints https://192.168.1.100:2379 --external-etcd-cafile /path/to/ca --external-etcd-certfile /path/to/cert --external-etcd-keyfile /path/to/privatekey</code></pre><h3 id="二、etcd-不可与-master-同在"><a href="#二、etcd-不可与-master-同在" class="headerlink" title="二、etcd 不可与 master 同在"></a>二、etcd 不可与 master 同在</h3><p>‘愿上帝与你同在’……这个坑是由于 kubeadm 的 check 机制的 bug 造成的，目前还没有修复；表现为 <strong>当 etcd 与 master 在同一节点时，kubeadm init 会失败，同时报错信息提示 ‘已经存在了 <code>/var/lib/etcd</code> 目录，或者 2379 端口被占用’</strong>；因为默认 kubeadm 会创建 etcd，而默认的 etcd 会占用这个目录和 2379 端口，<strong>即使你加了 <code>--external-etcd-endpoints</code> 参数，kubeadm 仍然会检测这两项条件是否满足，不满足则禁止 init 操作</strong></p><p><strong>解决办法就是要么外部的 etcd 更换数据目录(<code>/var/lib/etcd</code>)和端口，要么干脆不要和 master 放在同一主机即可</strong></p><h3 id="三、巨大的日志"><a href="#三、巨大的日志" class="headerlink" title="三、巨大的日志"></a>三、巨大的日志</h3><p>熟悉的小伙伴应该清楚，基本上每个 kubernetes 组件都会有个通用的参数 <code>--v</code>；这个参数用于控制 kubernetes 各个组件的日志级别，在早期(alpha)的 kubeadm 版本中，如果不进行调整，默认创建集群所有组件日志级别全部为 <code>--v=4</code> 即最高级别输出，这会导致在业务量大的时候磁盘空间以 <strong>‘我去尼玛’</strong> 的速度增长，尤其是 <code>kube-proxy</code> 组件的容器，会疯狂吃掉你的磁盘空间，然后剩下懵逼的你不知为何。在后续的版本中(beta)发现日志级别已经降到了 <code>--v=2</code>，不过对于完全不怎么看日志的我来说还是无卵用……</p><p><strong>解决办法有两种方案:</strong></p><p><strong>如果已经 <code>--v=4</code> 跑起来了(检查方法就是随便 describe 一个 kube-proxy 的容器，看下 command 字段就能看到)，并且无法停止重建集群，那么最简单的办法就是使用 <code>kubectl edit ds xxx</code> 方式编译一下相关 ds 文件等，然后手动杀掉相关 pod，让 kubernetes 自动重建即可，如果命令行用着不爽也可以通过 dashboard 更改</strong></p><p><strong>如果还没开始搭建，或者可以停掉重建，那么只需在 <code>kubeadm init</code> 之前 <code>export KUBE_COMPONENT_LOGLEVEL=&#39;--v=0&#39;</code> 即可</strong></p><h3 id="四、新节点加入-dns-要你命"><a href="#四、新节点加入-dns-要你命" class="headerlink" title="四、新节点加入 dns 要你命"></a>四、新节点加入 dns 要你命</h3><p>当 kubeadm 创建好集群以后，如果有需要增加新节点，那么在 <code>kubeadm join</code> 之后务必检查 <code>kube-dns</code> 组件，dns 在某些(weave 启动不完整或不正常)情况下，会由于新节点加入而挂掉，此时整个集群 dns 失效，<strong>所以最好 join 完观察一会 dns 状态，如果发现不正常马上杀掉  dns pod，让 kubernetes 自动重建；如果情况允许最好全部 join 完成后直接干掉 dns 让 kubernetes 重建一下</strong></p><h3 id="五、单点的-dns-浪起来让你怕"><a href="#五、单点的-dns-浪起来让你怕" class="headerlink" title="五、单点的 dns 浪起来让你怕"></a>五、单点的 dns 浪起来让你怕</h3><p>kubeadm 创建的 dns 默认也是单点的，而 dns 至关重要，只要一挂瞬间整个集群全部 <code>game over</code>；<strong>不过暂时还是没有发现能在 init 时候创建多个 dns 的方法；不过在集群创建后可以通过 <code>kubectl edit deploy kube-dns</code> 的方式修改其副本数量，让其创建多个副本即可</strong></p><h3 id="六、永远的-v1-4-4"><a href="#六、永远的-v1-4-4" class="headerlink" title="六、永远的 v1.4.4"></a>六、永远的 v1.4.4</h3><p>在一开始 kubeadm 创建集群时，采用的基础组件基本都是写死的，不过现在增加了 <code>--use-kubernetes-version</code> 选项，在 init 时使用该选项可以指定使用的基础组件(kube-proxy、apiserver 等)的版本，如 <code>kubeadm init --use-kubernetes-version v1.4.6</code> 即可使用 1.4.6 的镜像，目前最新版本的 rpm kubelet 版本为 1.4.4，从目前测试来看 1.4.4 的 kubelet 与 1.4.6 的其他版本组件一起运行尚未出现问题，不过最好在准备超版本运行之前，把 kubelet 的二进制文件也换成相应版本的</p><h3 id="七、reset-让你一无所有"><a href="#七、reset-让你一无所有" class="headerlink" title="七、reset 让你一无所有"></a>七、reset 让你一无所有</h3><p>kubeadm 在 <code>alpha</code> 时官方文档页面提供了重建集群脚本，如下</p><pre><code class="hljs sh">systemctl stop kubelet;docker rm -f -v $(docker ps -q);find /var/lib/kubelet | xargs -n 1 findmnt -n -t tmpfs -o TARGET -T | uniq | xargs -r umount -v;rm -r -f /etc/kubernetes /var/lib/kubelet /var/lib/etcd;</code></pre><p>从脚本上看很容易知道<strong>第二条命令很危险</strong>，他会干掉所有正在运行的容器，如果你的 node 上恰好有 <code>docker-compose</code> 启动的重要服务，这么这一下后果可想而知；<strong>kubeadm 到了 <code>alpha2</code> 之后，提供了 <code>kubeadm reset</code> 命令来重建集群，我已开始以为这是个很好的事情，既然重写了肯定很好用；但是上帝总是跟你讲 ‘Hello World’，经过测试(实际上是躺枪了)我发现 reset 其实就是把这四条 shell 命令封装一起变成一个 <code>kubeadm reset</code> 而已，所以说此命令慎用，不行就用上面的脚本手动档删除，否则一条 <code>reset</code> 倾家荡产</strong></p><p><strong>未完待续，欢迎补充，如果有坑继续添加……</strong></p><blockquote><p>本文参考 <a href="http://kubernetes.io/docs/admin/kubeadm/" target="_blank" rel="noopener">kubeadm reference</a></p></blockquote>]]></content>
    
    
    <categories>
      
      <category>Kubernetes</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Linux</tag>
      
      <tag>Docker</tag>
      
      <tag>Kubernetes</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>CentOS 升级 kernel</title>
    <link href="/2016/11/08/update-centos-kernel/"/>
    <url>/2016/11/08/update-centos-kernel/</url>
    
    <content type="html"><![CDATA[<blockquote><p>最紧要鼓捣 Dokcer Swarm，而 Swarm 的 overlay 网络需要 3.15 以上的 kernel，故记录一下升级内核的过程</p></blockquote><h3 id="一、手动档"><a href="#一、手动档" class="headerlink" title="一、手动档"></a>一、手动档</h3><p>手动档就是从源码开始编译内核安装，好处是可以自己选择任意版本的内核，缺点就是耗时长，编译安装消耗系统资源</p><h4 id="1-1、获取-kernel-源码"><a href="#1-1、获取-kernel-源码" class="headerlink" title="1.1、获取 kernel 源码"></a>1.1、获取 kernel 源码</h4><p>这世界上最伟大的 Linux 内核源码下载地址是 <a href="https://kernel.org" target="_blank" rel="noopener">kernel 官网</a>，选择一个稳定版本下载即可</p><p><img src="https://cdn.oss.link/markdown/3se7u.jpg" srcset="/img/loading.gif" alt="kernel homepage"></p><h4 id="1-2、解压并清理"><a href="#1-2、解压并清理" class="headerlink" title="1.2、解压并清理"></a>1.2、解压并清理</h4><p>官方要求将其解压到 <code>/usr/src</code> 目录，其实在哪都可以，为了规范一点索性也解压到此位置，然后为了防止编译残留先做一次清理动作</p><pre><code class="hljs sh"><span class="hljs-comment"># 下载内核源码</span>wget https://cdn.kernel.org/pub/linux/kernel/v4.x/linux-4.8.6.tar.xz<span class="hljs-comment"># 解压并移动到 /usr/src</span>tar -Jxvf linux-4.8.6.tar.xzmv linux-4.8.6 /usr/src/kernels<span class="hljs-comment"># 执行清理(没 gcc 的要装一下)</span><span class="hljs-built_in">cd</span> /usr/src/kernels/linux-4.8.6make mrproper &amp;&amp; make clean</code></pre><h4 id="1-3、生成编译配置表"><a href="#1-3、生成编译配置表" class="headerlink" title="1.3、生成编译配置表"></a>1.3、生成编译配置表</h4><p>kernel 在编译时需要一个配置文件(<code>.config</code>)，用于描述开启哪些特性等，该文件一般可通过一下四种途径获得:</p><ul><li>复制当前系统编译配置表，即 <code>cp /boot/config-xxx .config</code>；如果系统有多个内核，那么根据版本号选择最新的即可</li><li>使用 <code>make defconfig</code> 命令获取当前系统编译配置表，该命令会自动写入到 <code>.config</code> 中</li><li>使用 <code>make localmodconfig</code> 命令开启交互模式，然后根据提示生成编译配置表</li><li>使用 <code>make oldconfig</code> 命令根据旧的编译配置表生成新的编译配置表，<strong>刚方式会直接读取旧的便已配置表，并在以前没有设定过的配置时会自动开启交互模式</strong></li></ul><p>这里采用最后一种方式生成</p><p><img src="https://cdn.oss.link/markdown/f9j5r.jpg" srcset="/img/loading.gif" alt="create kernel compile param"></p><h4 id="1-4、编译并安装"><a href="#1-4、编译并安装" class="headerlink" title="1.4、编译并安装"></a>1.4、编译并安装</h4><p>内核配置表生成完成后便可进行编译和安装(需要安装 bc、openssl-devel等)</p><pre><code class="hljs sh">makemake modulesmake modules_installmake install</code></pre><p><strong>最后执行重启验证即可，验证成功后可删除旧的内核(<code>rpm -qa | grep kernel</code>)</strong></p><h3 id="二、自动档"><a href="#二、自动档" class="headerlink" title="二、自动档"></a>二、自动档</h3><p>相对于手动档编译安装，CentOS 还可以通过使用 elrepo 源的方式直接安装最新稳定版 kernel，脚本如下</p><pre><code class="hljs sh"><span class="hljs-comment"># import key</span>rpm --import https://www.elrepo.org/RPM-GPG-KEY-elrepo.org<span class="hljs-comment"># install elrepo repo</span>rpm -Uvh http://www.elrepo.org/elrepo-release-7.0-2.el7.elrepo.noarch.rpm<span class="hljs-comment"># install kernel</span>yum --enablerepo=elrepo-kernel install  kernel-ml-devel kernel-ml -y<span class="hljs-comment"># modify grub</span>grub2-set-default 0<span class="hljs-comment"># reboot</span>reboot</code></pre>]]></content>
    
    
    <categories>
      
      <category>Linux</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Linux</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>晚月</title>
    <link href="/2016/11/02/wanyue/"/>
    <url>/2016/11/02/wanyue/</url>
    
    <content type="html"><![CDATA[<h4 id="晚月"><a href="#晚月" class="headerlink" title="晚月"></a>晚月</h4><pre><code class="hljs sh">轻风细雨秋凉意，枯叶随风去，冷了心头，乱了谁的绪？待到寒风飘雪时，冰凌刺骨痛，旧伤再起，扰了谁的梦？又道凡尘多琐事，不想触则伤断魂；罢了罢了，坎坷千载遇伯乐，何思今朝无故人？</code></pre>]]></content>
    
    
    <categories>
      
      <category>随笔</category>
      
    </categories>
    
    
    <tags>
      
      <tag>随笔</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>kubeadm 搭建 kubernetes 集群</title>
    <link href="/2016/10/29/set-up-kubernetes-cluster-by-kubeadm/"/>
    <url>/2016/10/29/set-up-kubernetes-cluster-by-kubeadm/</url>
    
    <content type="html"><![CDATA[<blockquote><p>距离上一篇 <a href="https://mritd.me/2016/10/09/kubernetes-1.4-create-cluster/" target="_blank" rel="noopener">kubernetes 1.4 集群搭建</a> 发布间隔不算太久，自己也不断地在生产和测试环境鼓捣，有不少 “逗比” 的经历，准备写一下具体的 kubeadm 搭建集群的一些坑和踩坑的经验，如果没有使用过 kubeadm 的同学，最好先看下上面的文章，然后鼓捣一遍，也许并不会成功，但大部分坑再来看此文会有收获</p></blockquote><h3 id="一、环境准备"><a href="#一、环境准备" class="headerlink" title="一、环境准备"></a>一、环境准备</h3><p>首先环境还是三台虚拟机，虚拟机地址如下</p><table><thead><tr><th>IP 地址</th><th>节点</th></tr></thead><tbody><tr><td>192.168.1.167</td><td>master</td></tr><tr><td>192.168.1.189</td><td>node1</td></tr><tr><td>192.168.1.176</td><td>node2</td></tr></tbody></table><p>然后每台机器安装好 docker，至于 rpm 安装包版本下面介绍</p><h3 id="二、说点正经事"><a href="#二、说点正经事" class="headerlink" title="二、说点正经事"></a>二、说点正经事</h3><h4 id="2-1、安装包从哪来"><a href="#2-1、安装包从哪来" class="headerlink" title="2.1、安装包从哪来"></a>2.1、安装包从哪来</h4><p>官方的文档页面更新并不及时，同时他的 yum 源更新也很慢，再者…那他妈可是 Google 的服务器，能特么连上吗？以前总是在国外服务器使用 <code>yumdownloader</code> 下载，然后 <code>scp</code> 到本地，虽然能解决问题，但是蛋碎一地…最后找到了源头，如下</p><p><strong>Kubernetes 编译的各种发行版安装包来源于 Github 上的另一个叫 release 的项目，地址 <a href="https://github.com/kubernetes/release" target="_blank" rel="noopener">点这里</a>，把这个项目 <code>clone</code> 下来，由于本人是 Centos 用户，所以进入 rpm 目录，在安装好 docker 的机器上执行那个 <code>docker-build.sh</code> 脚本即可编译 rpm 包，最后会生成到当前目录的 <code>output</code> 目录下,截图如下</strong></p><p><img src="https://cdn.oss.link/markdown/3zs7u.jpg" srcset="/img/loading.gif" alt="release"></p><p><img src="https://cdn.oss.link/markdown/8b3a4.jpg" srcset="/img/loading.gif" alt="rpm目录"></p><h4 id="2-2、镜像从哪来"><a href="#2-2、镜像从哪来" class="headerlink" title="2.2、镜像从哪来"></a>2.2、镜像从哪来</h4><p>对的，没错，gcr.io 就是 Google 的域名，服务器更不用提，所以在进行 <code>kubeadm init</code> 操作时如果不先把这些镜像 load 进去绝对会卡死不动，以下列出了所需镜像，但是版本号根据 rpm 版本不同可能略有不同，具体怎么看下面介绍</p><table><thead><tr><th>镜像名称</th><th>版本号</th></tr></thead><tbody><tr><td>gcr.io/google_containers/kube-discovery-amd64</td><td>1.0</td></tr><tr><td>gcr.io/google_containers/kubedns-amd64</td><td>1.7</td></tr><tr><td>gcr.io/google_containers/kube-proxy-amd64</td><td>v1.4.1</td></tr><tr><td>gcr.io/google_containers/kube-scheduler-amd64</td><td>v1.4.1</td></tr><tr><td>gcr.io/google_containers/kube-controller-manager-amd64</td><td>v1.4.1</td></tr><tr><td>gcr.io/google_containers/kube-apiserver-amd64</td><td>v1.4.1</td></tr><tr><td>gcr.io/google_containers/etcd-amd64</td><td>2.2.5</td></tr><tr><td>gcr.io/google_containers/kube-dnsmasq-amd64</td><td>1.3</td></tr><tr><td>gcr.io/google_containers/exechealthz-amd64</td><td>1.1</td></tr><tr><td>gcr.io/google_containers/pause-amd64</td><td>3.0</td></tr></tbody></table><p><strong>这些镜像有两种办法可以获取，第一种是利用一台国外的服务器，在上面 pull 下来，然后再 save 成 tar 文件，最后 scp 到本地 load 进去；相对于第一种方式比较坑的是取决于服务器速度，每次搞起来也很蛋疼，第二种方式就是利用 docker hub 做中转，简单的说就是利用 docker hub 的自动构建功能，在 Github 中创建一个 Dockerfile，里面只需要 <code>FROM xxxx</code> 这些 gcr.io 的镜像即可，最后 pull 到本地，然后再 tag 一下</strong></p><p><strong>首先创建一个 github 项目，可以直接 fork 我的即可</strong></p><p><img src="https://cdn.oss.link/markdown/2eo34.jpg" srcset="/img/loading.gif" alt="docker-libray"></p><p>其中每个 Dockerfile 只需要 <code>FROM</code> 一下即可</p><p><img src="https://cdn.oss.link/markdown/cxva2.jpg" srcset="/img/loading.gif" alt="Dockerfile"></p><p><strong>最后在 Docker Hub 上创建自动构建项目</strong></p><p><img src="https://cdn.oss.link/markdown/p5khs.jpg" srcset="/img/loading.gif" alt="createproject"></p><p><img src="https://cdn.oss.link/markdown/gc8vl.jpg" srcset="/img/loading.gif" alt="from github"></p><p><img src="https://cdn.oss.link/markdown/9ufnd.jpg" srcset="/img/loading.gif" alt="selectproject"></p><p><img src="https://cdn.oss.link/markdown/ud42y.jpg" srcset="/img/loading.gif" alt="details"></p><p><strong>最后要手动触发一下，然后 Docker Hub 才会开始给你编译</strong></p><p><img src="https://cdn.oss.link/markdown/phgsg.jpg" srcset="/img/loading.gif" alt="Tigger"></p><p><strong>等待完成即可直接 pull 了</strong></p><p><img src="https://cdn.oss.link/markdown/itnw3.jpg" srcset="/img/loading.gif" alt="success"></p><h4 id="2-3、镜像版本怎么整"><a href="#2-3、镜像版本怎么整" class="headerlink" title="2.3、镜像版本怎么整"></a>2.3、镜像版本怎么整</h4><p>上面已经解决了镜像获取问题，但是一大心病就是 “我特么怎么知道是哪个版本的”，为了发扬 “刨根问底” 的精神，<strong>先进行一遍 <code>kubeadm init</code>，这时候绝对卡死，此时进入 <code>/etc/kubernetes/manifests</code> 可以看到许多 json 文件，这些文件中定义了需要哪些基础镜像</strong></p><p><img src="https://cdn.oss.link/markdown/3ovg8.jpg" srcset="/img/loading.gif" alt="all json"></p><p><img src="https://cdn.oss.link/markdown/uitnd.jpg" srcset="/img/loading.gif" alt="image version"></p><p>从上图中基本可以看到 <code>kubeadm init</code> 的时候会拉取哪些基础镜像了，<strong>但是还有一些镜像，仍然无法找到，比如<code>kubedns</code>、<code>pause</code> 等，至于其他的镜像版本，可以从源码中找到，源码位置是 <code>kubernetes/cmd/kubeadm/app/images/images.go</code> 这个文件中，如下所示:</strong> </p><p><img src="https://cdn.oss.link/markdown/ocgu4.jpg" srcset="/img/loading.gif" alt="image version"></p><p>剩余的一些镜像，比如 <code>kube-proxy-amd64</code>、<code>kube-discovery-amd64</code> 两个镜像，其中 <code>kube-discovery-amd64</code> 现在一直是 1.0 版本，源码如下所示</p><p><img src="https://cdn.oss.link/markdown/mp3qo.jpg" srcset="/img/loading.gif" alt="discovery version"></p><p><code>kube-proxy-amd64</code> 则是一直跟随基础组件的主版本，也就是说如果从 <code>manifests</code> 中看到 controller 等版本是 <code>v.1.4.4</code>，那么 <code>kube-proxy-amd64</code> 也是这个版本，源码如下</p><p><img src="https://cdn.oss.link/markdown/tienu.jpg" srcset="/img/loading.gif" alt="proxy version"></p><p>最后根据这些版本去 github 上准备相应的 Dockerfile，在利用 Docker Hub 的自动构建 build 一下，再 pull 下来 tag 成对应的镜像名称即可</p><h3 id="三、搭建集群"><a href="#三、搭建集群" class="headerlink" title="三、搭建集群"></a>三、搭建集群</h3><h4 id="3-1、主机名处理"><a href="#3-1、主机名处理" class="headerlink" title="3.1、主机名处理"></a>3.1、主机名处理</h4><p><strong>经过亲测，节点主机名最好为 <code>xxx.xxx</code> 这种域名格式，否则在某些情况下，POD 中跑的程序使用域名解析时可能出现问题，所以先要处理一下主机名</strong></p><pre><code class="hljs sh"><span class="hljs-comment"># 写入 hostname(node 节点后缀改成 .node)</span><span class="hljs-built_in">echo</span> <span class="hljs-string">"192-168-1-167.master"</span> &gt; /etc/hostname <span class="hljs-comment"># 加入 hosts</span><span class="hljs-built_in">echo</span> <span class="hljs-string">"127.0.0.1   192-168-1-167.master"</span> &gt;&gt; /etc/hosts<span class="hljs-comment"># 不重启情况下使内核生效</span>sysctl kernel.hostname=192-168-1-167.master<span class="hljs-comment"># 验证是否修改成功</span>➜  ~ hostname192-168-1-167.master</code></pre><h4 id="3-2、load-镜像"><a href="#3-2、load-镜像" class="headerlink" title="3.2、load 镜像"></a>3.2、load 镜像</h4><p>由于本人已经在 Docker Hub 上处理好了相关镜像，所以直接 pull 下来 tag 一下即可，</p><pre><code class="hljs sh">images=(kube-proxy-amd64:v1.4.4 kube-discovery-amd64:1.0 kubedns-amd64:1.7 kube-scheduler-amd64:v1.4.4 kube-controller-manager-amd64:v1.4.4 kube-apiserver-amd64:v1.4.4 etcd-amd64:2.2.5 kube-dnsmasq-amd64:1.3 exechealthz-amd64:1.1 pause-amd64:3.0 kubernetes-dashboard-amd64:v1.4.1)<span class="hljs-keyword">for</span> imageName <span class="hljs-keyword">in</span> <span class="hljs-variable">$&#123;images[@]&#125;</span> ; <span class="hljs-keyword">do</span>  docker pull mritd/<span class="hljs-variable">$imageName</span>  docker tag mritd/<span class="hljs-variable">$imageName</span> gcr.io/google_containers/<span class="hljs-variable">$imageName</span>  docker rmi mritd/<span class="hljs-variable">$imageName</span><span class="hljs-keyword">done</span></code></pre><h4 id="3-3、安装-rpm"><a href="#3-3、安装-rpm" class="headerlink" title="3.3、安装 rpm"></a>3.3、安装 rpm</h4><p>rpm 获取办法上文已经提到，可以自己编译，这里我已经编译好并维护了一个 yum 源，直接yum install 即可(懒)</p><pre><code class="hljs sh"><span class="hljs-comment"># 添加 yum 源</span>tee /etc/yum.repos.d/mritd.repo &lt;&lt; EOF[mritdrepo]name=Mritd Repositorybaseurl=https://rpm.mritd.me/centos/7/x86_64enabled=1gpgcheck=1gpgkey=https://cdn.oss.link/keys/rpm.public.keyEOF<span class="hljs-comment"># 刷新cache</span>yum makecache<span class="hljs-comment"># 安装</span>yum install -y kubelet kubectl kubernetes-cni kubeadm</code></pre><h4 id="3-4、初始化-master"><a href="#3-4、初始化-master" class="headerlink" title="3.4、初始化 master"></a>3.4、初始化 master</h4><p><strong>等会有个坑，kubeadm 等相关 rpm 安装后会生成 <code>/etc/kubernetes</code> 目录，而 kubeadm init 时候又会检测这些目录是否存在，如果存在则停止初始化，所以要先清理一下，以下清理脚本来源于 <a href="http://kubernetes.io/docs/getting-started-guides/kubeadm/" target="_blank" rel="noopener">官方文档 Tear down 部分</a>，该脚本同样适用于初始化失败进行重置</strong></p><pre><code class="hljs sh">systemctl stop kubelet;<span class="hljs-comment"># 注意: 下面这条命令会干掉所有正在运行的 docker 容器，</span><span class="hljs-comment"># 如果要进行重置操作，最好先确定当前运行的所有容器都能干掉(干掉不影响业务)，</span><span class="hljs-comment"># 否则的话最好手动删除 kubeadm 创建的相关容器(gcr.io 相关的)</span>docker rm -f -v $(docker ps -q);find /var/lib/kubelet | xargs -n 1 findmnt -n -t tmpfs -o TARGET -T | uniq | xargs -r umount -v;rm -r -f /etc/kubernetes /var/lib/kubelet /var/lib/etcd;</code></pre><p><strong>还有个坑，初始化以前记得一定要启动 kubelet，虽然你 <code>systemctl status kubelet</code> 看着他是启动失败，但是也得启动，否则绝壁卡死</strong></p><pre><code class="hljs sh">systemctl <span class="hljs-built_in">enable</span> kubeletsystemctl start kubelet</code></pre><p><strong>等会等会，还有坑，新版本直接 init 会提示 <code>ebtables not found in system path</code> 错误，所以还得先安装一下这个包在初始化</strong></p><pre><code class="hljs sh"><span class="hljs-comment"># 安装 ebtables</span>yum install -y ebtables</code></pre><p><strong>最后见证奇迹的时刻</strong></p><pre><code class="hljs sh"><span class="hljs-comment"># 初始化并指定 apiserver 监听地址</span>kubeadm init --api-advertise-addresses 192.168.1.167</code></pre><p><strong>完美截图如下</strong></p><p><img src="https://cdn.oss.link/markdown/rs2mw.jpg" srcset="/img/loading.gif" alt="init master"></p><p><strong>这里再爆料一个坑，底下的 <code>kubeadm join --token=b17964.5d8a3c14e99cf6aa 192.168.1.167</code> 这条命令一定保存好，因为后期没法重现的，你们老大再让你添加机器的时候如果没这个你会哭的</strong></p><h4 id="3-5、加入-node"><a href="#3-5、加入-node" class="headerlink" title="3.5、加入 node"></a>3.5、加入 node</h4><p>上面所有坑大约说的差不多了，直接上命令了</p><pre><code class="hljs sh"><span class="hljs-comment"># 处理主机名</span><span class="hljs-built_in">echo</span> <span class="hljs-string">"192-168-1-189.node"</span> &gt; /etc/hostname <span class="hljs-built_in">echo</span> <span class="hljs-string">"127.0.0.1   192-168-1-189.node"</span> &gt;&gt; /etc/hostssysctl kernel.hostname=192-168-1-189.node<span class="hljs-comment"># 拉取镜像</span>images=(kube-proxy-amd64:v1.4.4 kube-discovery-amd64:1.0 kubedns-amd64:1.7 kube-scheduler-amd64:v1.4.4 kube-controller-manager-amd64:v1.4.4 kube-apiserver-amd64:v1.4.4 etcd-amd64:2.2.5 kube-dnsmasq-amd64:1.3 exechealthz-amd64:1.1 pause-amd64:3.0 kubernetes-dashboard-amd64:v1.4.1)<span class="hljs-keyword">for</span> imageName <span class="hljs-keyword">in</span> <span class="hljs-variable">$&#123;images[@]&#125;</span> ; <span class="hljs-keyword">do</span>  docker pull mritd/<span class="hljs-variable">$imageName</span>  docker tag mritd/<span class="hljs-variable">$imageName</span> gcr.io/google_containers/<span class="hljs-variable">$imageName</span>  docker rmi mritd/<span class="hljs-variable">$imageName</span><span class="hljs-keyword">done</span><span class="hljs-comment"># 装 rpm</span>tee /etc/yum.repos.d/mritd.repo &lt;&lt; EOF[mritdrepo]name=Mritd Repositorybaseurl=https://rpm.mritd.me/centos/7/x86_64enabled=1gpgcheck=1gpgkey=https://cdn.oss.link/keys/rpm.public.keyEOFyum makecacheyum install -y kubelet kubectl kubernetes-cni kubeadm ebtables<span class="hljs-comment"># 清理目录(没初始化过只需要删目录)</span>rm -r -f /etc/kubernetes /var/lib/kubelet /var/lib/etcd;<span class="hljs-comment"># 启动 kubelet</span>systemctl <span class="hljs-built_in">enable</span> kubeletsystemctl start kubelet<span class="hljs-comment"># 初始化加入集群</span>kubeadm join --token=b17964.5d8a3c14e99cf6aa 192.168.1.167</code></pre><p><strong>同样完美截图</strong></p><p><img src="https://cdn.oss.link/markdown/9c8eu.jpg" srcset="/img/loading.gif" alt="join master"></p><p><img src="https://cdn.oss.link/markdown/ri4q9.jpg" srcset="/img/loading.gif" alt="get node"></p><h4 id="3-6、部署-weave-网络"><a href="#3-6、部署-weave-网络" class="headerlink" title="3.6、部署 weave 网络"></a>3.6、部署 weave 网络</h4><p>再没部署 weave 时，dns 是启动不了的，如下</p><p><img src="https://cdn.oss.link/markdown/fqjsg.jpg" srcset="/img/loading.gif" alt="dns not work"></p><p><strong>官方给出的命令是这样的</strong></p><pre><code class="hljs sh">kubectl create -f https://git.io/weave-kube</code></pre><p>本着 “刨根问底挖祖坟” 的精神，先把这个 yaml 搞下来</p><pre><code class="hljs sh">wget https://git.io/weave-kube -O weave-kube.yaml</code></pre><p>然后同样的套路，打开看一下镜像，利用 Docker Hub 做中转，搞下来再 load 进去，然后 <code>create -f</code> 就行了</p><pre><code class="hljs sh">docker pull mritd/weave-kube:1.7.2docker tag mritd/weave-kube:1.7.2 weaveworks/weave-kube:1.7.2docker rmi mritd/weave-kube:1.7.2kubectl create -f weave-kube.yaml</code></pre><p><strong>完美截图</strong></p><p><img src="https://cdn.oss.link/markdown/0ja5f.jpg" srcset="/img/loading.gif" alt="create weave"></p><h4 id="3-7、部署-dashboard"><a href="#3-7、部署-dashboard" class="headerlink" title="3.7、部署 dashboard"></a>3.7、部署 dashboard</h4><p><strong>dashboard 的命令也跟 weave 的一样，不过有个大坑，默认的 yaml 文件中对于 image 拉取策略的定义是 无论何时都会去拉取镜像，导致即使你 load 进去也无卵用，所以还得先把 yaml 搞下来然后改一下镜像拉取策略，最后再 <code>create -f</code> 即可</strong></p><pre><code class="hljs sh">wget https://rawgit.com/kubernetes/dashboard/master/src/deploy/kubernetes-dashboard.yaml -O kubernetes-dashboard.yaml</code></pre><p><strong>编辑 yaml 改一下 <code>imagePullPolicy</code>，把 <code>Always</code> 改成 <code>IfNotPresent</code>(本地没有再去拉取) 或者 <code>Never</code>(从不去拉取) 即可</strong></p><p><img src="https://cdn.oss.link/markdown/lqvh1.jpg" srcset="/img/loading.gif" alt="IfNotPresent"></p><p>最后再利用 Dokcer Hub 中转，然后创建(实际上 dashboard 已经有了 v1.4.1，我这里已经改了)</p><pre><code class="hljs sh">kubectl create -f kubernetes-dashboard.yaml</code></pre><p><strong>截图如下</strong></p><p><img src="https://cdn.oss.link/markdown/xsn9u.jpg" srcset="/img/loading.gif" alt="create dashboard"></p><p><strong>通过 describe 命令我们可以查看其暴露出的 <code>NodePoint</code>,然后便可访问</strong></p><p><img src="https://cdn.oss.link/markdown/5a94q.jpg" srcset="/img/loading.gif" alt="describe dashboard"></p><p><img src="https://cdn.oss.link/markdown/xwjvs.jpg" srcset="/img/loading.gif" alt="show dashboard"></p><h3 id="四、其他的一些坑"><a href="#四、其他的一些坑" class="headerlink" title="四、其他的一些坑"></a>四、其他的一些坑</h3><p>还有一些其他的坑等着大家去摸索，其中有一个是 DNS 解析错误，表现形式为 <strong>POD 内的程序通过域名访问解析不了，cat 一下容器的 <code>/etc/resolv.conf</code>发现指向的 dns 服务器与 <code>kubectl get svc --namespace=kube-system</code> 中的 kube-dsn 地址不符</strong>；解决办法就是 <strong>编辑节点的 <code>/etc/systemd/system/kubelet.service.d/10-kubeadm.conf</code> 文件，更改 <code>KUBELET_DNS_ARGS</code> 地址为 <code>get svc</code> 中的 kube-dns 地址，然后重启 kubelet 服务，重新杀掉 POD 让 kubernetes 重建即可</strong></p><p><img src="https://cdn.oss.link/markdown/hhozt.jpg" srcset="/img/loading.gif" alt="modify kube-dns"></p><p><strong>其他坑欢迎大家补充</strong></p>]]></content>
    
    
    <categories>
      
      <category>Kubernetes</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Linux</tag>
      
      <tag>Docker</tag>
      
      <tag>Kubernetes</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>人生就是会不断挥手说再见的啊</title>
    <link href="/2016/10/25/waved-goodbye/"/>
    <url>/2016/10/25/waved-goodbye/</url>
    
    <content type="html"><![CDATA[<blockquote><p>有些人，注定只能陪你一阵子，不是一辈子。人生就是会不断挥手说再见的啊…</p></blockquote><h3 id="1"><a href="#1" class="headerlink" title="1"></a>1</h3><p>前几天失眠，碰上小雨跟我聊天，她说她和男朋友分手了，因为我们习惯了他们俩每次分手后都说老死不相往来，转眼过几天又像连体婴一样出现在大家眼前，所以我对她说：“没事儿的，过几天你们俩就好了。”</p><p>“我今天做饭的时候，刀不小心划破了手，流血的时候我第一反应是拿起电话给他打电话，眼泪都出来了，情绪特别崩溃。”</p><p>“然后呢？”</p><p>“然后我突然想起来，那个可以撒娇的人没有了，以后没人可以让你撒娇了。自己默默回房间拿了个创可贴，发了一会呆就继续做饭了。”</p><p>我觉得有点心疼，除了心疼她被切破的手，更心疼她不能把电话打给想念的人。我想我们都曾有过拿起电话就想打给他却突然意识到，以后你都不能再打给这个人的时候。</p><p>和一个人最远的距离是你知道那个人就在那里，可你再也不能牵他的手，再也不能摸他的头发，你告诉自己：“别哭了，我们的故事结束了。”</p><h3 id="2"><a href="#2" class="headerlink" title="2"></a>2</h3><p>往后的日子里，你过得好就好，我就远远看着不打扰。</p><p>17岁的时候，小雨的身边是他，18岁的时候，小雨一想起就傻笑的人是他，19岁的时候，小雨牵着手的人还是他，今年20岁，小雨往后的生活里，再也没有他了。</p><p>我问她：“分手后悔吗？这么长时间说散就散了。”</p><p>她说：“我不后悔，我也没有遗憾。我曾经所做的事情，流过的的眼泪，都是成长。非要说有什么遗憾的话，可能就是没有和他走到终点。”</p><p>我们不该忘记生命中每一个爱过的人，我们爱过的不只是那个人，更是我们再也回不去的青春。</p><h3 id="3"><a href="#3" class="headerlink" title="3"></a>3</h3><p>之前我问过我的一个异性朋友，我问他：“你分手的时候会很难过很难过吗？”</p><p>他是这么跟我说的：“能说出口的难过都不是真的难过。</p><p>有一天晚上我手机突然响了，电话那头没有人说话，我也没说话，我知道是她，我知道她想我了。我没狠下心挂电话，我担心她害怕。我一根接一根的吸烟，她一声不吭，我们这样持续到我手机没电了自动关机。</p><p>与其说难过，倒不如说是担心。担心她饿了没人带她吃好吃的，担心她睡不着没人陪她说话，担心她的坏脾气没人包容…总之就是担心她以后过得不好，担心她过得不幸福。</p><p>一想到这些我才觉得难过。”</p><p>当这个人再出现在你生活里的时候，你看着她，你知道这个人以前你爱过，但以后你不能继续爱了，你希望别人好好爱她，又害怕别人没那么爱她。</p><p>原来，没有得到过不是最痛的，得到过再失去才是最痛的。</p><h3 id="fin"><a href="#fin" class="headerlink" title="fin"></a>fin</h3><p>对已经逝去的东西最好的尊重是绝口不提。</p><p>人生就是一个不断挥手说再见的过程啊，回不去的时候必须要说：再见。</p><p>说再见的时候不想说珍重，我想说：祝你幸福。</p><p>Author 蒋同学，微信公众号/有故事的蒋同学，ID/meiya54264<br>BGM</p><p><strong>转自 <a href="http://isujin.com/5990" target="_blank" rel="noopener">素锦</a></strong></p><audio  autoplay="autoplay">  <source src="https://cdn.oss.link/bgm/waved-goodbye.mp3" type="audio/mpeg" />Your browser does not support the audio element.</audio>]]></content>
    
    
    <categories>
      
      <category>随笔</category>
      
    </categories>
    
    
    <tags>
      
      <tag>随笔</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>二十多岁的我们，为什么觉得谈恋爱好难</title>
    <link href="/2016/10/25/twenty-love-so-hard/"/>
    <url>/2016/10/25/twenty-love-so-hard/</url>
    
    <content type="html"><![CDATA[<h3 id="01"><a href="#01" class="headerlink" title="01"></a>01</h3><p>前两天，我和朋友约在咖啡馆聊天虚度假期。有一个朋友为感情的事愁眉不展，她说在生活的压力下，感觉自己变得难以心动，对一切都感到麻木。</p><p>我也思考起，自己和身边人的感情生活。确实，好像一切都变了，但不是变得难以心动，而是心动之后所面对的一连串现实问题，让我们望而却步了。</p><p>二十多岁的我们，为什么觉得谈恋爱好难</p><p>才想起，过去的我并不觉得恋爱是件难事。</p><p>还记得18岁那年，我拿着卡里仅有的两千块钱，买了迪士尼的套票和酒店，因为那天是她的生日。</p><p>之后的一周时间里，每天吃方便面和咸方包，靠着同学的接济硬是撑了过去。那时候我并没有想太多，只是义无反顾地想让对方开心，再苦也值得。</p><p>四年后的今天，她留在北方的城市工作。有时候她会说：“想你了。可以来找我吗。”我回复一个笑容，然后迅速转移到下一个话题中。</p><p>我在想，如果是18岁时的我，一定会花光积蓄去找她吧。不知不觉，我已经不再会为爱情倾其所有。</p><p>而与此同时，似乎她也开始改变了。我们每天的对话，经常只有三句：“我起来了”，“我下班了”，“早点睡”。</p><p>从前，她事无大小总会跟我分享，但最近她再也没谈起过她的生活。反倒是在朋友圈里，我才看见她参加的各样聚会，跟朋友在周边城市旅游，有时玩到半夜才回家。</p><p>但我们都没有因此而感到难过或不适，反而开始适应了彼此独立的生活，不再盲目地追求当年义无反顾的激情。</p><p>从前我们年轻气盛，认为恋爱大过天，什么阻碍都不放在眼里，只要彼此相爱就好。但是二十多岁了，我们逐渐各自独立，而我再也不敢这样感性地义无反顾下去。</p><p>因为我们都心知肚明，异地恋成功的概率太小，成本太高了。也许，这几年的感情，很快就只能是一段回忆了。</p><p>而那些曾经让人羡慕的校园模范情侣们，到了现在的年纪，好像也走不过这一遭。</p><h3 id="02"><a href="#02" class="headerlink" title="02"></a>02</h3><p>坐在我身边的师姐，她曾经就是人人艳羡的对象。最近，她也分手了。谈起这段感情，她说男朋友的压力太大，向她坦白已经没有太多恋爱的资本了。</p><p>她回忆起上大学的时候，男朋友的父母每个月给他4000块生活费。这在学校可以生活得非常舒适了，他们总会在周末一起到周边城市旅游，或是互赠一些价值不菲的礼物。</p><p>有一次，我跟他们一起到香港。那个男生拿着早已准备好的购物清单，在一家潮牌店里用半个小时刷走了八千多块钱。</p><p>我盯着他那件上千块的T恤和三千多块的球鞋，一时觉得难以置信。而这也是学生时代的我所憧憬的情侣生活：钱袋鼓囊，时间充裕。</p><p>但是毕业以后，好像一切都变了。男生的父母需要他自力更生，于是每个月拿到手的4000块，变成了老板出的工资。</p><p>没有了宿舍以后，两人只能开始租房。为了省钱，他们住在了公司附近的城中村里，每个月1400。但是蟑螂乱窜，甚至要自己捡来半块床板盖厕所。再加上日常基本花销，每个月4000块工资所剩无几。</p><p>以前那些到处旅游购物的日子，再也没有了。平时出门打车的习惯也要改掉，更别说那些上千块的T恤、几千块的球鞋了。</p><p>今年的5月20号，师姐收到他发来的5.2元的红包。师姐说：“想到他如今面对的压力，租房、工作、爱情。要负担的东西变多了，而我们过去的生活也回不去了。”</p><p>终于在上个星期，他们分手了。原因很简单，他们不再有足够的时间和金钱来为彼此的感情付出了。</p><p>“也许分开会更好吧。”师姐喝了一口咖啡，淡淡的说到。“如今的他，更需要为自己的生活打拼，而不是我们互相操心了。”</p><h3 id="03"><a href="#03" class="headerlink" title="03"></a>03</h3><p>与恢复单身的师姐不同，我们的另一个朋友欣欣，最近突然订婚了。她与男朋友阿成从高中开始在一起，八年时间过去，但最终订婚的却不是他。</p><p>还记得在订婚宴上，万众瞩目的欣欣珠光宝气。她妈妈也在一旁不断夸赞着未来女婿，还苦口婆心地教我们，好好计划下自己该怎么过日子。</p><p>也正是因为她妈妈的这一番话，看着眼前不再满脸稚气的欣欣挽着身旁的男生，我意识到：我们都已经迈入到谈婚论嫁的年纪，身边的朋友生下小孩，再也不是一个“意外”。</p><p>毕业以后，身边不少同学开始组建自己的家庭，与此而来的是长辈们的压力。爱情已经不仅是两个人的事，也关乎到两个家庭的方方面面。有时候要面对父母的催婚，有时候还不得不听从父母的意愿。</p><p>回过头看看阿成，他们在一起这么多年，欣欣的妈妈对他并不满意。她的妈妈总说，阿成不会主动帮女生拎行李，不够上进，绝对不会让女儿嫁给这样的人。于是，千方百计地阻扰，并且对欣欣进行思想工作。</p><p>后来，在父母的牵线下，欣欣认识了一位年轻有为的男人。按她的话说，他性格不错，成熟稳重，会是一个可靠有担当的男人。她思前想后，经不住各方面的压力终于向阿成提出了分手。</p><p>然后，见家长，订婚。从结识到敲定婚事，仅仅是半年时间。</p><p>原来，在婚姻、家庭的压力面前，我们已经不能浪费太多时间去自由恋爱了。</p><p>曾经我们可能因为长相，因为性格，也可能是相互感觉不错，就能尝试走到一起。但现在，恋爱却多了很多前提。</p><p>工作是否稳定？</p><p>有房有车吗？</p><p>会不会经常分隔两地？</p><p>家庭环境如何？</p><p>双方父母怎么样？</p><p>在精打细算完一连串的问题之后，爱反而成为了最后才会思考的问题。</p><h3 id="04"><a href="#04" class="headerlink" title="04"></a>04</h3><p>我曾经遇到过十分喜欢的人，常常聊着天就会对着手机不自觉地笑出声。我们喜欢同样的歌曲和食物，喜欢同一座城市，还会幼稚地拍下身边有趣的东西发给对方，有着共同目标。</p><p>但我们都不敢轻易尝试开始，因为于我们而言，要在一起很容易，往后要面对的问题却很难。</p><p>同样刚毕业的我们，养活自己都已经很吃力，更不要说负担两个人的额外花销。更何况工作之余，已没有太多精力顾及另一个人。</p><p>朋友说：“我突然很害怕。害怕以后会迫不得已选择一个会给我良好生活、满足一切前提的人，但不是一个我真正喜欢的人。”</p><p>我也很害怕。我害怕自己无法给对方带来一个好的生活，只剩下我独自面对着现实带来的孤独和折磨。</p><p>昨天晚上，我和紫菜又聊起在咖啡店里听到的这几个故事。</p><p>她说，似乎恋爱中的问题，已经让我们逐渐变得可怕。</p><p>比如当她们得知一个大学朋友，即将嫁给一个三十多岁的有钱有势的人时，身边人的第一反应是羡慕和恭贺，但却没有一个人问她是否喜欢对方。</p><p>在那一瞬间，她觉得自己“这么多年来的书都白读了”。因为在现实面前，我们都不经意变得肤浅起来。</p><p>我反复思考她所说的话，一直到凌晨四点都没睡着。</p><p>也许，我们真的过了只要努力一下就能在一起的年纪了，过去藏在校服和学习背后的东西，终究被扯了出来。</p><p>Author：KC<br>BGM：天空の城ラピュタ~君をのせて by 高嶋ちさ子</p><p><strong>本文转自 <a href="http://isujin.com/6117" target="_blank" rel="noopener">素锦</a></strong></p><audio  autoplay="autoplay">  <source src="https://cdn.oss.link/bgm/高嶋ちさ子-天空の城ラピュタ~君をのせて.mp3" type="audio/mpeg" />Your browser does not support the audio element.</audio>]]></content>
    
    
    <categories>
      
      <category>随笔</category>
      
    </categories>
    
    
    <tags>
      
      <tag>随笔</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>StrongSwan 搭建 VPN</title>
    <link href="/2016/10/18/set-up-vpn-with-strongswan/"/>
    <url>/2016/10/18/set-up-vpn-with-strongswan/</url>
    
    <content type="html"><![CDATA[<blockquote><p>由于工作需要，记录一下使用 StrongSwan 搭建 VPN 的过程，支持 L2TP、IKEv2 PSK/CERT、IPsec 连接，基本上兼容大部分设备</p></blockquote><h3 id="一、环境准备"><a href="#一、环境准备" class="headerlink" title="一、环境准备"></a>一、环境准备</h3><p>基本环境如下</p><ul><li>CentOS 7 X64</li><li>StrongSwan 5.5</li></ul><h3 id="二、搭建-VPN"><a href="#二、搭建-VPN" class="headerlink" title="二、搭建 VPN"></a>二、搭建 VPN</h3><h4 id="2-1、安装依赖"><a href="#2-1、安装依赖" class="headerlink" title="2.1、安装依赖"></a>2.1、安装依赖</h4><p>以下采用源码编译安装，需要安装编译依赖环境</p><pre><code class="hljs sh">yum install -y gmp-devel xl2tpd module-init-tools gcc openssl-devel</code></pre><h4 id="2-2、编译安装"><a href="#2-2、编译安装" class="headerlink" title="2.2、编译安装"></a>2.2、编译安装</h4><p>首先下载源码</p><pre><code class="hljs sh">wget https://download.strongswan.org/strongswan-5.5.0.tar.gz -O /tmp/strongswan-5.5.0.tar.gz</code></pre><p>解压并编译安装</p><pre><code class="hljs sh"><span class="hljs-built_in">cd</span> /tmp &amp;&amp; tar -zxvf strongswan-5.5.0.tar.gz<span class="hljs-built_in">cd</span> /tmp/strongswan-5.5.0 &amp;&amp; ./configure --prefix=/usr --sysconfdir=/etc \--<span class="hljs-built_in">enable</span>-eap-radius \--<span class="hljs-built_in">enable</span>-eap-mschapv2 \--<span class="hljs-built_in">enable</span>-eap-identity \--<span class="hljs-built_in">enable</span>-eap-md5 \--<span class="hljs-built_in">enable</span>-eap-mschapv2 \--<span class="hljs-built_in">enable</span>-eap-tls \--<span class="hljs-built_in">enable</span>-eap-ttls \--<span class="hljs-built_in">enable</span>-eap-peap \--<span class="hljs-built_in">enable</span>-eap-tnc \--<span class="hljs-built_in">enable</span>-eap-dynamic \--<span class="hljs-built_in">enable</span>-xauth-eap \--<span class="hljs-built_in">enable</span>-openssl \&amp;&amp; make -j \&amp;&amp; make install</code></pre><h4 id="2-3、基础配置"><a href="#2-3、基础配置" class="headerlink" title="2.3、基础配置"></a>2.3、基础配置</h4><p>StrongSwan 的配置主要为 <code>ipsec.conf</code>、<code>strongswan.conf</code>、<code>xl2tpd.conf</code>、<code>options.xl2tpd</code> 这四个配置文件，以下为四个配置文件样例</p><h5 id="2-3-1、ipsec-conf"><a href="#2-3-1、ipsec-conf" class="headerlink" title="2.3.1、ipsec.conf"></a>2.3.1、ipsec.conf</h5><pre><code class="hljs sh"><span class="hljs-comment"># ipsec.conf - strongSwan IPsec configuration file</span>config setupuniqueids=nocharondebug=<span class="hljs-string">"cfg 2, dmn 2, ike 2, net 0"</span>conn %defaultdpdaction=cleardpddelay=300srekey=noleft=%defaultrouteleftfirewall=yesright=%anyikelifetime=60mkeylife=20mrekeymargin=3mkeyingtries=1auto=add<span class="hljs-comment">#######################################</span><span class="hljs-comment"># L2TP Connections</span><span class="hljs-comment">#######################################</span>conn L2TP-IKEv1-PSK<span class="hljs-built_in">type</span>=transportkeyexchange=ikev1authby=secretleftprotoport=udp/l2tpleft=%anyright=%anyrekey=noforceencaps=yes<span class="hljs-comment">#######################################</span><span class="hljs-comment"># Default non L2TP Connections</span><span class="hljs-comment">#######################################</span>conn Non-L2TPleftsubnet=0.0.0.0/0rightsubnet=10.0.0.0/24rightsourceip=10.0.0.0/24<span class="hljs-comment">#######################################</span><span class="hljs-comment"># EAP Connections</span><span class="hljs-comment">#######################################</span><span class="hljs-comment"># This detects a supported EAP method</span>conn IKEv2-EAPalso=Non-L2TPkeyexchange=ikev2eap_identity=%anyrightauth=eap-dynamic<span class="hljs-comment">#######################################</span><span class="hljs-comment"># PSK Connections</span><span class="hljs-comment">#######################################</span>conn IKEv2-PSKalso=Non-L2TPkeyexchange=ikev2authby=secret<span class="hljs-comment"># Cisco IPSec</span>conn IKEv1-PSK-XAuthalso=Non-L2TPkeyexchange=ikev1leftauth=pskrightauth=pskrightauth2=xauth<span class="hljs-comment">#######################################</span><span class="hljs-comment"># Certificate Connections</span><span class="hljs-comment">#######################################</span>conn windows7    keyexchange=ikev2    ike=aes256-sha1-modp1024!    rekey=no    left=%defaultroute    leftauth=pubkey    leftsubnet=0.0.0.0/0    leftcert=server.cert.pem    right=%any    rightauth=eap-mschapv2    rightsourceip=10.0.0.0/24    rightsendcert=never    eap_identity=%any    auto=add</code></pre><h5 id="2-3-2、options-xl2tpd"><a href="#2-3-2、options-xl2tpd" class="headerlink" title="2.3.2、options.xl2tpd"></a>2.3.2、options.xl2tpd</h5><pre><code class="hljs sh">ipcp-accept-localipcp-accept-remotems-dns 8.8.8.8ms-dns 8.8.4.4noccpauthcrtsctsidle 1800mtu 1280mru 1280locklcp-echo-failure 10lcp-echo-interval 60connect-delay 5000</code></pre><h5 id="2-3-3、strongswan-conf"><a href="#2-3-3、strongswan-conf" class="headerlink" title="2.3.3、strongswan.conf"></a>2.3.3、strongswan.conf</h5><pre><code class="hljs sh"><span class="hljs-comment"># /etc/strongswan.conf - strongSwan configuration file</span><span class="hljs-comment"># strongswan.conf - strongSwan configuration file</span><span class="hljs-comment">#</span><span class="hljs-comment"># Refer to the strongswan.conf(5) manpage for details</span>charon &#123;load_modular = yessend_vendor_id = yesplugins &#123;include strongswan.d/charon/*.confattr &#123;dns = 8.8.8.8, 8.8.4.4&#125;&#125;&#125;include strongswan.d/*.conf</code></pre><h5 id="2-3-4、xl2tpd-conf"><a href="#2-3-4、xl2tpd-conf" class="headerlink" title="2.3.4、xl2tpd.conf"></a>2.3.4、xl2tpd.conf</h5><pre><code class="hljs sh">[global]port = 1701auth file = /etc/ppp/l2tp-secretsdebug avp = yesdebug network = yesdebug state = yesdebug tunnel = yes[lns default]ip range = 10.1.0.2-10.1.0.254<span class="hljs-built_in">local</span> ip = 10.1.0.1require chap = yesrefuse pap = yesrequire authentication = yesname = l2tpd;ppp debug = yespppoptfile = /etc/ppp/options.xl2tpdlength bit = yes</code></pre><p><strong>创建好四个配置文件后将其复制到指定位置即可</strong></p><pre><code class="hljs sh"><span class="hljs-comment"># Strongswan Configuration</span>cp ipsec.conf /etc/ipsec.confcp strongswan.conf /etc/strongswan.conf<span class="hljs-comment"># XL2TPD Configuration</span>cp xl2tpd.conf /etc/xl2tpd/xl2tpd.confcp options.xl2tpd /etc/ppp/options.xl2tpd</code></pre><h4 id="2-4、创建证书"><a href="#2-4、创建证书" class="headerlink" title="2.4、创建证书"></a>2.4、创建证书</h4><p>对于 Windows、Android 等设备可能不支持某些登录方式，比如 IKEv2 PSK，这是需要创建证书，以支持使用 IKEv2 证书登录</p><h5 id="2-4-1、自签-CA"><a href="#2-4-1、自签-CA" class="headerlink" title="2.4.1、自签 CA"></a>2.4.1、自签 CA</h5><pre><code class="hljs sh"><span class="hljs-comment"># create CA certificate</span>ipsec pki --gen --outform pem &gt; ca.key.pemipsec pki --self --<span class="hljs-keyword">in</span> ca.key.pem --dn <span class="hljs-string">"C=CN, O=StrongSwan, CN=StrongSwan CA"</span> --ca --outform pem &gt; ca.cert.pem</code></pre><h5 id="2-4-2、创建服务器证书"><a href="#2-4-2、创建服务器证书" class="headerlink" title="2.4.2、创建服务器证书"></a>2.4.2、创建服务器证书</h5><p><strong>其中 <code>--san</code> 可以指定多个，但一般为一个是域名，一个是外网 IP，如果经过了路由，那么只需要写本机的对外暴露网卡的 IP 即可</strong></p><pre><code class="hljs sh"><span class="hljs-comment"># create server certificate</span>ipsec pki --gen --outform pem &gt; server.key.pemipsec pki --pub --<span class="hljs-keyword">in</span> server.key.pem | ipsec pki --issue --cacert ca.cert.pem \  --cakey ca.key.pem --dn <span class="hljs-string">"C=CN, O=StrongSwan, CN=服务器域名"</span> \  --san=<span class="hljs-string">"服务器域名"</span> --san=<span class="hljs-string">"网卡IP"</span> --flag serverAuth --flag ikeIntermediate \  --outform pem &gt; server.cert.pem</code></pre><h5 id="2-4-3、创建客户端证书"><a href="#2-4-3、创建客户端证书" class="headerlink" title="2.4.3、创建客户端证书"></a>2.4.3、创建客户端证书</h5><pre><code class="hljs sh">ipsec pki --gen --outform pem &gt; client.key.pemipsec pki --pub --<span class="hljs-keyword">in</span> client.key.pem | ipsec pki --issue --cacert ca.cert.pem \  --cakey ca.key.pem --dn <span class="hljs-string">"C=CN, O=StrongSwan, CN=Client"</span> \  --outform pem &gt; client.cert.pem</code></pre><h5 id="2-4-4、生成-p12"><a href="#2-4-4、生成-p12" class="headerlink" title="2.4.4、生成 p12"></a>2.4.4、生成 p12</h5><p>安卓等设备是不支持直接导入客户端证书的，需要转换成 p12 格式，转换过程中需要输入两次密码，该密码为证书使用密码，导入时需要输入</p><pre><code class="hljs sh">openssl pkcs12 -<span class="hljs-built_in">export</span> -inkey client.key.pem -<span class="hljs-keyword">in</span> client.cert.pem -name <span class="hljs-string">"Client"</span> \  -certfile ca.cert.pem -caname <span class="hljs-string">"StrongSwan CA"</span> -out client.cert.p12</code></pre><h5 id="2-4-5、安装证书"><a href="#2-4-5、安装证书" class="headerlink" title="2.4.5、安装证书"></a>2.4.5、安装证书</h5><p>创建完成后将证书复制到指定目录即可</p><pre><code class="hljs sh">cp -r ca.cert.pem /etc/ipsec.d/cacerts/cp -r server.cert.pem /etc/ipsec.d/certs/cp -r server.key.pem /etc/ipsec.d/private/cp -r client.cert.pem /etc/ipsec.d/certs/cp -r client.key.pem /etc/ipsec.d/private/</code></pre><h4 id="2-5、创建用户"><a href="#2-5、创建用户" class="headerlink" title="2.5、创建用户"></a>2.5、创建用户</h4><p>关于用户的登陆模式，比如使用 L2TP、IPsec、IKEv2 等请自行 Google，以下提供了一个简单的创建用户的脚本</p><pre><code class="hljs sh"><span class="hljs-meta">#!/bin/sh</span>vpn_user=<span class="hljs-variable">$1</span>vpn_password=<span class="hljs-variable">$2</span><span class="hljs-keyword">if</span> [ -z <span class="hljs-variable">$&#123;vpn_user&#125;</span> ] || [ -z <span class="hljs-variable">$&#123;vpn_password&#125;</span> ]; <span class="hljs-keyword">then</span><span class="hljs-built_in">echo</span> <span class="hljs-string">"Usage: <span class="hljs-variable">$0</span> user password"</span><span class="hljs-built_in">exit</span> 1<span class="hljs-keyword">fi</span>vpn_deluser <span class="hljs-variable">$&#123;vpn_user&#125;</span>cat &gt;&gt; /etc/ipsec.d/l2tp-secrets &lt;&lt;EOF<span class="hljs-string">"<span class="hljs-variable">$&#123;vpn_user&#125;</span>"</span> <span class="hljs-string">"*"</span> <span class="hljs-string">"<span class="hljs-variable">$&#123;vpn_password&#125;</span>"</span> <span class="hljs-string">"*"</span>EOFcat &gt;&gt; /etc/ipsec.d/ipsec.secrets &lt;&lt;EOF<span class="hljs-variable">$&#123;vpn_user&#125;</span> : EAP <span class="hljs-string">"<span class="hljs-variable">$&#123;vpn_password&#125;</span>"</span><span class="hljs-variable">$&#123;vpn_user&#125;</span> : XAUTH <span class="hljs-string">"<span class="hljs-variable">$&#123;vpn_password&#125;</span>"</span>EOF</code></pre><p><strong>将其保存为 <code>vpn_adduser.sh</code>，执行 <code>./vpn_adduser.sh USERNAME PASSWD</code> 即可添加用户</strong></p><h4 id="2-6、设置-PSK"><a href="#2-6、设置-PSK" class="headerlink" title="2.6、设置 PSK"></a>2.6、设置 PSK</h4><p>同样 PSK 也用于登录，如 IKEv2 PSK 登录，使用同样自行 Google，以下为设置 PSK 的脚本</p><pre><code class="hljs sh"><span class="hljs-meta">#!/bin/sh</span>psk=<span class="hljs-variable">$1</span><span class="hljs-keyword">if</span> [ -z <span class="hljs-variable">$&#123;psk&#125;</span> ]; <span class="hljs-keyword">then</span><span class="hljs-built_in">echo</span> <span class="hljs-string">"Usage: <span class="hljs-variable">$0</span> psk"</span><span class="hljs-built_in">exit</span> 1<span class="hljs-keyword">fi</span>vpn_unsetpsktouch /etc/ipsec.d/ipsec.secretscat &gt;&gt; /etc/ipsec.d/ipsec.secrets &lt;&lt;EOF: PSK <span class="hljs-string">"<span class="hljs-variable">$&#123;psk&#125;</span>"</span>EOF</code></pre><p>最后启动 VPN 连接即可</p><pre><code class="hljs sh">/usr/sbin/xl2tpd -c /etc/xl2tpd/xl2tpd.confipsec start</code></pre>]]></content>
    
    
    <categories>
      
      <category>Linux</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Linux</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>樱花 Docker 免费高速翻墙教程</title>
    <link href="/2016/10/14/arukas-docker-fq/"/>
    <url>/2016/10/14/arukas-docker-fq/</url>
    
    <content type="html"><![CDATA[<blockquote><p>免费梯子不多，且用亲珍惜，以下为利用樱花 Docker 翻墙教程</p></blockquote><h3 id="一、注册账号"><a href="#一、注册账号" class="headerlink" title="一、注册账号"></a>一、注册账号</h3><p>首先在 <a href="https://app.arukas.io/" target="_blank" rel="noopener">樱花 Docker 注册地址</a> 注册一个账号</p><p><img src="https://cdn.oss.link/markdown/xek39.jpg" srcset="/img/loading.gif" alt="crate_account"></p><p>填写邮箱、账户名、密码后点击确认</p><p><img src="https://cdn.oss.link/markdown/jax98.jpg" srcset="/img/loading.gif" alt="Fill_in_the_account"></p><p>稍等片刻在邮箱中点击验证连接即可</p><p><img src="https://cdn.oss.link/markdown/pewta.jpg" srcset="/img/loading.gif" alt="Verify_the_mailbox"></p><h3 id="二、创建容器"><a href="#二、创建容器" class="headerlink" title="二、创建容器"></a>二、创建容器</h3><p>注册号账号以后，登录控制台，点击 <code>Create</code> 创建容器 </p><p><img src="https://cdn.oss.link/markdown/3ai5c.jpg" srcset="/img/loading.gif" alt="login"></p><p>然后填写相关信息，如下</p><p><img src="https://cdn.oss.link/markdown/0dkkc.jpg" srcset="/img/loading.gif" alt="set_container"></p><p><strong>其中 <code>Image</code> 填写 <code>mritd/shadowsocks</code>，表示使用哪个镜像模板创建容器，关于 Docker 镜像含义等请自行 Google，<code>mritd/shadowsocks</code> 这个镜像是我维护的一个 shadowsocks 镜像，如果想使用其他 shadowsocks 可从 <a href="https://hub.docker.com/" target="_blank" rel="noopener">Docker Hub</a> 上自行搜索</strong></p><p><strong>其他各选项含义如下:</strong></p><ul><li>Instances: 启动多少个实例，一般 1 个就够用</li><li>Memory: 容器使用的内存大小，256 也可以</li><li>Endpoint: 暴露端口，可不填</li><li>Port: 容器对外提供服务的端口，默认为 5000</li><li>ENV: 是否使用环境变量，<code>mritd/shadowsocks</code> 镜像支持使用环境变量设置 shadowsocks 相关参数，这里可以省略</li><li>CMD: 要执行容器中的命令，<strong>首部必须填写 <code>/root/entrypoint.sh</code>，后面其他参数可以省略，但是一般会加上 <code>-k</code> 设置 shadowsocks 的密码，关于都能用哪些参数和参数意义请看考 <a href="https://hub.docker.com/r/mritd/shadowsocks/" target="_blank" rel="noopener">Docker Hub</a> 上的说明(shadowsocks 默认加密方式为 aes-256-cfb)</strong></li></ul><p>最后点击创建即可</p><p><img src="https://cdn.oss.link/markdown/e2d1j.jpg" srcset="/img/loading.gif" alt="create_container"></p><h3 id="三、启动并连接"><a href="#三、启动并连接" class="headerlink" title="三、启动并连接"></a>三、启动并连接</h3><p>创建完成后会回到主页列表，在主页列表中可以看到刚刚创建的容器，此时容器还没有启动，点击启动按钮即可</p><p><img src="https://cdn.oss.link/markdown/b9nts.jpg" srcset="/img/loading.gif" alt="start_container"></p><p>创建时间稍稍有点长，此时列表中容器变为黄色，点击容器名称后可看到如下所示</p><p><img src="https://cdn.oss.link/markdown/m093z.jpg" srcset="/img/loading.gif" alt="create_detail"></p><p>稍等片刻后容器变为绿色，并为 <code>Running</code> 状态表示创建完成</p><p><img src="https://cdn.oss.link/markdown/ybwqj.jpg" srcset="/img/loading.gif" alt="create_success"></p><p><strong>容器运行成功后会显示如下信息，其中包含了 shadowsocks 链接地址</strong></p><p><img src="https://cdn.oss.link/markdown/czjyx.jpg" srcset="/img/loading.gif" alt="ss_detail"></p><p>最后使用客户端连接即可</p><h3 id="四、高级扩展"><a href="#四、高级扩展" class="headerlink" title="四、高级扩展"></a>四、高级扩展</h3><p>樱花 Docker 提供了一个 CLI 命令行工具，可以通过相关 API Token 实现命令行下查询、创建 Docker 容器等操作，CLI 使用需要先创建 API Token，点击左侧按钮即可</p><p><img src="https://cdn.oss.link/markdown/ol0lv.jpg" srcset="/img/loading.gif" alt="crate_api_token"></p><p>创建成功后将 Token 声明道环境变量中，并使用 CLI 工具即可实现命令行下创建 shadowsocks 镜像，关于 CLI 使用说明和下载地址请移步 <a href="https://github.com/arukasio/cli" target="_blank" rel="noopener">Github</a> 查看</p><h3 id="五、其他说明"><a href="#五、其他说明" class="headerlink" title="五、其他说明"></a>五、其他说明</h3><p>关于镜像使用在 <a href="https://hub.docker.com/r/mritd/shadowsocks/" target="_blank" rel="noopener">Docker Hub</a> 页面详细描述了镜像参数，具体可以参考一下</p><p><strong>镜像可以理解为一个微型 Linux 系统，其中 CMD 选项代表要执行系统中的那个命令，填写 <code>/root/entrypoint.sh</code> 是因为制作镜像时指定了 shadowsocks 通过这个脚本启动，后面的 <code>-k</code> 等参数会被这个脚本执行，脚本支持哪些参数可以从上面的 Docker Hub 页面获取</strong></p><p><strong>镜像支持以环境变量的方式设置密码等，环境变量表现为一个 <code>key-value</code> (键值对) 形式，也就是说 CMD 其实可以只写 <code>/root/entrypoint.sh</code> 不加 <code>-k</code> 参数，而通过勾选 <code>ENV</code> 选项并添加一个 <code>PASSWORD=Your-PassWord</code> 环境变量来设置</strong></p><p><strong>目前镜像内集成了 kcptun，但是由于 kcptun 自定义设置需要挂载配置文件，所以在樱花 Docker 中还无法使用，后期准备支持环境变量设置</strong></p>]]></content>
    
    
    <categories>
      
      <category>Docker</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Linux</tag>
      
      <tag>Docker</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>kubernetes 1.4 集群搭建</title>
    <link href="/2016/10/09/kubernetes-1.4-create-cluster/"/>
    <url>/2016/10/09/kubernetes-1.4-create-cluster/</url>
    
    <content type="html"><![CDATA[<blockquote><p>距离 kubernetes 1.4 发布已经有段时间，1.4 版本新增了很多新特性，其中一个比较实用的功能就是增加了集群的快速创建，基本只需要 2 条命令就能搭建成功；但由于众所周知的原因(fuck GFW)，导致 kuadm 命令无法工作，以下记录了一下解决方案</p></blockquote><h3 id="一、环境准备"><a href="#一、环境准备" class="headerlink" title="一、环境准备"></a>一、环境准备</h3><p>基本环境为 3 台虚拟机，虚拟机信息如下</p><table><thead><tr><th>IP 地址</th><th>节点</th></tr></thead><tbody><tr><td>192.168.1.107</td><td>master</td></tr><tr><td>192.168.1.126</td><td>node1</td></tr><tr><td>192.168.1.217</td><td>node2</td></tr></tbody></table><h4 id="1-1、安装-docker"><a href="#1-1、安装-docker" class="headerlink" title="1.1、安装 docker"></a>1.1、安装 docker</h4><p>docker 这里使用的是 1.12.1 版本，安装直接根据官方教程来，如果网速较慢可切换国内源，如清华大 docker 源，具体请 Google</p><pre><code class="hljs sh">tee /etc/yum.repos.d/docker.repo &lt;&lt;-<span class="hljs-string">'EOF'</span>[dockerrepo]name=Docker Repositorybaseurl=https://yum.dockerproject.org/repo/main/centos/7/enabled=1gpgcheck=1gpgkey=https://yum.dockerproject.org/gpgEOFyum install docker-engine -ysystemctl <span class="hljs-built_in">enable</span> dockersystemctl start dockersystemctl status docker</code></pre><h4 id="1-2、修改主机名"><a href="#1-2、修改主机名" class="headerlink" title="1.2、修改主机名"></a>1.2、修改主机名</h4><p>由于 3 台虚拟机是从一个基础虚拟机复制而来，为了不影响 <code>kubectl get nodes</code> 查询，需要更改 3 台虚拟机的主机名，以下为 master 节点示例，其他节点对应修改即可</p><pre><code class="hljs sh"><span class="hljs-built_in">echo</span> <span class="hljs-string">"master"</span> &gt; /etc/hostname<span class="hljs-comment"># 替换一下 hosts 中的 localhost 等</span>vim /etc/hosts<span class="hljs-comment"># 修改后内容如下</span>127.0.0.1   master::1         master192.168.1.107 master192.168.1.126 node1192.168.1.217 node2</code></pre><h3 id="二、搭建-kubernetes-集群"><a href="#二、搭建-kubernetes-集群" class="headerlink" title="二、搭建 kubernetes 集群"></a>二、搭建 kubernetes 集群</h3><h4 id="2-1、安装基本组件"><a href="#2-1、安装基本组件" class="headerlink" title="2.1、安装基本组件"></a>2.1、安装基本组件</h4><p><strong>根据 <a href="http://kubernetes.io/docs/getting-started-guides/kubeadm/" target="_blank" rel="noopener">官方文档教程</a> 需要先安装 <code>kubelet</code>、<code>kubeadm</code>、<code>kubectl</code>、 <code>kubernetes-cni</code> 这四个 rpm 包，但是由于 GFW 原因实际上 Google 的 rpm 源无法下载，以下是我通过梯子下载到本地的，rpm 下载方法 可借助 yumdownloader 工具，具体请 Google</strong></p><pre><code class="hljs sh"><span class="hljs-comment"># 首先安装 socat</span>yum install -y socat<span class="hljs-comment"># 然后下载相关 rpm，我已经放到了 cdn 里</span>rpms=(5ce829590fb4d5c860b80e73d4483b8545496a13f68ff3033ba76fa72632a3b6-kubernetes-cni-0.3.0.1-0.07a8a2.x86_64.rpm \     bbad6f8b76467d0a5c40fe0f5a1d92500baef49dedff2944e317936b110524eb-kubeadm-1.5.0-0.alpha.0.1534.gcf7301f.x86_64.rpm \     c37966352c9d394bf2cc1f755938dfb679aa45ac866d3eb1775d9c9b87d5e177-kubelet-1.4.0-0.x86_64.rpm \     fac5b4cd036d76764306bd1df7258394b200be4c11f4e3fdd100bfb25a403ed4-kubectl-1.4.0-0.x86_64.rpm)<span class="hljs-keyword">for</span> rpmName <span class="hljs-keyword">in</span> <span class="hljs-variable">$&#123;rpms[@]&#125;</span>; <span class="hljs-keyword">do</span>  wget http://upyun.mritd.me/kubernetes/<span class="hljs-variable">$rpmName</span><span class="hljs-keyword">done</span><span class="hljs-comment"># 最后安装即可</span>rpm -ivh *.rpm</code></pre><h4 id="2-2、启动相关组件"><a href="#2-2、启动相关组件" class="headerlink" title="2.2、启动相关组件"></a>2.2、启动相关组件</h4><p>接下来启动 docker 和 kubelet </p><pre><code class="hljs sh">systemctl <span class="hljs-built_in">enable</span> dockersystemctl <span class="hljs-built_in">enable</span> kubeletsystemctl start dockersystemctl start kubelet</code></pre><p>此时查看 kubelet 其实是启动失败的，因为缺少相关配置，以下一部部署以后便会自动重启成功</p><p><strong>在正式使用 kubeadm 创建集群以前还需要关闭 selinux，在下一个版本这个问题已经被解决</strong></p><pre><code class="hljs sh"><span class="hljs-comment"># 关闭 selinux</span>setenforce 0</code></pre><h4 id="2-3、导入相关-image"><a href="#2-3、导入相关-image" class="headerlink" title="2.3、导入相关 image"></a>2.3、导入相关 image</h4><p>kubeadm 会 pull 相关的 image，由于 GFW 的原因会造成无法下载最终失败，所以最好的办法是先用梯子 pull 下来，再 load 进去即可，以下为需要 load 进的镜像</p><table><thead><tr><th>镜像名称</th><th>版本号</th></tr></thead><tbody><tr><td>gcr.io/google_containers/kube-proxy-amd64</td><td>v1.4.0</td></tr><tr><td>gcr.io/google_containers/kube-discovery-amd64</td><td>1.0</td></tr><tr><td>gcr.io/google_containers/kubedns-amd64</td><td>1.7</td></tr><tr><td>gcr.io/google_containers/kube-scheduler-amd64</td><td>v1.4.0</td></tr><tr><td>gcr.io/google_containers/kube-controller-manager-amd64</td><td>v1.4.0</td></tr><tr><td>gcr.io/google_containers/kube-apiserver-amd64</td><td>v1.4.0</td></tr><tr><td>gcr.io/google_containers/etcd-amd64</td><td>2.2.5</td></tr><tr><td>gcr.io/google_containers/kube-dnsmasq-amd64</td><td>1.3</td></tr><tr><td>gcr.io/google_containers/exechealthz-amd64</td><td>1.1</td></tr><tr><td>gcr.io/google_containers/pause-amd64</td><td>3.0</td></tr></tbody></table><p><strong>实际上不用梯子可以借助于 DockerHub 的自动构建功能，实现代理下载，如下所示</strong></p><pre><code class="hljs sh">images=(kube-proxy-amd64:v1.4.0 kube-discovery-amd64:1.0 kubedns-amd64:1.7 kube-scheduler-amd64:v1.4.0 kube-controller-manager-amd64:v1.4.0 kube-apiserver-amd64:v1.4.0 etcd-amd64:2.2.5 kube-dnsmasq-amd64:1.3 exechealthz-amd64:1.1 pause-amd64:3.0 kubernetes-dashboard-amd64:v1.4.0)<span class="hljs-keyword">for</span> imageName <span class="hljs-keyword">in</span> <span class="hljs-variable">$&#123;images[@]&#125;</span> ; <span class="hljs-keyword">do</span>  docker pull mritd/<span class="hljs-variable">$imageName</span>  docker tag mritd/<span class="hljs-variable">$imageName</span> gcr.io/google_containers/<span class="hljs-variable">$imageName</span>  docker rmi mritd/<span class="hljs-variable">$imageName</span><span class="hljs-keyword">done</span></code></pre><h4 id="2-4、创建集群"><a href="#2-4、创建集群" class="headerlink" title="2.4、创建集群"></a>2.4、创建集群</h4><p>首先在 master 上执行 init 操作</p><pre><code class="hljs sh">kubeadm init --api-advertise-addresses=192.168.1.107</code></pre><p>此时显示信息如下表示创建完成</p><pre><code class="hljs sh">➜  ~ kubeadm init --api-advertise-addresses=192.168.1.107&lt;master/tokens&gt; generated token: <span class="hljs-string">"42354d.e1fb733ed0c9a932"</span>&lt;master/pki&gt; created keys and certificates <span class="hljs-keyword">in</span> <span class="hljs-string">"/etc/kubernetes/pki"</span>&lt;util/kubeconfig&gt; created <span class="hljs-string">"/etc/kubernetes/kubelet.conf"</span>&lt;util/kubeconfig&gt; created <span class="hljs-string">"/etc/kubernetes/admin.conf"</span>&lt;master/apiclient&gt; created API client configuration&lt;master/apiclient&gt; created API client, waiting <span class="hljs-keyword">for</span> the control plane to become ready&lt;master/apiclient&gt; all control plane components are healthy after 18.921781 seconds&lt;master/apiclient&gt; waiting <span class="hljs-keyword">for</span> at least one node to register and become ready&lt;master/apiclient&gt; first node is ready after 2.014976 seconds&lt;master/discovery&gt; created essential addon: kube-discovery, waiting <span class="hljs-keyword">for</span> it to become ready&lt;master/discovery&gt; kube-discovery is ready after 3.505092 seconds&lt;master/addons&gt; created essential addon: kube-proxy&lt;master/addons&gt; created essential addon: kube-dnsKubernetes master initialised successfully!You can now join any number of machines by running the following on each node:kubeadm join --token 42354d.e1fb733ed0c9a932 192.168.1.107</code></pre><p>然后在子节点上使用 join 命令加入集群即可</p><pre><code class="hljs sh">kubeadm join --token 42354d.e1fb733ed0c9a932 192.168.1.107</code></pre><p>最后稍等片刻在 master 上 get nodes 即可查看，<strong>如果想让 master 也运行 pod，只需在 master 上运行 <code>kubectl taint nodes --all dedicated-</code> 即可</strong></p><pre><code class="hljs sh">➜  ~ kubectl get nodes                                   NAME      STATUS    AGEmaster    Ready     1mnode1     Ready     1mnode2     Ready     1m</code></pre><h4 id="2-5、创建-Pod-网络"><a href="#2-5、创建-Pod-网络" class="headerlink" title="2.5、创建 Pod 网络"></a>2.5、创建 Pod 网络</h4><p>创建好集群后，为了能让容器进行跨主机通讯还要部署 Pod 网络，这里使用官方推荐的 weave 方式，也可以采用 flannel，以下为 weave 示例</p><pre><code class="hljs sh"><span class="hljs-comment"># 在 master 上执行</span>kubectl apply -f https://git.io/weave-kube</code></pre><p>到此搭建完成</p><p><strong>本文参考 <a href="https://segmentfault.com/a/1190000007074726" target="_blank" rel="noopener">来自天国的 kubernetes</a>、<a href="http://kubernetes.io/docs/getting-started-guides/kubeadm" target="_blank" rel="noopener">Installing Kubernetes on Linux with kubeadm</a></strong></p>]]></content>
    
    
    <categories>
      
      <category>Kubernetes</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Linux</tag>
      
      <tag>Docker</tag>
      
      <tag>Kubernetes</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Jekyll 搭建静态博客</title>
    <link href="/2016/10/09/jekyll-create-a-static-blog/"/>
    <url>/2016/10/09/jekyll-create-a-static-blog/</url>
    
    <content type="html"><![CDATA[<blockquote><p>最近从 Hexo 切换到了 jekyll，发现 jekyll 搭建静态博客要比 Hexo 好得多；顺便吐槽一下 node 的依赖管理，感觉很蛋疼；同样的操作步骤往往在其他机器上无法搭建成功，每次挂掉的原因都是依赖不对等等，感觉没有一个良好的依赖管理系统；而 jekyll 使用 ruby 编写，暂不论语言哪个好那个坏，至少有 Genfile 等约束可以保证每次同样的操作搭建时不会出现依赖问题</p></blockquote><h3 id="一、环境准备"><a href="#一、环境准备" class="headerlink" title="一、环境准备"></a>一、环境准备</h3><ul><li>CentOS 7</li><li>rvm</li><li>ruby 2.3</li><li>jekyll 3.2.1</li></ul><p>本文基于 CentOS 7 系统，Mac OS 与其他系统基本操作类似，大致步骤相同，唯一差别就是搞定前期的 ruby 相关配置</p><h3 id="二、安装-ruby"><a href="#二、安装-ruby" class="headerlink" title="二、安装 ruby"></a>二、安装 ruby</h3><p>由于众所周知的原因(就是特么的有GFW)，导致 ruby 源等在国内不可用，所以可以使用淘宝源等替代</p><h4 id="2-1、安装-rvm"><a href="#2-1、安装-rvm" class="headerlink" title="2.1、安装 rvm"></a>2.1、安装 rvm</h4><p>rvm 是 ruby 管理工具，可以使用 rvm 在系统中安装配置多个版本的 ruby，同时可方便进行切换</p><pre><code class="hljs sh"><span class="hljs-comment"># 首先安装基本工具</span>yum install -y wget <span class="hljs-built_in">which</span><span class="hljs-comment"># 设置一下环境变量，下面要用到</span>PATH=<span class="hljs-variable">$PATH</span>:/usr/<span class="hljs-built_in">local</span>/rvm/bin:/usr/<span class="hljs-built_in">local</span>/rvm/rubies/ruby-2.3.0/bin<span class="hljs-comment"># 下载 rvm 的密钥文件(有墙太慢，我已经提取放到了 CDN 里)</span>wget http://upyun.mritd.me/keys/rvm.key -O rvm.key<span class="hljs-comment"># 导入密钥</span>gpg2 --import rvm.key<span class="hljs-comment"># 安装 rvm</span>curl -sSL https://get.rvm.io | bash -s stable<span class="hljs-comment"># 切换 ruby 源</span><span class="hljs-built_in">echo</span> <span class="hljs-string">"ruby_url=https://cache.ruby-china.org/pub/ruby"</span> &gt;&gt; /usr/<span class="hljs-built_in">local</span>/rvm/user/dbrvm requirements</code></pre><h4 id="2-2、安装-ruby"><a href="#2-2、安装-ruby" class="headerlink" title="2.2、安装 ruby"></a>2.2、安装 ruby</h4><p>安装好 rvm 后，可使用 rvm 直接安装 ruby</p><pre><code class="hljs sh"><span class="hljs-comment"># 安装 ruby</span>rvm install 2.3.0<span class="hljs-comment"># 设置默认使用的 ruby 版本</span>rvm use 2.3.0 --default<span class="hljs-comment"># 安装 bundler</span>gem install bundler</code></pre><h3 id="三、搭建-jekyll"><a href="#三、搭建-jekyll" class="headerlink" title="三、搭建 jekyll"></a>三、搭建 jekyll</h3><p>jekyll 搭建静态博客非常简单，只需要找一套可用的主题，然后安装好相关依赖即可，关于主题可参考这里 <a href="https://www.zhihu.com/question/20223939" target="_blank" rel="noopener">https://www.zhihu.com/question/20223939</a>，以下以 mzlogin.github.io 的主题为例</p><h4 id="3-1、clone-主题"><a href="#3-1、clone-主题" class="headerlink" title="3.1、clone 主题"></a>3.1、clone 主题</h4><pre><code class="hljs sh">git <span class="hljs-built_in">clone</span> https://github.com/mzlogin/mzlogin.github.io.git</code></pre><h4 id="3-2、安装-jekyll"><a href="#3-2、安装-jekyll" class="headerlink" title="3.2、安装 jekyll"></a>3.2、安装 jekyll</h4><pre><code class="hljs sh"><span class="hljs-comment"># 进入主题目录</span><span class="hljs-built_in">cd</span> mzlogin.github.io<span class="hljs-comment"># 安装 jekyll 等</span>bundle install</code></pre><h4 id="3-3、启动并修改"><a href="#3-3、启动并修改" class="headerlink" title="3.3、启动并修改"></a>3.3、启动并修改</h4><p>jekyll 等组件安装完成后便可直接启动预览博客</p><pre><code class="hljs sh"><span class="hljs-comment"># 启动 jekyll 并设置监听地址和端口</span>jekyll serve -H 0.0.0.0 -P 1234</code></pre><p>此时访问便可看到效果</p><p><img src="https://cdn.oss.link/markdown/jekyll_startjekyll.png" srcset="/img/loading.gif" alt="jekyll_startjekyll"></p><p><strong>接下来只需要根据主题作者的说明修改对应配置文件即可</strong></p><h3 id="四、推送到-Github"><a href="#四、推送到-Github" class="headerlink" title="四、推送到 Github"></a>四、推送到 Github</h3><p><strong>首先在 Github 上创建 <code>用户名.github.io</code> 项目，如 <code>mritd.github.io</code>，然后删除主题目录下的 <code>.git</code> 目录，在执行 <code>git init</code> 初始化一下 git 仓库，最后将 master 地址指向新建的仓库地址推送即可；Github 本身也是使用 jekyll 进行生成，所以会自动识别并生成博客；最后访问 <code>http://用户名.github.io</code> 即可；其他相关比如 sitemap 设置等可参考 <a href="https://github.com/mritd/mritd.github.io" target="_blank" rel="noopener">https://github.com/mritd/mritd.github.io</a></strong></p>]]></content>
    
    
    <categories>
      
      <category>Linux</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Linux</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>HotSpot 虚拟机的算法实现</title>
    <link href="/2006/01/02/algorithm-implementation-of-hotspot-vm/"/>
    <url>/2006/01/02/algorithm-implementation-of-hotspot-vm/</url>
    
    <content type="html"><![CDATA[<h2 id="一、概述"><a href="#一、概述" class="headerlink" title="一、概述"></a>一、概述</h2><p>HotSpot 虚拟机通过 GC Roots 枚举判定待回收的对象，通过安全点和安全区域确定 GC 的触发点，最后通过各种不同的回收算法完成垃圾回收。</p><h2 id="二、GC-Roots-枚举过程"><a href="#二、GC-Roots-枚举过程" class="headerlink" title="二、GC Roots 枚举过程"></a>二、GC Roots 枚举过程</h2><p><strong>GC Roots 枚举最大的困难点在于：检查范围比较大，并且必须在内存快照中进行，保证一致性，而且时间要求比较敏感。</strong></p><p>在生产环境中，即使不考虑其它部分内存，仅仅 Java 堆内存就可达几百兆甚至上G，在此范围内完成 GC Roots 确定是一件很困难的事情；同时，在进行 GC Roots 枚举时，<strong>必须保证一致性</strong>，即所有正在运行的程序必须停顿，不能出现正在枚举 GC Roots，而程序还在跑的情况，这会导致 GC Roots 不断变化，产生数据不一致导致统计不准确的情况；最后，由于所有工作线程必须停顿以完成 GC 过程，在大并发高访问量情况下，这个时间必须非常短。</p><h2 id="三、准确式-GC"><a href="#三、准确式-GC" class="headerlink" title="三、准确式 GC"></a>三、准确式 GC</h2><p>为解决上述问题，HotSpot 采用了一种 “准确式 GC” 的技术；该技术主要功能就是让虚拟机可以准确的知道内存中某个位置的数据类型是什么；比如某个内存位置到底是一个整型的变量，还是对某个对象的 reference；这样在进行 GC Roots 枚举时，只需要枚举 reference 类型的即可。</p><p>在能够准确地确定 Java 堆和方法区等 reference 准确位置之后，HotSpot 就能极大地缩短 GC Roots 枚举时间。</p><h2 id="四、OopMap"><a href="#四、OopMap" class="headerlink" title="四、OopMap"></a>四、OopMap</h2><p><strong>当类加载完成后，HotSpot 就将对象内存布局之中什么偏移量上数值是一个什么样的类型的数据这些信息存放到 OopMap 中；在 HotSpot 的 JIT 编译过程中，同样会插入相关指令来标明哪些位置存放的是对象引用等，这样在 GC 发生时，HotSpot 就可以直接扫描 OopMap 来获取这些信息，从而进行 GC Roots 枚举，下图展示了 JTI 编译的 OopMap 指令：</strong></p><p><img src="https://cdn.oss.link/markdown/hexo_jvm_jmm_oopmap.png" srcset="/img/loading.gif" alt="OopMap"></p><h2 id="五、安全点-Safepoint"><a href="#五、安全点-Safepoint" class="headerlink" title="五、安全点(Safepoint)"></a>五、安全点(Safepoint)</h2><p><strong>Safepoint：</strong>会导致 OopMap 内容变化的指令非常多，如果为每一条指令都生成对应的 OopMap，那么将需要大量的额外空间，这样对导致 GC 成本很高，所以 HotSpot 只在 “特定位置” 记录这些信息，这些位置被称为 <strong>安全点(Safepoint)。</strong></p><p><strong>并非程序在任意时刻都可以停顿下来进行 GC，而只有程序到达 安全点(Safepoint) 以后才可以停顿下来进行 GC；所以安全点既不能太少，以至于 GC 过程等待程序到达安全点的时间过长，也不能太多，以至于 GC 过程带来的成本过高。</strong></p><h2 id="六、安全点上停止线程方式"><a href="#六、安全点上停止线程方式" class="headerlink" title="六、安全点上停止线程方式"></a>六、安全点上停止线程方式</h2><p>由于在 GC 过程中必须保证程序已执行，那么也就是说 <strong>必须等待所有线程都到达安全点上方可进行 GC。</strong></p><p><strong>一般会有两种解决方案：</strong></p><p><strong>抢先式中断：</strong>不需要线程的执行代码去主动配合，当发生 GC 时，先强制中断所有线程，然后如果发现某些线程未处于安全点，那么将其唤醒，直至其到达安全点再次将其中断；这样一直等待所有线程都在安全点后开始 GC。</p><p><strong>主动式中断：</strong>不强制中断线程，只是简单地设置一个中断标记，各个线程在执行时轮询这个标记，一旦发现标记被改变(出现中断标记)时，那么将运行到安全点后自己中断挂起；目前所有商用虚拟机全部采用主动式中断。</p><p><strong>HotSpot 选定的标记位置与安全点位置是重合的，如下如图所示：</strong></p><p><img src="https://cdn.oss.link/markdown/hexo_jvm_jmm_safepoint.png" srcset="/img/loading.gif" alt="安全点与中断标记"></p><h2 id="七、安全区-Saferegion"><a href="#七、安全区-Saferegion" class="headerlink" title="七、安全区(Saferegion)"></a>七、安全区(Saferegion)</h2><p>安全点的机制似乎已经完美的解决了 “什么时候以及何时开始 GC” 的问题，但是实际情况并非如此；<strong>安全点机制仅仅是保证了程序执行时不需要太长时间就可以进入一个安全点进行 GC 动作，</strong>但是当特殊情况时，比如线程休眠、线程阻塞等状态的情况下，显然 JVM 不可能一直等待被阻塞或休眠的线程正常唤醒执行；此时就引入了安全区的概念。</p><p><strong>安全区(Saferegion)：</strong>安全区域是指在一段区域内，对象引用关系等不会发生变化，在此区域内任意位置开始 GC 都是安全的；线程运行时，首先标记自己进入了安全区，然后在这段区域内，如果线程发生了阻塞、休眠等操作，<strong>JVM 发起 GC 时将忽略这些处于安全区的线程。当线程再次被唤醒时，首先他会检查是否完成了 GC Roots枚举(或这个GC过程)，然后选择是否继续执行，否则将继续等待 GC 的完成。</strong></p>]]></content>
    
    
    <categories>
      
      <category>Java</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Java</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>apt-get 常用命令</title>
    <link href="/2006/01/02/apt-get%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/"/>
    <url>/2006/01/02/apt-get%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/</url>
    
    <content type="html"><![CDATA[<pre><code class="hljs bash">apt-cache search package    <span class="hljs-comment">#搜索包（相当于yum list | grep pkg）</span>apt-cache show package      <span class="hljs-comment">#显示包的相关信息，如说明、大小、版本等</span>apt-cache showpg package    <span class="hljs-comment">#显示包的相关信息，如Reverse Depends（反向依赖）、依赖等</span>apt-get install package       <span class="hljs-comment">#安装包</span>apt-get reinstall package     <span class="hljs-comment">#重新安装包</span>apt-get -f install package    <span class="hljs-comment">#强制安装</span></code></pre><pre><code class="hljs bash">apt-get remove package        <span class="hljs-comment">#删除包（只是删掉数据和可执行文件，不删除配置文件）</span>apt-get remove --purge package       <span class="hljs-comment">#删除包，包括删除配置文件等</span>apt-get autoremove --purge package   <span class="hljs-comment">#删除包及其依赖的软件包+配置文件等</span>apt-get update          <span class="hljs-comment">#更新源</span>apt-get upgrade         <span class="hljs-comment">#更新已安装的包</span>apt-get dist-upgrade    <span class="hljs-comment">#升级系统</span>apt-get dselect-upgrade        <span class="hljs-comment">#使用 dselect 升级</span>apt-cache depends package      <span class="hljs-comment">#了解使用依赖</span>apt-cache rdepends package     <span class="hljs-comment">#查看该包被哪些包依赖</span>apt-get build-dep package   <span class="hljs-comment">#安装相关的编译环境</span>apt-get <span class="hljs-built_in">source</span> package      <span class="hljs-comment">#下载该包的源代码</span>apt-get clean &amp;&amp; apt-get autoclean  <span class="hljs-comment">#清理下载文件的存档 &amp;&amp; 只清理过时的包</span>apt-get check             <span class="hljs-comment">#检查是否有损坏的依赖</span>dpkg -S filename          <span class="hljs-comment">#查找filename属于哪个软件包</span>apt-file search filename  <span class="hljs-comment">#查找filename属于哪个软件包</span>apt-file list packagename <span class="hljs-comment">#列出软件包的内容</span>apt-file update           <span class="hljs-comment">#更新apt-file的数据库</span>dpkg -l      <span class="hljs-comment">#列出当前系统中所有的包.可以和参数less一起使用在分屏查看（类似于rpm -qa）</span>dpkg -l |grep -i <span class="hljs-string">"pkg"</span>   <span class="hljs-comment">#查看系统中与"pkg"相关联的包（类似于rpm -qa | grep pkg）</span>dpkg -s pkg  <span class="hljs-comment">#查询一个已安装的包的详细信息（类似于rpm -qi）</span>dpkg -L pkg  <span class="hljs-comment">#查询一个已安装的软件包释放了哪些文件（类似于rpm -ql）</span>dpkg -S file     <span class="hljs-comment">#查询系统中某个文件属于哪个软件包（类似于rpm -qf）</span>dpkg -c pkg.deb  <span class="hljs-comment">#查询一个未安装的deb包将会释放哪些文件（类似于rpm -qpl）</span>dpkg -I pkg.deb  <span class="hljs-comment">#查看一个未安装的deb包的详细信息（类似于rpm -qpi）</span>dpkg -i pkg.deb  <span class="hljs-comment">#手动安装软件包（不能解决软依赖性问题，可以用apt-get -f install解决）</span>dpkg -r pkg      <span class="hljs-comment">#卸载软件包（不是完全的卸载，它的配置文件还存在）</span>dpkg -P pkg      <span class="hljs-comment">#全部卸载（不能解决依赖性的问题）</span>dpkg-reconfigure pkg     <span class="hljs-comment">#重新配置</span>dpkg -x pkg.deb dir      <span class="hljs-comment">#将一个deb包解开至dir目录</span>dpkg --pending --remove  <span class="hljs-comment">#移除多余的软件</span><span class="hljs-comment"># 强制安装一个包(忽略依赖及其它问题)</span>dpkg --force-all -i pkg.deb    <span class="hljs-comment">#可以参考dpkg --force-help</span><span class="hljs-comment"># 强制卸载一个包</span>dpkg --force-all -P pkg.debaptitude update   <span class="hljs-comment">#更新可用的包列表</span>aptitude upgrade  <span class="hljs-comment">#升级可用的包</span>aptitude dist-upgrade     <span class="hljs-comment">#将系统升级到新的发行版</span>aptitude install pkgname  <span class="hljs-comment">#安装包</span>aptitude remove pkgname   <span class="hljs-comment">#删除包</span>aptitude purge pkgname    <span class="hljs-comment">#删除包及其配置文件</span>aptitude search string    <span class="hljs-comment">#搜索包（相当于yum list | grep pkg，重要）</span>aptitude show pkgname     <span class="hljs-comment">#显示包的详细信息 （相当于yum info pkg，重要）</span>aptitude clean            <span class="hljs-comment">#删除下载的包文件</span>aptitude autoclean        <span class="hljs-comment">#仅删除过期的包文件</span>apt-get install xrdp      <span class="hljs-comment">#安装图形化</span></code></pre>]]></content>
    
    
    <categories>
      
      <category>Linux</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Linux</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>免费申请 StartSSL 证书</title>
    <link href="/2006/01/02/apply-for-a-startssl-certificate-for-free/"/>
    <url>/2006/01/02/apply-for-a-startssl-certificate-for-free/</url>
    
    <content type="html"><![CDATA[<h2 id="一、前言"><a href="#一、前言" class="headerlink" title="一、前言"></a>一、前言</h2><blockquote><p>一直想给博客搞个 SSL 证书装装逼，发现 StartSSL 可以申请免费的一年个人证书，所以决定搞来一个，以下为申请过程。</p></blockquote><h2 id="二、申请过程"><a href="#二、申请过程" class="headerlink" title="二、申请过程"></a>二、申请过程</h2><h3 id="2-1、注册账号"><a href="#2-1、注册账号" class="headerlink" title="2.1、注册账号"></a>2.1、注册账号</h3><p>既然要申请，首先需要注册一个 StartSSL 的账号，注册地址 <a href="https://www.startssl.com/SignUp" target="_blank" rel="noopener">https://www.startssl.com/SignUp</a>，按照提示一步一步往下搞就行。</p><p><img src="https://cdn.oss.link/markdown/hexo_startssl_register_account.png" srcset="/img/loading.gif" alt="hexo_startssl_register_account"></p><h3 id="2-2、登录"><a href="#2-2、登录" class="headerlink" title="2.2、登录"></a>2.2、登录</h3><p>注册完成后默认会下载一个 key，导入浏览器后可免密码登录；同时支持动态密码登录，不导入 key 每次登录都会向邮箱发送一个动态密码。</p><p><img src="https://cdn.oss.link/markdown/hexo_startssl_register_login.png" srcset="/img/loading.gif" alt="hexo_startssl_register_login"></p><p>接下来发送一个动态密码即可</p><p><img src="https://cdn.oss.link/markdown/hexo_startssl_login_sendpasswd.png" srcset="/img/loading.gif" alt="hexo_startssl_login_sendpasswd"></p><h3 id="2-3、创建-SSL"><a href="#2-3、创建-SSL" class="headerlink" title="2.3、创建 SSL"></a>2.3、创建 SSL</h3><p>首先点击 <code>Validations Wizard</code>，并选择第一项 <code>Domain Validation (for SSL certificate)</code>，创建一个域名的 SSL 证书</p><p><img src="https://cdn.oss.link/markdown/hexo_startssl_createssltype.png" srcset="/img/loading.gif" alt="hexo_startssl_createssltype"></p><p>然后输入你的域名地址</p><p><img src="https://cdn.oss.link/markdown/hexo_startssl_createssl_domain.png" srcset="/img/loading.gif" alt="hexo_startssl_createssl_domain"></p><h3 id="2-4、验证域名"><a href="#2-4、验证域名" class="headerlink" title="2.4、验证域名"></a>2.4、验证域名</h3><p>接下来 StartSSL 会要求验证域名的所有权，验证方法就是向域名注册邮箱发送动态验证码；<strong>注意 : 如果域名开启了隐私保护，那么 whois 查询得到的域名注册邮箱将是注册商的，所以验证时最好取消域名隐私保护，当然像 GoDaddy 等大型注册商会提供邮件转发，目前测试 GoDaddy 会转发邮件。</strong></p><p><img src="https://cdn.oss.link/markdown/hexo_startssl_domain_email.png" srcset="/img/loading.gif" alt="hexo_startssl_domain_email"></p><h3 id="2-5、生成-HTTPS-证书"><a href="#2-5、生成-HTTPS-证书" class="headerlink" title="2.5、生成 HTTPS 证书"></a>2.5、生成 HTTPS 证书</h3><p>验证好域名以后便可以选择 <code>Certificates Wizard</code> 来创建 SSL 证书</p><p><img src="https://cdn.oss.link/markdown/hexo_startssl_create_ssl1.png" srcset="/img/loading.gif" alt="hexo_startssl_create_ssl1"></p><p>随后输入证书使用的域名，包括二级域名，<strong>免费版最多可一次性输入5个</strong></p><p><img src="https://cdn.oss.link/markdown/hexo_startssl_input_domainname.png" srcset="/img/loading.gif" alt="hexo_startssl_input_domainname"></p><p>最后需要创建 CSR 来生成 SSL 证书，StartSSL支持在线生成以及离线(本地)生成并上传，以下为本地生成，并复制到 StartSSL</p><p><img src="https://cdn.oss.link/markdown/hexo_startssl_create_csr.png" srcset="/img/loading.gif" alt="hexo_startssl_create_csr"></p><p><img src="https://cdn.oss.link/markdown/hexo_startssl_mac_createcsr.png" srcset="/img/loading.gif" alt="hexo_startssl_mac_createcsr"></p><p><strong>生成好以后将 csr 文件内容复制到文本框即可</strong></p><p><img src="https://cdn.oss.link/markdown/hexo_startssl_copycsr.png" srcset="/img/loading.gif" alt="hexo_startssl_copycsr"></p><h3 id="2-6、下载证书"><a href="#2-6、下载证书" class="headerlink" title="2.6、下载证书"></a>2.6、下载证书</h3><p>输入完成后，待 StartSSL 签名完成便可下载证书</p><p><img src="https://cdn.oss.link/markdown/hexo_download_ssl.png" srcset="/img/loading.gif" alt="hexo_download_ssl"></p><p>文件内包含主流服务器的证书，如下</p><p><img src="https://cdn.oss.link/markdown/hexo_download_ssl_value.png" srcset="/img/loading.gif" alt="hexo_download_ssl"></p><p>到此申请结束，关于其实用教程请自行 Google</p>]]></content>
    
    
    <categories>
      
      <category>Linux</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Linux</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>AspectJ Hello World</title>
    <link href="/2006/01/02/aspectj-hello-world/"/>
    <url>/2006/01/02/aspectj-hello-world/</url>
    
    <content type="html"><![CDATA[<h2 id="一、简介"><a href="#一、简介" class="headerlink" title="一、简介"></a>一、简介</h2><p>AspectJ 是一个强大的面向切面编程框架，Spring 等 AOP 全部兼容该框架；它扩展了Java语言。AspectJ 定义了 AOP 语法所以它有一个专门的编译器用来生成遵守Java字节编码规范的Class文件。</p><h2 id="二、环境准备"><a href="#二、环境准备" class="headerlink" title="二、环境准备"></a>二、环境准备</h2><ul><li>IntelliJ IDEA 2016</li><li>Maven 3.3.9</li><li>AspectJ 1.8.9</li><li>aspectjtools 1.8.9</li></ul><h2 id="三、Hello-World"><a href="#三、Hello-World" class="headerlink" title="三、Hello World"></a>三、Hello World</h2><h3 id="1、首先新建一个-Java-项目"><a href="#1、首先新建一个-Java-项目" class="headerlink" title="1、首先新建一个 Java 项目 :"></a>1、首先新建一个 Java 项目 :</h3><p><img src="https://cdn.oss.link/markdown/hex_aspect_create_project.png" srcset="/img/loading.gif" alt="hex_aspect_create_project"></p><h3 id="2、将其转化为-Maven-项目"><a href="#2、将其转化为-Maven-项目" class="headerlink" title="2、将其转化为 Maven 项目"></a>2、将其转化为 Maven 项目</h3><p><img src="https://cdn.oss.link/markdown/hexo_aspect_add_maven.png" srcset="/img/loading.gif" alt="hexo_aspect_add_maven"></p><h3 id="3、加入相关依赖"><a href="#3、加入相关依赖" class="headerlink" title="3、加入相关依赖"></a>3、加入相关依赖</h3><p><img src="https://cdn.oss.link/markdown/hexo_aspect_maven_dependencies.png" srcset="/img/loading.gif" alt="hexo_aspect_maven_dependencies"></p><p><strong>POM 如下 :</strong></p><pre><code class="hljs xml"><span class="hljs-meta">&lt;?xml version="1.0" encoding="UTF-8"?&gt;</span><span class="hljs-tag">&lt;<span class="hljs-name">project</span> <span class="hljs-attr">xmlns</span>=<span class="hljs-string">"http://maven.apache.org/POM/4.0.0"</span></span><span class="hljs-tag">         <span class="hljs-attr">xmlns:xsi</span>=<span class="hljs-string">"http://www.w3.org/2001/XMLSchema-instance"</span></span><span class="hljs-tag">         <span class="hljs-attr">xsi:schemaLocation</span>=<span class="hljs-string">"http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd"</span>&gt;</span>    <span class="hljs-tag">&lt;<span class="hljs-name">modelVersion</span>&gt;</span>4.0.0<span class="hljs-tag">&lt;/<span class="hljs-name">modelVersion</span>&gt;</span>    <span class="hljs-tag">&lt;<span class="hljs-name">groupId</span>&gt;</span>me.mritd<span class="hljs-tag">&lt;/<span class="hljs-name">groupId</span>&gt;</span>    <span class="hljs-tag">&lt;<span class="hljs-name">artifactId</span>&gt;</span>Test1<span class="hljs-tag">&lt;/<span class="hljs-name">artifactId</span>&gt;</span>    <span class="hljs-tag">&lt;<span class="hljs-name">version</span>&gt;</span>1.0-SNAPSHOT<span class="hljs-tag">&lt;/<span class="hljs-name">version</span>&gt;</span>    <span class="hljs-tag">&lt;<span class="hljs-name">packaging</span>&gt;</span>jar<span class="hljs-tag">&lt;/<span class="hljs-name">packaging</span>&gt;</span>    <span class="hljs-tag">&lt;<span class="hljs-name">dependencies</span>&gt;</span>        <span class="hljs-comment">&lt;!-- https://mvnrepository.com/artifact/org.aspectj/aspectjrt --&gt;</span>        <span class="hljs-tag">&lt;<span class="hljs-name">dependency</span>&gt;</span>            <span class="hljs-tag">&lt;<span class="hljs-name">groupId</span>&gt;</span>org.aspectj<span class="hljs-tag">&lt;/<span class="hljs-name">groupId</span>&gt;</span>            <span class="hljs-tag">&lt;<span class="hljs-name">artifactId</span>&gt;</span>aspectjrt<span class="hljs-tag">&lt;/<span class="hljs-name">artifactId</span>&gt;</span>            <span class="hljs-tag">&lt;<span class="hljs-name">version</span>&gt;</span>1.8.9<span class="hljs-tag">&lt;/<span class="hljs-name">version</span>&gt;</span>        <span class="hljs-tag">&lt;/<span class="hljs-name">dependency</span>&gt;</span>        <span class="hljs-comment">&lt;!-- https://mvnrepository.com/artifact/org.aspectj/aspectjtools --&gt;</span>        <span class="hljs-tag">&lt;<span class="hljs-name">dependency</span>&gt;</span>            <span class="hljs-tag">&lt;<span class="hljs-name">groupId</span>&gt;</span>org.aspectj<span class="hljs-tag">&lt;/<span class="hljs-name">groupId</span>&gt;</span>            <span class="hljs-tag">&lt;<span class="hljs-name">artifactId</span>&gt;</span>aspectjtools<span class="hljs-tag">&lt;/<span class="hljs-name">artifactId</span>&gt;</span>            <span class="hljs-tag">&lt;<span class="hljs-name">version</span>&gt;</span>1.8.9<span class="hljs-tag">&lt;/<span class="hljs-name">version</span>&gt;</span>        <span class="hljs-tag">&lt;/<span class="hljs-name">dependency</span>&gt;</span>    <span class="hljs-tag">&lt;/<span class="hljs-name">dependencies</span>&gt;</span>    <span class="hljs-tag">&lt;<span class="hljs-name">build</span>&gt;</span>        <span class="hljs-tag">&lt;<span class="hljs-name">plugins</span>&gt;</span>            <span class="hljs-comment">&lt;!-- Maven 编译插件 --&gt;</span>            <span class="hljs-tag">&lt;<span class="hljs-name">plugin</span>&gt;</span>                <span class="hljs-tag">&lt;<span class="hljs-name">groupId</span>&gt;</span>org.apache.maven.plugins<span class="hljs-tag">&lt;/<span class="hljs-name">groupId</span>&gt;</span>                <span class="hljs-tag">&lt;<span class="hljs-name">artifactId</span>&gt;</span>maven-compiler-plugin<span class="hljs-tag">&lt;/<span class="hljs-name">artifactId</span>&gt;</span>                <span class="hljs-tag">&lt;<span class="hljs-name">version</span>&gt;</span>3.5.1<span class="hljs-tag">&lt;/<span class="hljs-name">version</span>&gt;</span>                <span class="hljs-tag">&lt;<span class="hljs-name">configuration</span>&gt;</span>                    <span class="hljs-tag">&lt;<span class="hljs-name">source</span>&gt;</span>1.8<span class="hljs-tag">&lt;/<span class="hljs-name">source</span>&gt;</span>                    <span class="hljs-tag">&lt;<span class="hljs-name">target</span>&gt;</span>1.8<span class="hljs-tag">&lt;/<span class="hljs-name">target</span>&gt;</span>                    <span class="hljs-tag">&lt;<span class="hljs-name">encoding</span>&gt;</span>UTF-8<span class="hljs-tag">&lt;/<span class="hljs-name">encoding</span>&gt;</span>                <span class="hljs-tag">&lt;/<span class="hljs-name">configuration</span>&gt;</span>            <span class="hljs-tag">&lt;/<span class="hljs-name">plugin</span>&gt;</span>            <span class="hljs-comment">&lt;!-- AspectJ 编译插件 --&gt;</span>            <span class="hljs-tag">&lt;<span class="hljs-name">plugin</span>&gt;</span>                <span class="hljs-tag">&lt;<span class="hljs-name">groupId</span>&gt;</span>org.codehaus.mojo<span class="hljs-tag">&lt;/<span class="hljs-name">groupId</span>&gt;</span>                <span class="hljs-tag">&lt;<span class="hljs-name">artifactId</span>&gt;</span>aspectj-maven-plugin<span class="hljs-tag">&lt;/<span class="hljs-name">artifactId</span>&gt;</span>                <span class="hljs-tag">&lt;<span class="hljs-name">version</span>&gt;</span>1.8<span class="hljs-tag">&lt;/<span class="hljs-name">version</span>&gt;</span>                <span class="hljs-tag">&lt;<span class="hljs-name">executions</span>&gt;</span>                    <span class="hljs-tag">&lt;<span class="hljs-name">execution</span>&gt;</span>                        <span class="hljs-tag">&lt;<span class="hljs-name">goals</span>&gt;</span>                            <span class="hljs-tag">&lt;<span class="hljs-name">goal</span>&gt;</span>compile<span class="hljs-tag">&lt;/<span class="hljs-name">goal</span>&gt;</span>       <span class="hljs-comment">&lt;!-- use this goal to weave all your main classes --&gt;</span>                            <span class="hljs-tag">&lt;<span class="hljs-name">goal</span>&gt;</span>test-compile<span class="hljs-tag">&lt;/<span class="hljs-name">goal</span>&gt;</span>  <span class="hljs-comment">&lt;!-- use this goal to weave all your test classes --&gt;</span>                        <span class="hljs-tag">&lt;/<span class="hljs-name">goals</span>&gt;</span>                    <span class="hljs-tag">&lt;/<span class="hljs-name">execution</span>&gt;</span>                <span class="hljs-tag">&lt;/<span class="hljs-name">executions</span>&gt;</span>            <span class="hljs-tag">&lt;/<span class="hljs-name">plugin</span>&gt;</span>        <span class="hljs-tag">&lt;/<span class="hljs-name">plugins</span>&gt;</span>    <span class="hljs-tag">&lt;/<span class="hljs-name">build</span>&gt;</span><span class="hljs-tag">&lt;/<span class="hljs-name">project</span>&gt;</span></code></pre><h3 id="4、创建一个普通类"><a href="#4、创建一个普通类" class="headerlink" title="4、创建一个普通类"></a>4、创建一个普通类</h3><pre><code class="hljs java"><span class="hljs-keyword">package</span> me.mritd.test;<span class="hljs-comment">/**</span><span class="hljs-comment"> * Created by mritd on 16/6/19.</span><span class="hljs-comment"> */</span><span class="hljs-keyword">public</span> <span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">Test1</span> </span>&#123;    <span class="hljs-function"><span class="hljs-keyword">public</span> <span class="hljs-keyword">static</span> <span class="hljs-keyword">void</span> <span class="hljs-title">main</span><span class="hljs-params">(String[] args)</span> </span>&#123;        System.out.println(<span class="hljs-string">"Test AspectJ..."</span>);    &#125;&#125;</code></pre><h3 id="5、创建一个切面"><a href="#5、创建一个切面" class="headerlink" title="5、创建一个切面"></a>5、创建一个切面</h3><p><strong>创建时选择 AspectJ 程序 :</strong></p><p><img src="https://cdn.oss.link/markdown/hexo_aspect_newaspect.png" srcset="/img/loading.gif" alt="hexo_aspect_newaspect"></p><pre><code class="hljs java"><span class="hljs-keyword">package</span> me.mritd.testaspect;<span class="hljs-comment">/**</span><span class="hljs-comment"> * Created by mritd on 16/6/19.</span><span class="hljs-comment"> */</span><span class="hljs-keyword">public</span> aspect TestAspect &#123;    <span class="hljs-comment">// 定义 pointcut</span>    <span class="hljs-function">pointcut <span class="hljs-title">TestAspectPointCutBefore</span><span class="hljs-params">()</span> : <span class="hljs-title">execution</span><span class="hljs-params">(* me.mritd.test.Test1.main(..)</span>)</span>;    <span class="hljs-function">pointcut <span class="hljs-title">TestAspectPointCutAfter</span><span class="hljs-params">()</span> : <span class="hljs-title">execution</span><span class="hljs-params">(* me.mritd.test.Test1.main(..)</span>)</span>;    <span class="hljs-comment">// 定义执行动作</span>    before() : TestAspectPointCutBefore()&#123;        System.out.println(<span class="hljs-string">"执行前切入..."</span>);    &#125;    after() : TestAspectPointCutAfter()&#123;        System.out.println(<span class="hljs-string">"执行前切入..."</span>);    &#125;&#125;</code></pre><h3 id="6、调整编译"><a href="#6、调整编译" class="headerlink" title="6、调整编译"></a>6、调整编译</h3><p>由于 AspectJ 需要单独的编译器编译，所以需要设置 Ajc 编译器，Maven 中已经加入了相关编译插件，直接 <code>compile</code> 也可以。</p><h4 id="6-1、设置项目依赖"><a href="#6-1、设置项目依赖" class="headerlink" title="6.1、设置项目依赖"></a>6.1、设置项目依赖</h4><p><img src="https://cdn.oss.link/markdown/hexo_aspect_add_ajc.png" srcset="/img/loading.gif" alt="hexo_aspect_add_ajc"></p><h4 id="6-2、设置编译器"><a href="#6-2、设置编译器" class="headerlink" title="6.2、设置编译器"></a>6.2、设置编译器</h4><p><img src="https://cdn.oss.link/markdown/hexo_aspect_set_ajc.png" srcset="/img/loading.gif" alt="hexo_aspect_set_ajc"></p><h3 id="7、运行测试"><a href="#7、运行测试" class="headerlink" title="7、运行测试"></a>7、运行测试</h3><p><img src="https://cdn.oss.link/markdown/hexo_aspect_runtest_helloworld.png" srcset="/img/loading.gif" alt="hexo_aspect_runtest_helloworld"></p>]]></content>
    
    
    <categories>
      
      <category>Java</category>
      
    </categories>
    
    
    <tags>
      
      <tag>AspectJ</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Can&#39;t connect to MySQL server on &#39;xxxx&#39; (61)</title>
    <link href="/2006/01/02/can-t-connect-to-mysql-server-on-xxxx-61/"/>
    <url>/2006/01/02/can-t-connect-to-mysql-server-on-xxxx-61/</url>
    
    <content type="html"><![CDATA[<p><strong>记录一下 Ubuntu 下安装 MySql 踩得坑；刚刚安装完 MySQL，连接时始终报 <code>Can&#39;t connect to MySQL server on &#39;10.211.55.14&#39; (61)</code>；后来查询此错误的原因就是网络不通，于是检查 ufw 防火墙、本机杀软、MySql远程访问开启情况…发现没问题以后，最终找到了答案：</strong></p><!--more--><p><strong>MySql 默认 监听 127.0.0.1，也就是说只有本地可以连接，需要将其改为 0.0.0.0 即可，更改方法如下：</strong></p><pre><code class="hljs sh">vim /etc/mysql/my.cnf</code></pre><p><strong>找到 bind-address 选项，将后面的 IP 改为 0.0.0.0，然后 执行重启 <code>service mysql restart</code>，截图如下:</strong></p><p><img src="https://cdn.oss.link/markdown/hexo_mysql_modify_listen_port.png" srcset="/img/loading.gif" alt="hexo_mysql_modify_listen_port"></p>]]></content>
    
    
    <categories>
      
      <category>Database</category>
      
    </categories>
    
    
    <tags>
      
      <tag>MySQL</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>CentOS 6.7_x64 minimal 网络初始化配置</title>
    <link href="/2006/01/02/centos-6-7-x64-minimal-%E7%BD%91%E7%BB%9C%E5%88%9D%E5%A7%8B%E5%8C%96%E9%85%8D%E7%BD%AE/"/>
    <url>/2006/01/02/centos-6-7-x64-minimal-%E7%BD%91%E7%BB%9C%E5%88%9D%E5%A7%8B%E5%8C%96%E9%85%8D%E7%BD%AE/</url>
    
    <content type="html"><![CDATA[<p><img src="https://cdn.oss.link/markdown/centos6.png" srcset="/img/loading.gif" alt="centos6"></p><h3 id="CentOS-6-7-x64-网络初始化配置"><a href="#CentOS-6-7-x64-网络初始化配置" class="headerlink" title="CentOS 6.7_x64 网络初始化配置"></a>CentOS 6.7_x64 网络初始化配置</h3><blockquote><p>最近准备折腾折腾Maven，想在本地CentOS下搭个Nexus私服，无奈CentOS LiveCD 版太大，很多功能也没啥用；so 搞了个minimal版本…..结果安装上无法联网，现记录一下初始化的网络配置。</p></blockquote><ul><li>/etc/sysconfig/network-scripts/ifcfg-eth0</li></ul><blockquote><p>这个文件主要是基本的配置文件，详细信息如下</p></blockquote><pre><code class="hljs bash">DEVICE=<span class="hljs-string">"eth0"</span>             <span class="hljs-comment">#启动的网卡</span>MM_Controlled=<span class="hljs-string">"no"</span>        <span class="hljs-comment">#没搞太明白，好像修改是实时生效的意思</span>ONBOOT=<span class="hljs-string">"yes"</span>              <span class="hljs-comment">#启动自动联网</span>BOOTPROTO=<span class="hljs-string">"static"</span>        <span class="hljs-comment">#ip获取方式 三个值 static、dhcp、none</span>IPADDR=<span class="hljs-string">"192.168.1.60"</span>     <span class="hljs-comment">#ip地址</span>IPV6INIT=no               <span class="hljs-comment">#ipv6支持</span>IPV6_AUTOCONF=no          <span class="hljs-comment">#ipv6自动配置</span>NETMASK=255.255.255.0     <span class="hljs-comment">#子网掩码</span>GATEWAY=192.168.1.1       <span class="hljs-comment">#网关 一般指向路由</span>BROADCAST=192.168.1.255   <span class="hljs-comment">#没太懂</span></code></pre><ul><li>/etc/sysconfig/network</li></ul><pre><code class="hljs bash">NETWORKING=yes                        <span class="hljs-comment">#网络是否可用</span>NETWORKING_IPV6=yesHOSTNAME=localhost.localdomain        <span class="hljs-comment">#主机名，主机名在/etc/hosts里面配置</span></code></pre><ul><li>/etc/resolv.conf</li></ul><pre><code class="hljs bash">nameserver 192.168.10.1              <span class="hljs-comment">#DNS服务器对应的IP</span>search localdomain                   <span class="hljs-comment">#搜索要找的域名，在/etc/hosts里面设定的有</span></code></pre><ul><li>最后重启网络</li></ul><pre><code class="hljs bash">service network restart</code></pre>]]></content>
    
    
    <categories>
      
      <category>Linux</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Linux</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>CentOS 上编译安装 Git</title>
    <link href="/2006/01/02/compile-and-install-git-on-centos/"/>
    <url>/2006/01/02/compile-and-install-git-on-centos/</url>
    
    <content type="html"><![CDATA[<p><img src="https://cdn.oss.link/markdown/hexo_git_install_logo.png" srcset="/img/loading.gif" alt="hexo_git_install_logo"></p><h2 id="一、概述"><a href="#一、概述" class="headerlink" title="一、概述"></a>一、概述</h2><blockquote><p>Git是目前世界上最先进的分布式版本控制系统(没有之一)，高端大气上档次的 Git 值得我们去学习，由于CentOS本身的Git版本太低，所以这里记录一下CentOS下编译安装Git的过程。</p></blockquote><!--more--><h2 id="二、安装前准备工作"><a href="#二、安装前准备工作" class="headerlink" title="二、安装前准备工作"></a>二、安装前准备工作</h2><ol><li>下载 git 源码 <a href="https://github.com/git/git/releases" target="_blank" rel="noopener">下载地址</a></li><li>上传到 CentOS (略过)</li><li>解压文件 执行 <code>tar -zxvf git-2.7.0.tar.gz</code></li></ol><h2 id="三、编译安装"><a href="#三、编译安装" class="headerlink" title="三、编译安装"></a>三、编译安装</h2><h3 id="1、安装编译所需的依赖环境"><a href="#1、安装编译所需的依赖环境" class="headerlink" title="1、安装编译所需的依赖环境"></a>1、安装编译所需的依赖环境</h3><pre><code class="hljs sh">yum -y install curl-devel expat-devel gettext-devel openssl-devel zlib-devel perl-ExtUtils-MakeMaker</code></pre><h3 id="2、配置安装目录"><a href="#2、配置安装目录" class="headerlink" title="2、配置安装目录"></a>2、配置安装目录</h3><blockquote><p>git 默认编译安装会安装到当前用户目录下，所以需要配置安装目录</p></blockquote><pre><code class="hljs sh"><span class="hljs-built_in">cd</span> git-2.7.0autoconf./configure --prefix=/usr/<span class="hljs-built_in">local</span></code></pre><h3 id="3、编译并安装"><a href="#3、编译并安装" class="headerlink" title="3、编译并安装"></a>3、编译并安装</h3><pre><code class="hljs sh">make &amp;&amp; make install</code></pre>]]></content>
    
    
    <categories>
      
      <category>Linux</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Linux</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>DBA视频笔记</title>
    <link href="/2006/01/02/dba%E8%A7%86%E9%A2%91%E7%AC%94%E8%AE%B0/"/>
    <url>/2006/01/02/dba%E8%A7%86%E9%A2%91%E7%AC%94%E8%AE%B0/</url>
    
    <content type="html"><![CDATA[<p><img src="https://cdn.oss.link/markdown/hexo_oracledatabase_log.png" srcset="/img/loading.gif" alt="Oracle Logo"></p><blockquote><p>数据库是个硬伤，最近在X宝(真特么是个神奇的地方)搞了点OCP、OCA、OCM的视频，看点记点……不定时更新……</p></blockquote><h2 id="SQL部分"><a href="#SQL部分" class="headerlink" title="SQL部分"></a>SQL部分</h2><h3 id="SQL分类"><a href="#SQL分类" class="headerlink" title="SQL分类"></a>SQL分类</h3><ol><li>DML 数据操纵语言<br>SELECT、INSERT、DELETE、UPDATE、MERGE(11G)<br>SELECT 对表数据进行读操作<br>INSERT、DELETE、UPDATE 对表数据进行读操作 会引起UNDO，UBDO会引起REDO</li><li>DDL 数据定义语言<br>CREATE、ALTER、DROP、TRUNCAT、RENAME、COMMON (会更新数据字典)</li><li>TCL 事务操作语言<br>COMMIT、ROLLBAK、SAVEPOINT</li><li>DCL 权限操作语言<br>GRANT、REVOKE</li></ol><h3 id="基本SQL"><a href="#基本SQL" class="headerlink" title="基本SQL"></a>基本SQL</h3><ol><li><p>数据类型</p><ul><li><p>字符型<br>CHAR                    固定字符，最长2000 (建表后无数据可改变，有数据只能涨)<br>VARCHAR2                可变长度，最长4000，最小1<br>NCHAR/NVARCHAR2         NCAHR/NVARCHAR2类型的列使用国家字符集(<strong>字母汉字一视同仁，都视为同一长度</strong>)<br>RAW/LANG RAW            固定/可变长二进制数据长度，最大2G (老类型，逐步淘汰)<br>LONG                    可变长字符数据，最大2G，具有VARCHAR2特性，<strong>一个表最多1列</strong> (老类型，逐步淘汰)</p></li><li><p>数值型<br>NUMBER(P,S)             实数类型，以可变长内部类型存储数据，内部格式精度高达38位<br>INT                     NUMBER子类型，范围同上</p></li><li><p>日期型<br>DATE                    日期普通格式，精度只能到秒<br>TIMESTAMP               日期扩展格式，精度可达秒后小数点9位(十亿分之一秒)<br>TIMESTAMP WHITH TIMEZONE          带时区<br>TIMESTAMP WITH LOCAL TIMEZONE     时区转换成本地日期</p></li><li><p>LOB型(大对象)<br>10G引入，11G重新定义，在字段中存储大数据，所有大对象最大4G<br>CLOB                    用于存储单字节字符数据，包含在数据库内(查询必须使用 like)<br>NCLOB                   用于存储多字节字符数据<br>BLOB                    用于存储二进制数据，包含在数据库内<br>BFILE                   存储在数据库之外的二进制文件中，文件中数据只能只读访问</p><blockquote><p>CLOB、NCLOB、BLOB都是内部LOB类型，没有LONG每个表只能有一列的限制；保存图片或者电影BLOB最好，小说等文本使用VLOB最好；LONG、RAW即将废弃，但并未完全废弃，11G重要视图dba_views，对于text(视图定义)仍沿用LONG类型</p><p>Oracle 11g 重新设计了大对象，提出Secure Lobs的概念，相关参数是 <code>db_securefile</code>，采用Secure File的前提是11g以上版本，ASSM管理等；符合这些条件的BasicFile Lobs也可以转换成 SecureFile Lobs，<strong>较之前的BasicFile Lobs，SecureFile Lobs有极大改进：压缩、去重、加密</strong></p><p><strong>当 <code>CREATE TABLE</code> 定义LOB列时，往往用到 <code>LOB_storage_clause</code>；之后对LOB的操作往往是ORACLE提供的 <code>DBMS_LOB</code> 包，通过编写 PL/SQL 块来对LOB数据进行管理</strong></p></blockquote></li></ul></li><li><p>数据类型转换</p><ul><li>隐性类型转换<br>是指ORACLE自动完成的类型转换；自一些带有明显意图的字面值上，可由ORACLE自主判断进行数制转换，如<pre><code class="hljs sql"><span class="hljs-comment">-- ORACLE 自主将字符型'7788'转换为数值型7788进行比较</span><span class="hljs-keyword">SELECT</span> EMPNO <span class="hljs-keyword">FROM</span> EMP <span class="hljs-keyword">WHERE</span> EMPNO =<span class="hljs-string">'7788'</span>;<span class="hljs-comment">-- 将日期型转换为字符型进行长度判断</span><span class="hljs-keyword">SELECT</span> <span class="hljs-keyword">LENGTH</span>(<span class="hljs-keyword">SYSDATE</span>) <span class="hljs-keyword">FROM</span> DUAL;<span class="hljs-comment">-- 将字符型 '1028' 转换成数值型进行计算</span><span class="hljs-keyword">SELECT</span> <span class="hljs-string">'1028'</span>+<span class="hljs-number">123</span> <span class="hljs-keyword">FROM</span> DUAL;<span class="hljs-comment">-- 将数值型123转换为字符型进行拼接</span><span class="hljs-keyword">SELECT</span> <span class="hljs-string">'1028'</span>||<span class="hljs-number">123</span> <span class="hljs-keyword">FROM</span> DUAL;</code></pre></li><li>显性类型转换<br>TO_CHAR、TO_DATE、TO_NUMBER<br><img src="https://cdn.oss.link/markdown/hexo_ORACLE_datacoversion.png" srcset="/img/loading.gif" alt="转换图"></li></ul></li><li><p>单行函数</p><pre><code class="hljs sql">  <span class="hljs-comment">--字符函数</span>  lower('SQL')                    <span class="hljs-comment">--返回小写</span>  upper('sql')                    <span class="hljs-comment">--返回大写</span>  initcap('sql')                  <span class="hljs-comment">--首字母大写</span>  concat('sql','test')            <span class="hljs-comment">--字符串拼接</span>  substr('String',1,3)            <span class="hljs-comment">--字符串截取</span>  instr('sqlsq<span class="hljs-comment">#l#sql','#',3)      --从第3个位置查找'#'字符绝对位置</span>  length('String')                <span class="hljs-comment">--返回字符串长度</span>  lpad('test'10,'<span class="hljs-comment">#')              --左填充</span>  rpad('test',10,'*')             --右填充  <span class="hljs-keyword">replace</span>(<span class="hljs-string">'11111q111'</span>,<span class="hljs-string">'q'</span>,<span class="hljs-string">'0'</span>)    <span class="hljs-comment">--字符串替换</span>  <span class="hljs-keyword">trim</span>(<span class="hljs-string">'  1  '</span>)                   <span class="hljs-comment">--首尾去空格</span>  <span class="hljs-comment">--数值函数</span>  <span class="hljs-keyword">round</span>(p,s)                <span class="hljs-comment">--对指定值做四舍五入，s为正数时表示小数点后保留位数，</span>                              <span class="hljs-comment">--也可位负数，意义不大；可用于按照指定精度对十进制数做四舍五入</span>  <span class="hljs-keyword">round</span>(<span class="hljs-number">45.923</span>,<span class="hljs-number">1</span>)                 <span class="hljs-comment">--结果45.9</span>  <span class="hljs-keyword">round</span>(<span class="hljs-number">45.923</span>,<span class="hljs-number">0</span>)                 <span class="hljs-comment">--结果46</span>  <span class="hljs-keyword">round</span>(<span class="hljs-number">45.923</span>,<span class="hljs-number">-1</span>)                <span class="hljs-comment">--结果50</span>  trunc(p,s)                      <span class="hljs-comment">--对指定数值进行取整，可按精度截断指定十进制数</span>  trun(<span class="hljs-number">45.923</span>,<span class="hljs-number">0</span>)                  <span class="hljs-comment">--结果45</span>  <span class="hljs-keyword">mod</span>(<span class="hljs-number">100</span>,<span class="hljs-number">12</span>)                     <span class="hljs-comment">--取余</span>  <span class="hljs-comment">--日期函数</span>  <span class="hljs-comment">--日期数值在ORALCE中按照数字存储，所以可以进行加减运算，计算时以天为单位，缺省格式 DD-MON-RR</span>  <span class="hljs-keyword">SELECT</span> <span class="hljs-keyword">SYSDATE</span>+<span class="hljs-number">2</span> <span class="hljs-keyword">FROM</span> DUAL;                                     <span class="hljs-comment">--当前日期+2天</span>  <span class="hljs-keyword">SELECT</span> <span class="hljs-keyword">SYSDATE</span>+<span class="hljs-number">2</span>/<span class="hljs-number">24</span> <span class="hljs-keyword">FROM</span> DUAL;                                  <span class="hljs-comment">--当前日期+2小时</span>  TO_DATE('2016-01-01 21:40:30','YYYY-MM-DD HH24:MI:SS')          <span class="hljs-comment">--字符串转日期</span>  TO_CHAR(SYSDATE,'YYYY-MM-DD HH24:MI:SS')                        <span class="hljs-comment">--日期转字符串</span>  MONTHS_BETWEEN                                                  <span class="hljs-comment">--计算两个日期之间相差月数</span>  ADD_MONTHS                                                      <span class="hljs-comment">--给指定日期加月份</span>  LAST_DAY                                                        <span class="hljs-comment">--指定日期月份最后一天</span>  NEXT_DAY                                                        <span class="hljs-comment">--指定日期下一天 第二个参数为1~7，表示周日~周六</span>                                                                <span class="hljs-comment">--NEXT_DAY(SYSDATE,7)  表示下一个星期六</span>  <span class="hljs-comment">--对于日期可使用 四舍五入、取整操作 s是'MONTH'按30天计算15舍16进，s是'YEAR'5舍7如</span>ROUND(p,s)、TRUNC(p,s)       <span class="hljs-comment">--其它函数(表达式)</span>  DECODE(COLUMN,KEY1,VALUE1，KEY2，VALUE2...DEFAULT)  CASE COLUMN WHEN XXXX THEN XXXX    <span class="hljs-comment">--等值判断相当于 DECODE</span>              WHEN XXXX THEN XXXX              ELSE XXXX <span class="hljs-keyword">END</span>  <span class="hljs-keyword">CASE</span> <span class="hljs-keyword">WHEN</span> 条件 <span class="hljs-keyword">THEN</span> XXXX           <span class="hljs-comment">--条件判断</span>       <span class="hljs-keyword">WHEN</span> 条件 <span class="hljs-keyword">THEN</span> XXXX       <span class="hljs-keyword">ELSE</span> XXXX <span class="hljs-keyword">END</span>  <span class="hljs-keyword">DISTINCT</span>                           <span class="hljs-comment">--去重</span>  <span class="hljs-keyword">CHR</span>()                              <span class="hljs-comment">--字符转ASCII</span>  <span class="hljs-keyword">ASCII</span>()                            <span class="hljs-comment">--ASCII转字符</span>  SYS_CONTEXT                        <span class="hljs-comment">--获取上下文环境函数</span></code></pre><p><strong>SYS_CONTEXT函数参考</strong> <strong><a href="http://alany.blog.51cto.com/6125308/1418163" target="_blank" rel="noopener">其他参考</a></strong></p><pre><code class="hljs sql"><span class="hljs-keyword">select</span> sys_context(<span class="hljs-string">'USERENV'</span>,<span class="hljs-string">'AUTHENTICATION_TYPE'</span>) <span class="hljs-keyword">from</span> dual;<span class="hljs-comment">--用户的认证类型</span><span class="hljs-keyword">select</span> sys_context(<span class="hljs-string">'USERENV'</span>,<span class="hljs-string">'AUTHENTICATION_DATA'</span>) <span class="hljs-keyword">from</span> dual;<span class="hljs-comment">--未知</span><span class="hljs-keyword">select</span> sys_context(<span class="hljs-string">'USERENV'</span>,<span class="hljs-string">'BG_JOB_ID'</span>) <span class="hljs-keyword">from</span> dual;<span class="hljs-comment">--当前指定id的会话是否为oracle后台程序建立，不是则返回null</span><span class="hljs-keyword">select</span> sys_context(<span class="hljs-string">'USERENV'</span>,<span class="hljs-string">'CLIENT_INFO'</span>) <span class="hljs-keyword">from</span> dual;<span class="hljs-comment">--通过dbms_application_info包可以存储高达64字节的用户会话信息</span><span class="hljs-keyword">select</span> sys_context(<span class="hljs-string">'USERENV'</span>,<span class="hljs-string">'CURRENT_SCHEMA'</span>) <span class="hljs-keyword">from</span> dual;<span class="hljs-comment">--默认的schema将被当做当前的schema。当在当前会话中使用ALTER SESSION SET CURRENT_SCHEMA语句的时候，它的查询返回值将被改变</span><span class="hljs-keyword">select</span> sys_context(<span class="hljs-string">'USERENV'</span>,<span class="hljs-string">'CURRENT_SCHEMAID'</span>) <span class="hljs-keyword">from</span> dual;<span class="hljs-comment">--当前schema的id</span><span class="hljs-keyword">select</span> sys_context(<span class="hljs-string">'USERENV'</span>,<span class="hljs-string">'CURRENT_USER'</span>) <span class="hljs-keyword">from</span> dual;<span class="hljs-comment">--当前的登陆用户</span><span class="hljs-keyword">select</span> <span class="hljs-keyword">REPLACE</span>(<span class="hljs-keyword">SUBSTR</span>(sys_context(<span class="hljs-string">'USERENV'</span>,<span class="hljs-string">'HOST'</span>),<span class="hljs-number">1</span>,<span class="hljs-number">30</span>),<span class="hljs-string">'\'</span>,<span class="hljs-string">':'</span>) <span class="hljs-keyword">from</span> dual;'<span class="hljs-comment">--当前会话主机操作系统名</span><span class="hljs-keyword">select</span> sys_context(<span class="hljs-string">'USERENV'</span>,<span class="hljs-string">'CURRENT_USERID'</span>) <span class="hljs-keyword">from</span> dual;<span class="hljs-comment">--当前登陆的用户的id</span><span class="hljs-keyword">select</span> sys_context(<span class="hljs-string">'USERENV'</span>,<span class="hljs-string">'DB_DOMAIN'</span>) <span class="hljs-keyword">from</span> dual;<span class="hljs-comment">--为数据库的域指定初始化参数</span><span class="hljs-keyword">select</span> sys_context(<span class="hljs-string">'USERENV'</span>,<span class="hljs-string">'DB_NAME'</span>) <span class="hljs-keyword">from</span> dual;<span class="hljs-comment">--数据库实例名</span><span class="hljs-keyword">select</span> sys_context(<span class="hljs-string">'USERENV'</span>,<span class="hljs-string">'ENTRYID'</span>) <span class="hljs-keyword">from</span> dual;<span class="hljs-comment">--可用的审计标示符。不能再分布式sql语句中使用此选项。使用USERENV关键字必须置AUDIT_TRAIL的初始化参数为真。</span><span class="hljs-keyword">select</span> sys_context(<span class="hljs-string">'USERENV'</span>,<span class="hljs-string">'EXTERNAL_NAME'</span>) <span class="hljs-keyword">from</span> dual;<span class="hljs-comment">--数据库用户的扩展名</span><span class="hljs-keyword">select</span> sys_context(<span class="hljs-string">'USERENV'</span>,<span class="hljs-string">'FG_JOB_ID'</span>) <span class="hljs-keyword">from</span> dual;<span class="hljs-comment">--返回作业id当此会话是客户端进程创建。否则，返回null</span><span class="hljs-keyword">select</span> sys_context(<span class="hljs-string">'USERENV'</span>,<span class="hljs-string">'INSTANCE'</span>) <span class="hljs-keyword">from</span> dual;<span class="hljs-comment">--当前数据库实例的标示id</span><span class="hljs-keyword">select</span> sys_context(<span class="hljs-string">'USERENV'</span>,<span class="hljs-string">'ISDBA'</span>) <span class="hljs-keyword">from</span> dual;<span class="hljs-comment">--当前用户是否是以dba身份登录</span><span class="hljs-keyword">select</span> sys_context(<span class="hljs-string">'USERENV'</span>,<span class="hljs-string">'LANG'</span>) <span class="hljs-keyword">from</span> dual;<span class="hljs-comment">--iso对‘LANGUAGE’的简称，查询的参数比“LANGUAGE”短</span><span class="hljs-keyword">select</span> sys_context(<span class="hljs-string">'USERENV'</span>,<span class="hljs-string">'LANGUAGE'</span>) <span class="hljs-keyword">from</span> dual;<span class="hljs-comment">--结果为当前数据库使用的存储语言，跟上面查询意义一样</span><span class="hljs-keyword">select</span> sys_context(<span class="hljs-string">'USERENV'</span>,<span class="hljs-string">'NETWORK_PROTOCOL'</span>) <span class="hljs-keyword">from</span> dual;<span class="hljs-comment">--用于通信的网络协议</span><span class="hljs-keyword">select</span> sys_context(<span class="hljs-string">'USERENV'</span>,<span class="hljs-string">'NLS_CALENDAR'</span>) <span class="hljs-keyword">from</span> dual;<span class="hljs-comment">--当前会话使用的，格林尼治时间</span><span class="hljs-keyword">select</span> sys_context(<span class="hljs-string">'USERENV'</span>,<span class="hljs-string">'NLS_CURRENCY'</span>) <span class="hljs-keyword">from</span> dual;<span class="hljs-comment">--本地化的货币符，如人民币为￥，美元符为$</span><span class="hljs-keyword">select</span> sys_context(<span class="hljs-string">'USERENV'</span>,<span class="hljs-string">'NLS_DATE_FORMAT'</span>) <span class="hljs-keyword">from</span> dual;<span class="hljs-comment">--当前使用的日期格式，一般中国为dd-mon-rr</span><span class="hljs-keyword">select</span> sys_context(<span class="hljs-string">'USERENV'</span>,<span class="hljs-string">'NLS_DATE_LANGUAGE'</span>) <span class="hljs-keyword">from</span> dual;<span class="hljs-comment">--表示日期的语言，如中文简体SIMPLIFIED CHINESE</span><span class="hljs-keyword">select</span> sys_context(<span class="hljs-string">'USERENV'</span>,<span class="hljs-string">'NLS_TERRITORY'</span>) <span class="hljs-keyword">from</span> dual;<span class="hljs-comment">--数据库服务器所在区域，如中国CHINA</span><span class="hljs-keyword">select</span> sys_context(<span class="hljs-string">'USERENV'</span>,<span class="hljs-string">'OS_USER'</span>) <span class="hljs-keyword">from</span> dual;<span class="hljs-comment">--操作系统的用户名</span><span class="hljs-keyword">select</span> sys_context(<span class="hljs-string">'USERENV'</span>,<span class="hljs-string">'PROXY_USER'</span>) <span class="hljs-keyword">from</span> dual;<span class="hljs-comment">--是否使用代理用户。否返回null</span><span class="hljs-keyword">select</span> sys_context(<span class="hljs-string">'USERENV'</span>,<span class="hljs-string">'PROXY_USERID'</span>) <span class="hljs-keyword">from</span> dual;<span class="hljs-comment">--代理用户id</span><span class="hljs-keyword">select</span> sys_context(<span class="hljs-string">'USERENV'</span>,<span class="hljs-string">'SESSION_USER'</span>) <span class="hljs-keyword">from</span> dual;<span class="hljs-comment">--当前认证的数据库用户名</span><span class="hljs-keyword">select</span> sys_context(<span class="hljs-string">'USERENV'</span>,<span class="hljs-string">'SESSION_USERID'</span>) <span class="hljs-keyword">from</span> dual;<span class="hljs-comment">--当前认证的数据库用户名id</span><span class="hljs-keyword">select</span> sys_context(<span class="hljs-string">'USERENV'</span>,<span class="hljs-string">'SESSIONID'</span>) <span class="hljs-keyword">from</span> dual;<span class="hljs-comment">--当前会话id</span><span class="hljs-keyword">select</span> sys_context(<span class="hljs-string">'USERENV'</span>,<span class="hljs-string">'TERMINAL'</span>) <span class="hljs-keyword">from</span> dual;<span class="hljs-comment">--操作系统用户组</span><span class="hljs-keyword">select</span> sys_context(<span class="hljs-string">'USERENV'</span>,<span class="hljs-string">'IP_ADDRESS'</span>) <span class="hljs-keyword">from</span> dual;<span class="hljs-comment">--当前会话主机ip</span><span class="hljs-keyword">select</span> sys_context(<span class="hljs-string">'USERENV'</span>,<span class="hljs-string">'HOST'</span>) <span class="hljs-keyword">from</span> dual;<span class="hljs-comment">--当前会话主机操作系统名</span></code></pre></li><li><p>多行函数</p></li></ol>]]></content>
    
    
    <categories>
      
      <category>Database</category>
      
    </categories>
    
    
    <tags>
      
      <tag>SQL</tag>
      
      <tag>Oracle</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Docker Contianer 挂掉强制修改文件</title>
    <link href="/2006/01/02/docker-contianer-%E6%8C%82%E6%8E%89%E5%BC%BA%E5%88%B6%E4%BF%AE%E6%94%B9%E6%96%87%E4%BB%B6/"/>
    <url>/2006/01/02/docker-contianer-%E6%8C%82%E6%8E%89%E5%BC%BA%E5%88%B6%E4%BF%AE%E6%94%B9%E6%96%87%E4%BB%B6/</url>
    
    <content type="html"><![CDATA[<h2 id="一、扯犊子"><a href="#一、扯犊子" class="headerlink" title="一、扯犊子"></a>一、扯犊子</h2><blockquote><p>起因是为了装逼，想申请一个 StartSSL 证书给博客装下逼…… 目前博客全部采用 Dokcer 部署，前端一个 Nginx 的 Contianer 做反代，后端起了静态博客、WP老博客什么的一些东西；手残党开始肆无忌惮的改前端 Nginx Contianer 中的配置；发现 <code>service nginx reload</code> 没效果，这特么哪行，果断 <code>docker restart nginx</code>…..后面你懂的，<code>docker logs nginx</code> 发现配置报错了……所以就有了下文……</p></blockquote><h2 id="二、Dokcer-Contianer-Images-存储位置"><a href="#二、Dokcer-Contianer-Images-存储位置" class="headerlink" title="二、Dokcer Contianer/Images 存储位置"></a>二、Dokcer Contianer/Images 存储位置</h2><h3 id="2-1、images-存储-aufs-摘自-DockerOne"><a href="#2-1、images-存储-aufs-摘自-DockerOne" class="headerlink" title="2.1、images 存储(aufs) 摘自 DockerOne"></a>2.1、images 存储(aufs) 摘自 <a href="http://dockone.io/question/70" target="_blank" rel="noopener">DockerOne</a></h3><ul><li><code>/var/lib/graph/&lt;image id&gt;</code> 下面没有layer目录，只有每个镜像的json描述文件和layersize大小</li><li><code>/var/lib/docker/repositories-aufs</code> TagStore的存储地方，里面有image id与reponame ，tag之间的映射关系. aufs是driver名</li><li><code>/var/lib/docker/aufs/diff/&lt;image id or container id&gt;</code> 每层layer与其父layer之间的文件差异，有的为空，有的有一些文件(镜像实际存储的地方)</li><li><code>/var/lib/docker/aufs/layers/&lt;image id or container id&gt;</code> 每层layer一个文件，记录其父layer一直到根layer之间的ID，每个ID一行。大部分文件的最后一行都一样，表示继承自同一个layer.</li><li><code>/var/lib/docker/aufs/mnt/&lt;image id or container id&gt;</code> 有容器运行时里面有数据(容器数据实际存储的地方,包含整个文件系统数据)，退出时里面为空</li></ul><h3 id="2-2、Contianer-文件修改"><a href="#2-2、Contianer-文件修改" class="headerlink" title="2.2、Contianer 文件修改"></a>2.2、Contianer 文件修改</h3><p>所有 Contianer 文件存放于 <code>/var/lib/docker/aufs/mnt</code> 目录，改废掉的文件名为 <code>hexo.conf</code>，so  直接开搜 :</p><p><img src="https://cdn.oss.link/markdown/hexo_docker_modify_contianerfile.png" srcset="/img/loading.gif" alt="hexo_docker_modify_contianerfile"></p><p>然后直接修改保存即可，此时 Contianer 可恢复启动，此坑完结；目测企业级应用不会出现这种坑爹情况，一般都是编排工具……像我这么作死的应该没有</p><p><strong>做个安静的美男(dou)子(bi)</strong></p>]]></content>
    
    
    <categories>
      
      <category>Docker</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Linux</tag>
      
      <tag>Docker</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Docker Registry Mirror</title>
    <link href="/2006/01/02/docker-mirror-registry/"/>
    <url>/2006/01/02/docker-mirror-registry/</url>
    
    <content type="html"><![CDATA[<h2 id="一、扯淡"><a href="#一、扯淡" class="headerlink" title="一、扯淡"></a>一、扯淡</h2><p>撸 Dokcer 这么长时间以来，由于国内众所周知的网络原因，Docker pull 镜像会非常慢，慢到你怀疑这个世界，甚至怀疑你来到这个世界的正确性与合理性，为了为了让自己不怀疑世界，记录一下如何撸一个 docker mirror registry</p><h2 id="二、动手撸一个"><a href="#二、动手撸一个" class="headerlink" title="二、动手撸一个"></a>二、动手撸一个</h2><h3 id="2-1、基本环境"><a href="#2-1、基本环境" class="headerlink" title="2.1、基本环境"></a>2.1、基本环境</h3><p>以下操作基本环境如下</p><ul><li>Docker 1.12.1</li><li>registry 2.5.1</li><li>nginx 1.10.1</li></ul><h3 id="2-2、导出-registry-配置"><a href="#2-2、导出-registry-配置" class="headerlink" title="2.2、导出 registry 配置"></a>2.2、导出 registry 配置</h3><p>Docker 官方提供了一个 registry，Github 地址 <a href="https://github.com/docker/distribution" target="_blank" rel="noopener">点这里</a>，而大部分能找到的资料都是如何撸一个 private registry，就是启动一下这个官方 registry container 即可，然后就可以 docker push 什么的；而 mirror registry 其实也很简单，就是增加一个配置即可</p><p><strong>首先把官方 registry 中的配置文件导出</strong></p><pre><code class="hljs sh">docker run -it --rm --entrypoint cat registry:2.5.1 \/etc/docker/registry/config.yml &gt; config.yml</code></pre><p><strong>registry 的配置文件内容如下</strong></p><pre><code class="hljs yml"><span class="hljs-attr">version:</span> <span class="hljs-number">0.1</span><span class="hljs-attr">log:</span>  <span class="hljs-attr">fields:</span>    <span class="hljs-attr">service:</span> <span class="hljs-string">registry</span><span class="hljs-attr">storage:</span>  <span class="hljs-attr">cache:</span>    <span class="hljs-attr">blobdescriptor:</span> <span class="hljs-string">inmemory</span>  <span class="hljs-attr">filesystem:</span>    <span class="hljs-attr">rootdirectory:</span> <span class="hljs-string">/var/lib/registry</span><span class="hljs-attr">http:</span>  <span class="hljs-attr">addr:</span> <span class="hljs-string">:5000</span>  <span class="hljs-attr">headers:</span>    <span class="hljs-attr">X-Content-Type-Options:</span> <span class="hljs-string">[nosniff]</span><span class="hljs-attr">health:</span>  <span class="hljs-attr">storagedriver:</span>    <span class="hljs-attr">enabled:</span> <span class="hljs-literal">true</span>    <span class="hljs-attr">interval:</span> <span class="hljs-string">10s</span>    <span class="hljs-attr">threshold:</span> <span class="hljs-number">3</span></code></pre><h3 id="2-3、修改-registry-配置"><a href="#2-3、修改-registry-配置" class="headerlink" title="2.3、修改 registry 配置"></a>2.3、修改 registry 配置</h3><p><strong>上一步已经将配置导出了，接下来如果想使用 mirror 功能只需在下面增加 proxy 选项即可</strong></p><pre><code class="hljs yml"><span class="hljs-attr">version:</span> <span class="hljs-number">0.1</span><span class="hljs-attr">log:</span>  <span class="hljs-attr">fields:</span>    <span class="hljs-attr">service:</span> <span class="hljs-string">registry</span><span class="hljs-attr">storage:</span>  <span class="hljs-attr">cache:</span>    <span class="hljs-attr">blobdescriptor:</span> <span class="hljs-string">inmemory</span>  <span class="hljs-attr">filesystem:</span>    <span class="hljs-attr">rootdirectory:</span> <span class="hljs-string">/var/lib/registry</span><span class="hljs-attr">http:</span>  <span class="hljs-attr">addr:</span> <span class="hljs-string">:5000</span>  <span class="hljs-attr">headers:</span>    <span class="hljs-attr">X-Content-Type-Options:</span> <span class="hljs-string">[nosniff]</span><span class="hljs-attr">health:</span>  <span class="hljs-attr">storagedriver:</span>    <span class="hljs-attr">enabled:</span> <span class="hljs-literal">true</span>    <span class="hljs-attr">interval:</span> <span class="hljs-string">10s</span>    <span class="hljs-attr">threshold:</span> <span class="hljs-number">3</span><span class="hljs-attr">proxy:</span>  <span class="hljs-attr">remoteurl:</span> <span class="hljs-string">https://registry-1.docker.io</span>  <span class="hljs-attr">username:</span> <span class="hljs-string">[username]</span>  <span class="hljs-attr">password:</span> <span class="hljs-string">[password]</span></code></pre><p><strong>username 与 password 是可选项，当填写 username 与 password 以后就可以从 hub pull 私有镜像</strong></p><h3 id="2-4、启动-mirror-registry"><a href="#2-4、启动-mirror-registry" class="headerlink" title="2.4、启动 mirror registry"></a>2.4、启动 mirror registry</h3><p><strong>最后只需要在启动 registry 时候将配置塞回去即可</strong></p><pre><code class="hljs sh">docker run -dt --name v2-mirror \-v /data/registry:/var/lib/registry \-v /etc/registry/config.yml:/etc/docker/registry/config.yml \-p 5000:5000 registry:2.5.1</code></pre><p><strong>以上命令将启动一个 mirror registry，并且数据持久化到 <code>/data/registry</code></strong></p><h3 id="2-5、nginx-配置-ssl"><a href="#2-5、nginx-配置-ssl" class="headerlink" title="2.5、nginx 配置 ssl"></a>2.5、nginx 配置 ssl</h3><p>当然此时直接在 docker 启动参数总增加 <code>--registry-mirror=http://IP:5000</code>，然后重启 docker 再进行 pull 即可生效，但是 5000 端口外加 http 总有点那么不装逼，所以最好增加一个 nginx 做反向代理，同时可以使用 ssl 加密，以下是一个 nginx 配置仅供参考，ssl 证书可采用 <a href="https://www.startssl.com" target="_blank" rel="noopener">StartSSL</a> 免费一年的 DV 证书</p><p><strong>nginx.conf</strong></p><pre><code class="hljs sh"><span class="hljs-comment"># worker 运行用户组</span>user  nginx nginx;<span class="hljs-comment"># worker 进程自动</span>worker_processes  auto;<span class="hljs-comment"># 单个 worker 进程能打开的最大文件描述符数量</span>worker_rlimit_nofile  51200;<span class="hljs-comment"># 指定每个用户能够发往 worker 的信号数量(rtsig)</span><span class="hljs-comment"># worker_rlimit_sigpending</span><span class="hljs-comment"># CPU 亲源性，用于将具体的 worker 绑定到 CPU 核心</span><span class="hljs-comment"># worker_cpu_affinity 0001 0010 0100 1000</span><span class="hljs-comment"># 指定 worker 优先级</span><span class="hljs-comment"># worker_priority</span><span class="hljs-comment"># 是否以守护进程方式启动 nginx</span><span class="hljs-comment"># daemon off|on</span><span class="hljs-comment"># 是否以 master/worker 方式运行</span><span class="hljs-comment"># master_process</span><span class="hljs-comment"># 错误日志文件及级别</span>error_log  /var/<span class="hljs-built_in">log</span>/nginx/error.log  info;pid        /var/run/nginx/nginx.pid;events &#123;    <span class="hljs-comment"># 每个 worker 进程所能响应的最大并发连接数</span>    worker_connections  51200;    <span class="hljs-comment"># 指明使用的事件模型 一般自动选择</span>    <span class="hljs-comment"># use [epoll|rgsig|select|poll]</span>    use epoll;    <span class="hljs-comment"># 定义内部各请求调用 worker 时使用的 负载均衡锁</span>    <span class="hljs-comment"># on: 能够让多个 worker 轮流的序列化的响应新请求，会有一定性能损失</span>    <span class="hljs-comment"># off:</span>    <span class="hljs-comment"># accept_mutex [on|off]</span>    <span class="hljs-comment"># 定义锁文件位置</span>    <span class="hljs-comment"># lock_file /PATH/TO/LOCK_FILE</span>    multi_accept on;&#125;http &#123;    include       mime.types;    default_type  application/octet-stream;    log_format    main  <span class="hljs-string">'$server_name $remote_addr - $remote_user [$time_local] "$request" - $request_body '</span>                        <span class="hljs-string">'$status $body_bytes_sent "$http_referer" '</span>                        <span class="hljs-string">'"$http_user_agent" "$http_x_forwarded_for" '</span>                        <span class="hljs-string">'$ssl_protocol $ssl_cipher $request_time '</span>;    server_names_hash_bucket_size 128;    client_header_buffer_size 32k;    large_client_header_buffers 4 32k;    client_max_body_size 1024m;    sendfile on;    tcp_nopush on;    keepalive_timeout 120;    server_tokens off;    tcp_nodelay on;    gzip  on;    gzip_buffers 16 8k;    gzip_comp_level 6;    gzip_http_version 1.1;    gzip_min_length 256;    gzip_proxied any;    gzip_vary on;    gzip_types        text/xml application/xml application/atom+xml application/rss+xml application/xhtml+xml image/svg+xml        text/javascript application/javascript application/x-javascript        text/x-json application/json application/x-web-app-manifest+json        text/css text/plain text/x-component        font/opentype application/x-font-ttf application/vnd.ms-fontobject        image/x-icon;    gzip_disable <span class="hljs-string">"MSIE [1-6]\.(?!.*SV1)"</span>;    open_file_cache max=1000 inactive=20s;    open_file_cache_valid 30s;    open_file_cache_min_uses 2;    open_file_cache_errors on;    include /etc/nginx/conf.d/*.conf;&#125;</code></pre><p><strong>mirror.conf</strong></p><pre><code class="hljs sh">server &#123;  listen 80;  server_name your-domain;  rewrite ^(.*) https://<span class="hljs-variable">$server_name</span><span class="hljs-variable">$1</span> permanent;&#125;server &#123;  listen 443;  server_name your-domain;  access_log /var/<span class="hljs-built_in">log</span>/nginx/your-domain.log main;  ssl on;  ssl_certificate      /etc/nginx/ssl/your-domain.crt;  ssl_certificate_key  /etc/nginx/ssl/your-domain.key;  location / &#123;    log_not_found on;    proxy_pass http://mirror:5000;    proxy_read_timeout 300;    proxy_connect_timeout 300;    proxy_redirect off;    proxy_set_header X-Forwarded-Proto <span class="hljs-variable">$scheme</span>;    proxy_set_header Host              <span class="hljs-variable">$http_host</span>;    proxy_set_header X-Real-IP         <span class="hljs-variable">$remote_addr</span>;  &#125;&#125;</code></pre>]]></content>
    
    
    <categories>
      
      <category>Docker</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Linux</tag>
      
      <tag>Docker</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Dokcer 使用 Flannel 跨主机通讯</title>
    <link href="/2006/01/02/dokcer-%E4%BD%BF%E7%94%A8-flannel-%E8%B7%A8%E4%B8%BB%E6%9C%BA%E9%80%9A%E8%AE%AF/"/>
    <url>/2006/01/02/dokcer-%E4%BD%BF%E7%94%A8-flannel-%E8%B7%A8%E4%B8%BB%E6%9C%BA%E9%80%9A%E8%AE%AF/</url>
    
    <content type="html"><![CDATA[<h2 id="一、简介"><a href="#一、简介" class="headerlink" title="一、简介"></a>一、简介</h2><p>Flannel 是 CoreOS 提供用于解决 Dokcer 集群跨主机通讯的覆盖网络工具，Flannel 跨主机通讯在分配网络时，依赖于 Etcd，Etcd可参考 <a href="http://mritd.me/2016/09/01/Etcd-%E9%9B%86%E7%BE%A4%E6%90%AD%E5%BB%BA/" target="_blank" rel="noopener">Etcd 集群搭建</a></p><h2 id="二、二进制文件安装"><a href="#二、二进制文件安装" class="headerlink" title="二、二进制文件安装"></a>二、二进制文件安装</h2><h3 id="2-1、环境准备"><a href="#2-1、环境准备" class="headerlink" title="2.1、环境准备"></a>2.1、环境准备</h3><p>以下环境为 3 台虚拟机，同时 Etcd 集群已经配置好，同样安装在 3 台虚拟机上，节点 IP 如下</p><table><thead><tr><th>节点</th><th>地址</th></tr></thead><tbody><tr><td>etcd0</td><td>192.168.1.154</td></tr><tr><td>etcd1</td><td>192.168.1.156</td></tr><tr><td>etcd2</td><td>192.168.1.249</td></tr></tbody></table><h3 id="2-2、安装-Flannel"><a href="#2-2、安装-Flannel" class="headerlink" title="2.2、安装 Flannel"></a>2.2、安装 Flannel</h3><p>首先现在 Flannel 编译好的二进制文件 <a href="https://github.com/coreos/flannel/releases" target="_blank" rel="noopener">Github 下载地址</a></p><pre><code class="hljs sh"><span class="hljs-comment"># 首先下载压缩包(可能需要自备梯子)</span>wget https://github.com/coreos/flannel/releases/download/v0.6.1/flannel-v0.6.1-linux-amd64.tar.gz<span class="hljs-comment"># 解压</span>tar -zxvf flannel-v0.6.1-linux-amd64.tar.gz</code></pre><p>解压后的到 <code>flanneld</code>、<code>mk-docker-opts.sh</code> 两个文件，其中 <code>flanneld</code> 为主要的执行文件，sh 脚本用于生成 Docker 启动参数</p><h3 id="2-3、配置-Flannel"><a href="#2-3、配置-Flannel" class="headerlink" title="2.3、配置 Flannel"></a>2.3、配置 Flannel</h3><p>解压好 Flannel 后将其复制到可执行目录</p><pre><code class="hljs sh">cp flanneld /usr/<span class="hljs-built_in">local</span>/bin/</code></pre><p>由于 Flannel 需要依赖 Etcd 来保证集群 IP 分配不冲突的问题，所以首先要在 Etcd 中设置 Flannel 节点所使用的 IP 段</p><pre><code class="hljs sh">etcdctl --endpoints http://etcd1.mritd.me:4001 \<span class="hljs-built_in">set</span> /coreos.com/network/config <span class="hljs-string">'&#123;"NetWork":"10.0.0.0/16"&#125;'</span></code></pre><p>接下来启动 Flannel，并指定 Etcd 的集群位置即可，Etcd 集群前端可使用 nginx 或 haproxy 反向代理</p><pre><code class="hljs sh">flanneld --etcd-endpoints=<span class="hljs-string">"http://etcd1.mritd.me:2379"</span> --ip-masq=<span class="hljs-literal">true</span> &gt;&gt; /var/<span class="hljs-built_in">log</span>/flanneld.log 2&gt;&amp;1 &amp;</code></pre><p><img src="https://cdn.oss.link/markdown/hexo_flannel_start.png" srcset="/img/loading.gif" alt="hexo_flannel_start"></p><h3 id="2-4、配置-Docker"><a href="#2-4、配置-Docker" class="headerlink" title="2.4、配置 Docker"></a>2.4、配置 Docker</h3><p>在各个节点安装好以后最后要更改 Docker 的启动参数，使其能够使用 Flannel 进行 IP 分配，以及网络通讯</p><p><strong>Flannel 运行后会生成一个环境环境变量文件，包含了当前主机要使用 Flannel 通讯的相关参数</strong></p><pre><code class="hljs sh"><span class="hljs-comment"># 查看 Flannel 分配的网络参数</span>cat /run/flannel/subnet.env<span class="hljs-comment"># 相关网络参数如下</span>FLANNEL_NETWORK=10.0.0.0/16FLANNEL_SUBNET=10.0.72.1/24FLANNEL_MTU=1472FLANNEL_IPMASQ=<span class="hljs-literal">true</span></code></pre><p><strong>修改 docker0 网卡参数</strong></p><pre><code class="hljs sh"><span class="hljs-built_in">source</span> /run/flannel/subnet.envifconfig docker0 <span class="hljs-variable">$&#123;FLANNEL_SUBNET&#125;</span></code></pre><p>此时可以看到 docker0 的网卡 ip 地址已经处于 Flannel 网卡网段之内</p><p><img src="https://cdn.oss.link/markdown/hexo_flannel_modifydocker0.png" srcset="/img/loading.gif" alt="hexo_flannel_modifydocker0"></p><p><strong>创建 Docker 运行变量</strong></p><p>接下来使用 Flannel 提供方的脚本创建 Docker 启动参数，创建好的启动参数位于 <code>/run/docker_opts.env</code> 文件中</p><pre><code class="hljs sh">./mk-docker-opts.sh -d /run/docker_opts.env -c</code></pre><p><strong>修改 Docker 启动参数</strong></p><p>将 docker0 与 flannel0 绑定后，还需要修改 docker 的启动参数，并使其启动后使用由 Flannel 生成的配置参数，修改如下</p><pre><code class="hljs sh"><span class="hljs-comment"># 编辑 systemd service 配置文件</span>vim /usr/lib/systemd/system/docker.service<span class="hljs-comment"># 在启动时增加 Flannel 提供的启动参数</span>ExecStart=/usr/bin/dockerd <span class="hljs-variable">$DOCKER_OPTS</span><span class="hljs-comment"># 指定这些启动参数所在的文件位置(这个配置是新增的，同样放在 Service标签下)</span>EnvironmentFile=/run/docker_opts.env</code></pre><p>然后重新加载 systemd 配置，并重启 Docker 即可</p><pre><code class="hljs sh">systemctl daemon-reloadsystemctl restart docker</code></pre><p><strong>整个完成流程截图如下</strong></p><p><img src="https://cdn.oss.link/markdown/hexo_flannel_configall.png" srcset="/img/loading.gif" alt="hexo_flannel_configall"></p><h3 id="2-5、测试"><a href="#2-5、测试" class="headerlink" title="2.5、测试"></a>2.5、测试</h3><p>每个节点分别启动一个 Contianer ，并相互 ping 测试即可</p><h2 id="三、rpm-安装"><a href="#三、rpm-安装" class="headerlink" title="三、rpm 安装"></a>三、rpm 安装</h2><p>CentOS 官方已经提供了 flannel 的 rpm 包，使用 rpm 包安装的好处是会自动创建一些配置文件，同时方便管理。</p><p><strong>目前官方提供的 rpm 包一般版本都会低一些，可以使用 <a href="https://github.com/mritd/shell_scripts" target="_blank" rel="noopener">shell_scripts</a> 中的 <code>build-flannel-rpm.sh</code> 脚本创建指定版本的 rpm 包，以下基于我创建好的 0.6.1 版本的 flannel rpm 安装</strong></p><h3 id="3-1、安装-flannel"><a href="#3-1、安装-flannel" class="headerlink" title="3.1、安装 flannel"></a>3.1、安装 flannel</h3><p>首先使用脚本创建一个 0.6.1 版本的 rpm</p><pre><code class="hljs sh">git <span class="hljs-built_in">clone</span> https://github.com/mritd/shell_scripts.git<span class="hljs-built_in">cd</span> shell_script./build-flannel-rpm.sh 0.6.1</code></pre><p>创建好以后可以直接进行安装</p><pre><code class="hljs sh">rpm -ivh flannel-0.6.1-1.x86_64.rpm</code></pre><h3 id="3-2、配置-flannel"><a href="#3-2、配置-flannel" class="headerlink" title="3.2、配置 flannel"></a>3.2、配置 flannel</h3><p>rpm 安装的 flannel 已经做了很多自动配置，我们只需要修改一下 flannel 的配置即可</p><pre><code class="hljs sh"><span class="hljs-comment"># 同样先在 etcd 中放入 flannel ip 分配地址</span>etcdctl --endpoints http://etcd1.mritd.me:4001 \<span class="hljs-built_in">set</span> /coreos.com/network/config <span class="hljs-string">'&#123;"NetWork":"10.0.0.0/16"&#125;'</span><span class="hljs-comment"># 然后编辑 flannel 配置文件</span>vim /etc/sysconfig/flanneld<span class="hljs-comment"># 修改 Etcd 地址和 Etcd 中 flannel ip 分配地址的目录即可</span>FLANNEL_ETCD=<span class="hljs-string">"http://etcd1.mritd.me:2379"</span>FLANNEL_ETCD_KEY=<span class="hljs-string">"/coreos.com/network"</span><span class="hljs-comment"># 最后重启 flannel 和 docker</span>systemctl restart flannelsystemctl restart docker</code></pre><p><strong>测试同样创建两个 Contianer 相互ping即可</strong></p>]]></content>
    
    
    <categories>
      
      <category>Docker</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Linux</tag>
      
      <tag>Docker</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>互联网加密及 OpenSSL 介绍和简单使用</title>
    <link href="/2006/01/02/encryption-and-introduction-to-openssl/"/>
    <url>/2006/01/02/encryption-and-introduction-to-openssl/</url>
    
    <content type="html"><![CDATA[<h2 id="一、互联网通信安全简述"><a href="#一、互联网通信安全简述" class="headerlink" title="一、互联网通信安全简述"></a>一、互联网通信安全简述</h2><h3 id="1-1、安全问题"><a href="#1-1、安全问题" class="headerlink" title="1.1、安全问题"></a>1.1、安全问题</h3><p>随着互联网不断发展，网络安全日益重要，而在互联网上的两台主机间通讯安全成为隐患，其主要面临两大问题 : <strong>身份认证与数据安全</strong>。</p><h3 id="1-2、身份认证"><a href="#1-2、身份认证" class="headerlink" title="1.2、身份认证"></a>1.2、身份认证</h3><p>当互联网上两台从未通讯过的主机进行通讯时，比如我们第一次访问支付宝网站，我们如何确信对方主机是真实的，比如我们在浏览器中输入支付宝的地址最终到达的主机是支付宝的主机，而并非别人仿造的假的钓鱼网站？此时面临的问题就是一个 <strong>身份认证</strong> 问题，即对方要能证明其身份。</p><h3 id="1-3、数据安全"><a href="#1-3、数据安全" class="headerlink" title="1.3、数据安全"></a>1.3、数据安全</h3><p>假设已经解决了身份认证问题，那么接下来的操作如何保证数据不被窃取？比如在支付宝网站进行转账操作，如何保证密码传输过程中不会被别人窃取；此时面临的就是 <strong>数据安全</strong> 问题。</p><h2 id="二、加密算法"><a href="#二、加密算法" class="headerlink" title="二、加密算法"></a>二、加密算法</h2><h3 id="2-1、加密方式分类"><a href="#2-1、加密方式分类" class="headerlink" title="2.1、加密方式分类"></a>2.1、加密方式分类</h3><p>目前互联网上的加密算法主要分为三大类 : <strong>对称加密(双向加密)、非对称加密、公钥加密。</strong></p><h3 id="2-2、对称加密"><a href="#2-2、对称加密" class="headerlink" title="2.2、对称加密"></a>2.2、对称加密</h3><p>对称加密也称作双向加密，即 <strong>加密与解密使用同一密码</strong>，对明文使用密码加密后，对方可以通过同一密码进行反向解密，<strong>加密安全性完全依赖于加密密码。</strong>常见的加密算法有 <code>DES(56bits)</code>、<code>3DES</code>、<code>AES(128bits)</code>、<code>Blowfish</code>、<code>Twofish</code>、<code>IDEA</code>、<code>RC6</code>、<code>CAST5</code>、<code>Serpent</code>等等；此种方式加密容易遭受 <strong>字典攻击</strong>，同时解密方需要得到加密密码，中间可能涉及密码传输泄露问题；此种加密一般都是 <strong>将原文分割成固定大小的数据块，对这些块进行加密(ECB，CBC)。</strong></p><h3 id="2-3、非对称加密"><a href="#2-3、非对称加密" class="headerlink" title="2.3、非对称加密"></a>2.3、非对称加密</h3><p>非对称加密与对称加密相反，其加密解密可使用不同的密码，常见的如 <code>RSA</code>、<code>EIGmal</code>、<code>DSA</code> 等。</p><h4 id="2-3-1、单向加密"><a href="#2-3-1、单向加密" class="headerlink" title="2.3.1、单向加密"></a>2.3.1、单向加密</h4><p>在非对称加密中，有一种比较特殊的加密叫做单向加密，<strong>单向机密也称消息摘要算法，其主要目的是提取消息特征码，加密后的密文不可逆。</strong>常见的单向加密算法有 : <code>MD5</code>、<code>SHA1</code>、<code>SHA512</code>、<code>CRC-32(循环冗余校验码)</code> 等；单向加密有两个重要的特性 : <strong>定长输出、雪崩效应；</strong>定长输出可以理解为 <strong>无论消息体多大，最终输出加密后结果长度一致，雪崩效应顾名思义，即消息体的微小变化，会导致加密结果的巨大变化。</strong></p><h3 id="2-4、公钥加密"><a href="#2-4、公钥加密" class="headerlink" title="2.4、公钥加密"></a>2.4、公钥加密</h3><p>公钥加密时会有两个文件，即一个公钥一个私钥；并且公钥与私钥成对出现，其特性是 <strong>使用公钥加密的内容必须使用与其匹配的私钥解密，反之亦然。</strong></p><h2 id="三、解决方案"><a href="#三、解决方案" class="headerlink" title="三、解决方案"></a>三、解决方案</h2><h3 id="3-1、身份认证的解决"><a href="#3-1、身份认证的解决" class="headerlink" title="3.1、身份认证的解决"></a>3.1、身份认证的解决</h3><p>由上可知，首先我们面临的第一个问题就是身份认证问题，即 “支付宝要能证明他是支付宝”；纵观以上3中加密算法，比较适合做身份认证的就是 <strong>公钥加密</strong>，<strong>即在互联网上从未通讯过的主机想向对方证明自己的身份，只需先生成一对密钥，然后将公钥放在互联网上大家可任意获取，私钥自己保留；需要证明身份时只需用自己的私钥加密一段信息发给对方，对方若能使用其公钥解密，就能证明其身份。</strong>简单的例子如下所示，支付宝服务器先将自己的公钥放到互联网上，然后我们需要支付宝服务器证明其身份的时候，支付宝服务器先用自己的私钥加密一段文字，若我们使用互联网上的支付宝公钥能够将其解密，则就能证明此服务器是真实的</p><p><img src="https://cdn.oss.link/markdown/hexo_openssl_shenfenrenzheng1.png" srcset="/img/loading.gif" alt="hexo_openssl_shenfenrenzheng1"></p><h3 id="3-2、CA-机构"><a href="#3-2、CA-机构" class="headerlink" title="3.2、CA 机构"></a>3.2、CA 机构</h3><p>上面的做法似乎完美的解决了身份认证问题，但并非如此，这中间存在一个巨大漏洞；即 <strong>如何保证从互联网上获取的公钥就是对方的，而不是别人恶意放到互联网上的假冒公钥。</strong>此时的解决方案就是 <strong>大家找一个公认的知名机构，把公钥全部放到这个机构那，然后谁需要谁从那里下载</strong>，此时这个机构称之为 CA，CA 为每个使用者颁发一个证书，证书中包含其公钥和CA 的私钥签名，以及一些使用者信息；<strong>同时证书具有级联信任关系，即 我们信任了一家 CA 的根证书，那么由其颁发的其他证书将都被信任，类似于一棵树的结构，一旦我们信任了根节点那么以下所有子节点将全部被信任。</strong></p><p><img src="https://cdn.oss.link/markdown/hexo_openssl_shenfenrenzheng2.png" srcset="/img/loading.gif" alt="hexo_openssl_shenfenrenzheng2"></p><h3 id="3-3、数据加密的解决"><a href="#3-3、数据加密的解决" class="headerlink" title="3.3、数据加密的解决"></a>3.3、数据加密的解决</h3><p>当我们使用公钥加密和 CA 机制解决了身份认证以后，接下来的问题就是数据传输过程中的加密问题；既然是传输数据，那么数据内容肯定需要双向加密才可以被接收方解密读取，但是很大的问题是 <strong>双向加密时，解密方需要知道解密密码；</strong>而将密码在互联网上传输明显是不明智的，此时出现了另一种解决方案，即 <strong>密钥交换算法</strong>，专门用于在互联网上传输密钥信息。</p><h4 id="3-3-1、Diffie-Hellman-算法"><a href="#3-3-1、Diffie-Hellman-算法" class="headerlink" title="3.3.1、Diffie-Hellman 算法"></a>3.3.1、Diffie-Hellman 算法</h4><p>Diffie–Hellman 是著名的密钥交换算法，其基本原理如下：</p><ul><li>首先两台计算机互相通讯，并约定选取一个大素数 g</li><li>然后两台计算机再协商一个计算数 p</li><li>接下来每台计算机随机选定一个随机数，此随机数并不会发给对方(x、y)</li><li>通讯时，A 计算机计算 <code>g^x % p</code> 发给 B 计算机</li><li>接下来同样 B 计算机计算 <code>g^y % p</code> 发给 A 计算机</li><li>B 计算机拿到 <code>g^x % p</code> 后再进行 <code>(g^x % p)^y</code> 即最终结果为 <code>g^xy % p</code></li><li>同样 A 计算机拿到 <code>g^y % p</code> 后再进行 <code>(g^y % p)^x</code> 即最终结果为 <code>g^xy % p</code></li><li>此时两台计算机结果将一致，接下来传送的数据便以此结果为密码进行加密即可</li></ul><p><strong>此过程中双方都保留了私有的 x、y 随机数，x、y 并未在互联网上传输，所以是安全的，并且由离散数学原理 x、y 被推测的可能性很小，以当前计算机性能基本不可能推测(不要提超级计算机，黑客没那个资源)。</strong></p><h4 id="3-3-2、Diffie-Hellman-算法示意图"><a href="#3-3-2、Diffie-Hellman-算法示意图" class="headerlink" title="3.3.2、Diffie-Hellman 算法示意图"></a>3.3.2、Diffie-Hellman 算法示意图</h4><p>Diffie-Hellman 算法示意图如下</p><p><img src="https://cdn.oss.link/markdown/hexo_openssl_DH.png" srcset="/img/loading.gif" alt="hexo_openssl_DH"></p><h3 id="3-4、数据完整性"><a href="#3-4、数据完整性" class="headerlink" title="3.4、数据完整性"></a>3.4、数据完整性</h3><p>以上部分已经实现了身份校验以及数据加密操作，接下来我们需要保证数据的完整性，对于完整性最好的应用就是采用单向加密即消息摘要算法，提取数据的特征码；<strong>简单地说就是每发送一段数据，就对发送的数据做一次数据特征提取，然后将特征码一并发送，接收方接收到数据并解密后可对其使用同样的算法进行特征码提取比对，如果计算结果不一致则证明数据被篡改，直接丢弃即可。</strong></p><p><img src="https://cdn.oss.link/markdown/hexo_openssl_datawanzheng.png" srcset="/img/loading.gif" alt="hexo_openssl_datawanzheng"></p><h2 id="四、OpenSSL-使用"><a href="#四、OpenSSL-使用" class="headerlink" title="四、OpenSSL 使用"></a>四、OpenSSL 使用</h2><h3 id="4-1、OpenSSL-简介"><a href="#4-1、OpenSSL-简介" class="headerlink" title="4.1、OpenSSL 简介"></a>4.1、OpenSSL 简介</h3><p>OpenSSL 是一组加密套件，其提供了功能强大的加密解密功能，以及证书相关的操作工具，OpenSSL 分为三大部分 : <code>libcrypto</code> 通用功能的加密库、<code>libssl</code> 用于实现TLS/SSL的功能 和 <code>openssl</code> 多功能命令工具；而一般 OpenSSL 使用指的就是多功能命令行工具的使用。</p><p>OpenSSL 有许多子命令，可通过 <code>openssl ?</code> 查看其支持子命令和加密算法等信息</p><pre><code class="hljs sh">Standard commandsasn1parse         ca                ciphers           cmscrl               crl2pkcs7         dgst              dhdhparam           dsa               dsaparam          ececparam           enc               engine            errstrgendh             gendsa            genpkey           genrsanseq              ocsp              passwd            pkcs12pkcs7             pkcs8             pkey              pkeyparampkeyutl           prime             rand              reqrsa               rsautl            s_client          s_servers_time            sess_id           smime             speedspkac             srp               ts                verifyversion           x509Message Digest commands (see the `dgst<span class="hljs-string">' command for more details)</span><span class="hljs-string">md4               md5               rmd160            sha</span><span class="hljs-string">sha1</span><span class="hljs-string"></span><span class="hljs-string">Cipher commands (see the `enc'</span> <span class="hljs-built_in">command</span> <span class="hljs-keyword">for</span> more details)aes-128-cbc       aes-128-ecb       aes-192-cbc       aes-192-ecbaes-256-cbc       aes-256-ecb       base64            bfbf-cbc            bf-cfb            bf-ecb            bf-ofbcamellia-128-cbc  camellia-128-ecb  camellia-192-cbc  camellia-192-ecbcamellia-256-cbc  camellia-256-ecb  cast              cast-cbccast5-cbc         cast5-cfb         cast5-ecb         cast5-ofbdes               des-cbc           des-cfb           des-ecbdes-ede           des-ede-cbc       des-ede-cfb       des-ede-ofbdes-ede3          des-ede3-cbc      des-ede3-cfb      des-ede3-ofbdes-ofb           des3              desx              rc2rc2-40-cbc        rc2-64-cbc        rc2-cbc           rc2-cfbrc2-ecb           rc2-ofb           rc4               rc4-40seed              seed-cbc          seed-cfb          seed-ecbseed-ofb</code></pre><h3 id="4-2、OpenSSL-实现加解密"><a href="#4-2、OpenSSL-实现加解密" class="headerlink" title="4.2、OpenSSL 实现加解密"></a>4.2、OpenSSL 实现加解密</h3><h4 id="4-2-1、加密操作"><a href="#4-2-1、加密操作" class="headerlink" title="4.2.1、加密操作"></a>4.2.1、加密操作</h4><p>OpenSSL 使用子命令 <code>enc</code> 实现加密，命令格式如下</p><pre><code class="hljs sh">openssl enc -算法 -salt -a -salt -<span class="hljs-keyword">in</span> 输入文件 -out 输出文件 -k passwd</code></pre><p>如下为加密一个文件示例</p><pre><code class="hljs sh">openssl enc -aes-256-ecb  -a -salt -<span class="hljs-keyword">in</span> /etc/profile -out ~/<span class="hljs-built_in">test</span> -k test123</code></pre><p><code>-a</code> 选项用于将加密后内容以 base64 格式输出，加密后内容如下</p><pre><code class="hljs sh">U2FsdGVkX1/tVWQ7cGR5rTLU9Phnx7csis12ukuGpZTWx7sDiJ8Qdb4Jn8hvShnW7gPouyOwJlKufoDHXT2hqOnw/i8Rf5QisLc+EOXgUWDFun3AqHte6YmNbelDFchuhDsph4Vq7TNBmZ/C1LHSCnMoA39qHBWxyrIstpOWs5TWkdPjDgEVZkIXNiWIwoRFCDVv7AuZXO9qF3sMb4fLWPF9cM7FkbvGkkhzBJZ0dAdq8DetKaZyjOX4UfeWPSavU61t9s7hlmjEPmXpUSSfnO8/7u0B/EEnujpkwyNZXbQBcQ9QFDh7x1vdzQIAuMRw0SLsVm1OSOifvMOpOAJCmvhExrKsznWgQlQabLOWsr29yrs0YBuwSl6oUSf8Qx1S3z/GC7l13zzmHwXJTgPRe0WzdLYsy6GMj2/0DveLvcXT/cXGid69tzIf4JaczIEXzSS4N12C3CFS/60poXZmiwoNsC3n6ESBf3dZum6LKA9sNPdxveb/RTZ4KYl/iw9SNgAqNGa9Yp1xdyl2NBaruvDxVAGbwC+rGn9UjbIYbEdT7ZjKPjJ6BP8yzwv6jnOU3E58UVIuIEVWJY3kF77+Vk99JuQepFRGX9sHYdafQR7AISiS88eipn6qkMMDmrkTkwZtebWyhvmWTZABOa7tckzzYxJ/Ke1jkBudasUJr9RUEiNNmSASdRr0dlZvM/tSCOuPeNo8ZU7ad/yd/OpvPKShn9Hh9oXCKSB4vl/DQVefVXWAaTp49FFAGFrQThiQrAwHwrJ/N2ab0Do0iArctNTqz5u7MtEHwVtFsV45YJfAGQRwTg06Y6qKjhgWmBjXEGNGbP7c0A1SiI/g8maKl3ciTwWPWXCcpf14MFcYBvjpA7EVYSYTh7hSxzUtY0gkl3EUVrqnY0M8u+0LBgAcrA==</code></pre><h4 id="4-2-2、解密操作"><a href="#4-2-2、解密操作" class="headerlink" title="4.2.2、解密操作"></a>4.2.2、解密操作</h4><p>解密操作与加密基本相同，只不过需要加上 <code>-d</code> 选项，代表解密，命令格式如下</p><pre><code class="hljs sh">openssl enc -算法 -salt -d -a -<span class="hljs-keyword">in</span> 输入文件 -out 输出文件 -k passwd</code></pre><p>如下为解密一个文件的示例</p><pre><code class="hljs sh">openssl enc -aes-256-ecb -salt -d -a -<span class="hljs-keyword">in</span> <span class="hljs-built_in">test</span> -out test1 -k test123</code></pre><p>解密后文件即为原来的 <code>/etc/profile</code> 文件内容</p><h3 id="4-3、OpenSSL-创建-CA"><a href="#4-3、OpenSSL-创建-CA" class="headerlink" title="4.3、OpenSSL 创建 CA"></a>4.3、OpenSSL 创建 CA</h3><p>关于 CA 证书本文并未做过多解释，限于篇幅，完全解释完太多，以下为一篇写的比较好的文章 <a href="http://seanlook.com/2015/01/15/openssl-certificate-encryption/" target="_blank" rel="noopener">点这里</a>；下面主要介绍 OpenSSL 工具建立私有 CA。</p><h4 id="4-3-1、OpenSSL-CA-相关配置"><a href="#4-3-1、OpenSSL-CA-相关配置" class="headerlink" title="4.3.1、OpenSSL CA 相关配置"></a>4.3.1、OpenSSL CA 相关配置</h4><p>默认的 Ubuntu 下 OpenSSL 的配置文件位于 <code>/etc/ssl/openssl.cnf</code>，CentOS 位于 <code>/etc/pki/tls/openssl.cnf</code>，我们只需要关注如下部分，关于具体配置含义 Google 之。</p><pre><code class="hljs sh"><span class="hljs-comment">####################################################################</span>[ ca ]default_ca      = CA_default            <span class="hljs-comment"># The default ca section</span><span class="hljs-comment">####################################################################</span>[ CA_default ]dir             = ./demoCA              <span class="hljs-comment"># Where everything is kept</span>certs           = <span class="hljs-variable">$dir</span>/certs            <span class="hljs-comment"># Where the issued certs are kept</span>crl_dir         = <span class="hljs-variable">$dir</span>/crl              <span class="hljs-comment"># Where the issued crl are kept</span>database        = <span class="hljs-variable">$dir</span>/index.txt        <span class="hljs-comment"># database index file.</span><span class="hljs-comment">#unique_subject = no                    # Set to 'no' to allow creation of</span>                                        <span class="hljs-comment"># several ctificates with same subject.</span>new_certs_dir   = <span class="hljs-variable">$dir</span>/newcerts         <span class="hljs-comment"># default place for new certs.</span>certificate     = <span class="hljs-variable">$dir</span>/cacert.pem       <span class="hljs-comment"># The CA certificate</span>serial          = <span class="hljs-variable">$dir</span>/serial           <span class="hljs-comment"># The current serial number</span>crlnumber       = <span class="hljs-variable">$dir</span>/crlnumber        <span class="hljs-comment"># the current crl number</span>                                        <span class="hljs-comment"># must be commented out to leave a V1 CRL</span>crl             = <span class="hljs-variable">$dir</span>/crl.pem          <span class="hljs-comment"># The current CRL</span>private_key     = <span class="hljs-variable">$dir</span>/private/cakey.pem<span class="hljs-comment"># The private key</span>RANDFILE        = <span class="hljs-variable">$dir</span>/private/.rand    <span class="hljs-comment"># private random number file</span>x509_extensions = usr_cert              <span class="hljs-comment"># The extentions to add to the cert</span><span class="hljs-comment"># Comment out the following two lines for the "traditional"</span><span class="hljs-comment"># (and highly broken) format.</span>name_opt        = ca_default            <span class="hljs-comment"># Subject Name options</span>cert_opt        = ca_default            <span class="hljs-comment"># Certificate field options</span><span class="hljs-comment"># Extension copying option: use with caution.</span><span class="hljs-comment"># copy_extensions = copy</span><span class="hljs-comment"># Extensions to add to a CRL. Note: Netscape communicator chokes on V2 CRLs</span><span class="hljs-comment"># so this is commented out by default to leave a V1 CRL.</span><span class="hljs-comment"># crlnumber must also be commented out to leave a V1 CRL.</span><span class="hljs-comment"># crl_extensions        = crl_ext</span>default_days    = 365                   <span class="hljs-comment"># how long to certify for</span>default_crl_days= 30                    <span class="hljs-comment"># how long before next CRL</span>default_md      = default               <span class="hljs-comment"># use public key default MD</span>preserve        = no                    <span class="hljs-comment"># keep passed DN ordering</span><span class="hljs-comment"># A few difference way of specifying how similar the request should look</span><span class="hljs-comment"># For type CA, the listed attributes must be the same, and the optional</span><span class="hljs-comment"># and supplied fields are just that :-)</span>policy          = policy_match<span class="hljs-comment"># For the CA policy</span>[ policy_match ]countryName             = matchstateOrProvinceName     = matchorganizationName       = matchorganizationalUnitName  = optionalcommonName              = suppliedemailAddress            = optional</code></pre><h4 id="4-3-2、生成密钥对"><a href="#4-3-2、生成密钥对" class="headerlink" title="4.3.2、生成密钥对"></a>4.3.2、生成密钥对</h4><p>由配置文件可知，OpenSSL 默认的工作目录在 <code>./demoCA</code> 下，所以需要先创建 CA 所需相关目录，执行 <code>mkdir -p demoCA/{private,certs,crl,newcerts}</code>，目录结构如下</p><p><img src="https://cdn.oss.link/markdown/hexo_openssl_dirtree.png" srcset="/img/loading.gif" alt="hexo_openssl_dirtree"></p><p>然后生成一个私钥(<strong>公钥其实是从私钥中提取的，所以创建私钥就可以从中提取出公钥</strong>)</p><pre><code class="hljs sh"><span class="hljs-comment"># 使用 () 开启子 shell 执行，同时设置掩码</span>(<span class="hljs-built_in">umask</span> 077; openssl genrsa -out demoCA/private/cakey.pem 2048)<span class="hljs-comment"># 如果想查看公钥可执行以下命令(非必须)</span>openssl rsa -<span class="hljs-keyword">in</span> demoCA/private/cakey.pem -pubout</code></pre><h4 id="4-3-3、生成自签证书"><a href="#4-3-3、生成自签证书" class="headerlink" title="4.3.3、生成自签证书"></a>4.3.3、生成自签证书</h4><p>作为根 CA，可以生成一个自签名的证书，来标明自己的身份，签名方法如下</p><pre><code class="hljs sh">openssl req -new -x509 -key demoCA/private/cakey.pem -days 3655 -out demoCA/cacert.pem</code></pre><p>然后输入相关信息，注意不要乱输入，其中 <code>Common Name</code> 一般为使用者域名，这里暂时填写的是 IP，截图如下</p><p><img src="https://cdn.oss.link/markdown/hexo_openssl_createcapb.png" srcset="/img/loading.gif" alt="hexo_openssl_createcapb"></p><p><strong>最后要创建用于签名证书的相关文件，</strong>执行 <code>touch demoCA/{index.txt,serial,crlnumber}</code> 创建所需文件，执行 <code>echo &quot;01&quot; &gt; demoCA/serial</code> 初始化序列号，最终文件目录结构如下</p><p><img src="https://cdn.oss.link/markdown/hexo_openssl_createcafiledirtree.png" srcset="/img/loading.gif" alt="hexo_openssl_createcafiledirtree"></p><h3 id="4-4、OpneSSL-申请证书"><a href="#4-4、OpneSSL-申请证书" class="headerlink" title="4.4、OpneSSL 申请证书"></a>4.4、OpneSSL 申请证书</h3><p>同样，先生成一对密钥</p><pre><code class="hljs sh"><span class="hljs-comment"># 创建目录</span>mkdir mycrt<span class="hljs-comment"># 生成公钥 扩展名随便取</span>openssl genrsa -out mritd.key 2048<span class="hljs-comment"># 提取公钥</span>openssl rsa -<span class="hljs-keyword">in</span> mritd.key -pubout &gt; mritd.public.key</code></pre><p><strong>接下来要申请一个证书签署请求，信息要保证与 CA 创建时一致(具体可调整 openssl.cnf 配置文件)，密码直接回车默认为空密码</strong></p><pre><code class="hljs sh">openssl req -new -key mritd.key -out mritd.csr</code></pre><p>截图如下</p><p><img src="https://cdn.oss.link/markdown/hexo_openssl_createprivatekey.png" srcset="/img/loading.gif" alt="hexo_openssl_createprivatekey"></p><p>最后使用 CA 进行签署即可(注意，要回到 <code>demoCA</code> 上一级目录)，提示是否确认全部 y 即可</p><pre><code class="hljs sh">openssl ca -<span class="hljs-keyword">in</span> mycrt/mritd.csr -out mycrt/mritd.crt -days 3655</code></pre><p>截图如下</p><p><img src="https://cdn.oss.link/markdown/hexo_openssl_signedcrt.png" srcset="/img/loading.gif" alt="hexo_openssl_signedcrt"></p><p><strong>最终生成的  <code>mritd.crt</code> 即为可用的证书</strong></p>]]></content>
    
    
    <categories>
      
      <category>Linux</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Linux</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Quartz 任务强制删除</title>
    <link href="/2006/01/02/force-delete-quartz-job/"/>
    <url>/2006/01/02/force-delete-quartz-job/</url>
    
    <content type="html"><![CDATA[<h2 id="一、扯淡"><a href="#一、扯淡" class="headerlink" title="一、扯淡"></a>一、扯淡</h2><blockquote><p>公司调度平台使用 Quartz 框架实现，Job 信息持久化到 Oracle 数据库中；坑爹队友在开发环境开发了一个 Job，结果后期需求变更，又将此 Job 实现类移除，但移除代码前未删除相关的数据信息，导致数据库中残留相关 trigger、JobDetails 等信息，最终项目启动初始化 scheduler 失败，整个调度平台不可用……</p></blockquote><h2 id="二、强删-Job"><a href="#二、强删-Job" class="headerlink" title="二、强删 Job"></a>二、强删 Job</h2><h3 id="1、排查报错原因"><a href="#1、排查报错原因" class="headerlink" title="1、排查报错原因 :"></a>1、排查报错原因 :</h3><pre><code class="hljs sh">DefinitionInitializer: Quartz Scheduler failed to initialize: org.quartz.SchedulerConfigException: Failure occured during job recovery. [See nested exception: org.quartz.JobPersistenceException: Couldn<span class="hljs-string">'t store trigger: com.xxxxx.xxxx.task.ContAutoCancelToStatuJob [See nested exception: java.lang.ClassNotFoundException: com.xxxxx.xxxx.task.ContAutoCancelToStatuJob]]</span></code></pre><p>从日志中可以看出，某个 trigger 有问题，原因是其引用的 Job 实现类已经被删除</p><h3 id="2、查出所有-Quartz-的表"><a href="#2、查出所有-Quartz-的表" class="headerlink" title="2、查出所有 Quartz 的表 :"></a>2、查出所有 Quartz 的表 :</h3><pre><code class="hljs sql"><span class="hljs-keyword">SELECT</span> * <span class="hljs-keyword">FROM</span> user_tab_comments T1 <span class="hljs-keyword">WHERE</span> T1.table_name <span class="hljs-keyword">LIKE</span> <span class="hljs-string">'QRTZ_%'</span>  ;</code></pre><p>所有 Quartz 表如下 :</p><p><img src="https://cdn.oss.link/markdown/hexo_delete_quartzjob_alltable.png" srcset="/img/loading.gif" alt="hexo_delete_quartzjob_alltable"></p><h3 id="3、删掉-trigger"><a href="#3、删掉-trigger" class="headerlink" title="3、删掉 trigger"></a>3、删掉 trigger</h3><p>查询所有 triger 相关的表，发现在 QRTZ_CRON_TRIGGERS 中有导致报错的 trigger(我们任务触发用的 cron 表达式方式)，所以果断删除 :</p><pre><code class="hljs sql"><span class="hljs-keyword">DELETE</span>  <span class="hljs-keyword">FROM</span> QRTZ_CRON_TRIGGERS T1 <span class="hljs-keyword">WHERE</span> T1.JOB_CLASS_NAME=<span class="hljs-string">'com.xxxxx.xxxx.task.ContAutoCancelToStatuJob'</span>;</code></pre><h3 id="4、启动测试"><a href="#4、启动测试" class="headerlink" title="4、启动测试"></a>4、启动测试</h3><p>再次启动发现报错信息已经没有了，但是悲剧的发现无法创建任何任务，其报错信息大致的意思还是 <code>class no found</code>，后排查发现 Job 相关的表中仍有残留的记录，所以同样需要清除</p><pre><code class="hljs sql"><span class="hljs-keyword">DELETE</span>  <span class="hljs-keyword">FROM</span> QRTZ_JOB_DETAILS T1 <span class="hljs-keyword">WHERE</span> T1.JOB_CLASS_NAME=<span class="hljs-string">'com.xxxxx.xxxx.task.ContAutoCancelToStatuJob'</span>;</code></pre><p><strong>删除时发现，此条记录无法删除，报错是有级联信息，但是我翻了半天没找到，最终无奈根据报错信息，先找到其约束，将其置为级联删除，再删主记录，最后把约束级联删除改回 no action 即可</strong></p><h3 id="5、修改约束-amp-amp-级联删除"><a href="#5、修改约束-amp-amp-级联删除" class="headerlink" title="5、修改约束&amp;&amp;级联删除"></a>5、修改约束&amp;&amp;级联删除</h3><p>删除 QRTZ_JOB_DETAILS 时报错如下:</p><p><img src="https://cdn.oss.link/markdown/hexo_delete_quartzjob_deleteerror.png" srcset="/img/loading.gif" alt="hexo_delete_quartzjob_deleteerror"></p><p>查询该约束(图后截的，约束编号已变):</p><pre><code class="hljs sql"><span class="hljs-keyword">SELECT</span> * <span class="hljs-keyword">FROM</span> user_constraints T1 <span class="hljs-keyword">WHERE</span> T1.constraint_name =<span class="hljs-string">'SYS_C0080707'</span>;</code></pre><p>发现其约束 QRTZ_TRIGGERS 修改其约束级联删除关系:</p><p><img src="https://cdn.oss.link/markdown/hexo_delete_quartzjob_modify.png" srcset="/img/loading.gif" alt="hexo_delete_quartzjob_modify"></p><p>此时再次删除即可，到此强删完成。</p>]]></content>
    
    
    <categories>
      
      <category>Java</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Java</tag>
      
      <tag>Quartz</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Linux 下 MySQL 密码强制修改</title>
    <link href="/2006/01/02/forced-change-mysql-passowrd-on-linux/"/>
    <url>/2006/01/02/forced-change-mysql-passowrd-on-linux/</url>
    
    <content type="html"><![CDATA[<blockquote><p>好吧好吧，这事不怨我，二逼同事打扰我，就输错了……</p></blockquote><h3 id="一、停掉mysql服务"><a href="#一、停掉mysql服务" class="headerlink" title="一、停掉mysql服务"></a>一、停掉mysql服务</h3><pre><code class="hljs bash"><span class="hljs-comment">#单独安装的mysql</span>service mysqld stop<span class="hljs-comment">#其他如lnmp脚本安装的</span>lnmp mysql stop</code></pre><h3 id="二、启动mysql，禁止权限验证"><a href="#二、启动mysql，禁止权限验证" class="headerlink" title="二、启动mysql，禁止权限验证"></a>二、启动mysql，禁止权限验证</h3><pre><code class="hljs bash"><span class="hljs-comment">#在mysql安装目录的bin目录下执行(&amp;后台执行)</span>/usr/<span class="hljs-built_in">local</span>/mysql/bin/mysqld_safe --skip-grant-tables &amp;</code></pre><h3 id="三、本地登录，更改密码"><a href="#三、本地登录，更改密码" class="headerlink" title="三、本地登录，更改密码"></a>三、本地登录，更改密码</h3><pre><code class="hljs bash"><span class="hljs-comment">#本地 localhost登录无需密码</span>mysql -u root<span class="hljs-comment">#切换数据库</span>mysql&gt;use mysql;<span class="hljs-comment">#更改密码</span>mysql&gt;update user <span class="hljs-built_in">set</span> password=password(<span class="hljs-string">"newpasswd"</span>) <span class="hljs-built_in">where</span> user=<span class="hljs-string">"root"</span>;<span class="hljs-comment">#刷新权限</span>mysql&gt;flush privileges;<span class="hljs-comment">#退出</span>mysql&gt;\q</code></pre><h3 id="四、启动mysql"><a href="#四、启动mysql" class="headerlink" title="四、启动mysql"></a>四、启动mysql</h3><ul><li>先停掉已经跳过权限的mysql启动进程</li></ul><pre><code class="hljs bash"><span class="hljs-comment">#查询mysql是否启动</span>ps aux | grep <span class="hljs-string">"mysql"</span><span class="hljs-comment">#如果发现 有  --skip-grant-tables  类似的进程 找到其 PID kill掉</span><span class="hljs-comment">#然后在 kill掉其他进程</span><span class="hljs-comment">#PID : 用户名后面的 数字为PID</span><span class="hljs-built_in">kill</span> -9 PID</code></pre><ul><li>正常启动mysql</li></ul><pre><code class="hljs bash"><span class="hljs-comment">#普通直接安装</span>service mysqld start<span class="hljs-comment">#lnmp脚本安装</span>lnmp mysql start</code></pre><ul><li>登录测试</li></ul><pre><code class="hljs bash"><span class="hljs-comment">#输入密码</span>mysql -uroot -p</code></pre>]]></content>
    
    
    <categories>
      
      <category>Linux</category>
      
      <category>Database</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Linux</tag>
      
      <tag>MySQL</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Git 中文乱码解决方案</title>
    <link href="/2006/01/02/git-chinese-garbled-solution/"/>
    <url>/2006/01/02/git-chinese-garbled-solution/</url>
    
    <content type="html"><![CDATA[<p><img src="https://cdn.oss.link/markdown/hexo_git_encoding.png" srcset="/img/loading.gif" alt="hexo_git_encoding"></p><blockquote><p>今天在虚拟机中提交时 <code>git status</code> 了一下，发现中文文件名都变成了 <code>\344\270\255\346\226\207</code> 这种形式，记录一下解决方案。</p></blockquote><ul><li>没修改配置前的状态</li></ul><p><img src="https://cdn.oss.link/markdown/hexo_git_luanma1.png" srcset="/img/loading.gif" alt="hexo_git_luanma1"></p><ul><li>更改 git 状态输出的编码显示方式</li></ul><pre><code class="hljs bash"><span class="hljs-comment"># 设置 不会对0×80以上的字符进行quote</span>git config --global core.quotepath <span class="hljs-literal">false</span></code></pre><ul><li>再次查看效果如下</li></ul><p><img src="https://cdn.oss.link/markdown/hexo_git_luanma2.png" srcset="/img/loading.gif" alt="hexo_git_luanma2"></p><ul><li>记录一下其他的配置 摘自 <a href="https://gist.github.com/hidoos/7866314" target="_blank" rel="noopener">Github</a></li></ul><blockquote><p>步骤：</p><ol><li>下载：<a href="http://loaden.googlecode.com/files/gitconfig.7z" target="_blank" rel="noopener">http://loaden.googlecode.com/files/gitconfig.7z</a></li><li>解压到：&lt;MsysGit安装目录&gt;/cmd/，例如：D:\Program Files\Git\cmd</li><li>进入Bash，执行gitconfig</li></ol></blockquote><blockquote><p>搞定什么了？<br>看看gitconfig的内容先：</p></blockquote><pre><code class="hljs bash"><span class="hljs-meta">#!/bin/sh</span><span class="hljs-comment"># 全局提交用户名与邮箱</span>git config --global user.name <span class="hljs-string">"Yuchen Deng"</span>git config --global user.email 邮箱名@gmail.com<span class="hljs-comment"># 中文编码支持</span><span class="hljs-built_in">echo</span> <span class="hljs-string">"export LESSCHARSET=utf-8"</span> &gt; <span class="hljs-variable">$HOME</span>/.profilegit config --global gui.encoding utf-8git config --global i18n.commitencoding utf-8git config --global i18n.logoutputencoding gbk<span class="hljs-comment"># 全局编辑器，提交时将COMMIT_EDITMSG编码转换成UTF-8可避免乱码</span>git config --global core.editor notepad2<span class="hljs-comment"># 差异工具配置</span>git config --global diff.external git-diff-wrapper.shgit config --global diff.tool tortoisegit config --global difftool.tortoise.cmd <span class="hljs-string">'TortoiseMerge -base:"$LOCAL" -theirs:"$REMOTE"'</span>git config --global difftool.prompt <span class="hljs-literal">false</span><span class="hljs-comment"># 合并工具配置</span>git config --global merge.tool tortoisegit config --global mergetool.tortoise.cmd <span class="hljs-string">'TortoiseMerge -base:"$BASE" -theirs:"$REMOTE" -mine:"$LOCAL" -merged:"$MERGED"'</span>git config --global mergetool.prompt <span class="hljs-literal">false</span><span class="hljs-comment"># 别名设置</span>git config --global alias.dt difftoolgit config --global alias.mt mergetool<span class="hljs-comment"># 取消 $ git gui 的中文界面，改用英文界面更易懂</span><span class="hljs-keyword">if</span> [ -f <span class="hljs-string">"/share/git-gui/lib/msgs/zh_cn.msg"</span> ]; <span class="hljs-keyword">then</span>rm /share/git-gui/lib/msgs/zh_cn.msg<span class="hljs-keyword">fi</span></code></pre><blockquote><p>这个脚本解决了：</p><ol><li>中文乱码</li><li>图形化Diff/Merge</li><li>还原英文界面，更好懂<br>其中最有价值的，就是Git的Diff/Merge外部工具TortoiseMerge配置。<br>安装MsysGit后，一个命令即可完成配置。<br>适用于MsysGit安装版与绿色版。</li></ol></blockquote><blockquote><p>网上关于为Git配置TortoiseMerge来进行diff和merge的介绍几乎没有（反正我没有搜索到），但我认为TortoiseMerge是最好用的，单文件（一个可执行程序，绿色版，下载地址：<a href="http://sourceforge.net/projects/tortoisesvn/files/Tools/1.6.7/TortoiseDiff-1.6.7.zip/download)，实在是绝配！" target="_blank" rel="noopener">http://sourceforge.net/projects/tortoisesvn/files/Tools/1.6.7/TortoiseDiff-1.6.7.zip/download)，实在是绝配！</a></p></blockquote><blockquote><p>为什么不使用TortoiseGit？他们不是集成了TortoiseMerge吗？<br>理由：TortoiseGit只有Windows才有，我更喜欢git gui，结合gitk，跨平台实在相同的操作方式，更爽！<br>如果您离不开TortoiseGit，这篇文章就直接无视吧。</p></blockquote>]]></content>
    
    
    <categories>
      
      <category>Linux</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Linux</tag>
      
      <tag>Git</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Gradle 构建基础笔记</title>
    <link href="/2006/01/02/gradle-note/"/>
    <url>/2006/01/02/gradle-note/</url>
    
    <content type="html"><![CDATA[<p><img src="https://cdn.oss.link/markdown/hexo_gradle_logo.png" srcset="/img/loading.gif" alt="Gradle_logo"></p><blockquote><p>本文参考自 <a href="https://www.gitbook.com/book/dongchuan/gradle-user-guide-/details" target="_blank" rel="noopener">Gradle User Guide 中文版</a> 感谢其翻译贡献者</p></blockquote><h2 id="安装Gradle"><a href="#安装Gradle" class="headerlink" title="安装Gradle"></a>安装Gradle</h2><ul><li>下载安装包 <a href="http://gradle.org/gradle-download/" target="_blank" rel="noopener">下载地址</a></li><li>配置环境变量 如下</li></ul><p><img src="https://cdn.oss.link/markdown/hexo_gradle_home.jpg" srcset="/img/loading.gif" alt="GRADLE_HOME"></p><p><img src="https://cdn.oss.link/markdown/hexo_gradle_path.jpg" srcset="/img/loading.gif" alt="GRADLE_PATH"></p><h2 id="Gradle-基本概念"><a href="#Gradle-基本概念" class="headerlink" title="Gradle 基本概念"></a>Gradle 基本概念</h2><h3 id="Projects"><a href="#Projects" class="headerlink" title="Projects"></a>Projects</h3><blockquote><p>每一个构建由一个或者多个 Project 构成，每个 Project 代表一个资源，具体做什么取决与如何定义该 Project，它可能代表一个发布的 zip 文件，也可能代表某项动作，比如部署应用等。</p></blockquote><h3 id="Tasks"><a href="#Tasks" class="headerlink" title="Tasks"></a>Tasks</h3><blockquote><p>每个 Project 由一个或者多个 task 组成，每个 task 可以是更细致化的构建操作，比如编译 class文件，创建 jar 文件，生成 javadoc 文档等。</p></blockquote><h2 id="Gradle-Hello-World"><a href="#Gradle-Hello-World" class="headerlink" title="Gradle Hello World"></a>Gradle Hello World</h2><blockquote><p>我们可以通过 <code>gradle</code> 命令来完成一次构建，<strong><code>gradle</code> 命令会在当前目录查找一个叫 <code>build.gradle</code> 的描述文件来完成构建</strong>。严格来说它是一个构建脚本，类似于 Maven 的 <code>POM.xml</code> 文件；用来描述整个构建过程；以下是一个 <code>Hello World</code> 示例。</p></blockquote><pre><code class="hljs groovy">task helloWorld &#123;    doLast &#123;        println <span class="hljs-string">'Hello World!'</span>    &#125;&#125;</code></pre><p>将以上脚本保存为 <code>build.gradle</code>；并在当前目录执行 <code>gradle -q helloWorld</code> 尝试运行该 task。<br><strong>注意：<code>-q</code> 代表 quite 模式，不显示 Gradle 的日志信息。</strong></p><h2 id="快捷定义任务"><a href="#快捷定义任务" class="headerlink" title="快捷定义任务"></a>快捷定义任务</h2><p>以上代码，<code>doLast</code> 作为 <code>helloWorld</code> 的一个 Action，此时可以通过如下方式快捷定义：</p><pre><code class="hljs groovy">task helloWorld &lt;&lt; &#123;    println <span class="hljs-string">'Hello World!'</span>&#125;</code></pre><h2 id="在-Gradle-中使用-Groovy"><a href="#在-Gradle-中使用-Groovy" class="headerlink" title="在 Gradle 中使用 Groovy"></a>在 Gradle 中使用 Groovy</h2><p>Gradle 是基于 Groovy 的，在 build.gradle 中可以使用 Groovy 书写(虽然我不会…)；样例如下(测试命令行中文乱码)：</p><pre><code class="hljs groovy">task upper &lt;&lt; &#123;    String testStr = <span class="hljs-string">"AbcD"</span>    println <span class="hljs-string">"原始字符："</span> + testStr    println <span class="hljs-string">"转换大写："</span> + testStr.toUpperCase()&#125;</code></pre><h2 id="任务依赖性"><a href="#任务依赖性" class="headerlink" title="任务依赖性"></a>任务依赖性</h2><p>Gradle 支持任务间的依赖关系，如下所示：</p><pre><code class="hljs grovvy">task testDepends &lt;&lt; &#123;    println &quot;taskDepends&quot;&#125;&#x2F;&#x2F; 设置依赖关系task test1(dependsOn: testDepends) &lt;&lt; &#123;    println &quot;test1&quot;&#125;</code></pre><p>执行 <code>gradle -q test1</code> 命令运行 task1 任务，此时由于 task1 任务依赖于 testDepends 任务，所以会先执行 testDepends 任务，在执行 test1 任务。</p><p><strong>注意：build.gradle 脚本中，任务依赖时没有顺序区分，也就是说可以先声明一个任务依赖于另一个任务，而后定义另一个任务，总结来说就是 task 书写顺序不会影响 任务间依赖关系。</strong></p><h2 id="任务的动态性"><a href="#任务的动态性" class="headerlink" title="任务的动态性"></a>任务的动态性</h2><p>由于 Gradle 使用 Groovy 作为构建语言，Groovy 函数式的特性可以让我们动态的创建任务，而无需预先设定好(个人认为实际构建可能没这么变态的需要)；样例如下：</p><pre><code class="hljs groovy"><span class="hljs-comment">// 此时相当于定义了 4个 task 分别是 taskDynamic0、...1、...2、...3</span><span class="hljs-number">4.</span>times &#123; counter -&gt;    task <span class="hljs-string">"taskDynamic$counter"</span> &lt;&lt; &#123;        println <span class="hljs-string">"taskDynamic$counter"</span>    &#125;&#125;</code></pre><h2 id="通过API使用已存在的任务"><a href="#通过API使用已存在的任务" class="headerlink" title="通过API使用已存在的任务"></a>通过API使用已存在的任务</h2><p>Gradle 中可以先定义 task，然后再指定这些 task 之间的关系，就像下面这样：</p><pre><code class="hljs groovy"><span class="hljs-comment">// 先利用动态性 创建4个任务</span><span class="hljs-number">4.</span>times &#123; counter -&gt;    task <span class="hljs-string">"task$counter"</span> &lt;&lt; &#123;        println <span class="hljs-string">"task----&gt; $counter"</span>    &#125;&#125;<span class="hljs-comment">// 再指定依赖关系</span>task0.dependsOn task1,task3</code></pre><p>同时 Gradle 还允许在任务定以后动态的改变或添加其行为，如下所示：</p><pre><code class="hljs groovy"><span class="hljs-comment">// 首先定义任务</span>task test1 &lt;&lt; &#123;    println <span class="hljs-string">"default"</span>&#125;<span class="hljs-comment">// 然后动态添加 任务开始动作</span>test1.doFirst &#123;    println <span class="hljs-string">"First"</span>&#125;<span class="hljs-comment">// 动态添加 任务结束动作</span>test1.doLast &#123;    println <span class="hljs-string">"Last"</span>&#125;<span class="hljs-comment">// 在添加一个结束动作</span>test1 &lt;&lt; &#123;    println <span class="hljs-string">"the end"</span>&#125;</code></pre><p><strong>注意：上面一直使用的 <code>&lt;&lt;</code> 实际是 doLast 的简写形式。</strong></p><h2 id="任务引用-短标记语法"><a href="#任务引用-短标记语法" class="headerlink" title="任务引用(短标记语法)"></a>任务引用(短标记语法)</h2><p>Gradle 可以通过 <code>$</code> 符号引用另一个任务，可以是另一个任务的执行结果；也可以是其属性，比如 <code>task.name</code> 将返回某个任务的名字，样例如下：</p><pre><code class="hljs groovy"><span class="hljs-comment">// 首先定义一个任务</span>task test1 &lt;&lt; &#123;    prinln <span class="hljs-string">"test1111111"</span>&#125;<span class="hljs-comment">// 然后在另一个任务里引用</span>task test2 &lt;&lt; &#123;    println <span class="hljs-string">"test2---&gt;$test1.name"</span>&#125;</code></pre><h2 id="自定义任务属性"><a href="#自定义任务属性" class="headerlink" title="自定义任务属性"></a>自定义任务属性</h2><p>在 Gradle 中我们可以自定义任务的属性，然后再恰当的时候加以引用，在设置某个任务属性时 <strong>采用 <code>ext</code> 关键字定义该属性</strong>；个人理解类似 java 的 this 关键字；样例如下：</p><pre><code class="hljs groovy"><span class="hljs-comment">// 定义 testTask1 的两个属性</span>task testTask1 &#123;    ext.property1 = <span class="hljs-string">"1"</span>    ext.property2 = <span class="hljs-string">"2"</span>&#125;<span class="hljs-comment">// 可直接打印引用，字符串中可采用短标记 $ 引用</span>task testTask2 &lt;&lt; &#123;    println testTask1.property1    println <span class="hljs-string">"$testTask1.property2"</span>&#125;</code></pre><p><strong>注意：定义任务属性时必须在任务体中，而不能在 doLast 或 doFirst Action 中定义！</strong></p><h2 id="调用-Ant-任务"><a href="#调用-Ant-任务" class="headerlink" title="调用 Ant 任务"></a>调用 Ant 任务</h2><blockquote><p>以下直接 copy 的，本人没玩过 Ant……</p></blockquote><p>Ant 任务是 Gradle 的一等公民. Gradle 通过 Groovy 出色的集成了 Ant 任务. Groovy 自带了一个 AntBuilder. 相比于从一个 build.xml 文件中使用 Ant 任务, 在 Gradle 里使用 Ant 任务更为方便和强大. 从下面的例子中, 你可以学习如何执行 Ant 任务以及如何访问 ant 属性：</p><pre><code class="hljs groovy">task loadfile &lt;&lt; &#123;    <span class="hljs-keyword">def</span> files = file(<span class="hljs-string">'../antLoadfileResources'</span>).listFiles().sort()        files.each &#123; File file -&gt;        <span class="hljs-keyword">if</span> (file.isFile()) &#123;            ant.loadfile(<span class="hljs-string">srcFile:</span> file, <span class="hljs-string">property:</span> file.name)            println <span class="hljs-string">" *** $file.name ***"</span>            println <span class="hljs-string">"$&#123;ant.properties[file.name]&#125;"</span>        &#125;    &#125;&#125;</code></pre><h2 id="使用方法"><a href="#使用方法" class="headerlink" title="使用方法"></a>使用方法</h2><blockquote><p>又用 Ant…… My God ….. 接着 copy</p></blockquote><p>Gradle 能很好地衡量你编写脚本的逻辑能力. 首先要做的是如何提取一个方法.</p><pre><code class="hljs groovy">task checksum &lt;&lt; &#123;    fileList(<span class="hljs-string">'../antLoadfileResources'</span>).each &#123;File file -&gt;        ant.checksum(<span class="hljs-string">file:</span> file, <span class="hljs-string">property:</span> <span class="hljs-string">"cs_$file.name"</span>)        println <span class="hljs-string">"$file.name Checksum: $&#123;ant.properties["</span>cs_$file.name<span class="hljs-string">"]&#125;"</span>    &#125;&#125;task loadfile &lt;&lt; &#123;    fileList(<span class="hljs-string">'../antLoadfileResources'</span>).each &#123;File file -&gt;        ant.loadfile(<span class="hljs-string">srcFile:</span> file, <span class="hljs-string">property:</span> file.name)        println <span class="hljs-string">"I'm fond of $file.name"</span>    &#125;&#125;File[] fileList(String dir) &#123;    file(dir).listFiles(&#123;file -&gt; file.isFile() &#125; <span class="hljs-keyword">as</span> FileFilter).sort()&#125;</code></pre><h2 id="默认任务"><a href="#默认任务" class="headerlink" title="默认任务"></a>默认任务</h2><blockquote><p>终于没有 Ant 了……</p></blockquote><p>Gradle 允许在一个构建脚本(<code>build.gradle</code>) 中定义默认需要执行的任务，只需要使用 <code>defaultTasks</code> 关键字即可，此时可直接在构建脚本目录执行 <code>gradle [-q]</code> 来执行构建过程，样例如下</p><pre><code class="hljs groovy"><span class="hljs-comment">// 定义 默认的任务</span>defaultTasks <span class="hljs-string">"testTask1"</span>,<span class="hljs-string">"testTask2"</span>task testTask1 &lt;&lt; &#123;    println <span class="hljs-string">"testTask1"</span>&#125;task testTask2 &lt;&lt; &#123;    println <span class="hljs-string">"testTask2"</span>&#125;task testTask3 &lt;&lt; &#123;    println <span class="hljs-string">"testTask3"</span>&#125;</code></pre><h2 id="DAG-配置"><a href="#DAG-配置" class="headerlink" title="DAG 配置"></a>DAG 配置</h2><p><strong>原文：</strong>正如我们之后的详细描述, Gradle 有一个配置阶段和执行阶<br>段. 在配置阶段后, Gradle 将会知道应执行的所有任务. Gradle 为你提供一个”钩子”, 以便利用这些信息. 举个例子, 判断发布的任务是否在要被执行的任务当中. 根据这一点, 你可以给一些变量指定不同的值.</p><p><strong>样例代码：</strong></p><pre><code class="hljs groovy">task distribution &lt;&lt; &#123;    println <span class="hljs-string">"We build the zip with version=$version"</span>&#125;task release(<span class="hljs-string">dependsOn:</span> <span class="hljs-string">'distribution'</span>) &lt;&lt; &#123;    println <span class="hljs-string">'We release now'</span>&#125;gradle.taskGraph.whenReady &#123;taskGraph -&gt;    <span class="hljs-keyword">if</span> (taskGraph.hasTask(release)) &#123;        version = <span class="hljs-string">'1.0'</span>    &#125; <span class="hljs-keyword">else</span> &#123;        version = <span class="hljs-string">'1.0-SNAPSHOT'</span>    &#125;&#125;</code></pre><p><strong>个人理解：</strong>关于 DAG 配置的描述，大致意思应该是说，Gradle 构建时会执行两个阶段：配置阶段、执行阶段；在配置阶段；Gradle 主要完成所有的配置工作，应该会读取整个构建脚本，分析里面的语义；这也可能是 Gradle 定义 task 等不分先后顺序的原因；在这个阶段，我们可以通过内置变量(DAG)和内置方法来动态的判断并调整后续的构建流程；当所有构建流程确定后，再进行第二个构建阶段。</p>]]></content>
    
    
    <categories>
      
      <category>Java</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Java</tag>
      
      <tag>Gradle</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Harbor 企业级 Docker Registry 初试</title>
    <link href="/2006/01/02/harbor-enterprise-docker-registry-part1/"/>
    <url>/2006/01/02/harbor-enterprise-docker-registry-part1/</url>
    
    <content type="html"><![CDATA[<h2 id="一、介绍"><a href="#一、介绍" class="headerlink" title="一、介绍"></a>一、介绍</h2><blockquote><p>Project Harbor is an enterprise-class registry server, which extends the open source Docker Registry server by adding the functionality usually required by an enterprise, such as security, control, and management. Harbor is primarily designed to be a private registry - providing the needed security and control that enterprises require. It also helps minimize bandwidth usage, which is helpful to both improve productivity (local network access) as well as performance (for those with poor internet connectivity).</p></blockquote><p>简单的说，Harbor 是一个企业级的 Docker Registry，可以实现 images 的私有存储和日志统计权限控制等功能，并支持创建多项目(Harbor 提出的概念)，基于官方 Registry V2 实现。</p><h2 id="二、环境准备"><a href="#二、环境准备" class="headerlink" title="二、环境准备"></a>二、环境准备</h2><p>本文所使用的环境如下 :</p><ul><li>Ubuntu 14.04</li><li>Docker 1.11.2</li><li>docker-compose 1.6.2</li></ul><h3 id="2-1、安装-Docker"><a href="#2-1、安装-Docker" class="headerlink" title="2.1、安装 Docker"></a>2.1、安装 Docker</h3><p>执行以下命令安装 Docker</p><pre><code class="hljs sh">curl -fsSL https://get.docker.io | bash</code></pre><h3 id="2-2、安装-docker-compose"><a href="#2-2、安装-docker-compose" class="headerlink" title="2.2、安装 docker-compose"></a>2.2、安装 docker-compose</h3><p>默认的 <a href="https://docs.docker.com/compose/install/" target="_blank" rel="noopener">官方文档</a> 安装命令如下 :</p><pre><code class="hljs sh">curl -L https://github.com/docker/compose/releases/download/1.6.2/docker-compose-`uname -s`-`uname -m` &gt; /usr/<span class="hljs-built_in">local</span>/bin/docker-compose</code></pre><p><strong>经过本人测试，其文件托管在亚马逊上，伟大的防火墙成功阻止下载……</strong></p><p><strong>有能力的童鞋可以使用梯子，我已经下载好了一个 <a href="https://cdn.oss.link/files/docker-compose" target="_blank" rel="noopener">点击下载</a></strong>；下载后直接 cp 到 <code>/usr/local/bin</code> 下并给与可执行权限即可。</p><h2 id="三、搭建-Harbor"><a href="#三、搭建-Harbor" class="headerlink" title="三、搭建 Harbor"></a>三、搭建 Harbor</h2><h3 id="3-1、克隆源码"><a href="#3-1、克隆源码" class="headerlink" title="3.1、克隆源码"></a>3.1、克隆源码</h3><pre><code class="hljs sh">git <span class="hljs-built_in">clone</span> https://github.com/vmware/harbor</code></pre><h3 id="3-2、修改配置"><a href="#3-2、修改配置" class="headerlink" title="3.2、修改配置"></a>3.2、修改配置</h3><pre><code class="hljs sh"><span class="hljs-built_in">cd</span> harbor/Deploy/vim harbor.cfg</code></pre><p>配置样例如下 :</p><pre><code class="hljs sh"><span class="hljs-comment">## Configuration file of Harbor</span><span class="hljs-comment">#The IP address or hostname to access admin UI and registry service.</span><span class="hljs-comment">#DO NOT use localhost or 127.0.0.1, because Harbor needs to be accessed by external clients.</span><span class="hljs-comment"># 指定 hostname，一般为IP，或者域名，用于登录 Web UI 界面</span>hostname = 10.211.55.17<span class="hljs-comment">#The protocol for accessing the UI and token/notification service, by default it is http.</span><span class="hljs-comment">#It can be set to https if ssl is enabled on nginx.</span><span class="hljs-comment"># URL 访问方式，SSL 需要配置 nginx</span>ui_url_protocol = http<span class="hljs-comment">#Email account settings for sending out password resetting emails.</span><span class="hljs-comment"># 邮件相关信息配置，如忘记密码发送邮件</span>email_server = smtp.xxxxxx.comemail_server_port = 465email_username = reg@mritd.meemail_password = xxxxxxemail_from = docker &lt;reg@mritd.me&gt;email_ssl = <span class="hljs-literal">true</span><span class="hljs-comment">##The password of Harbor admin, change this before any production use.</span><span class="hljs-comment"># 默认的 Harbor 的管理员密码，管理员用户名默认 admin</span>harbor_admin_password = Harbor12345<span class="hljs-comment">##By default the auth mode is db_auth, i.e. the credentials are stored in a local database.</span><span class="hljs-comment">#Set it to ldap_auth if you want to verify a user's credentials against an LDAP server.</span><span class="hljs-comment"># 指定 Harbor 的权限验证方式，Harbor 支持本地的 mysql 数据存储密码，同时也支持 LDAP</span>auth_mode = db_auth<span class="hljs-comment">#The url for an ldap endpoint.</span><span class="hljs-comment"># 如果采用了 LDAP，此处填写 LDAP 地址</span>ldap_url = ldaps://ldap.mydomain.com<span class="hljs-comment">#The basedn template to look up a user in LDAP and verify the user's password.</span><span class="hljs-comment"># LADP 验证密码的方式(我特么没用过这么高级的玩意)</span>ldap_basedn = uid=%s,ou=people,dc=mydomain,dc=com<span class="hljs-comment">#The password for the root user of mysql db, change this before any production use.</span><span class="hljs-comment"># mysql 数据库 root 账户密码</span>db_password = root123<span class="hljs-comment">#Turn on or off the self-registration feature</span><span class="hljs-comment"># 是否允许开放注册</span>self_registration = on<span class="hljs-comment">#Turn on or off the customize your certicate</span><span class="hljs-comment"># 允许自签名证书</span>customize_crt = on<span class="hljs-comment">#fill in your certicate message</span><span class="hljs-comment"># 自签名证书信息</span>crt_country = CNcrt_state = Statecrt_location = CNcrt_organization = mritdcrt_organizationalunit = mritdcrt_commonname = mritd.mecrt_email = reg.mritd.me<span class="hljs-comment">#####</span></code></pre><h3 id="3-3、生成相关配置"><a href="#3-3、生成相关配置" class="headerlink" title="3.3、生成相关配置"></a>3.3、生成相关配置</h3><pre><code class="hljs sh"><span class="hljs-built_in">cd</span> harbor/Deploy/./prepare</code></pre><p><img src="https://cdn.oss.link/markdown/hexo_docker_harbor_prepare.png" srcset="/img/loading.gif" alt="hexo_docker_harbor_prepare"></p><h3 id="3-4、编译-image-并启动"><a href="#3-4、编译-image-并启动" class="headerlink" title="3.4、编译 image 并启动"></a>3.4、编译 image 并启动</h3><pre><code class="hljs sh"><span class="hljs-built_in">cd</span> harbor/Deploy/docker-compose up -d</code></pre><p><img src="https://cdn.oss.link/markdown/hexo_docker_harbor_up.png" srcset="/img/loading.gif" alt="hexo_docker_harbor_up"></p><h3 id="3-5、启动后相关容器"><a href="#3-5、启动后相关容器" class="headerlink" title="3.5、启动后相关容器"></a>3.5、启动后相关容器</h3><p><strong>正常启动成功后会有 5 个 Contianer :</strong></p><ul><li>Proxy : 由Nginx 服务器构成的反向代理</li><li>Registry : 由Docker官方的开源registry 镜像构成的容器实例</li><li>UI : 即架构中的core services, 构成此容器的代码是Harbor项目的主体</li><li>Mysql : 由官方MySql镜像构成的数据库容器</li><li>Log : 运行着rsyslogd的容器，通过log-driver的形式收集其他容器的日志</li></ul><p><strong>这几个 Contianer 通过 Docker link 的形式连接在一起，在容器之间通过容器名字互相访问。对终端用户而言，只需要暴露 proxy（即Nginx）的服务端口</strong></p><p><img src="https://cdn.oss.link/markdown/hexo_docker_harbor_contianer.png" srcset="/img/loading.gif" alt="hexo_docker_harbor_contianer"></p><h2 id="四、访问-Web-UI-并测试"><a href="#四、访问-Web-UI-并测试" class="headerlink" title="四、访问 Web UI 并测试"></a>四、访问 Web UI 并测试</h2><h3 id="4-1、主页"><a href="#4-1、主页" class="headerlink" title="4.1、主页"></a>4.1、主页</h3><p><strong>默认的访问地址即为 <code>harbor.cfg</code> 中 <code>hostname</code> 地址，直接访问即可，如下</strong></p><p><img src="https://cdn.oss.link/markdown/hexo_docker_harbor_homepage.png" srcset="/img/loading.gif" alt="hexo_docker_harbor_homepage"></p><p><strong>如果 <code>harbor.cfg</code> 中 <code>self_registration</code> 属性设置为 <code>off</code>，那么普通用户将无法自己实现注册，只能由管理员创建用户，主页右上角的注册按钮也会消失。</strong></p><h3 id="4-2、登录"><a href="#4-2、登录" class="headerlink" title="4.2、登录"></a>4.2、登录</h3><p><strong>Harbor 默认管理员用户为 <code>admin</code>，密码在 <code>harbor.cfg</code> 中设置过，默认的是 <code>Harbor12345</code>，可直接登陆</strong></p><p><img src="https://cdn.oss.link/markdown/hexo_docker_harbor_userspace.png" srcset="/img/loading.gif" alt="hexo_docker_harbor_userspace"></p><h3 id="4-3、创建私有项目"><a href="#4-3、创建私有项目" class="headerlink" title="4.3、创建私有项目"></a>4.3、创建私有项目</h3><p><strong>Harbor 有一个项目的概念，项目名可以理解为 Docker Hub 的用户名，其下可以后很多 images，Harbor 的项目必须登录后方可 push，公有项目和私有项目的区别是对其他用户是否可见</strong></p><p><img src="https://cdn.oss.link/markdown/hexo_docker_harbor_createproject.png" srcset="/img/loading.gif" alt="hexo_docker_harbor_createproject"></p><h3 id="4-4、push-镜像"><a href="#4-4、push-镜像" class="headerlink" title="4.4、push 镜像"></a>4.4、push 镜像</h3><h4 id="4-4-1、设置-http-仓库地址"><a href="#4-4-1、设置-http-仓库地址" class="headerlink" title="4.4.1、设置 http 仓库地址"></a>4.4.1、设置 http 仓库地址</h4><p>由于采用了默认的 http 方式连接，而 Docker 认为这是不安全的，所以在 push 之前需要调整一下 docker 配置，编辑 <code>/etc/default/docker</code> 增加如下内容</p><pre><code class="hljs sh">DOCKER_OPTS=<span class="hljs-string">"<span class="hljs-variable">$DOCKER_OPTS</span> --insecure-registry 10.211.55.17"</span></code></pre><p><strong>其中 IP 地址要指向 <code>harbor.cfg</code> 中的 <code>hostname</code></strong>，然后执行 <code>docker-compose stop</code> 停掉所有 Contianer，再执行 <code>service docker restart</code> 重启 Dokcer 服务，最后执行 <code>docker-compose start</code> 即可。</p><p><strong>注意 : Docker 服务重启后，执行 <code>docker-compose start</code> 时有一定几率出现如下错误(或者目录已存在等错误)，此时在 <code>docker-compose stop</code> 一下然后在启动即可，实在不行再次重启 Dokcer 服务，千万不要手贱的去删文件(别问我怎么知道的)</strong></p><p><img src="https://cdn.oss.link/markdown/hexo_docker_harbor_composeerror.jpeg" srcset="/img/loading.gif" alt="hexo_docker_harbor_composeerror"></p><h4 id="4-4-2、Harbor-项目和权限-角色"><a href="#4-4-2、Harbor-项目和权限-角色" class="headerlink" title="4.4.2、Harbor 项目和权限(角色)"></a>4.4.2、Harbor 项目和权限(角色)</h4><p><strong>用户本身拥有的项目，登陆后可直接 push，其他的用户创建的项目取决于项目是否添加了对应用户和权限，</strong></p><p><strong>也就是说用户是否可以向一个项目 push 镜像，取决于权限(角色)设置，如下所示，在项目中可以设置成员和其权限</strong></p><p><img src="https://cdn.oss.link/markdown/hexo_docker_harbor_projectuser.png" srcset="/img/loading.gif" alt="hexo_docker_harbor_projectuser"></p><p><img src="https://cdn.oss.link/markdown/hexo_docker_harbor_projectuserrole.png" srcset="/img/loading.gif" alt="hexo_docker_harbor_projectuserrole"></p><p><strong>对于权限(角色)，<code>Project Admin</code> 和 <code>Developer</code> 可以有 push 的权限，而 <code>Guest</code> 只能查看和 pull</strong></p><h4 id="4-4-3、push-镜像"><a href="#4-4-3、push-镜像" class="headerlink" title="4.4.3、push 镜像"></a>4.4.3、push 镜像</h4><p>首先使用一个对目标项目具有 push 权限的用户登录，以下 push 的目标是 mritd 项目，test1 用户在项目里定义为 <code>Developer</code>，所以登录后 push 即可</p><p><img src="https://cdn.oss.link/markdown/hexo_docker_harbor_loginmritd.png" srcset="/img/loading.gif" alt="hexo_docker_harbor_loginmritd"></p><p>然后 <code>tag</code> 一个 image，名称一定要标准( <code>registryAddress[:端口]/项目/imageName[:tag]</code> )，最后将其 push 即可</p><p><img src="https://cdn.oss.link/markdown/hexo_docker_harbor_pushmritdimages.png" srcset="/img/loading.gif" alt="hexo_docker_harbor_pushmritdimages"></p><p>最后可在 Web UI 中查看刚刚 push 的 image</p><p><img src="https://cdn.oss.link/markdown/hexo_docker_harbor_mritdshow.png" srcset="/img/loading.gif" alt="hexo_docker_harbor_mritdshow"></p><p><strong>到此结束 Thanks</strong></p>]]></content>
    
    
    <categories>
      
      <category>Docker</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Linux</tag>
      
      <tag>Docker</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Harbor 企业级 Docker Registry 第二弹</title>
    <link href="/2006/01/02/harbor-enterprise-docker-registry-part2/"/>
    <url>/2006/01/02/harbor-enterprise-docker-registry-part2/</url>
    
    <content type="html"><![CDATA[<h2 id="一、简介"><a href="#一、简介" class="headerlink" title="一、简介"></a>一、简介</h2><p>Harbor 是 VMware 中国开发的一款 Dokcer Registry 工具，其主要致力于企业级的 Registry 管理，并提供了 LDAP 等高级权限认证功能，从第一次尝试到现在的版本已经有了很大变化，故决定重写一下 Harbor 的相关文章</p><h2 id="二、Harbor-搭建私服"><a href="#二、Harbor-搭建私服" class="headerlink" title="二、Harbor 搭建私服"></a>二、Harbor 搭建私服</h2><p>Harbor 最主要的功能就是搭建一个企业级的 Registry 私服，并对其进行完善的安全管理等，最新版本的 Harbor 已经支持 Dokcer 容器化的启动方式，各个组件使用 docker-compose 来进行编排，以下为搭建过程</p><h3 id="2-1、获取安装脚本"><a href="#2-1、获取安装脚本" class="headerlink" title="2.1、获取安装脚本"></a>2.1、获取安装脚本</h3><p>安装脚本在 Github 上，直接 wget 下来即可</p><pre><code class="hljs sh">wget https://github.com/vmware/harbor/releases/download/0.3.5/harbor-installer.tgztar -zxvf harbor-installer.tgz</code></pre><p>将其解压开后目录结构如下</p><p><img src="https://cdn.oss.link/markdown/hexo_harbor_install_scripts.png" srcset="/img/loading.gif" alt="hexo_harbor_install_scripts"></p><p>其中最外层有一个 <code>install.sh</code> 脚本，用于安装 Harbor，config 目录存放了一些配置信息，如 registry 和 ui 目录中存放了相关证书用于组件间加密通讯，<code>harbor.cfg</code> 是全局配置文件，里面主要包含了一些常用设置，比如是否启用 https 等，<code>prepare</code> 是一个 python 写的预处理脚本，主要负责初始化一些 <code>harbor.cfg</code> 的相关配置，<code>docker-compose.yml</code> 顾名思义，里面顶一个各个组件的依赖关系以及配置挂载、数据持久化等设置。</p><h3 id="2-2、基础配置"><a href="#2-2、基础配置" class="headerlink" title="2.2、基础配置"></a>2.2、基础配置</h3><p>修改配置直接编辑 <code>harbor.cfg</code> 即可</p><pre><code class="hljs sh">vim harbor.cfg<span class="hljs-comment"># 其配置信息如下</span>hostname = registry.mritd.me                      <span class="hljs-comment"># Harbor 服务器域名</span>ui_url_protocol = https                           <span class="hljs-comment"># UI 组件访问协议</span>email_server = smtp.mydomain.com                  <span class="hljs-comment"># email 服务器地址</span>email_server_port = 25                            <span class="hljs-comment"># email 端口</span>email_username = sample_admin@mydomain.com        <span class="hljs-comment"># email 账号</span>email_password = abc                              <span class="hljs-comment"># email 密码</span>email_from = admin &lt;sample_admin@mydomain.com&gt;    <span class="hljs-comment"># email 发件人</span>email_ssl = <span class="hljs-literal">false</span>                                 <span class="hljs-comment"># 是否启用 SSL</span>harbor_admin_password = Harbor12345               <span class="hljs-comment"># Harbor 初始化管理员(admin)密码</span>auth_mode = db_auth                               <span class="hljs-comment"># 权限管理模型(db_auth/ldap_auth)</span>ldap_url = ldaps://ldap.mydomain.com              <span class="hljs-comment"># ldap 地址</span>ldap_basedn = uid=%s,ou=people,dc=mydomain,dc=com <span class="hljs-comment"># ldap 权限模型</span>db_password = root123                             <span class="hljs-comment"># 数据库 管理员密码</span>self_registration = on                            <span class="hljs-comment"># 是否打开自动注册</span>use_compressed_js = on                            <span class="hljs-comment"># 是否启用压缩js</span>max_job_workers = 3                               <span class="hljs-comment"># 最大任务数</span>token_expiration = 30                             <span class="hljs-comment"># token 超时</span>verify_remote_cert = on                           <span class="hljs-comment"># 是否验证远程证书</span>customize_crt = on                                <span class="hljs-comment"># 是否启用自定义证书</span><span class="hljs-comment"># 以下为自定义证书信息</span>crt_country = CNcrt_state = Statecrt_location = CNcrt_organization = organizationcrt_organizationalunit = organizational unitcrt_commonname = example.comcrt_email = example@example.com</code></pre><h3 id="2-3、HTTPS-配置"><a href="#2-3、HTTPS-配置" class="headerlink" title="2.3、HTTPS 配置"></a>2.3、HTTPS 配置</h3><p>基础配置中如果启用了 https 协议，那么需要手动生成 nginx 的证书，生成过程如下</p><p><strong>CentOS7 下首先需要修改 OpenSSL CA 工作目录</strong></p><pre><code class="hljs sh"><span class="hljs-comment">#  编辑 OpenSSL 配置</span>vim /etc/pki/tls/openssl.cnf<span class="hljs-comment"># 主要修改 CA_default 标签下的 dir 为 ./demoCA</span>[ CA_default ]dir             = ./demoCA</code></pre><p><strong>创建 CA</strong></p><pre><code class="hljs sh">openssl req \    -newkey rsa:4096 -nodes -sha256 -keyout ca.key \    -x509 -days 365 -out ca.crt</code></pre><p><img src="https://cdn.oss.link/markdown/hexo_harbor_createcacert.png" srcset="/img/loading.gif" alt="hexo_harbor_createcacert"></p><p><strong>创建签名请求</strong></p><pre><code class="hljs sh">openssl req \    -newkey rsa:4096 -nodes -sha256 -keyout yourdomain.com.key \    -out yourdomain.com.csr</code></pre><p><img src="https://cdn.oss.link/markdown/hexo_harbor_createcsr.png" srcset="/img/loading.gif" alt="hexo_harbor_createcsr"></p><p><strong>初始化 CA 信息</strong></p><pre><code class="hljs sh">mkdir demoCA<span class="hljs-built_in">cd</span> demoCAtouch index.txt<span class="hljs-built_in">echo</span> <span class="hljs-string">'01'</span> &gt; serial<span class="hljs-built_in">cd</span> ../</code></pre><p><strong>签署证书</strong></p><pre><code class="hljs sh">openssl ca -<span class="hljs-keyword">in</span> yourdomain.com.csr -out yourdomain.com.crt -cert ca.crt -keyfile ca.key -outdir .</code></pre><p><img src="https://cdn.oss.link/markdown/hexo_harbor_signcrt.png" srcset="/img/loading.gif" alt="hexo_harbor_signcrt"></p><p><strong>复制证书到配置目录，并修改 nginx 配置</strong></p><pre><code class="hljs sh"><span class="hljs-comment"># 复制证书</span>cp registry.mritd.me.crt config/nginx/certcp ca/registry.mritd.me.key config/nginx/cert<span class="hljs-comment"># 备份配置</span>mv config/nginx/nginx.conf config/nginx/nginx.conf.bak<span class="hljs-comment"># 使用模板文件</span>mv config/nginx/nginx.https.conf config/nginx/nginx.conf<span class="hljs-comment"># 编辑 Nginx 配置</span>vim config/nginx/nginx.conf<span class="hljs-comment"># 主要修改 监听域名 和 ssl 证书位置</span>server_name registry.mritd.me;ssl_certificate /etc/nginx/cert/registry.mritd.me.crt;ssl_certificate_key /etc/nginx/cert/registry.mritd.me.key;</code></pre><p><strong>最后执行 install 访问域名即可</strong></p><h2 id="三、Harbor-搭建镜像仓库"><a href="#三、Harbor-搭建镜像仓库" class="headerlink" title="三、Harbor 搭建镜像仓库"></a>三、Harbor 搭建镜像仓库</h2><p>搭建镜像仓库只需要简单修改配置即可，不过镜像仓库不允许 push 操作，只作为官方仓库缓存</p><pre><code class="hljs sh">vim templates/registry/config.yml<span class="hljs-comment"># 增加以下内容</span>proxy:  remoteurl: https://registry-1.docker.io<span class="hljs-comment"># 然后重新部署即可</span>docker-compose downrm -rf /data/*docker up -d</code></pre>]]></content>
    
    
    <categories>
      
      <category>Docker</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Linux</tag>
      
      <tag>Docker</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Harbor 企业级 Docker Registry HTTPS配置</title>
    <link href="/2006/01/02/harbor-enterprise-docker-registry-part3/"/>
    <url>/2006/01/02/harbor-enterprise-docker-registry-part3/</url>
    
    <content type="html"><![CDATA[<blockquote><p>本文参考自 <a href="https://github.com/vmware/harbor/blob/master/docs/configure_https.md" target="_blank" rel="noopener">Harbor Github</a>、<a href="http://liaoph.com/openssl-san/" target="_blank" rel="noopener">OpenSSL 创建 SAN 证书</a></p></blockquote><h2 id="一、创建-CA-并自签证书"><a href="#一、创建-CA-并自签证书" class="headerlink" title="一、创建 CA 并自签证书"></a>一、创建 CA 并自签证书</h2><p>创建 CA 即自签名证书请看 <a href="http://mritd.me/2016/07/02/%E4%BA%92%E8%81%94%E7%BD%91%E5%8A%A0%E5%AF%86%E5%8F%8AOpenSSL%E4%BB%8B%E7%BB%8D%E5%92%8C%E7%AE%80%E5%8D%95%E4%BD%BF%E7%94%A8/" target="_blank" rel="noopener">互联网加密及OpenSSL介绍和简单使用</a>，以下简单介绍一下踩的坑，堆一下命令</p><h3 id="1-1、SAN-证书扩展域名配置"><a href="#1-1、SAN-证书扩展域名配置" class="headerlink" title="1.1、SAN 证书扩展域名配置"></a>1.1、SAN 证书扩展域名配置</h3><p><strong>默认的 OpenSSL 生成的签名请求只适用于生成时填写的域名，即 <code>Common Name</code> 填的是哪个域名，证书就只能应用于哪个域名，但是一般内网都是以 IP 方式部署，所以需要添加 SAN(Subject Alternative Name) 扩展信息，以支持多域名和IP</strong></p><pre><code class="hljs sh"><span class="hljs-comment"># 首先 cp 一份 openssl 配置</span>cp /etc/ssl/openssl.cnf .<span class="hljs-comment"># 主要修改 内容如下</span>[ req ]<span class="hljs-comment"># 上面的内容省略，主要增加这个属性(默认在最后一行被注释了，解开即可)</span>req_extensions = v3_req[ v3_req ]<span class="hljs-comment"># 修改 subjectAltName</span>subjectAltName = @alt_names[ alt_names ]<span class="hljs-comment"># 此节点[ alt_names ]为新增的，内容如下</span>IP.1=10.211.55.16   <span class="hljs-comment"># 扩展IP(私服所在服务器IP)</span>DNS.1=*.xran.me     <span class="hljs-comment"># 扩展域名(一般用于公网这里做测试)</span>DNS.2=*.baidu.com   <span class="hljs-comment"># 可添加多个扩展域名和IP</span></code></pre><p>完整的配置文件如下</p><pre><code class="hljs sh">[ req ]default_bits            = 2048default_keyfile         = privkey.pemdistinguished_name      = req_distinguished_nameattributes              = req_attributesx509_extensions = v3_ca <span class="hljs-comment"># The extentions to add to the self signed cert</span><span class="hljs-comment"># Passwords for private keys if not present they will be prompted for</span><span class="hljs-comment"># input_password = secret</span><span class="hljs-comment"># output_password = secret</span><span class="hljs-comment"># This sets a mask for permitted string types. There are several options.</span><span class="hljs-comment"># default: PrintableString, T61String, BMPString.</span><span class="hljs-comment"># pkix   : PrintableString, BMPString (PKIX recommendation before 2004)</span><span class="hljs-comment"># utf8only: only UTF8Strings (PKIX recommendation after 2004).</span><span class="hljs-comment"># nombstr : PrintableString, T61String (no BMPStrings or UTF8Strings).</span><span class="hljs-comment"># MASK:XXXX a literal mask value.</span><span class="hljs-comment"># WARNING: ancient versions of Netscape crash on BMPStrings or UTF8Strings.</span>string_mask = utf8onlyreq_extensions = v3_req <span class="hljs-comment"># The extensions to add to a certificate request</span>[ v3_req ]<span class="hljs-comment"># Extensions to add to a certificate request</span>basicConstraints = CA:FALSEkeyUsage = nonRepudiation, digitalSignature, keyEnciphermentsubjectAltName = @alt_names[ alt_names ]IP.1=10.211.55.16DNS.1=*.xran.meDNS.2=*.baidu.com</code></pre><h3 id="1-2、创建-CA-及自签名"><a href="#1-2、创建-CA-及自签名" class="headerlink" title="1.2、创建 CA 及自签名"></a>1.2、创建 CA 及自签名</h3><p>具体原理这里不做过多阐述，直接堆命令，详细请看 <a href="http://mritd.me/2016/07/02/%E4%BA%92%E8%81%94%E7%BD%91%E5%8A%A0%E5%AF%86%E5%8F%8AOpenSSL%E4%BB%8B%E7%BB%8D%E5%92%8C%E7%AE%80%E5%8D%95%E4%BD%BF%E7%94%A8/" target="_blank" rel="noopener">互联网加密及OpenSSL介绍和简单使用</a></p><pre><code class="hljs sh"><span class="hljs-built_in">cd</span> ~<span class="hljs-comment"># 创建 CA 工作目录</span>mkdir -p demoCA/&#123;private,certs,crl,newcerts&#125;<span class="hljs-comment"># 创建 CA 私钥</span>(<span class="hljs-built_in">umask</span> 077; openssl genrsa -out demoCA/private/cakey.pem 2048)<span class="hljs-comment"># 执行自签名(信息不要乱填，参考下面截图)</span>openssl req -new -x509 -key demoCA/private/cakey.pem -days 3655 -out demoCA/cacert.pem<span class="hljs-comment"># 初始化相关文件</span>touch demoCA/&#123;index.txt,serial,crlnumber&#125;<span class="hljs-comment"># 初始化序列号</span><span class="hljs-built_in">echo</span> <span class="hljs-string">"01"</span> &gt; demoCA/serial</code></pre><p>自签名证书截图如下</p><p><img src="https://cdn.oss.link/markdown/hexo_docker_harbor_https_createcacrt.png" srcset="/img/loading.gif" alt="hexo_harbor_https_createcacrt"></p><h3 id="1-3、创建证书并通过-CA-签名"><a href="#1-3、创建证书并通过-CA-签名" class="headerlink" title="1.3、创建证书并通过 CA 签名"></a>1.3、创建证书并通过 CA 签名</h3><p>同样，直接上命令……嘎嘣脆，奏是这个味</p><p><strong>注意: 创建签名请求(csr文件)命令和签名命令(ca)与 <a href="http://mritd.me/2016/07/02/%E4%BA%92%E8%81%94%E7%BD%91%E5%8A%A0%E5%AF%86%E5%8F%8AOpenSSL%E4%BB%8B%E7%BB%8D%E5%92%8C%E7%AE%80%E5%8D%95%E4%BD%BF%E7%94%A8/" target="_blank" rel="noopener">互联网加密及OpenSSL介绍和简单使用</a> 中稍有不同，openssl.cnf 为第一步修改后的，签名请求密码留空即可</strong></p><pre><code class="hljs sh"><span class="hljs-comment"># 证书存放目录</span>mkdir dockercrt<span class="hljs-comment"># 创建私钥</span>openssl genrsa -out dockercrt/docker.key 2048<span class="hljs-comment"># 生成带有 SAN 的证书请求</span>openssl req -new -key dockercrt/docker.key -out dockercrt/docker.csr -config openssl.cnf<span class="hljs-comment"># 签名带有 SAN 的证书</span>openssl ca -<span class="hljs-keyword">in</span> dockercrt/docker.csr -out dockercrt/docker.crt -config openssl.cnf -extensions v3_req</code></pre><p>创建签名请求信息填写截图如下</p><p><img src="https://cdn.oss.link/markdown/hexo_docker_harbor_https_createcsr.png" srcset="/img/loading.gif" alt="hexo_harbor_https_createcsr"></p><h2 id="二、配置-Harbor-HTTPS"><a href="#二、配置-Harbor-HTTPS" class="headerlink" title="二、配置 Harbor HTTPS"></a>二、配置 Harbor HTTPS</h2><h3 id="2-1、服务端配置"><a href="#2-1、服务端配置" class="headerlink" title="2.1、服务端配置"></a>2.1、服务端配置</h3><p>服务端配置相对简单，只需要修改一下 Harbor 的 Nginx 配置文件，并把签名好的证书和私钥复制过去即可</p><pre><code class="hljs sh"><span class="hljs-built_in">cd</span> ~/harbor/Deploy<span class="hljs-comment"># 复制 crt、key</span>cp ~/dockercrt/docker.crt config/nginx/certcp ~/dockercrt/docker.key config/nginx/cert<span class="hljs-comment"># 修改配置</span>vim config/nginx/nginx.conf</code></pre><p><strong>Nginx 样例配置如下</strong></p><pre><code class="hljs sh">worker_processes auto;events &#123;  worker_connections 1024;  use epoll;  multi_accept on;&#125;http &#123;  tcp_nodelay on;  <span class="hljs-comment"># this is necessary for us to be able to disable request buffering in all cases</span>  proxy_http_version 1.1;  upstream registry &#123;    server registry:5000;  &#125;  upstream ui &#123;    server ui:80;  &#125;  server &#123;    <span class="hljs-comment"># listen 80;</span>    listen 443 ssl;    <span class="hljs-comment"># disable any limits to avoid HTTP 413 for large image uploads</span>    client_max_body_size 0;    ssl on;    ssl_certificate /etc/nginx/cert/docker.crt;    ssl_certificate_key /etc/nginx/cert/docker.key;    location / &#123;      proxy_pass http://ui/;      proxy_set_header Host <span class="hljs-variable">$host</span>;      proxy_set_header X-Real-IP <span class="hljs-variable">$remote_addr</span>;      proxy_set_header X-Forwarded-For <span class="hljs-variable">$proxy_add_x_forwarded_for</span>;      <span class="hljs-comment"># When setting up Harbor behind other proxy, such as an Nginx instance, remove the below line if the proxy already</span>has similar settings.      proxy_set_header X-Forwarded-Proto <span class="hljs-variable">$scheme</span>;      proxy_buffering off;      proxy_request_buffering off;    &#125;    location /v1/ &#123;      <span class="hljs-built_in">return</span> 404;    &#125;    location /v2/ &#123;      proxy_pass http://registry/v2/;      proxy_set_header Host <span class="hljs-variable">$http_host</span>;      proxy_set_header X-Real-IP <span class="hljs-variable">$remote_addr</span>;      proxy_set_header X-Forwarded-For <span class="hljs-variable">$proxy_add_x_forwarded_for</span>;      <span class="hljs-comment"># When setting up Harbor behind other proxy, such as an Nginx instance, remove the below line if the proxy already has similar settings.</span>      proxy_set_header X-Forwarded-Proto <span class="hljs-variable">$scheme</span>;      proxy_buffering off;      proxy_request_buffering off;    &#125;    location /service/ &#123;      proxy_pass http://ui/service/;      proxy_set_header Host <span class="hljs-variable">$host</span>;      proxy_set_header X-Real-IP <span class="hljs-variable">$remote_addr</span>;      proxy_set_header X-Forwarded-For <span class="hljs-variable">$proxy_add_x_forwarded_for</span>;      <span class="hljs-comment"># When setting up Harbor behind other proxy, such as an Nginx instance, remove the below line if the proxy already has similar settings.</span>      proxy_set_header X-Forwarded-Proto <span class="hljs-variable">$scheme</span>;      proxy_buffering off;      proxy_request_buffering off;    &#125;  &#125;  server &#123;    listen 80;    rewrite ^/(.*) https://<span class="hljs-variable">$server_name</span><span class="hljs-variable">$1</span> permanent;  &#125;&#125;</code></pre><p><strong>最后重新创建 contianer 即可</strong></p><pre><code class="hljs sh"><span class="hljs-built_in">cd</span> ~/harbor/Deploy./prepare<span class="hljs-comment"># 先 down 一下删除原有配置</span>docker-compose downdocker-compose up -d</code></pre><p>此时访问 <code>https://hostname</code> 即可</p><h3 id="2-2、客户端配置"><a href="#2-2、客户端配置" class="headerlink" title="2.2、客户端配置"></a>2.2、客户端配置</h3><p><strong>客户端需要将签名 CA 的自签名根证书加入到本机的信任列表中，Ubuntu 下操作如下</strong></p><pre><code class="hljs sh"><span class="hljs-built_in">cd</span> ~<span class="hljs-comment"># 本人测试用的两个 虚拟机，需要远程拷贝</span>scp root@10.211.55.16:~/demoCA/cacert.pem .<span class="hljs-comment"># 备份一下 系统原有的根证书信任列表</span>cp /etc/ssl/certs/ca-certificates.crt /etc/ssl/certs/ca-certificates.crt.bak<span class="hljs-comment"># 追加 自签名的 CA 根证书到 系统信任列表</span>cat cacert.pem &gt;&gt; /etc/ssl/certs/ca-certificates.crt<span class="hljs-comment"># 重启 docker 服务</span>service docker restart</code></pre><h3 id="2-3、客户端测试"><a href="#2-3、客户端测试" class="headerlink" title="2.3、客户端测试"></a>2.3、客户端测试</h3><p>客户端直接登录，并 push 即可，<strong>如果原来修改过 <code>/etc/default/docker</code> 文件的，并加入了 <code>--insecure-registry</code> 选项的需要将其去除</strong></p><pre><code class="hljs sh"><span class="hljs-comment"># 登录 如果登录成功就代表没问题了</span>docker login 10.211.55.16<span class="hljs-comment"># push 测试</span>docker push 10.211.55.16/mritd/nginx:1.9</code></pre>]]></content>
    
    
    <categories>
      
      <category>Docker</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Linux</tag>
      
      <tag>Docker</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Nexus 私服使用教程</title>
    <link href="/2006/01/02/how-to-use-nexus/"/>
    <url>/2006/01/02/how-to-use-nexus/</url>
    
    <content type="html"><![CDATA[<h3 id="一、仓库简介"><a href="#一、仓库简介" class="headerlink" title="一、仓库简介"></a>一、仓库简介</h3><ul><li>默认情况下，Nexus有如下几个仓库<br><img src="https://cdn.oss.link/markdown/Maven-Nexus-allRepository-1024x286.png" srcset="/img/loading.gif" alt="仓库简介"></li></ul><ul><li><p>这些仓库按照其 Type 大致分为4种</p><ul><li><p>hosted类型</p><blockquote><p>这种类型的仓库只提供对内服务，也就是说他不面向公网</p></blockquote><pre><code class="hljs bash">Releases  <span class="hljs-comment">#存放 maven install 的 Releases 版本的jar</span>Snapshots <span class="hljs-comment">#存放 maven install 的 Snapshots 版本的jar</span>3rd party <span class="hljs-comment">#当某些jar我们无法从中央仓库下载时，我们往往从其官网下载，当我们希望此种jar也被Nexus管理时，则需要手动将其上传至此仓库，并定义坐标</span></code></pre></li><li><p>proxy类型</p><blockquote><p>这种类型则相对于hosted，只提供面向公网的服务，我们知道当Maven向私服请求的jar私服中木有时，私服会请求中央仓库；<br>此时则有这种proxy类型仓库请求并缓存下载下来的jar包</p></blockquote><pre><code class="hljs bash">Central <span class="hljs-comment">#中央仓库的代理仓库，主要负责请求中央仓库并缓存结果</span>Apache Snapshots <span class="hljs-comment">#从名字可以看出来只负责请求Apache 的Snapshots jar</span>Codehaus Snapshots <span class="hljs-comment"># 同上</span></code></pre></li><li><p>virtual 可忽略，基本用不到，面向Maven1类型……</p></li><li><p>group类型</p><blockquote><p>这种类型的仓库可看做是其他仓库的组合产品，默认的这个 Public Repository 可以下载其他几个仓库的jar；<br>实际就是将其他仓库组合起来，用一个地址就可下载组合仓库的jar，可通过菜单配置和新建。</p></blockquote></li></ul></li></ul><h3 id="二、Nexus使用的一些基本设置"><a href="#二、Nexus使用的一些基本设置" class="headerlink" title="二、Nexus使用的一些基本设置"></a>二、Nexus使用的一些基本设置</h3><ul><li><p>更改中央仓库地址为私服地址</p><blockquote><p>既然我们配置了私服，那么相应的，我们的项目就应该使用Nexus的地址(Public Repository)来下载jar包</p></blockquote><ul><li><p>基于POM文件的配置，只需在项目的pom文件中使用 Repositories 标签指定即可<br><img src="https://cdn.oss.link/markdown/Maven-Nexus-setRepository1.png" srcset="/img/loading.gif" alt="POM文件更改Mavne私服地址"></p></li><li><p>基于全局性的Maven setting.xml配置方式，这种方式会影响到全局的Maven(配置应该没问题，但布吉岛为啥我的eclipse不好使)<br><img src="https://cdn.oss.link/markdown/Maven-Nexus-settingnexus.png" srcset="/img/loading.gif" alt="setting.xml更改私服地址"></p></li><li><p>使用镜像来指定Maven私服<br><img src="https://cdn.oss.link/markdown/Maven-Nexus-Mirror.png" srcset="/img/loading.gif" alt="镜像修改私服地址"></p><blockquote><p>在实际使用中，不管是通过POM文件还是Maven的setting文件来指定Nexus私服，当私服出现问题不可访问时，Maven默认都会再次请求中央服务器，这可能导致jar的版本不一致问题，也就是说当私服挂掉后，开人员若不查看Maven控制台，则不可感知，相当于没有Nexus一样；为了避免这种情况，我们可以在Maven的setting配置文件中设置镜像站点；达到的效果就是Maven的所有请求必须经过镜像站点(Nexus);当Nexus出现问题后，那么Maven将强制不可用。</p></blockquote></li></ul></li></ul>]]></content>
    
    
    <categories>
      
      <category>Java</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Java</tag>
      
      <tag>Maven</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Java 内置命令-jps</title>
    <link href="/2006/01/02/java-builtin-command-jps/"/>
    <url>/2006/01/02/java-builtin-command-jps/</url>
    
    <content type="html"><![CDATA[<h2 id="jps-命令"><a href="#jps-命令" class="headerlink" title="jps 命令"></a>jps 命令</h2><blockquote><p>本文参考自 <a href="http://www.hollischuang.com/archives/105" target="_blank" rel="noopener">HollisChuang’s Blog</a></p></blockquote><h3 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h3><p>jps 命令用于查看当前 java 进程及其 pid 等相关信息，同 <code>ps -aux | grep java</code> 这种命令不同的是，<strong>jps 并不依赖于应用程序名来搜索进程(比如 <code>grep java</code> )</strong>； 这使得它可以显示出没有执行体的 java 进程；<strong>该命令 1.5+ 可用</strong>。</p><h3 id="原理"><a href="#原理" class="headerlink" title="原理"></a>原理</h3><p>在 Java 程序运行后，其会在 <code>java.io.tmpdir</code> 参数指定的位置生成一个 <code>hsperfdata_{UserName}</code> 的目录，该目录下会生成类似 Linux 程序运行时的二进制 pid 文件，jsp 命令实质上就是列出该目录下的所有文件；其相关信息通过解析该文件获取。在 Linux 系统下，这个目录位于 <code>/tmp/hsperfdata_{UserName}</code>，示例如下：</p><p><img src="https://cdn.oss.link/markdown/hexo_jps_dir.png" srcset="/img/loading.gif" alt="hexo_jps_dir"></p><h3 id="命令详解"><a href="#命令详解" class="headerlink" title="命令详解"></a>命令详解</h3><p>首先可以查看 jps 命令的帮助，获取相关信息，执行 <code>jps --help</code> 如下：</p><p><img src="https://cdn.oss.link/markdown/hexo_jps_help.png" srcset="/img/loading.gif" alt="hexo_jps_help.png"></p><h4 id="q-参数"><a href="#q-参数" class="headerlink" title="-q 参数"></a>-q 参数</h4><p><code>-q</code> 参数只显示pid，不显示class名称,jar文件名和传递给main 方法的参数</p><pre><code class="hljs sh">[mritd@iZ257uxyg2vZ ~]$ jps -q1743315645[mritd@iZ257uxyg2vZ ~]$</code></pre><h4 id="m-参数"><a href="#m-参数" class="headerlink" title="-m 参数"></a>-m 参数</h4><p><code>-m</code> 输出传递给 <code>main</code> 方法的参数，在嵌入式jvm上可能是null</p><pre><code class="hljs sh">[mritd@iZ257uxyg2vZ ~]$ jps -m17478 Jps -m15645 Bootstrap start[mritd@iZ257uxyg2vZ ~]$</code></pre><h4 id="l-参数"><a href="#l-参数" class="headerlink" title="-l 参数"></a>-l 参数</h4><p><code>-l</code> 参数输出应用程序main class的完整package名 或者 应用程序的jar文件完整路径名</p><pre><code class="hljs sh">[mritd@iZ257uxyg2vZ ~]$ jps -l17498 sun.tools.jps.Jps15645 org.apache.catalina.startup.Bootstrap[mritd@iZ257uxyg2vZ ~]$</code></pre><h4 id="v-参数"><a href="#v-参数" class="headerlink" title="-v 参数"></a>-v 参数</h4><p><code>-v</code> 参数输出传递给JVM的参数</p><pre><code class="hljs sh">[mritd@iZ257uxyg2vZ ~]$ jps -v15645 Bootstrap -Djava.util.logging.config.file=/home/mritd/usr/<span class="hljs-built_in">local</span>/apache-tomcat-8.0.32/conf/logging.properties -Djava.util.logging.manager=org.apache.juli.ClassLoaderLogManager -Djava.endorsed.dirs=/home/mritd/usr/<span class="hljs-built_in">local</span>/apache-tomcat-8.0.32/endorsed -Dcatalina.base=/home/mritd/usr/<span class="hljs-built_in">local</span>/apache-tomcat-8.0.32 -Dcatalina.home=/home/mritd/usr/<span class="hljs-built_in">local</span>/apache-tomcat-8.0.32 -Djava.io.tmpdir=/home/mritd/usr/<span class="hljs-built_in">local</span>/apache-tomcat-8.0.32/temp17518 Jps -Dapplication.home=/home/mritd/usr/<span class="hljs-built_in">local</span>/jdk1.8.0_73 -Xms8m[mritd@iZ257uxyg2vZ ~]$</code></pre><h4 id="V-参数"><a href="#V-参数" class="headerlink" title="-V 参数"></a>-V 参数</h4><p><code>-V</code> 参数输出通过标记的文件传递给JVM的参数( <code>.hotspotrc</code> 文件，或者是通过参数 <code>-XX:Flags=&lt;filename&gt;</code> 指定的文件)。</p><pre><code class="hljs sh">[mritd@iZ257uxyg2vZ ~]$ jps -V17595 Jps15645 Bootstrap[mritd@iZ257uxyg2vZ ~]$</code></pre><h3 id="hostid-详解"><a href="#hostid-详解" class="headerlink" title="hostid 详解"></a>hostid 详解</h3><p><code>hostid</code> 即 <strong>服务器标识</strong>，它指定了目标的服务器，它的语法如下：</p><pre><code class="hljs sh">[protocol:][[//]hostname][:port][/servername]</code></pre><ul><li>protocol: 如果 <code>protocol</code>及 <code>hostname</code> 都没有指定，那表示的是与当前环境相关的本地协议，如果指定了 <code>hostname</code> 却没有指定 <code>protocol</code>，那么 <code>protocol</code> 的默认就是 <code>rmi</code>。</li><li>hostname: 服务器的IP或者名称，没有指定则表示本机。</li><li>port: 远程 <code>rmi</code> 的端口，如果没有指定则默认为 <code>1099</code>。</li><li>Servername: 注册到 <code>RMI</code> 注册中心中的 <code>jstatd</code> 的名称。</li></ul>]]></content>
    
    
    <categories>
      
      <category>Java</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Java</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>JAVA 异常处理</title>
    <link href="/2006/01/02/java-exception/"/>
    <url>/2006/01/02/java-exception/</url>
    
    <content type="html"><![CDATA[<h2 id="一、异常分类"><a href="#一、异常分类" class="headerlink" title="一、异常分类"></a>一、异常分类</h2><p>在JAVA中，所有异常都由 <code>Throwable</code> 继承而来；但在下一层立即分解为两个分支：<code>Error</code> 和 <code>Exception</code>。</p><h3 id="Error"><a href="#Error" class="headerlink" title="Error"></a>Error</h3><blockquote><p>Error 类层次结构描述了JAVA运行时系统的内部错误和资源耗尽。应用程序不该抛出此种异常；如果出现了内部错误，除了告知用户和安全的退出，对于其他处理我们是无能为力的；比如出现 断电、硬盘损坏等等；但这种情况一般很少出现。</p></blockquote><h3 id="Exception"><a href="#Exception" class="headerlink" title="Exception"></a>Exception</h3><blockquote><p>在我们进行JAVA开发时，我们需要关注的是 Exception 的层次结构，Exception大致分为两个分支：RuntimeException 和 非RuntimeException。<strong>划分两个分支的规则是：由程序设计错误导致的异常为 RuntimeException；而程序本身没问题，由像 I/O 错误这类造成的异常属于非 RuntimeException。</strong></p></blockquote><p><strong>派生于RuntimeException的异常包含以下几种情况：</strong></p><ul><li>错误的类型转换</li><li>数组下标越界</li><li>使用了null引用(空指针)</li></ul><p><strong>其他非RuntimeException异常也有几种常见情况：</strong></p><ul><li>试图在文件尾部后读取数据</li><li>试图打开不存在的文件</li><li>通过字符串查找类，而类实际不存在(反射)</li></ul><p><strong>Java 异常层次图</strong></p><p><img src="https://cdn.oss.link/markdown/hexo_java_exception.jpeg" srcset="/img/loading.gif" alt="hexo_java_exception.jpeg"></p><p><strong>从上面可以看出，”一旦出现 RuntimeException 那么一定是你的问题！” 这句话很有道理。</strong>JAVA语言规范将 派生于Error类或RuntimeException类的异常称之为 <code>未检查异常(unchecked)</code>；其他异常称之为 <code>已检查异常(checked)</code>；对于已检查，编译器将要求提供对应的异常处理器。</p><h2 id="二、声明和抛出异常"><a href="#二、声明和抛出异常" class="headerlink" title="二、声明和抛出异常"></a>二、声明和抛出异常</h2><h3 id="异常的声明"><a href="#异常的声明" class="headerlink" title="异常的声明"></a>异常的声明</h3><blockquote><p>在JAVA中如果方法内存在可能出现的异常，那么方法不光要告诉编译器返回什么，同时还要告诉编译器有可能产生哪些异常，如果文件的I/O异常等。</p></blockquote><p><strong>方法在其首部通过 <code>throws</code> 关键字声明该方法内可能抛出的已检查异常，如下：</strong></p><pre><code class="hljs java"><span class="hljs-function"><span class="hljs-keyword">public</span> <span class="hljs-keyword">void</span> testException <span class="hljs-keyword">throws</span> <span class="hljs-title">FileNotFoundException</span><span class="hljs-params">()</span></span>&#123;    <span class="hljs-comment">// 其他代码...</span>&#125;</code></pre><blockquote><p>在继承关系中，<strong>子类声明的已检查异常不能比超类声明的已检查异常更加通用，简而言之就是子类声明的已检查异常范围不能大于父类声明的已检查异常范围，子类声明的已检查异常可以更精确。如果超类中某个方法未声明已检查异常，那么子类在重写该方法后同样不能声明任何已检查异常。</strong></p></blockquote><h3 id="异常的抛出"><a href="#异常的抛出" class="headerlink" title="异常的抛出"></a>异常的抛出</h3><blockquote><p>在JAVA中抛出一个异常可以通过 <code>throw</code> 关键字进行抛出。但需要注意的是，我们抛出异常是要注意异常的作用场景，比如绝对不能抛出 Error 这种异常，抛出异常的样例如下：</p></blockquote><pre><code class="hljs java"><span class="hljs-function"><span class="hljs-keyword">public</span> <span class="hljs-keyword">void</span> testException <span class="hljs-keyword">throws</span> <span class="hljs-title">FileNotFoundException</span><span class="hljs-params">()</span></span>&#123;   <span class="hljs-comment">// 其他逻辑代码</span>   <span class="hljs-keyword">throw</span> <span class="hljs-keyword">new</span> FileNotFoundException();&#125;</code></pre><p>在书写程序时，可能遇到所有标准异常类型都无法描述我们的异常，此时我们可以自定义一个异常类，并继承相应的父异常类；比如一个文件操作异常，如果要自定义的话我们应该继承自 IOException；一般这种需要自定义异常的情况很少见。</p><h2 id="三、捕获异常"><a href="#三、捕获异常" class="headerlink" title="三、捕获异常"></a>三、捕获异常</h2><h3 id="try-catch-捕获异常"><a href="#try-catch-捕获异常" class="headerlink" title="try-catch 捕获异常"></a>try-catch 捕获异常</h3><blockquote><p>当调用的方法声明(抛出)了一个已检查异常时，我们有两种选择：</p></blockquote><ul><li>1、继续向上(声明)抛出该已检查异常；</li><li>2、捕获该已检查异常。</li></ul><blockquote><p>通过 try-catch 语句块捕获异常的样例如下</p></blockquote><pre><code class="hljs java"><span class="hljs-keyword">try</span>&#123;    <span class="hljs-comment">// 调用声明已检查异常的方法相关代码</span>&#125;<span class="hljs-keyword">catch</span>(ExceptionType e)&#123;    <span class="hljs-comment">// 对应出现异常的后续处理</span>&#125;</code></pre><p><strong><code>try-catch</code> 语句块的作用是：</strong></p><ul><li>如果在 try 块中出现了在 catch 块中说明的异常(ExceptionType类型的异常)，那么 try 块中余下代码将不会被执行，程序将执行 catch 块中的处理代码。</li><li>如果在 try 块中没有出现异常，则程序正常执行，不会执行 catch 中的代码。</li><li>如果 try 块中出现了未在 catch 块中说明的异常，则程序将异常一直向上抛出，直到jvm处理；一般为jvm爆出错误信息，然后程序终止。</li></ul><h3 id="捕获多个异常"><a href="#捕获多个异常" class="headerlink" title="捕获多个异常"></a>捕获多个异常</h3><blockquote><p>try-catch 支持多异常的捕获，如下</p></blockquote><pre><code class="hljs java"><span class="hljs-keyword">try</span> &#123;    <span class="hljs-comment">// 异常代码</span>&#125; <span class="hljs-keyword">catch</span>(FileNotFoundException e)&#123;    <span class="hljs-comment">// 对应异常处理代码</span>&#125; <span class="hljs-keyword">catch</span>(EOFException e)&#123;    <span class="hljs-comment">// 对应异常处理代码</span>&#125; ...</code></pre><p>*<em>注意：在进行多异常捕获时，应尽量根据业务需要保证精确性；除非你知道你想要干什么，否则尽量不要直接捕获更高级的类似于 <code>Exception</code> 这种超类异常，因为这可能造成我们无法判定具体是 <code>FileNotFoundException</code> 还是 <code>EOFException</code>；对后续的处理会相当麻烦；除非真正的业务不关心具体细节；但还是那句话，异常捕获的界别取决于业务需要。 *</em></p><blockquote><p>在 Java SE 7中，同一 catch 块可以捕获 <strong>彼此不存在子类关系</strong> 的多个异常进行统一处理，从而达到合并 catch 块，样例如下：</p></blockquote><pre><code class="hljs java"><span class="hljs-keyword">try</span> &#123;    <span class="hljs-comment">// 异常代码</span>&#125; <span class="hljs-keyword">catch</span>(FileNotFoundException | EOFException e)&#123;    <span class="hljs-comment">// 对应异常处理代码</span>&#125; ...</code></pre><p><strong>在 catch 块中的异常变量 e 默认是 final 的；所以不能再 catch 块中为异常对象 e 重新赋值(改变引用指向)。</strong></p><h2 id="四、异常的重复抛出和异常连"><a href="#四、异常的重复抛出和异常连" class="headerlink" title="四、异常的重复抛出和异常连"></a>四、异常的重复抛出和异常连</h2><blockquote><p>实际上，在 catch 块中如初出现了新的异常或者有意的抛出新的异常，那么原本 try 块中的异常信息将被覆盖；这种情况一般一会出现，比如处理一些特定情况下的IO异常，样例如下：</p></blockquote><pre><code class="hljs java"><span class="hljs-keyword">try</span> &#123;    <span class="hljs-comment">// 异常代码</span>&#125; <span class="hljs-keyword">catch</span>(FileNotFoundException e)&#123;    <span class="hljs-comment">// 这里可能由于业务逻辑需要，我们并不关心具体是那种异常，我们只要知道是IO出了问题即可</span>    <span class="hljs-keyword">throw</span> <span class="hljs-keyword">new</span> IOException();&#125;</code></pre><p><strong>当然这里由于新抛出了IO异常，所以原来的具体异常信息可能丢失，一般我们会采用以下两种方案：</strong></p><ul><li>将原来的异常信息当做新异常的构造参数传入：<code>throw new IOException(&quot;文件未找到: &quot;+e.getMessage());</code></li><li>也可以调用新异常的 <code>newException.initCause(oldException)</code> 完成异常信息的封装；后续可通过 <code>newException.getCause();</code> 获得原始的异常信息。</li></ul><h2 id="五、finally-语句块"><a href="#五、finally-语句块" class="headerlink" title="五、finally 语句块"></a>五、finally 语句块</h2><blockquote><p>在异常处理机制中，允许使用 finally 语句块已完成一些必要的资源释放，比如关闭数据源、释放数据库连接等。</p></blockquote><pre><code class="hljs java"><span class="hljs-keyword">try</span> &#123;    <span class="hljs-comment">// 异常代码</span>&#125; <span class="hljs-keyword">catch</span>(Exception e)&#123;    <span class="hljs-comment">// 异常处理代码</span>&#125; <span class="hljs-keyword">finally</span>&#123;    <span class="hljs-comment">// 资源释放</span>&#125;</code></pre><p><strong>注意：</strong></p><ul><li>finally 语句块中的代码总被执行！</li><li>如果方法有返回值，finally 语句块中的代码总会在 try/catch 语句块中的 return 之前执行！</li><li>如果 finally 语句块中也包含 return 返回值，那么这个值将作为最终返回值 覆盖掉 try/catch 中的 return 返回值！</li><li>如果在 finally 语句块中执行相关操作(如资源释放)出现了异常，那么同 catch 块中一样，该异常将会覆盖原始异常！</li></ul><h2 id="六、Java-SE-7-带资源释放的-try-语句"><a href="#六、Java-SE-7-带资源释放的-try-语句" class="headerlink" title="六、Java SE 7 带资源释放的 try 语句"></a>六、Java SE 7 带资源释放的 try 语句</h2><blockquote><p>在 Java SE 7 中，如果资源类实现了 <code>AutoCloseable</code> 接口或其子接口(Closeable)，那么可以使用带资源的 try-catch 块来释放资源，省去 finally 语句块，示例如下：</p></blockquote><pre><code class="hljs java"><span class="hljs-comment">// 首先定义需要释放的资源</span><span class="hljs-keyword">try</span> (InputStream in = <span class="hljs-keyword">new</span> FileInputStream(<span class="hljs-string">"/testFile"</span>))&#123;    <span class="hljs-comment">// 异常代码</span>&#125; <span class="hljs-keyword">catch</span>(ExceptionType e)&#123;    <span class="hljs-comment">// 异常处理代码</span>&#125;</code></pre><h3 id="注意事项"><a href="#注意事项" class="headerlink" title="注意事项"></a>注意事项</h3><ul><li>1、catch 块中需要捕获 <strong>资源释放中的</strong> 已检查已检查异常；也就是说如果 try 块后面的括号内的异常也必须在 catch 中体现(捕获)；</li><li>2、同原来的 catch/finally 中出现异常不一样，资源释放中出现异常将自动被抑制，原来的会重新抛出，不会覆盖原来的异常信息；资源释放产生的异常通过 <code>addSuppressed</code> 方法添加到原来异常的抑制列表中，可通过 <code>getSuppressed</code> 方法重新获取资源释放时产生的异常信息。</li></ul>]]></content>
    
    
    <categories>
      
      <category>Java</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Java</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>List 的 UnsupportedOperationException 异常</title>
    <link href="/2006/01/02/java-list-unsupportedoperation-exception/"/>
    <url>/2006/01/02/java-list-unsupportedoperation-exception/</url>
    
    <content type="html"><![CDATA[<h2 id="一、前言"><a href="#一、前言" class="headerlink" title="一、前言"></a>一、前言</h2><p>今天偶尔测试 <code>List.remove()</code> 和 <code>List.add()</code> 方法很奇怪的出现了 <code>UnsupportedOperationException</code> 异常，但是 “某些情况下” 调用则不会出现这个异常，由于只考虑了局部代码，所以让我很困惑，以下分析一下 “搞笑的 UnsupportedOperationException” 异常。</p><h2 id="二、测试代码"><a href="#二、测试代码" class="headerlink" title="二、测试代码"></a>二、测试代码</h2><p><strong>首先一个正常的</strong></p><pre><code class="hljs java"><span class="hljs-comment">// 创建一个 ArrayList</span>List&lt;String&gt; list = <span class="hljs-keyword">new</span> ArrayList&lt;String&gt;();<span class="hljs-comment">// add 操作</span>list.add(<span class="hljs-string">"a"</span>);list.add(<span class="hljs-string">"b"</span>);list.add(<span class="hljs-string">"c"</span>);<span class="hljs-comment">// remove 操作</span>list.remove(<span class="hljs-number">2</span>);System.out.println(list);</code></pre><p><strong>接着一个报错的</strong></p><pre><code class="hljs java"><span class="hljs-comment">// 使用数组来创建 "ArrayList"</span>String s[] = &#123;<span class="hljs-string">"a"</span>,<span class="hljs-string">"b"</span>,<span class="hljs-string">"c"</span>&#125;;List&lt;String&gt; list = Arrays.asList(s);<span class="hljs-comment">// UnsupportedOperationException</span>list.add(<span class="hljs-string">"d"</span>);<span class="hljs-comment">// UnsupportedOperationException</span>list.remove(<span class="hljs-number">3</span>);</code></pre><h2 id="三、原因分析"><a href="#三、原因分析" class="headerlink" title="三、原因分析"></a>三、原因分析</h2><p>首先两个代码唯一差别就是第一个直接 <code>new ArrayList()</code>，第二个通过 <code>Arrays.asList()</code> 创建；</p><h3 id="1、Arrays-asList-方法"><a href="#1、Arrays-asList-方法" class="headerlink" title="1、Arrays.asList() 方法"></a>1、Arrays.asList() 方法</h3><p>查看源码如下：</p><pre><code class="hljs java"><span class="hljs-meta">@SafeVarargs</span><span class="hljs-meta">@SuppressWarnings</span>(<span class="hljs-string">"varargs"</span>)<span class="hljs-keyword">public</span> <span class="hljs-keyword">static</span> &lt;T&gt; <span class="hljs-function">List&lt;T&gt; <span class="hljs-title">asList</span><span class="hljs-params">(T... a)</span> </span>&#123;    <span class="hljs-keyword">return</span> <span class="hljs-keyword">new</span> ArrayList&lt;&gt;(a);&#125;</code></pre><p>乍眼一看，也是返回 <code>ArrayList</code>，这就搞笑了，同样是 <code>ArrayList</code> 一个报错一个不报错……</p><p>当点进去这个 <code>ArrayList</code> 构造方法后，<strong>实质上 <code>Arrays.aslist()</code> 方法 new 出的 <code>ArrayList</code> 并非 <code>java.util.ArrayList</code>，其实质是 <code>Arrays</code> 的内部类，</strong>如下：</p><pre><code class="hljs java"><span class="hljs-comment">/**</span><span class="hljs-comment"> * <span class="hljs-doctag">@serial</span> include</span><span class="hljs-comment"> */</span><span class="hljs-keyword">private</span> <span class="hljs-keyword">static</span> <span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">ArrayList</span>&lt;<span class="hljs-title">E</span>&gt; <span class="hljs-keyword">extends</span> <span class="hljs-title">AbstractList</span>&lt;<span class="hljs-title">E</span>&gt;</span><span class="hljs-class">    <span class="hljs-keyword">implements</span> <span class="hljs-title">RandomAccess</span>, <span class="hljs-title">java</span>.<span class="hljs-title">io</span>.<span class="hljs-title">Serializable</span></span><span class="hljs-class"></span>&#123;    <span class="hljs-keyword">private</span> <span class="hljs-keyword">static</span> <span class="hljs-keyword">final</span> <span class="hljs-keyword">long</span> serialVersionUID = -<span class="hljs-number">2764017481108945198L</span>;    <span class="hljs-keyword">private</span> <span class="hljs-keyword">final</span> E[] a;    ArrayList(E[] array) &#123;        a = Objects.requireNonNull(array);    &#125;    <span class="hljs-meta">@Override</span>    <span class="hljs-function"><span class="hljs-keyword">public</span> <span class="hljs-keyword">int</span> <span class="hljs-title">size</span><span class="hljs-params">()</span> </span>&#123;        <span class="hljs-keyword">return</span> a.length;    &#125;    <span class="hljs-meta">@Override</span>    <span class="hljs-keyword">public</span> Object[] toArray() &#123;        <span class="hljs-keyword">return</span> a.clone();    &#125;    <span class="hljs-meta">@Override</span>    <span class="hljs-meta">@SuppressWarnings</span>(<span class="hljs-string">"unchecked"</span>)    <span class="hljs-keyword">public</span> &lt;T&gt; T[] toArray(T[] a) &#123;        <span class="hljs-keyword">int</span> size = size();        <span class="hljs-keyword">if</span> (a.length &lt; size)            <span class="hljs-keyword">return</span> Arrays.copyOf(<span class="hljs-keyword">this</span>.a, size,                                 (Class&lt;? extends T[]&gt;) a.getClass());        System.arraycopy(<span class="hljs-keyword">this</span>.a, <span class="hljs-number">0</span>, a, <span class="hljs-number">0</span>, size);        <span class="hljs-keyword">if</span> (a.length &gt; size)            a[size] = <span class="hljs-keyword">null</span>;        <span class="hljs-keyword">return</span> a;    &#125;    <span class="hljs-meta">@Override</span>    <span class="hljs-function"><span class="hljs-keyword">public</span> E <span class="hljs-title">get</span><span class="hljs-params">(<span class="hljs-keyword">int</span> index)</span> </span>&#123;        <span class="hljs-keyword">return</span> a[index];    &#125;    <span class="hljs-meta">@Override</span>    <span class="hljs-function"><span class="hljs-keyword">public</span> E <span class="hljs-title">set</span><span class="hljs-params">(<span class="hljs-keyword">int</span> index, E element)</span> </span>&#123;        E oldValue = a[index];        a[index] = element;        <span class="hljs-keyword">return</span> oldValue;    &#125;    <span class="hljs-meta">@Override</span>    <span class="hljs-function"><span class="hljs-keyword">public</span> <span class="hljs-keyword">int</span> <span class="hljs-title">indexOf</span><span class="hljs-params">(Object o)</span> </span>&#123;        E[] a = <span class="hljs-keyword">this</span>.a;        <span class="hljs-keyword">if</span> (o == <span class="hljs-keyword">null</span>) &#123;            <span class="hljs-keyword">for</span> (<span class="hljs-keyword">int</span> i = <span class="hljs-number">0</span>; i &lt; a.length; i++)                <span class="hljs-keyword">if</span> (a[i] == <span class="hljs-keyword">null</span>)                    <span class="hljs-keyword">return</span> i;        &#125; <span class="hljs-keyword">else</span> &#123;            <span class="hljs-keyword">for</span> (<span class="hljs-keyword">int</span> i = <span class="hljs-number">0</span>; i &lt; a.length; i++)                <span class="hljs-keyword">if</span> (o.equals(a[i]))                    <span class="hljs-keyword">return</span> i;        &#125;        <span class="hljs-keyword">return</span> -<span class="hljs-number">1</span>;    &#125;    <span class="hljs-meta">@Override</span>    <span class="hljs-function"><span class="hljs-keyword">public</span> <span class="hljs-keyword">boolean</span> <span class="hljs-title">contains</span><span class="hljs-params">(Object o)</span> </span>&#123;        <span class="hljs-keyword">return</span> indexOf(o) != -<span class="hljs-number">1</span>;    &#125;    <span class="hljs-meta">@Override</span>    <span class="hljs-function"><span class="hljs-keyword">public</span> Spliterator&lt;E&gt; <span class="hljs-title">spliterator</span><span class="hljs-params">()</span> </span>&#123;        <span class="hljs-keyword">return</span> Spliterators.spliterator(a, Spliterator.ORDERED);    &#125;    <span class="hljs-meta">@Override</span>    <span class="hljs-function"><span class="hljs-keyword">public</span> <span class="hljs-keyword">void</span> <span class="hljs-title">forEach</span><span class="hljs-params">(Consumer&lt;? <span class="hljs-keyword">super</span> E&gt; action)</span> </span>&#123;        Objects.requireNonNull(action);        <span class="hljs-keyword">for</span> (E e : a) &#123;            action.accept(e);        &#125;    &#125;    <span class="hljs-meta">@Override</span>    <span class="hljs-function"><span class="hljs-keyword">public</span> <span class="hljs-keyword">void</span> <span class="hljs-title">replaceAll</span><span class="hljs-params">(UnaryOperator&lt;E&gt; operator)</span> </span>&#123;        Objects.requireNonNull(operator);        E[] a = <span class="hljs-keyword">this</span>.a;        <span class="hljs-keyword">for</span> (<span class="hljs-keyword">int</span> i = <span class="hljs-number">0</span>; i &lt; a.length; i++) &#123;            a[i] = operator.apply(a[i]);        &#125;    &#125;    <span class="hljs-meta">@Override</span>    <span class="hljs-function"><span class="hljs-keyword">public</span> <span class="hljs-keyword">void</span> <span class="hljs-title">sort</span><span class="hljs-params">(Comparator&lt;? <span class="hljs-keyword">super</span> E&gt; c)</span> </span>&#123;        Arrays.sort(a, c);    &#125;&#125;</code></pre><h3 id="2、AbstractList-抽象类"><a href="#2、AbstractList-抽象类" class="headerlink" title="2、AbstractList 抽象类"></a>2、AbstractList 抽象类</h3><p>通过对比代码发现 <code>java.util.ArrayList</code> 和 <code>Arrays$ArrayList</code> 同样继承自 <code>java.util.AbstractList</code>；而 <code>add()</code> 和 <code>remove()</code> 方法同样在此抽象类中定义，以下为两个方法在抽象类中的默认实现：</p><p><strong>add() 方法</strong></p><pre><code class="hljs java"><span class="hljs-function"><span class="hljs-keyword">public</span> <span class="hljs-keyword">boolean</span> <span class="hljs-title">add</span><span class="hljs-params">(E e)</span> </span>&#123;    add(size(), e);    <span class="hljs-keyword">return</span> <span class="hljs-keyword">true</span>;&#125;<span class="hljs-function"><span class="hljs-keyword">public</span> <span class="hljs-keyword">void</span> <span class="hljs-title">add</span><span class="hljs-params">(<span class="hljs-keyword">int</span> index, E element)</span> </span>&#123;    <span class="hljs-keyword">throw</span> <span class="hljs-keyword">new</span> UnsupportedOperationException();&#125;</code></pre><p><strong>remove() 方法</strong></p><pre><code class="hljs java"><span class="hljs-function"><span class="hljs-keyword">public</span> E <span class="hljs-title">remove</span><span class="hljs-params">(<span class="hljs-keyword">int</span> index)</span> </span>&#123;    <span class="hljs-keyword">throw</span> <span class="hljs-keyword">new</span> UnsupportedOperationException();&#125;</code></pre><h3 id="3、结论"><a href="#3、结论" class="headerlink" title="3、结论"></a>3、结论</h3><p><code>Arrays.asList</code> 最终返回的 <code>ArrayList</code> 实质上是其内部类，<code>java.util.Arrays$ArrayList</code> 和 <code>java.util.ArrayList</code> 全部继承自 <code>java.util.AbstractList</code> 抽象类，而 <code>java.util.AbstractList</code> 中默认的 <code>add()</code> 和 <code>remove()</code> 方法默认将抛出 <code>UnsupportedOperationException</code> 异常，不巧的是 <code>java.util.Arrays$ArrayList</code> 并未重写这两个方法，导致调用后抛出此异常。</p>]]></content>
    
    
    <categories>
      
      <category>Java</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Java</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Java 内存之直接内存</title>
    <link href="/2006/01/02/java-memory-direct-memory/"/>
    <url>/2006/01/02/java-memory-direct-memory/</url>
    
    <content type="html"><![CDATA[<h2 id="一、概念和特征"><a href="#一、概念和特征" class="headerlink" title="一、概念和特征"></a>一、概念和特征</h2><ul><li>直接内存并非 JVMS 定义的标准 Java 运行时内存。</li><li>JDK1.4 加入了新的 NIO 机制，目的是防止 Java 堆 和 Native 堆之间往复的数据复制带来的性能损耗，此后 NIO 可以使用 Native 的方式直接在 Native 堆分配内存。</li><li>直接内存区域是全局共享的内存区域。</li></ul><ul><li>直接内存区域可以进行自动内存管理(GC)，但机制并不完善。</li><li>本机的 Native 堆(直接内存) 不受 JVM 堆内存大小限制。</li><li>可能出现 OutOfMemoryError 异常。</li></ul><h2 id="二、异常演示"><a href="#二、异常演示" class="headerlink" title="二、异常演示"></a>二、异常演示</h2><p>测试代码：</p><pre><code class="hljs java"><span class="hljs-keyword">public</span> <span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">TestNativeHeap</span> </span>&#123;<span class="hljs-comment">/**</span><span class="hljs-comment"> * VM Args: -XX:MaxDirectMemorySize=10M</span><span class="hljs-comment"> */</span><span class="hljs-comment">// 每次内存分配大小</span><span class="hljs-keyword">private</span> <span class="hljs-keyword">static</span> <span class="hljs-keyword">int</span> _1M =<span class="hljs-number">1024</span>*<span class="hljs-number">1024</span>;<span class="hljs-function"><span class="hljs-keyword">public</span> <span class="hljs-keyword">static</span> <span class="hljs-keyword">void</span> <span class="hljs-title">main</span><span class="hljs-params">(String[] args)</span> <span class="hljs-keyword">throws</span> IllegalArgumentException, IllegalAccessException </span>&#123;Field unsafeFiled = Unsafe.class.getFields()[0];unsafeFiled.setAccessible(<span class="hljs-keyword">true</span>);Unsafe unsafe = (Unsafe) unsafeFiled.get(<span class="hljs-keyword">null</span>);<span class="hljs-keyword">while</span> (<span class="hljs-keyword">true</span>) &#123;unsafe.allocateMemory(_1M);&#125;&#125;&#125;</code></pre><p>以上代码运行会出现 OutOfMemoryError 异常，原因是不断申请内存将超出 Native 堆限制。</p><p><strong>直接内存导致的 OutOfMemoryError 异常，在异常信息中不会有明显的堆栈区错误提示；同时另一大特点就是内存转储文件(dump)出来会非常小，如果项目中出现这种情况，并且直接或者间接地使用了 NIO 技术，那么应该考虑是否为直接内存导致的内存溢出。</strong></p>]]></content>
    
    
    <categories>
      
      <category>Java</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Java</tag>
      
      <tag>JMM</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Java 内存之 Java 堆</title>
    <link href="/2006/01/02/java-memory-java-heap/"/>
    <url>/2006/01/02/java-memory-java-heap/</url>
    
    <content type="html"><![CDATA[<h2 id="一、基本概念和特性"><a href="#一、基本概念和特性" class="headerlink" title="一、基本概念和特性"></a>一、基本概念和特性</h2><ul><li>Java 堆内存是全局共享的</li><li>Java 堆通常是 JVM 中最大的一块内存区域</li><li>Java 堆得主要作用是用于存放创建的对象实例</li><li>JVMS 明确要求，此区域必须实现内存自动管理，即 GC；但不要求具体的 GC 实现，包括实现算法和技术</li><li>Java 堆可以在物理上不连续空间分配，只要逻辑上连续即可</li><li>Java 堆可能出现 OutOfMemoryError 异常</li><li>Java 堆可以使固定大小的，也可以实现为动态扩展的，当前主流 JVM 都是可扩展的</li></ul><p><strong>注意：虽然 Java 堆是全集共享的，但是为了避免并发吗创建对象实例进行堆内存分配时的竞争关系，很有可能在 Java 堆内存在私有的线程分配缓冲区(TLAB 线程本地分配缓冲)，当 TLAB 不够用时才会加锁向 Java 堆申请更多的内存。</strong></p><p><strong>由 C/C++ 延伸的 “堆内存比占内存更快、对象分配在堆/栈内存” 等相关讨论都是没有意义的，因为 JVM 规范中，所有对象都必须在堆内分配。</strong></p><h2 id="二、对象分配过程"><a href="#二、对象分配过程" class="headerlink" title="二、对象分配过程"></a>二、对象分配过程</h2><p>Java 堆分配对象实例的图例大致如下：</p><p><img src="https://cdn.oss.link/markdown/hexo_java_jmm_heap1.png" srcset="/img/loading.gif" alt="Java 堆对象分配1"></p><p><strong>左侧代码会让 JVM 编译时在 Java 虚拟机栈的局部变量表中预留一个 Solt，且类型为 reference；右侧的代码在运行时会在 Java 堆中创建对象实例，并在对象头中存放该对象对应数据类型的指针，其指向方法区中的对象数据类型。</strong></p><h2 id="三、JVM-内存布局选择"><a href="#三、JVM-内存布局选择" class="headerlink" title="三、JVM 内存布局选择"></a>三、JVM 内存布局选择</h2><p><strong>一般在 JVM 实现时有两种内存布局方案可以选择：</strong></p><p>第一种：如上图所示，根据 JMM 规范要求 “一个 reference 至少能间接或者直接的查找到该对象实例的内存地址和对应数据类型”；那么局部变量表中存储的 Solt 中的 reference 将直接指向 Java 堆中的对象实例，而该对象实例头部指针指向方法区中的对象类型数据。</p><p><strong>第二种：如下图，局部变量表中的 reference 指向 Java 堆内句柄池中的某个句柄，这个句柄同时存储对象实例的指针和对象类型数据的指针，然后两个指针分别指向对象实例数据和对象类型数据。</strong></p><p><img src="https://cdn.oss.link/markdown/hexo_java_jmm_heap2.png" srcset="/img/loading.gif" alt="Java 堆对象分配2"></p><p><strong>两种内存布局各有好处，第二种内存布局保证了局部变量表中 reference 指向的稳定性，因为其一直指向 Java 堆中的句柄，当 GC 时对象的频繁移动导致对象内存地址移动时，不会影响局部变量表中的 reference 的改变；而第一种内存布局的好处在于查找对象速度会很快，对比第二种 reference 引用句柄的方式，reference 直接指向对象实例在查找对象时节省了一次指针切换开销。Oracle HotSpot 采用的是第一种内存布局。</strong></p><h2 id="四、异常"><a href="#四、异常" class="headerlink" title="四、异常"></a>四、异常</h2><p><strong>如果实际所需的堆大小超过了自动内存管理所能提供的容量，那么 JVM 将抛出 <code>OutOfMemoryError</code> 异常；该区域是最大出现内存异常的区域。</strong></p><p><strong>-Xms和-Xmx 用于设置 Java 堆的最大和最小内存；如果两个值相同，则证明 Java 堆不允许扩展。</strong></p><p><strong>-XX:+HeapDumpOnOutOfMemoryError 用于指定当 Java 堆出现内存溢出时，将当前内存映像信息 Dump 到磁盘镜像中，供后续分析。</strong></p>]]></content>
    
    
    <categories>
      
      <category>Java</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Java</tag>
      
      <tag>JMM</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Java 内存之方法区和运行时常量池</title>
    <link href="/2006/01/02/java-memory-method-area-and-runtime-constant-pool/"/>
    <url>/2006/01/02/java-memory-method-area-and-runtime-constant-pool/</url>
    
    <content type="html"><![CDATA[<h2 id="一、相关特征"><a href="#一、相关特征" class="headerlink" title="一、相关特征"></a>一、相关特征</h2><h3 id="1、方法区特征"><a href="#1、方法区特征" class="headerlink" title="1、方法区特征"></a>1、方法区特征</h3><ul><li>同 Java 堆一样，方法区也是全局共享的一块内存区域</li><li>方法区的作用是存储 Java 类的结构信息，当我们创建对象实例后，<strong>对象的类型信息存储在方法堆之中，实例数据存放在堆中；实例数据指的是在 Java 中创建的各种实例对象以及它们的值，类型信息指的是定义在 Java 代码中的常量、静态变量、以及在类中声明的各种方法、方法字段等等；同事可能包括即时编译器编译后产生的代码数据。</strong></li></ul><ul><li>JVMS 不要求该区域实现自动的内存管理，但是商用 JVM 一般都已实现该区域的自动内存管理。</li><li>方法区分配内存可以不连续，可以动态扩展。</li><li>该区域并非像 JMM 规范描述的那样数据一旦放进去就属于 “永久代”；<strong>在该区域进行内存回收的主要目的是对常量池的回收和对内存数据的卸载；一般来说这个区域的内存回收效率比起 Java 堆要低得多。</strong></li><li>当方法区无法满足内存需求时，将抛出 OutOfMemoryError 异常。</li></ul><h3 id="2、运行时常量池的特征"><a href="#2、运行时常量池的特征" class="headerlink" title="2、运行时常量池的特征"></a>2、运行时常量池的特征</h3><ul><li><strong>运行时常量池是方法区的一部分，</strong>所以也是全局共享的。</li><li><strong>其作用是存储 Java 类文件常量池中的符号信息。</strong></li><li><strong>class 文件中存在常量池(非运行时常量池)，其在编译阶段就已经确定；JVM 规范对 class 文件结构有着严格的规范，必须符合此规范的 class 文件才会被 JVM 认可和装载。</strong></li><li><strong>运行时常量池</strong> 中保存着一些 class 文件中描述的符号引用，同时还会将这些符号引用所翻译出来的直接引用存储在 <strong>运行时常量池</strong> 中。</li><li><strong>运行时常量池相对于 class 常量池一大特征就是其具有动态性，Java 规范并不要求常量只能在运行时才产生，也就是说运行时常量池中的内容并不全部来自 class 常量池，class 常量池并非运行时常量池的唯一数据输入口；在运行时可以通过代码生成常量并将其放入运行时常量池中。</strong></li><li>同方法区一样，当运行时常量池无法申请到新的内存时，将抛出 OutOfMemoryError 异常。</li></ul><h2 id="二、HotSpot-方法区变迁"><a href="#二、HotSpot-方法区变迁" class="headerlink" title="二、HotSpot 方法区变迁"></a>二、HotSpot 方法区变迁</h2><h3 id="1、JDK1-2-JDK6"><a href="#1、JDK1-2-JDK6" class="headerlink" title="1、JDK1.2 ~ JDK6"></a>1、JDK1.2 ~ JDK6</h3><p>在 JDK1.2 ~ JDK6 的实现中，HotSpot 使用永久代实现方法区；HotSpot 使用 GC 分代实现方法区带来了很大便利；</p><h3 id="2、JDK7"><a href="#2、JDK7" class="headerlink" title="2、JDK7"></a>2、JDK7</h3><p>由于 GC 分代技术的影响，使之许多优秀的内存调试工具无法在 Oracle HotSpot之上运行，必须单独处理；并且 Oracle 同时收购了 BEA 和 Sun 公司，同时拥有 JRockit 和 HotSpot，在将 JRockit 许多优秀特性移植到 HotSpot 时由于 GC 分代技术遇到了种种困难，<strong>所以从 JDK7 开始 Oracle HotSpot 开始移除永久代。</strong></p><p><strong>JDK7中符号表被移动到 Native Heap中，字符串常量和类引用被移动到 Java Heap中。</strong></p><h3 id="3、JDK8"><a href="#3、JDK8" class="headerlink" title="3、JDK8"></a>3、JDK8</h3><p><strong>在 JDK8 中，永久代已完全被元空间(Meatspace)所取代。</strong></p><h2 id="三、永久代变迁产生的影响"><a href="#三、永久代变迁产生的影响" class="headerlink" title="三、永久代变迁产生的影响"></a>三、永久代变迁产生的影响</h2><h4 id="1、测试代码1"><a href="#1、测试代码1" class="headerlink" title="1、测试代码1"></a>1、测试代码1</h4><pre><code class="hljs java"><span class="hljs-keyword">public</span> <span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">Test1</span> </span>&#123;    <span class="hljs-function"><span class="hljs-keyword">public</span> <span class="hljs-keyword">static</span> <span class="hljs-keyword">void</span> <span class="hljs-title">main</span><span class="hljs-params">(String[] args)</span> </span>&#123; String s1 = <span class="hljs-keyword">new</span> StringBuilder(<span class="hljs-string">"漠"</span>).append(<span class="hljs-string">"然"</span>).toString(); System.out.println(s1.intern() == s1); String s2 = <span class="hljs-keyword">new</span> StringBuilder(<span class="hljs-string">"漠"</span>).append(<span class="hljs-string">"然"</span>).toString(); System.out.println(s2.intern() == s2);&#125;&#125;</code></pre><p>以上代码，在 JDK6 下执行结果为 false、false，在 JDK7 以上执行结果为 true、false。</p><p><strong>首先明确两点：</strong><br>1、在 Java 中直接使用双引号展示的字符串将会在常量池中直接创建。<br>2、String 的 intern 方法首先将尝试在常量池中查找该对象，如果找到则直接返回该对象在常量池中的地址；找不到则将该对象放入常量池后再返回其地址。<strong>JDK6 常量池在方法区，频繁调用该方法可能造成 OutOfMemoryError。</strong></p><p><strong>产生两种结果的原因：</strong></p><p>在 JDK6 下 s1、s2 指向的是新创建的对象，<strong>该对象将在 Java Heap 中创建，所以 s1、s2 指向的是 Java Heap 中的内存地址；</strong>调用 intern 方法后将尝试在常量池中查找该对象，没找到后将其放入常量池并返回，<strong>所以此时 s1/s2.intern() 指向的是常量池中的地址，JDK6常量池在方法区，与堆隔离，；所以 s1.intern()==s1 返回false。</strong></p><h4 id="2、测试代码2"><a href="#2、测试代码2" class="headerlink" title="2、测试代码2"></a>2、测试代码2</h4><pre><code class="hljs java"><span class="hljs-keyword">public</span> <span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">Test2</span> </span>&#123;<span class="hljs-function"><span class="hljs-keyword">public</span> <span class="hljs-keyword">static</span> <span class="hljs-keyword">void</span> <span class="hljs-title">main</span><span class="hljs-params">(String[] args)</span> </span>&#123;<span class="hljs-comment">/**</span><span class="hljs-comment"> * 首先设置 持久代最大和最小内存占用(限定为10M)</span><span class="hljs-comment"> * VM args: -XX:PermSize=10M -XX:MaxPremSize=10M</span><span class="hljs-comment"> */</span>List&lt;String&gt; list  = <span class="hljs-keyword">new</span> ArrayList&lt;String&gt;();<span class="hljs-comment">// 无限循环 使用 list 对其引用保证 不被GC  intern 方法保证其加入到常量池中</span><span class="hljs-keyword">int</span> i = <span class="hljs-number">0</span>;<span class="hljs-keyword">while</span> (<span class="hljs-keyword">true</span>) &#123;    <span class="hljs-comment">// 此处永久执行，最多就是将整个 int 范围转化成字符串并放入常量池</span>list.add(String.valueOf(i++).intern());&#125;&#125;&#125;</code></pre><p>以上代码在 JDK6 下会出现 Perm 内存溢出，JDK7 or high 则没问题。</p><p><strong>原因分析：</strong></p><p><strong>JDK6 常量池存在方法区，设置了持久代大小后，不断while循环必将撑满 Perm 导致内存溢出；JDK7 常量池被移动到 Native Heap(Java Heap)，所以即使设置了持久代大小，也不会对常量池产生影响；不断while循环在当前的代码中，所有int的字符串相加还不至于撑满 Heap 区，所以不会出现异常。</strong></p>]]></content>
    
    
    <categories>
      
      <category>Java</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Java</tag>
      
      <tag>JMM</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Java 内存自动管理-虚拟机和内存区域概述</title>
    <link href="/2006/01/02/java-memory-overview-of-vm-memory-auto-management-and-memory-regions/"/>
    <url>/2006/01/02/java-memory-overview-of-vm-memory-auto-management-and-memory-regions/</url>
    
    <content type="html"><![CDATA[<blockquote><p>本文参考 <a href="http://www.jikexueyuan.com/course/1793_1.html?ss=2" target="_blank" rel="noopener">JVM自动内存管理：内存区域基础概念</a></p></blockquote><h2 id="一、虚拟机及其定义"><a href="#一、虚拟机及其定义" class="headerlink" title="一、虚拟机及其定义"></a>一、虚拟机及其定义</h2><h3 id="1、虚拟机概述"><a href="#1、虚拟机概述" class="headerlink" title="1、虚拟机概述"></a>1、虚拟机概述</h3><p><strong>虚拟机：</strong> 模拟某种计算机体系结构，执行特定指令的软件；虚拟机一般分为 <strong>系统虚拟机、进程虚拟机。</strong></p><p><strong>系统虚拟机：</strong>如 <code>Virtual Box</code>、<code>VMware</code> 等，完整的模拟整个操作系统。</p><p><strong>进程虚拟机：</strong>如 <code>JVM</code>、<code>Adobe Flash Player</code>、<code>FC模拟器</code> 等，<strong>进程虚拟机不会完整的模拟系统，而只是模拟某个特定指令级的虚拟机。</strong></p><p><strong>高级语言虚拟机：</strong>如 <code>JVM</code>、<code>.NET CLR</code>、<code>P-Code</code> 等，高级语言虚拟机属于进程虚拟机的一种，只不过更加细致化，将模拟某个指令级限定为某个高级语言。</p><p><strong>Java 语言虚拟机：</strong>将高级语言限定为 Java 语言，<strong>但是并不一定所有的 Java 虚拟机都能称之为 JVM；</strong></p><p><strong>JVM：</strong></p><ul><li>必须通过 <code>Java TCK</code> (Technology Compatibility Kit) 的兼容性测试才能称之为 JVM。</li><li>JVM 并非一定执行 “Java” 程序，JVM 面对的是 class 字节码；比如 Scala 语言，其生成也是 class ，便可运行在 JVM 上。</li><li>业界三大商用虚拟机：Oracle HotSpot、Oracle JRockit VM、IBM J9 VM</li></ul><p><strong>Java 虚拟机概念遵循 “公有设计，私有实现”，即设计规范必须遵循，但是具体实现可以不同，只要在外表上一致即可，比如 JVM 规范要求内存可以自动释放，但每个虚拟机所采用的 GC 算法实现可能不尽相同；但最终要保证和达到的目的就是：同一份代码不做任何修改，在不同 JVM 上都能正确运行，</strong></p><h3 id="2、Java虚拟机运行时数据区"><a href="#2、Java虚拟机运行时数据区" class="headerlink" title="2、Java虚拟机运行时数据区"></a>2、Java虚拟机运行时数据区</h3><p><strong>JVM 数据区定义：</strong></p><p>在 JVM 规范中定义了若干种程序运行时需要的存储不同数据类型的数据区域；某些区域是全局共享的，随着虚拟机启动而创建，随着虚拟机关闭而销毁；某系区域是线程私有的，随着线程启动而创建，随着线程停止而销毁。所有的 Java 虚拟机都遵从该规范。</p><p><strong>JVM 数据区划分：</strong></p><p><strong>Java 虚拟机数据区主要分为5大区块：程序计数器、Java 堆、Java 虚拟机栈、本地方法栈、方法区</strong>；图例如下：</p><p><img src="https://cdn.oss.link/markdown/hexo_java_VM_data_area.png" srcset="/img/loading.gif" alt="数据区截图"></p><p>从上图可以看出，<strong>方法区和堆是所有线程共享的，虚拟机栈、本地方法栈、程序计数器是每个线程私有的。</strong></p><p><strong>程序计数器：</strong></p><p>程序计数器是一块非常小的内存空间，其作用是记录当前线程执行字节码的行号。<strong>如果当前线程执行的是一个 Java 方法，则该区域记录的是 class 地址，如果执行的是一个 本地的 Native 方法，则此区域为空。此区域是唯一一个在 JVM 规范中没有规定内存溢出等内存异常的区域。</strong></p><h2 id="二、Java-虚拟机栈和本地方法栈"><a href="#二、Java-虚拟机栈和本地方法栈" class="headerlink" title="二、Java 虚拟机栈和本地方法栈"></a>二、Java 虚拟机栈和本地方法栈</h2><h3 id="1、Java-虚拟机栈的特征"><a href="#1、Java-虚拟机栈的特征" class="headerlink" title="1、Java 虚拟机栈的特征"></a>1、Java 虚拟机栈的特征</h3><ul><li>线程私有：Java虚拟机栈是线程私有的，其生命周期与线程相同；Java 虚拟机栈描述的是 Java 方法执行时的内存模型；<strong>每个方法在执行时都会创建一个栈帧，用来保存这个方法的操作数栈、局部变量表、方法出口、动态链接等信息；每个方法运行的过程就对应了一个栈帧在 Java 虚拟机栈中入栈和出栈的过程。</strong></li><li>后进先出(LIFO)栈：Java 虚拟机栈是一个后进先出的栈，后面进入的栈帧会优先出栈；</li><li>存储栈帧：每个 Java 方法的调用、执行和退出，都和存储的栈帧有着密切的联系。</li><li>异常：<strong>JVM 规范对此定义了两种异常状况：OutOfMemoryError、StackOverflowError；</strong>如果线程请求的栈深度大于 JVM 允许的栈深度，就会抛出 StackOverflowError异常。</li></ul><h3 id="2、本地方法栈的特征"><a href="#2、本地方法栈的特征" class="headerlink" title="2、本地方法栈的特征"></a>2、本地方法栈的特征</h3><ul><li>线程私有：同上</li><li>后进先出(LIFO)栈：同上</li><li>作用是支撑 Native 的调用、执行和退出</li><li>异常：同上</li></ul><p><strong>本地方法栈实现各不相同，如 Oracle HotSport 直接将 Java 虚拟机栈和 本地方法栈合二为一。</strong></p><h3 id="3、栈帧的概念和特征"><a href="#3、栈帧的概念和特征" class="headerlink" title="3、栈帧的概念和特征"></a>3、栈帧的概念和特征</h3><ul><li>栈帧被设计为用于存储数据和部分过程结果的数据结构，同时也被用于存储动态链接、方法返回值和异常分派。</li><li>每一个完整的栈帧都包含：局部变量表、操作数栈、动态链接信息、方法正常完成和异常完成信息；<strong>在程序运行时，栈帧需要多大的局部变量表、多深的操作数栈在编译期就已经确定了；Java 编译器会将其写入 class 字节码的 code 表中。</strong></li><li>在程序执行时，方法调用链可能会很长，很多方法都处于执行状态，对于执行引擎而言，<strong>在活动线程中只有位于 Java 虚拟机栈栈顶的栈帧才是有效的；</strong>该栈帧被称为当前栈帧，与之对应的关联方法被称之为当前方法，虚拟机执行中所有执行的字节码指令都针对当前栈帧和当前方法进行操作；</li></ul><p><strong>关于局部变量表和操作数栈大体内存图如下：</strong></p><p><img src="https://cdn.oss.link/markdown/hexo_java_jmm_jvm_stack.png" srcset="/img/loading.gif" alt="Java 虚拟机栈内存图"></p><h3 id="4、局部变量表"><a href="#4、局部变量表" class="headerlink" title="4、局部变量表"></a>4、局部变量表</h3><p><strong>定义：</strong>局部变量表是一组局部变量值得存储空间，用于存储方法参数、方法内部定义的变量等；</p><p><strong>在 JVM 编译时，就已经在 class code表中确定了该方法所需要局部变量表的最大容量；局部变量表以变量槽(Slot)为最小单位，JVM 规范并未明确说明一个 Solt 所占用的空间大小。</strong>只是非常有导向性的描述了一个 Solt 应该能存放一个布尔型、字节型、字符型、短整型、整型、浮点型、引用型(<br>reference)、返回地址(returnAddress)型的数据(两个 Solat 可以存放 long 或 double 型的数据)；在这8中数据类型中，他们都可以使用32位甚至更小的内存空间来存储；这种规定允许 Solt 的大小随着不同 JVM 实现、不同操作系统、甚至是不同硬件如 CPU 等而变化，但是必须保证每个 JVM 上栈帧的 Solt 至少在外观上保持一致。</p><ul><li>reference：即对象实例引用类型，JVM 规范并未完全说明该类型应该是何种数据结构甚至占用空间大小等，<strong>但此类型至少能完成2件事：1、通过该 reference 至少能直接或者间接的找到该引用引用的对象在 Java 堆中实例的具体内存地址；2、通过该 reference 能直接或者间接的找到这个对象所属的数据类型在 Java 方法区中的类型信息。</strong></li><li>returnAddress：即返回地址型，该种类型目前已经不被使用，以前主要是为了字节码的3条指令：jsr、jsrw、ret服务的；他指向了一条字节码指令的地址，在以前的 JVM 中使用这几条命令和 returnAddress 实现异常处理。</li><li>long &amp; double：同 JMM 规范类似，在 Solt 中同样允许对64位的 long、double类型分成2个32位的操作进行，<strong>不同的是由于局部变量表建立在 Java 虚拟机栈之上，属于线程私有，所以不会产生原子性的线程安全问题，同时 JVM 也不允许通过 class 直接访问 Solt 中的 long 和 double 类型。</strong></li></ul><p>JVM 通过索引值方式定位局部变量表中的数据，<strong>索引值从0开始，到局部变量表最大范围结束，</strong>对于32位的类型数据，索引n就代表访问第n个Solt中的数据，<strong>对于64位的类型数据，索引n代表第n和n+1个Solt中的数据。对于64位的数据，JVM不允许通过任何方式单独访问其中一个 Solt 数据。</strong></p><p><strong>局部变量表用于方法间参数传递，以及方法执行过程中存储基础数据类型和对象引用；如果正在执行的方法是实例方法(new 的对象里的)，那么该方法的局部变量表第0个Solt将存储该方法所对应的实例引用；可以通过关键字 bis(不一定对) 来访问这个隐含参数，其他变量按顺序存储在局部变量表中；为节省空间，Solt可以被重用，如果方法体中定义的变量作用域，如果当前程序计数器范围查过了其作用域，那么该变量所占用的局部变量表中的 Solt 将被释放并重用；</strong></p><h3 id="5、操作数栈"><a href="#5、操作数栈" class="headerlink" title="5、操作数栈"></a>5、操作数栈</h3><p><strong>定义：</strong>操作数栈也称操作栈，与局部变量表相同，其大小在编译期确定。</p><p>操作数栈由若干个 Entry 组成，JVM 规范定义，在操作数栈中，任意一个栈元素(Entry)都可以存储任意数据类型的数据，包括64位的 long 和 double；但普通32位的数据类型 Entry 深度为1，long 和 double Entry 深度为2；</p><p>在方法执行过程中，操作数栈栈帧用于存储计算参数和计算结果；在方法调用时，<strong>操作数栈也用来准备调用方法的参数以及接收返回值；在方法刚开始执行时，操作数栈是空的，在执行过程中，各种不同的字节码指令向操作数栈中写入和读取数据。</strong>比如一个int型加法操作，JVM必须保证操作数栈栈最接近栈顶的两个元素都是int型，相加的过程对应两个元素出栈相加并入栈的过程。<strong>出栈的数据类型必须与字节码的类型严格匹配，在编译时，编译器会严格检查该项。</strong></p><p>在 Java 虚拟机栈中，往往两个栈帧是相互独立的，<strong>但在 JVM 实现中，往往对其进行优化，比如让两个逻辑上独立的栈帧出现一部分重叠，让下面栈帧的操作数栈和上面栈帧的局部变量表重叠，这样在方法互相调用参数传递时他们就共用一部分数据，从而避免了数据复制带来的性能损失。</strong></p><blockquote><p>可参考视频 <a href="http://www.jikexueyuan.com/course/1793_2.html?ss=2" target="_blank" rel="noopener">Java 虚拟机栈和本地方法栈</a> ，<strong>个人理解：局部变量表类似于方法执行时的永久存储区，而操作数栈类似于temp缓存区，所有运算借助操作数栈完成，最终结果返回到方法局部变量表。</strong></p></blockquote><h3 id="6、异常情况"><a href="#6、异常情况" class="headerlink" title="6、异常情况"></a>6、异常情况</h3><ul><li>如果线程请求的栈容量超出 Java 虚拟机栈容量，那么将抛出 <code>StackOverflowError</code> 异常。</li><li>如果 Java 虚拟机栈可以动态扩展，但尝试扩展的动作无法申请到足够的内存去完成扩展，或者在建立新线程时无法获得足够的内存去创建对应的虚拟机栈，那么将抛出 <code>OutOfMemoryError</code> 异常。</li><li><strong>JVM 参数：-Xss用于控制 Java 虚拟机栈的栈深度，注意：Oracle HotSpot 虚拟机 Java 虚拟机栈和本地方法栈是一个，所以此参数将控制两个栈的总体栈深度。</strong></li></ul>]]></content>
    
    
    <categories>
      
      <category>Java</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Java</tag>
      
      <tag>JMM</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Java 多线程可见性问题</title>
    <link href="/2006/01/02/java-multithreading-variable-visibility/"/>
    <url>/2006/01/02/java-multithreading-variable-visibility/</url>
    
    <content type="html"><![CDATA[<h2 id="一、相关定义"><a href="#一、相关定义" class="headerlink" title="一、相关定义"></a>一、相关定义</h2><h3 id="1、可见性"><a href="#1、可见性" class="headerlink" title="1、可见性"></a>1、可见性</h3><p>在多线程中，如果一个线程对某一 <strong>共享变量</strong> 的修改，<strong>能及时被其他线程所感知</strong>，这个特性或者说过程称之为线程可见性。</p><h3 id="2、共享变量"><a href="#2、共享变量" class="headerlink" title="2、共享变量"></a>2、共享变量</h3><p>当多线程同时操作一个变量时，该变量在多线程的 <strong>工作内存(私有内存)</strong> 中都存在一个副本，那么这个变量称之为这几个线程的共享变量。</p><h3 id="3、工作内存"><a href="#3、工作内存" class="headerlink" title="3、工作内存"></a>3、工作内存</h3><p>多线程工作时，每个线程都会复制主内存中的变量副本到自己的私有内存，这个私有内存称之为每个线程的工作内存。</p><h2 id="二、Java-内存模型-Java-Memory-Model"><a href="#二、Java-内存模型-Java-Memory-Model" class="headerlink" title="二、Java 内存模型(Java Memory Model)"></a>二、Java 内存模型(Java Memory Model)</h2><blockquote><p><strong>Java 内存模型(JMM) 描述了 Java 程序中对线程共享变量的访问规则，以及在 JVM 层面对内存读写变量的底层细节，详细可参考 <a href="http://www.infoq.com/cn/articles/java-memory-model-1" target="_blank" rel="noopener">InfoQ-深入理解Java内存模型</a></strong></p></blockquote><h3 id="1、概述"><a href="#1、概述" class="headerlink" title="1、概述"></a>1、概述</h3><p>简单地说，Java 内存模型规定，所有实例域、静态域和数组元素存储在堆内存中，堆内存在线程之间共享，局部变量(Local variables)，方法定义参数(java语言规范称之为formal method parameters)和异常处理器参数(exception handler parameters)不会在线程之间共享；运行时将所有变量放入 <strong>主内存中</strong>，同时在多线程访问的情况下，每个线程会开辟自己的 <strong>工作内存(抽象)</strong>；<strong>工作内存中用于存放该线程所用到的每个主内存中的变量副本</strong>，图例如下：</p><p><img src="https://cdn.oss.link/markdown/hexo_java_thread_memory1.png" srcset="/img/loading.gif" alt="Java 内存模型1"></p><h3 id="2、线程读写变量规则"><a href="#2、线程读写变量规则" class="headerlink" title="2、线程读写变量规则"></a>2、线程读写变量规则</h3><p>JMM 规定，多线程情况下，每个线程对变量的操作 <strong>必须在自己的工作内存</strong> 中完成，<strong>不允许线程直接对主内存进行操作</strong>。</p><p>多线程变量传递需要借助主内存中转；<strong>也就是说，当A线程需要改变变量值时，需要先改变当前工作内存中的变量，再将其刷新到主内存，最后通过主内存刷新到线程B的工作内存。</strong>图例如下：</p><p><img src="https://cdn.oss.link/markdown/hexo_java_thread_memory2.png" srcset="/img/loading.gif" alt="Java 内存模型2"></p><p><img src="https://cdn.oss.link/markdown/hexo_java_thread_memory3.png" srcset="/img/loading.gif" alt="Java 内存模型3"></p><h2 id="三、synchronized-实现可见性"><a href="#三、synchronized-实现可见性" class="headerlink" title="三、synchronized 实现可见性"></a>三、synchronized 实现可见性</h2><p>Java 中对代码块或方法使用 <code>symchroized</code> 关键字，可保证其内部的共享变量实现多线程可见性。</p><h3 id="1、JMM-对-synchroized-相关规定"><a href="#1、JMM-对-synchroized-相关规定" class="headerlink" title="1、JMM 对 synchroized 相关规定"></a>1、JMM 对 synchroized 相关规定</h3><ul><li>线程解锁前，必须将工作内存中数据刷新到主内存。</li><li>线程加锁时，必须先清空工作内存，然后将主内存数据刷新到工作内存</li></ul><h3 id="2、线程执行互斥代码过程"><a href="#2、线程执行互斥代码过程" class="headerlink" title="2、线程执行互斥代码过程"></a>2、线程执行互斥代码过程</h3><ul><li>1、线程在 <code>synchroized</code> 入口处获得互斥锁</li><li>2、线程清空自己的工作内存</li><li>3、线程将主内存数据刷新到工作内存</li><li>4、线程执行互斥代码</li><li>5、线程将工作内存数据刷新到主内存</li><li>6、线程退出 <code>synchroized</code> 代码块并释放互斥锁</li></ul><h3 id="3、指令重排序"><a href="#3、指令重排序" class="headerlink" title="3、指令重排序"></a>3、指令重排序</h3><blockquote><p>具体可参考 <a href="http://www.infoq.com/cn/articles/java-memory-model-2" target="_blank" rel="noopener">InfoQ 深入理解Java内存模型（二）-重排序</a></p></blockquote><p><strong>定义：</strong>指令重排序，简单地说就是编译器和CPU为了提高并行执行速度，进行的代码重排序执行。</p><p>在CPU执行层面，每执行一个指令也会类似 Java 语言的 I/O 操作，在某一个命令未执行完成时必须进入等待(阻塞状态)，然后执行下一个命令；而编译器和CPU为了最大限度利用有限时间执行更多的任务，可能会进行指令重排序；指令重排序结果如下：</p><pre><code class="hljs java"><span class="hljs-comment">// 原有代码</span><span class="hljs-keyword">int</span> a = <span class="hljs-number">10</span>;<span class="hljs-keyword">int</span> b = <span class="hljs-number">20</span>;</code></pre><p>在编译器和CPU优化后，重排序可能会出现如下结果：</p><pre><code class="hljs java"><span class="hljs-comment">// 重排序后</span><span class="hljs-keyword">int</span> b = <span class="hljs-number">20</span>;<span class="hljs-keyword">int</span> a = <span class="hljs-number">10</span>;</code></pre><p><strong>但是指令重排序会遵循一点：as-if-serail 语义，即在单线程的情况下，保证最终执行结果不变，这时才会进行指令重排序；</strong>如上所示，a与b哪个先定义都不会产生结果的变更时才会重排序。</p><h3 id="4、代码示例"><a href="#4、代码示例" class="headerlink" title="4、代码示例"></a>4、代码示例</h3><pre><code class="hljs java"><span class="hljs-keyword">package</span> mkw.demo.syn;<span class="hljs-keyword">public</span> <span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">SynchronizedDemo</span> </span>&#123;<span class="hljs-comment">//共享变量</span>    <span class="hljs-keyword">private</span> <span class="hljs-keyword">boolean</span> ready = <span class="hljs-keyword">false</span>;    <span class="hljs-keyword">private</span> <span class="hljs-keyword">int</span> result = <span class="hljs-number">0</span>;    <span class="hljs-keyword">private</span> <span class="hljs-keyword">int</span> number = <span class="hljs-number">1</span>;       <span class="hljs-comment">//写操作</span>    <span class="hljs-function"><span class="hljs-keyword">public</span> <span class="hljs-keyword">void</span> <span class="hljs-title">write</span><span class="hljs-params">()</span></span>&#123;    ready = <span class="hljs-keyword">true</span>;       <span class="hljs-comment">//1.1</span>    number = <span class="hljs-number">2</span>;                      <span class="hljs-comment">//1.2    </span>    &#125;    <span class="hljs-comment">//读操作</span>    <span class="hljs-function"><span class="hljs-keyword">public</span> <span class="hljs-keyword">void</span> <span class="hljs-title">read</span><span class="hljs-params">()</span></span>&#123;        <span class="hljs-keyword">if</span>(ready)&#123;         <span class="hljs-comment">//2.1</span>    result = number*<span class="hljs-number">3</span>; <span class="hljs-comment">//2.2</span>    &#125;       System.out.println(<span class="hljs-string">"result的值为："</span> + result);    &#125;    <span class="hljs-comment">//内部线程类</span>    <span class="hljs-keyword">private</span> <span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">ReadWriteThread</span> <span class="hljs-keyword">extends</span> <span class="hljs-title">Thread</span> </span>&#123;    <span class="hljs-comment">//根据构造方法中传入的flag参数，确定线程执行读操作还是写操作</span>    <span class="hljs-keyword">private</span> <span class="hljs-keyword">boolean</span> flag;    <span class="hljs-function"><span class="hljs-keyword">public</span> <span class="hljs-title">ReadWriteThread</span><span class="hljs-params">(<span class="hljs-keyword">boolean</span> flag)</span></span>&#123;    <span class="hljs-keyword">this</span>.flag = flag;    &#125;        <span class="hljs-meta">@Override</span>                                                                            <span class="hljs-function"><span class="hljs-keyword">public</span> <span class="hljs-keyword">void</span> <span class="hljs-title">run</span><span class="hljs-params">()</span> </span>&#123;        <span class="hljs-keyword">if</span>(flag)&#123;        <span class="hljs-comment">//构造方法中传入true，执行写操作</span>        write();        &#125;<span class="hljs-keyword">else</span>&#123;        <span class="hljs-comment">//构造方法中传入false，执行读操作</span>        read();        &#125;        &#125;    &#125;    <span class="hljs-function"><span class="hljs-keyword">public</span> <span class="hljs-keyword">static</span> <span class="hljs-keyword">void</span> <span class="hljs-title">main</span><span class="hljs-params">(String[] args)</span>  </span>&#123;    SynchronizedDemo synDemo = <span class="hljs-keyword">new</span> SynchronizedDemo();    <span class="hljs-comment">//启动线程执行写操作</span>    synDemo .<span class="hljs-keyword">new</span> ReadWriteThread(<span class="hljs-keyword">true</span>).start();    <span class="hljs-comment">//启动线程执行读操作</span>    synDemo.<span class="hljs-keyword">new</span> ReadWriteThread(<span class="hljs-keyword">false</span>).start();    &#125;&#125;</code></pre><p>运行 mian 方法，由于可见性问题，可能出现多种情况，比如 写线程执行到 1.1，读线程获取CPU资源立即执行、写线程指令 1.1、1.2重排序等等情况。</p><h2 id="四、volatile-实现可见性控制"><a href="#四、volatile-实现可见性控制" class="headerlink" title="四、volatile 实现可见性控制"></a>四、volatile 实现可见性控制</h2><h3 id="1、实现原理"><a href="#1、实现原理" class="headerlink" title="1、实现原理"></a>1、实现原理</h3><p>volatile 通过加入内存屏障和禁止指令重排序实现可见性控制，其过程大致分为以下两个过程：</p><ul><li>对 volatile 变量进行写操作时，会在写操作后加入一条 <code>store</code> 屏蔽指令；<strong>该命令会将工作内存中内容强制刷新到主内存(覆盖)；同时会防止编译器/CPU指令重排序时将前面的变量重排到 volatile 修饰的变量之后(禁止颠倒顺序)。</strong></li><li>对 volatile 变量进行读操作时，会在读操作前加入一条 <code>load</code> 屏蔽指令；<strong>该指令会强制使工作内存失效，从而达到必须从主内存刷新到工作内存的效果</strong></li></ul><p>总结：使用 valotile 修饰变量后，线程在每次读取该变量时，会强制从主内存刷新最新状态到工作内存；在每次写入该变量时，会强制从工作内存刷新到主内存，以保证线程可见性。</p><h3 id="2、volatile-不能保证原子性"><a href="#2、volatile-不能保证原子性" class="headerlink" title="2、volatile 不能保证原子性"></a>2、volatile 不能保证原子性</h3><p>由上可知 volatile 在保证可见性的原理大致和 synchroized 类似，主要是控制工作内存与主内存的刷新关系；但是相对于 synchroized ，volatile 不能保证原子性操作，测试代码如下：</p><pre><code class="hljs java"><span class="hljs-keyword">package</span> mkw.demo.vol;<span class="hljs-keyword">import</span> java.util.concurrent.locks.Lock;<span class="hljs-keyword">import</span> java.util.concurrent.locks.ReentrantLock;<span class="hljs-keyword">public</span> <span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">VolatileDemo</span> </span>&#123;        <span class="hljs-comment">// 测试变量</span><span class="hljs-keyword">private</span> <span class="hljs-keyword">volatile</span> <span class="hljs-keyword">int</span> number = <span class="hljs-number">0</span>;<span class="hljs-function"><span class="hljs-keyword">public</span> <span class="hljs-keyword">int</span> <span class="hljs-title">getNumber</span><span class="hljs-params">()</span></span>&#123;<span class="hljs-keyword">return</span> <span class="hljs-keyword">this</span>.number;&#125;<span class="hljs-function"><span class="hljs-keyword">public</span> <span class="hljs-keyword">void</span> <span class="hljs-title">increase</span><span class="hljs-params">()</span></span>&#123;<span class="hljs-keyword">try</span> &#123;Thread.sleep(<span class="hljs-number">100</span>);&#125; <span class="hljs-keyword">catch</span> (InterruptedException e) &#123;e.printStackTrace();&#125;<span class="hljs-keyword">this</span>.number++;&#125;<span class="hljs-function"><span class="hljs-keyword">public</span> <span class="hljs-keyword">static</span> <span class="hljs-keyword">void</span> <span class="hljs-title">main</span><span class="hljs-params">(String[] args)</span> </span>&#123;<span class="hljs-comment">// TODO Auto-generated method stub</span><span class="hljs-keyword">final</span> VolatileDemo volDemo = <span class="hljs-keyword">new</span> VolatileDemo();<span class="hljs-keyword">for</span>(<span class="hljs-keyword">int</span> i = <span class="hljs-number">0</span> ; i &lt; <span class="hljs-number">500</span> ; i++)&#123;<span class="hljs-keyword">new</span> Thread(<span class="hljs-keyword">new</span> Runnable() &#123;<span class="hljs-meta">@Override</span><span class="hljs-function"><span class="hljs-keyword">public</span> <span class="hljs-keyword">void</span> <span class="hljs-title">run</span><span class="hljs-params">()</span> </span>&#123;volDemo.increase();&#125;&#125;).start();&#125;<span class="hljs-comment">//如果还有子线程在运行，主线程就让出CPU资源，</span><span class="hljs-comment">//直到所有的子线程都运行完了，主线程再继续往下执行</span><span class="hljs-keyword">while</span>(Thread.activeCount() &gt; <span class="hljs-number">1</span>)&#123;Thread.yield();&#125;System.out.println(<span class="hljs-string">"number : "</span> + volDemo.getNumber());&#125;&#125;</code></pre><p>当以上代码运行多次后，number 打印的值会出现很多小于500的情况；其原因是 volatile 无法保证 number++ 这行代码的原子性操作；实质上 <code>number++</code> 执行了3个动作，首先读取 number 值，然后进行+1，最后写会 number；</p><p>首先假设 number=2，在多线程并发的情况下，A线程由于 volatile 原因，首先将 number 从主内存强制刷新到工作内存，然后进行 +1 操作，此时A线程工作内存中 number=3，当要回写到工作内存时。线程 B 获取了CPU资源开始执行；不难想象，B线程对 number+1 后，可能被A线程覆盖掉；此时 volatile 无法保证原子性的问题就暴露了出来。</p><h3 id="3、volatile-适用场景"><a href="#3、volatile-适用场景" class="headerlink" title="3、volatile 适用场景"></a>3、volatile 适用场景</h3><ul><li>对变量的写不依赖于当前值；即直接强制写，同时写的值跟上一次没关系。</li><li>该变量不包含在其他变量的不变式中；即与其他 volatile 变量没关系。</li></ul><h3 id="4、volatile-与-synchroized-比较"><a href="#4、volatile-与-synchroized-比较" class="headerlink" title="4、volatile 与 synchroized 比较"></a>4、volatile 与 synchroized 比较</h3><p>在可见性上，volatile 与 synchroized 基本是相同的；但是在原子性上，volatile不支持；<strong>同时 synchroized 会执行加锁动作，开销较大，而 volatile 更轻量级。</strong></p>]]></content>
    
    
    <categories>
      
      <category>Java</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Java</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>JAXB 笔记</title>
    <link href="/2006/01/02/jaxb-note/"/>
    <url>/2006/01/02/jaxb-note/</url>
    
    <content type="html"><![CDATA[<h2 id="一、简介"><a href="#一、简介" class="headerlink" title="一、简介"></a>一、简介</h2><p>JAXB（Java Architecture for XML Binding简称JAXB）允许Java开发人员将Java类映射为XML表示方式。JAXB提供两种主要特性：将一个Java对象序列化为XML，以及反向操作，将XML解析成Java对象。换句话说，JAXB允许以XML格式存储和读取数据，而不需要程序的类结构实现特定的读取XML和保存XML的代码！</p><p>JAXB 原本是 J2EE 的一部分，从 JDK6 开始，JAXB 已经加入到 J2SE 中，以下为目前各个版本 JDK 中 JAXB 版本：</p><ul><li>J2SE 6 : JAXB 2.0 (JSR 222)</li><li>J2SE 7 : JAXB 2.2.3 (JSR 222, maintenance release 2)</li><li>J2SE 8 : JAXB 2.2.8</li></ul><h2 id="二、JAXB-与-DOM-SAX-JDOM-DOM4J-等比较"><a href="#二、JAXB-与-DOM-SAX-JDOM-DOM4J-等比较" class="headerlink" title="二、JAXB 与 DOM/SAX/JDOM/DOM4J 等比较"></a>二、JAXB 与 DOM/SAX/JDOM/DOM4J 等比较</h2><p>JAXB 致力于解决 XML 与 Java 之间的对象映射关系，而 DOM/SAX 等是解析 XML 的底层 API；从这里可以看出两者出发点有着本质不同，<strong>JAXB 底层也会调用 SAX 来解析 XML 文件</strong>。单纯的解析 XML 文件时，我们要考虑使用 DOM/SAX 等解析技术，而涉及到 XML 到 Java 对象映射时，我们应当考虑使用 JAXB 实现。</p><h2 id="三、Object2XML-编组-marshal"><a href="#三、Object2XML-编组-marshal" class="headerlink" title="三、Object2XML 编组(marshal)"></a>三、Object2XML 编组(marshal)</h2><p>从 Java 对象转化到 XML 文件的过程我们称之为 <strong>编组(malshal)</strong>；以下为代码示例：</p><p><strong>Persion 对象 带转化的 Object</strong></p><pre><code class="hljs java"><span class="hljs-keyword">package</span> me.mritd.test;<span class="hljs-keyword">import</span> javax.xml.bind.annotation.XmlRootElement;<span class="hljs-comment">/**</span><span class="hljs-comment"> * Created by mritd on 16/4/7.</span><span class="hljs-comment"> */</span><span class="hljs-comment">// JAXB 注解,标注根节点名称</span><span class="hljs-meta">@XmlRootElement</span>(name = <span class="hljs-string">"Persion"</span>)<span class="hljs-keyword">public</span> <span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">Persion</span> </span>&#123;    <span class="hljs-keyword">private</span> String name;    <span class="hljs-keyword">private</span> String address;    <span class="hljs-keyword">private</span> <span class="hljs-keyword">int</span> age;    <span class="hljs-comment">// 省略 SET GET 方法</span>&#125;</code></pre><p><strong>编组(Marshal)测试类</strong></p><pre><code class="hljs java"><span class="hljs-keyword">public</span> <span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">JaxbMarshalTest</span> </span>&#123;    <span class="hljs-function"><span class="hljs-keyword">public</span> <span class="hljs-keyword">static</span> <span class="hljs-keyword">void</span> <span class="hljs-title">main</span><span class="hljs-params">(String[] args)</span> </span>&#123;        <span class="hljs-comment">// 首先 创建一个 待编组的 Object</span>        Persion persion = <span class="hljs-keyword">new</span> Persion(<span class="hljs-string">"name"</span>,<span class="hljs-string">"beijing"</span>,<span class="hljs-number">10</span>);        <span class="hljs-keyword">try</span> &#123;            <span class="hljs-comment">// 初始化 marshal</span>            JAXBContext jaxbContext = JAXBContext.newInstance(Persion<span class="hljs-class">.<span class="hljs-keyword">class</span>)</span>;            Marshaller marshaller = jaxbContext.createMarshaller();            <span class="hljs-comment">// 设置格式化输出 xml</span>            marshaller.setProperty(Marshaller.JAXB_FORMATTED_OUTPUT,<span class="hljs-keyword">true</span>);            <span class="hljs-comment">// 执行编组</span>            marshaller.marshal(persion,<span class="hljs-keyword">new</span> File(<span class="hljs-string">"Persion.xml"</span>));            <span class="hljs-comment">// 输出到控制台</span>            marshaller.marshal(persion,System.out);        &#125; <span class="hljs-keyword">catch</span> (JAXBException e) &#123;            e.printStackTrace();        &#125;    &#125;</code></pre><h2 id="四、Object2XML-解组-反编组-UnMarshal"><a href="#四、Object2XML-解组-反编组-UnMarshal" class="headerlink" title="四、Object2XML 解组/反编组(UnMarshal)"></a>四、Object2XML 解组/反编组(UnMarshal)</h2><p>从已有的 XML 文件转化为 Object 对象的过程我们称之为解组/反编组(UnMarshal)，以下伪代码示例：</p><pre><code class="hljs java"><span class="hljs-keyword">public</span> <span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">JaxbUnMarshalTest</span> </span>&#123;    <span class="hljs-function"><span class="hljs-keyword">public</span> <span class="hljs-keyword">static</span> <span class="hljs-keyword">void</span> <span class="hljs-title">main</span><span class="hljs-params">(String[] args)</span> </span>&#123;        <span class="hljs-comment">// 实例化文件</span>        File file = <span class="hljs-keyword">new</span> File(<span class="hljs-string">"Persion.xml"</span>);        <span class="hljs-keyword">try</span> &#123;            <span class="hljs-comment">// 创建 UnMarshaller</span>            JAXBContext jaxbContext = JAXBContext.newInstance(Persion<span class="hljs-class">.<span class="hljs-keyword">class</span>)</span>;            Unmarshaller unmarshaller = jaxbContext.createUnmarshaller();            <span class="hljs-comment">// 解组 操作</span>            Persion persion = (Persion) unmarshaller.unmarshal(file);            System.out.println(persion);        &#125; <span class="hljs-keyword">catch</span> (JAXBException e) &#123;            e.printStackTrace();        &#125;    &#125;&#125;</code></pre><h2 id="五、适配器-Adapters"><a href="#五、适配器-Adapters" class="headerlink" title="五、适配器(Adapters)"></a>五、适配器(Adapters)</h2><p>在普通的 POJO 与 XML 互相转化时，简单的编组和解组已经完全可以满足需要，但是当包含复杂类型的 POJO 等情况出现时，组需要使用适配器加以辅助编组与解组，样例代码如下：</p><p>首先定义的 Persion 中有一属性为 LocalDate 类型，此类型为复杂类型，需要定义适配器：</p><p><strong>LocalDate Adapter</strong></p><pre><code class="hljs java"><span class="hljs-keyword">public</span> <span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">DateAdapter</span> <span class="hljs-keyword">extends</span> <span class="hljs-title">XmlAdapter</span>&lt;<span class="hljs-title">String</span>, <span class="hljs-title">LocalDate</span>&gt; </span>&#123;    <span class="hljs-meta">@Override</span>    <span class="hljs-function"><span class="hljs-keyword">public</span> LocalDate <span class="hljs-title">unmarshal</span><span class="hljs-params">(String v)</span> <span class="hljs-keyword">throws</span> Exception </span>&#123;        <span class="hljs-keyword">return</span> LocalDate.parse(v);    &#125;    <span class="hljs-meta">@Override</span>    <span class="hljs-function"><span class="hljs-keyword">public</span> String <span class="hljs-title">marshal</span><span class="hljs-params">(LocalDate v)</span> <span class="hljs-keyword">throws</span> Exception </span>&#123;        <span class="hljs-keyword">return</span> v.toString();    &#125;&#125;</code></pre><p><strong>POJO 如下</strong></p><pre><code class="hljs java"><span class="hljs-comment">// JAXB 注解,标注根节点名称</span><span class="hljs-meta">@XmlRootElement</span>(name = <span class="hljs-string">"Persion"</span>)<span class="hljs-keyword">public</span> <span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">Persion</span> </span>&#123;    <span class="hljs-keyword">private</span> String name;    <span class="hljs-keyword">private</span> String address;    <span class="hljs-keyword">private</span> <span class="hljs-keyword">int</span> age;    <span class="hljs-keyword">private</span> LocalDate date;    <span class="hljs-comment">// 自定义适配器</span>    <span class="hljs-meta">@XmlJavaTypeAdapter</span>(DateAdapter<span class="hljs-class">.<span class="hljs-keyword">class</span>)</span><span class="hljs-class">    <span class="hljs-title">public</span> <span class="hljs-title">LocalDate</span> <span class="hljs-title">getDate</span>() </span>&#123;        <span class="hljs-keyword">return</span> date;    &#125;    <span class="hljs-comment">// 省略其他 SET GET 方法</span>&#125;</code></pre><p><strong>测试类</strong></p><pre><code class="hljs java"><span class="hljs-keyword">public</span> <span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">PersionMarshalAndUnMarshal</span> </span>&#123;    <span class="hljs-function"><span class="hljs-keyword">public</span> <span class="hljs-keyword">static</span> <span class="hljs-keyword">void</span> <span class="hljs-title">main</span><span class="hljs-params">(String[] args)</span> </span>&#123;        <span class="hljs-comment">// 创建对象</span>        Persion persion = <span class="hljs-keyword">new</span> Persion(<span class="hljs-string">"mritd"</span>,<span class="hljs-string">"beijing"</span>,<span class="hljs-number">18</span>,LocalDate.of(<span class="hljs-number">2016</span>,<span class="hljs-number">4</span>,<span class="hljs-number">8</span>));        <span class="hljs-keyword">try</span> &#123;            <span class="hljs-comment">// 创建 marshaller</span>            JAXBContext jaxbContext = JAXBContext.newInstance(Persion<span class="hljs-class">.<span class="hljs-keyword">class</span>)</span>;            Marshaller marshaller = jaxbContext.createMarshaller();            <span class="hljs-comment">// 创建编组文件</span>            File file = <span class="hljs-keyword">new</span> File(<span class="hljs-string">"Persion.xml"</span>);            <span class="hljs-comment">// 执行编组</span>            marshaller.setProperty(Marshaller.JAXB_FORMATTED_OUTPUT,<span class="hljs-keyword">true</span>);            marshaller.marshal(persion,file);            marshaller.marshal(persion,System.out);            <span class="hljs-comment">// 创建 unMarshaller</span>            Unmarshaller unmarshaller = jaxbContext.createUnmarshaller();            Persion unPersion = (Persion)unmarshaller.unmarshal(file);            System.out.println(unPersion);        &#125; <span class="hljs-keyword">catch</span> (JAXBException e) &#123;            e.printStackTrace();        &#125;    &#125;&#125;</code></pre><h2 id="六、其他参考"><a href="#六、其他参考" class="headerlink" title="六、其他参考"></a>六、其他参考</h2><ul><li>JAXB 常用注解 <a href="http://www.cnblogs.com/fragranting/archive/2012/03/25/xml--jaxb.html" target="_blank" rel="noopener">http://www.cnblogs.com/fragranting/archive/2012/03/25/xml–jaxb.html</a></li><li>JAXB使用教程一 <a href="http://www.blogways.net/blog/2015/05/05/jaxb-tutorial-1.html" target="_blank" rel="noopener">http://www.blogways.net/blog/2015/05/05/jaxb-tutorial-1.html</a></li><li>使用JAXB2.0实现OXM <a href="http://my.oschina.net/zzx0421/blog/98186" target="_blank" rel="noopener">http://my.oschina.net/zzx0421/blog/98186</a></li></ul>]]></content>
    
    
    <categories>
      
      <category>Java</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Java</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>JDK8 新特性</title>
    <link href="/2006/01/02/jdk8-new-features/"/>
    <url>/2006/01/02/jdk8-new-features/</url>
    
    <content type="html"><![CDATA[<h2 id="一、接口的默认方法实现"><a href="#一、接口的默认方法实现" class="headerlink" title="一、接口的默认方法实现"></a>一、接口的默认方法实现</h2><p>在 JDK8 中，接口允许有默认的实现方法，这些默认的实现方法需要使用 <code>default</code> 关键字修饰即可，同时其实现类可直接调用这些方法；代码示例如下：</p><h3 id="1、自定义接口"><a href="#1、自定义接口" class="headerlink" title="1、自定义接口"></a>1、自定义接口</h3><pre><code class="hljs java"><span class="hljs-keyword">public</span> <span class="hljs-class"><span class="hljs-keyword">interface</span> <span class="hljs-title">TestInterface</span> </span>&#123;    <span class="hljs-comment">// 接口方法</span>    <span class="hljs-function"><span class="hljs-keyword">public</span> <span class="hljs-keyword">void</span> <span class="hljs-title">test</span><span class="hljs-params">()</span></span>;    <span class="hljs-comment">// 接口默认方法1</span>    <span class="hljs-function"><span class="hljs-keyword">default</span> <span class="hljs-keyword">public</span> <span class="hljs-keyword">void</span> <span class="hljs-title">defaultMethod1</span><span class="hljs-params">()</span></span>&#123;        System.out.println(<span class="hljs-string">"Hello World!"</span>);    &#125;    <span class="hljs-comment">// 接口默认方法2</span>    <span class="hljs-function"><span class="hljs-keyword">default</span> <span class="hljs-keyword">public</span> <span class="hljs-keyword">void</span> <span class="hljs-title">defaultMethod2</span><span class="hljs-params">()</span></span>&#123;        System.out.println(<span class="hljs-string">"World Hello!"</span>);    &#125;&#125;</code></pre><h3 id="2、实现类及测试"><a href="#2、实现类及测试" class="headerlink" title="2、实现类及测试"></a>2、实现类及测试</h3><pre><code class="hljs java"><span class="hljs-keyword">public</span> <span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">TestInterfaceImpl</span> <span class="hljs-keyword">implements</span> <span class="hljs-title">TestInterface</span> </span>&#123;    <span class="hljs-meta">@Override</span>    <span class="hljs-function"><span class="hljs-keyword">public</span> <span class="hljs-keyword">void</span> <span class="hljs-title">test</span><span class="hljs-params">()</span> </span>&#123;        System.out.println(<span class="hljs-string">"Test 方法实现"</span>);    &#125;    <span class="hljs-function"><span class="hljs-keyword">public</span> <span class="hljs-keyword">static</span> <span class="hljs-keyword">void</span> <span class="hljs-title">main</span><span class="hljs-params">(String[] args)</span> </span>&#123;        TestInterfaceImpl testInterface = <span class="hljs-keyword">new</span> TestInterfaceImpl();        testInterface.test();        <span class="hljs-comment">// default 修饰的默认方法可直接调用</span>        testInterface.defaultMethod1();        testInterface.defaultMethod2();    &#125;&#125;</code></pre><h3 id="3、默认方法的继承"><a href="#3、默认方法的继承" class="headerlink" title="3、默认方法的继承"></a>3、默认方法的继承</h3><p>同普通的类一样，接口的默认方法也可以被继承；而一旦被继承就会出现继承的顺序即优先级问题，接口的默认方法继承遵续以下几个原则：</p><ul><li>1、当某一个类实现了某一接口后，那么该类便继承了该接口的默认方法。</li><li>2、当某一个类实现某一接口，同时又继承另一个类，而另一个类中有一方法和接口中的默认方法相同，那么最终以类中的方法为准，即 <strong>类中的方法优先级高于接口中的方法。</strong></li><li>3、当某一类实现了两个接口，而两个接口中又有同方法签名的默认方法时，必须显示在该类中覆盖两个接口中的同方法签名的方法，否则编译不通过。</li><li>4、当某一类实现了两个接口，而其中一个接口的父接口，与另一个接口中的默认方法方法签名一致时，同样需要显示覆盖，否则编译报错。</li><li>5、当某一类实现了两个接口，而其中一个接口的父接口，与另一个接口中的默认方法方法签名一致，同时该接口又覆盖了父类的默认方法，此时也必须显式覆盖该方法，否则编译报错。</li></ul><h4 id="3-1、第2中情况测试-1省略"><a href="#3-1、第2中情况测试-1省略" class="headerlink" title="3.1、第2中情况测试(1省略)"></a>3.1、第2中情况测试(1省略)</h4><p><strong>有一默认方法的接口</strong></p><pre><code class="hljs java"><span class="hljs-keyword">public</span> <span class="hljs-class"><span class="hljs-keyword">interface</span> <span class="hljs-title">TestIntefaceDefaultMethod</span> </span>&#123;    <span class="hljs-comment">// 接口中的默认 test 方法</span>    <span class="hljs-function"><span class="hljs-keyword">default</span> <span class="hljs-keyword">public</span> <span class="hljs-keyword">void</span> <span class="hljs-title">test</span><span class="hljs-params">()</span></span>&#123;        System.out.println(<span class="hljs-string">"TestIntefaceDefaultMethod"</span>);    &#125;&#125;</code></pre><p><strong>有一普通方法的父类</strong></p><pre><code class="hljs java"><span class="hljs-keyword">public</span> <span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">TestInterfaceDefaultMethodClazz</span> </span>&#123;    <span class="hljs-comment">// 类中的 test 方法</span>    <span class="hljs-function"><span class="hljs-keyword">public</span> <span class="hljs-keyword">void</span> <span class="hljs-title">test</span><span class="hljs-params">()</span></span>&#123;        System.out.println(<span class="hljs-string">"TestInterfaceDefaultMethodClazz"</span>);    &#125;&#125;</code></pre><p><strong>测试类，实现并继承上述接口和类</strong></p><pre><code class="hljs java"><span class="hljs-comment">// 继承一个类和一个接口</span><span class="hljs-keyword">public</span> <span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">Test</span> <span class="hljs-keyword">extends</span> <span class="hljs-title">TestInterfaceDefaultMethodClazz</span> <span class="hljs-keyword">implements</span> <span class="hljs-title">TestIntefaceDefaultMethod</span></span>&#123;    <span class="hljs-function"><span class="hljs-keyword">public</span> <span class="hljs-keyword">static</span> <span class="hljs-keyword">void</span> <span class="hljs-title">main</span><span class="hljs-params">(String[] args)</span> </span>&#123;        <span class="hljs-comment">// 打印 TestInterfaceDefaultMethodClazz</span>        <span class="hljs-comment">// 说明调用的是类中的 test 方法</span>        <span class="hljs-keyword">new</span> Test().test();    &#125;&#125;</code></pre><h4 id="3-2、第3中情况测试"><a href="#3-2、第3中情况测试" class="headerlink" title="3.2、第3中情况测试"></a>3.2、第3中情况测试</h4><p><strong>第一个有默认方法的接口</strong></p><pre><code class="hljs java"><span class="hljs-keyword">public</span> <span class="hljs-class"><span class="hljs-keyword">interface</span> <span class="hljs-title">TestIntefaceDefaultMethod1</span> </span>&#123;    <span class="hljs-function"><span class="hljs-keyword">default</span> <span class="hljs-keyword">public</span> <span class="hljs-keyword">void</span> <span class="hljs-title">test</span><span class="hljs-params">()</span></span>&#123;        System.out.println(<span class="hljs-string">"TestIntefaceDefaultMethod1"</span>);    &#125;&#125;</code></pre><p><strong>第二个有默认方法的接口</strong></p><pre><code class="hljs java"><span class="hljs-keyword">public</span> <span class="hljs-class"><span class="hljs-keyword">interface</span> <span class="hljs-title">TestIntefaceDefaultMethod2</span> </span>&#123;    <span class="hljs-function"><span class="hljs-keyword">default</span> <span class="hljs-keyword">public</span> <span class="hljs-keyword">void</span> <span class="hljs-title">test</span><span class="hljs-params">()</span></span>&#123;        System.out.println(<span class="hljs-string">"TestIntefaceDefaultMethod2"</span>);    &#125;&#125;</code></pre><p><strong>同时实现两个接口的测试类</strong></p><pre><code class="hljs java"><span class="hljs-keyword">public</span> <span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">TestExtendsInteface</span> <span class="hljs-keyword">implements</span> <span class="hljs-title">TestIntefaceDefaultMethod1</span>,<span class="hljs-title">TestIntefaceDefaultMethod2</span></span>&#123;    <span class="hljs-comment">// 必须显式覆盖 否则编译不通过</span>    <span class="hljs-meta">@Override</span>    <span class="hljs-function"><span class="hljs-keyword">public</span> <span class="hljs-keyword">void</span> <span class="hljs-title">test</span><span class="hljs-params">()</span> </span>&#123;        <span class="hljs-comment">// 在这里显式的调用要使用 哪个实现接口的方法</span>        TestIntefaceDefaultMethod1.<span class="hljs-keyword">super</span>.test();    &#125;    <span class="hljs-function"><span class="hljs-keyword">public</span> <span class="hljs-keyword">static</span> <span class="hljs-keyword">void</span> <span class="hljs-title">main</span><span class="hljs-params">(String[] args)</span> </span>&#123;        <span class="hljs-keyword">new</span> TestExtendsInteface().test();    &#125;&#125;</code></pre><h4 id="3-3、第4种情况测试"><a href="#3-3、第4种情况测试" class="headerlink" title="3.3、第4种情况测试"></a>3.3、第4种情况测试</h4><p><strong>TestIntefaceDefaultMethod1、TestIntefaceDefaultMethod2 接口与上面相同，新增一个 TestIntefaceDefaultMethod3 接口，该接口继承自 TestIntefaceDefaultMethod1，并且是一个空的接口</strong></p><pre><code class="hljs java"><span class="hljs-keyword">public</span> <span class="hljs-class"><span class="hljs-keyword">interface</span> <span class="hljs-title">TestIntefaceDefaultMethod3</span> <span class="hljs-keyword">extends</span> <span class="hljs-title">TestIntefaceDefaultMethod1</span></span>&#123;&#125;</code></pre><p><strong>测试类同时实现 2、3两个接口，其中3接口继承1接口，并且为空接口</strong></p><pre><code class="hljs java"><span class="hljs-keyword">public</span> <span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">TestExtendsInteface</span> <span class="hljs-keyword">implements</span> <span class="hljs-title">TestIntefaceDefaultMethod3</span>,<span class="hljs-title">TestIntefaceDefaultMethod2</span></span>&#123;    <span class="hljs-comment">// 同样必须显式覆盖 否则编译报错</span>    <span class="hljs-meta">@Override</span>    <span class="hljs-function"><span class="hljs-keyword">public</span> <span class="hljs-keyword">void</span> <span class="hljs-title">test</span><span class="hljs-params">()</span> </span>&#123;        <span class="hljs-comment">// 在这里显式的调用要使用 哪个实现接口的方法</span>        TestIntefaceDefaultMethod3.<span class="hljs-keyword">super</span>.test();    &#125;    <span class="hljs-function"><span class="hljs-keyword">public</span> <span class="hljs-keyword">static</span> <span class="hljs-keyword">void</span> <span class="hljs-title">main</span><span class="hljs-params">(String[] args)</span> </span>&#123;        <span class="hljs-keyword">new</span> TestExtendsInteface().test();    &#125;&#125;</code></pre><h4 id="3-4、第5种情况"><a href="#3-4、第5种情况" class="headerlink" title="3.4、第5种情况"></a>3.4、第5种情况</h4><p><strong>TestIntefaceDefaultMethod3 覆盖了 TestIntefaceDefaultMethod1 的默认方法</strong></p><pre><code class="hljs java"><span class="hljs-keyword">public</span> <span class="hljs-class"><span class="hljs-keyword">interface</span> <span class="hljs-title">TestIntefaceDefaultMethod3</span> <span class="hljs-keyword">extends</span> <span class="hljs-title">TestIntefaceDefaultMethod1</span></span>&#123;    <span class="hljs-meta">@Override</span>    <span class="hljs-function"><span class="hljs-keyword">default</span> <span class="hljs-keyword">public</span> <span class="hljs-keyword">void</span> <span class="hljs-title">test</span><span class="hljs-params">()</span></span>&#123;        System.out.println(<span class="hljs-string">"TestIntefaceDefaultMethod3"</span>);    &#125;&#125;</code></pre><p><strong>测试类</strong></p><pre><code class="hljs java"><span class="hljs-keyword">public</span> <span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">TestExtendsInteface</span> <span class="hljs-keyword">implements</span> <span class="hljs-title">TestIntefaceDefaultMethod3</span>,<span class="hljs-title">TestIntefaceDefaultMethod2</span></span>&#123;    <span class="hljs-comment">// 同样必须显式覆盖</span>    <span class="hljs-meta">@Override</span>    <span class="hljs-function"><span class="hljs-keyword">public</span> <span class="hljs-keyword">void</span> <span class="hljs-title">test</span><span class="hljs-params">()</span> </span>&#123;        System.out.println(<span class="hljs-string">"TestExtendsInteface"</span>);    &#125;    <span class="hljs-function"><span class="hljs-keyword">public</span> <span class="hljs-keyword">static</span> <span class="hljs-keyword">void</span> <span class="hljs-title">main</span><span class="hljs-params">(String[] args)</span> </span>&#123;        <span class="hljs-keyword">new</span> TestExtendsInteface().test();    &#125;&#125;</code></pre><h4 id="3-5、总结"><a href="#3-5、总结" class="headerlink" title="3.5、总结"></a>3.5、总结</h4><p><strong>单接口实现情况下，默认方法可以直接用，多接口实现情况下一旦出现同方法签名的默认方法，那么必须显式覆盖，否则编译不通过。</strong></p><h2 id="二、接口的静态方法"><a href="#二、接口的静态方法" class="headerlink" title="二、接口的静态方法"></a>二、接口的静态方法</h2><p>JDK 8 中的接口允许使用静态的方法实现，如下所示：</p><p><strong>静态方法接口</strong></p><pre><code class="hljs java"><span class="hljs-keyword">public</span> <span class="hljs-class"><span class="hljs-keyword">interface</span> <span class="hljs-title">TestStaticInterface</span> </span>&#123;    <span class="hljs-comment">// 静态方法</span>    <span class="hljs-function"><span class="hljs-keyword">public</span> <span class="hljs-keyword">static</span> <span class="hljs-keyword">void</span> <span class="hljs-title">testStaticMethod</span><span class="hljs-params">()</span></span>&#123;        System.out.println(<span class="hljs-string">"JDK8 Static Method!"</span>);    &#125;&#125;</code></pre><p><strong>测试类</strong></p><pre><code class="hljs java"><span class="hljs-keyword">public</span> <span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">TestStaticInterfaceImpl</span> </span>&#123;    <span class="hljs-function"><span class="hljs-keyword">public</span> <span class="hljs-keyword">static</span> <span class="hljs-keyword">void</span> <span class="hljs-title">main</span><span class="hljs-params">(String[] args)</span> </span>&#123;        <span class="hljs-comment">// 直接调用</span>        TestStaticInterface.testStaticMethod();    &#125;&#125;</code></pre><h2 id="三、Lambda-表达式"><a href="#三、Lambda-表达式" class="headerlink" title="三、Lambda 表达式"></a>三、Lambda 表达式</h2><p>Lambda 表达式是 JDK8 新增的重要特性，Lambda 使 Java 具有了类似函数式编程的风格，其实 Lambda 表达式也是一个 “语法糖”，其实质也是由编译器根据表达式推断最终生成原始语法的字节码方式。</p><h3 id="1、基本语法"><a href="#1、基本语法" class="headerlink" title="1、基本语法"></a>1、基本语法</h3><p>Lambda 基本语法以 <code>(argument) -&gt; (body)</code> 表示，以下为具体的一些例子：</p><pre><code class="hljs java"><span class="hljs-comment">// 不需要参数，返回值为5</span>() -&gt; <span class="hljs-number">5</span><span class="hljs-comment">// 接受一个 String 型的参数，返回值为 字符串+Hello World</span>(String str) -&gt; str+<span class="hljs-string">" Hello World"</span><span class="hljs-comment">// 接受两个 int 型的参数，返回两数之和</span>(<span class="hljs-keyword">int</span> a,<span class="hljs-keyword">int</span> b) -&gt; a+b<span class="hljs-comment">// 接受一个 String 型的参数，并将其打印到控制台，不反悔任何参数(void)</span>(String str) -&gt; System.out.println(str)</code></pre><h3 id="2、Lambda-表达式结构"><a href="#2、Lambda-表达式结构" class="headerlink" title="2、Lambda 表达式结构"></a>2、Lambda 表达式结构</h3><ul><li>一个 Lambda 可以有零个或者多个参数；</li><li>参数类型可以声明，也可以由上下文推断，如 <code>(int a)</code> 与 <code>(a)</code> 效果一致；</li><li>所有参数包含在圆括号内，参数之间用分号分隔，如 <code>(int a,int b)</code> 或 <code>(a,b)</code>;</li><li>空圆括号代表参数集为空，如 <code>() -&gt; 2</code>；</li><li>当只有一个参数，且类型可被推倒时，圆括号可省略，如 <code>a -&gt; a+1</code>；</li><li>Lambda 表达式主体可包含一条或多条语句；</li><li>如果 Lamdba 表达式主体部分只有一条语句，那么花括号可以省略，同时匿名函数返回类型与该语句返回类型一致，如 <code>(int a,int b) -&gt; return a+b</code>；</li><li>如果 Lambda 表达式主体部分含有多条语句，则必须使用花括号，此时匿名函数返回类型与该代码块返回一致，没有返回则为空。</li></ul><h3 id="3、Lambda-表达式原理-函数式接口"><a href="#3、Lambda-表达式原理-函数式接口" class="headerlink" title="3、Lambda 表达式原理(函数式接口)"></a>3、Lambda 表达式原理(函数式接口)</h3><p>在 Java 中有两种描述层面的特殊接口：</p><p><strong>Marker(标记) 接口</strong>：这种接口内部不定义任何待实现的抽象方法，就像他的名字那样，只用作标记一个类，从而使其能够通过 <code>instanceof</code> 关键字检查。</p><p><strong>Function(函数) 接口</strong>：与标记接口不同的是，这种接口内部内有一个待实现的抽象方法，此种接口的作用是提供函数式编程需要，例如为 Lambda 表达式提供便利。</p><p><strong>其实每个 Lambda 表达式都隐式的赋值给了函数式接口，也就是说每个 Lambda 表达式其实都是函数式接口的内部抽象方法实现；</strong>从而我们便可以通过 Lambda 表达式来完成对函数式接口的简化操作，例如下面的 Runnable 接口：</p><pre><code class="hljs java">Runnable runnable = () -&gt; System.out.println(<span class="hljs-string">"Hello World"</span>);</code></pre><p><strong>当不指明 Lambda 表达式是哪个函数式接口的实现时，编译器还可以自动根据构造器将 Lambda 赋值给对应的函数式接口，</strong>如下：</p><pre><code class="hljs java"><span class="hljs-keyword">public</span> <span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">TestLambda</span> </span>&#123;    <span class="hljs-function"><span class="hljs-keyword">public</span> <span class="hljs-keyword">static</span> <span class="hljs-keyword">void</span> <span class="hljs-title">main</span><span class="hljs-params">(String[] args)</span> </span>&#123;        <span class="hljs-keyword">new</span> Thread(() -&gt; System.out.println(<span class="hljs-string">"Hello World!"</span>)).start();    &#125;&#125;</code></pre><p>以上代码，由于 <code>Thread</code> 类存在一个接受 <code>Runnable</code> 接口的构造器，而 <code>Runnable</code> 又是函数式接口，所以编译器自动推断将 Lambda 表达式隐式的赋值给了 <code>Runnable</code> 接口。</p><p><strong>注意：根据本人测试，当某个类有两个有参构造器，且两个有参构造器入参全部为函数式接口，同时该函数式接口返回值相同时，Lambda 表达式必须强制转换为其中一种类型，因为编译器无法推断出 Lambda 表达式到底是实现的那种函数式接口，测试代码如下：</strong></p><p><strong>两个函数式接口：Interface1、Interface2</strong></p><pre><code class="hljs java"><span class="hljs-keyword">public</span> <span class="hljs-class"><span class="hljs-keyword">interface</span> <span class="hljs-title">Interface1</span> </span>&#123;    <span class="hljs-function"><span class="hljs-keyword">public</span> <span class="hljs-keyword">void</span> <span class="hljs-title">sayHello</span><span class="hljs-params">()</span></span>;&#125;</code></pre><pre><code class="hljs java"><span class="hljs-keyword">public</span> <span class="hljs-class"><span class="hljs-keyword">interface</span> <span class="hljs-title">Interface2</span> </span>&#123;    <span class="hljs-function"><span class="hljs-keyword">public</span> <span class="hljs-keyword">void</span> <span class="hljs-title">sayHello</span><span class="hljs-params">()</span></span>;&#125;</code></pre><p><strong>对应的使用类：TestLambdaInterface</strong></p><pre><code class="hljs java"><span class="hljs-keyword">public</span> <span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">TestLambdaInterface</span> </span>&#123;    <span class="hljs-comment">// 两个有参构造器，参数都为函数式接口</span>    <span class="hljs-function"><span class="hljs-keyword">public</span> <span class="hljs-title">TestLambdaInterface</span> <span class="hljs-params">(Interface1 interface1)</span></span>&#123;        interface1.sayHello();    &#125;    <span class="hljs-function"><span class="hljs-keyword">public</span> <span class="hljs-title">TestLambdaInterface</span> <span class="hljs-params">(Interface2 interface2)</span></span>&#123;        interface2.sayHello();    &#125;    <span class="hljs-function"><span class="hljs-keyword">public</span> <span class="hljs-keyword">static</span> <span class="hljs-keyword">void</span> <span class="hljs-title">main</span><span class="hljs-params">(String[] args)</span> </span>&#123;        <span class="hljs-comment">// 直接这么干是无法通过编译的</span>        <span class="hljs-comment">// new TestLambdaInterface(() -&gt; System.out.println("Hello World!"));</span>        <span class="hljs-comment">// 必须强制转换为某一具体接口类型</span>        <span class="hljs-keyword">new</span> TestLambdaInterface((Interface1)() -&gt; System.out.println(<span class="hljs-string">"Hello World!"</span>));    &#125;&#125;</code></pre><h3 id="4、FunctionalInterface-注解"><a href="#4、FunctionalInterface-注解" class="headerlink" title="4、FunctionalInterface 注解"></a>4、FunctionalInterface 注解</h3><p>JDK 8 新增了 <code>FunctionalInterface</code> 注解，该注解只能作用在接口上，<strong>用于标注该接口是一个函数式接口；</strong>从而显式的告诉 Lambda 表达式将其赋值到此接口，这个注解标注了函数式接口，<strong>同时也限定了该接口内只能有一个抽象方法，</strong>否则将无法编译通过，示例如下：</p><p><strong>标注 @FunctionalInterface 注解的函数式接口</strong></p><pre><code class="hljs java"><span class="hljs-meta">@FunctionalInterface</span><span class="hljs-keyword">public</span> <span class="hljs-class"><span class="hljs-keyword">interface</span> <span class="hljs-title">TestLambdaAnnotation</span> </span>&#123;    <span class="hljs-function"><span class="hljs-keyword">public</span> <span class="hljs-keyword">void</span> <span class="hljs-title">sayHello</span><span class="hljs-params">()</span></span>;&#125;</code></pre><p><strong>测试使用类</strong></p><pre><code class="hljs java"><span class="hljs-keyword">public</span> <span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">TestLambdaAnnotationImpl</span> </span>&#123;    <span class="hljs-function"><span class="hljs-keyword">public</span> <span class="hljs-title">TestLambdaAnnotationImpl</span><span class="hljs-params">(TestLambdaAnnotation lambdaAnnotation)</span></span>&#123;    &#125;    <span class="hljs-function"><span class="hljs-keyword">public</span> <span class="hljs-keyword">static</span> <span class="hljs-keyword">void</span> <span class="hljs-title">main</span><span class="hljs-params">(String[] args)</span> </span>&#123;        <span class="hljs-keyword">new</span> TestLambdaAnnotationImpl(() -&gt; System.out.println(<span class="hljs-string">"Hello World"</span>));    &#125;&#125;</code></pre><h3 id="5、Lambda-表达式作用域"><a href="#5、Lambda-表达式作用域" class="headerlink" title="5、Lambda 表达式作用域"></a>5、Lambda 表达式作用域</h3><p>Lambda 表达式和原来的匿名内部类相似，但在作用域等方面还有些细微差别：</p><h4 id="5-1、局部变量"><a href="#5-1、局部变量" class="headerlink" title="5.1、局部变量"></a>5.1、局部变量</h4><p>局部变量不必使用 <code>final</code> 关键字修饰，即可在 Lambda 表达式中直接使用，但是此后的该变量不可改变，相当于隐式 final，如下：</p><pre><code class="hljs java"><span class="hljs-keyword">public</span> <span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">TestLocalVariables</span> </span>&#123;    <span class="hljs-function"><span class="hljs-keyword">public</span> <span class="hljs-keyword">static</span> <span class="hljs-keyword">void</span> <span class="hljs-title">main</span><span class="hljs-params">(String[] args)</span> </span>&#123;        <span class="hljs-comment">// 不必声明为 final</span>        <span class="hljs-keyword">int</span> a = <span class="hljs-number">10</span>;        <span class="hljs-comment">// 直接使用</span>        <span class="hljs-keyword">new</span> Thread(() -&gt; System.out.println(a)).start();        <span class="hljs-comment">// 不可改变该变量,否则编译报错</span>        <span class="hljs-comment">// a = 20;</span>    &#125;&#125;</code></pre><h4 id="5-2、对象属性与静态字段"><a href="#5-2、对象属性与静态字段" class="headerlink" title="5.2、对象属性与静态字段"></a>5.2、对象属性与静态字段</h4><p>同匿名对象一样，Lambda 表达式对于对象的属性和静态属性既可读又可写，如下：</p><pre><code class="hljs java"><span class="hljs-keyword">public</span> <span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">TestField</span> </span>&#123;    <span class="hljs-comment">// 普通属性</span>    String field = <span class="hljs-string">"Hello World"</span>;    <span class="hljs-comment">// 静态属性</span>    <span class="hljs-keyword">static</span> <span class="hljs-keyword">int</span> a = <span class="hljs-number">10</span>;    <span class="hljs-function"><span class="hljs-keyword">public</span> <span class="hljs-keyword">void</span> <span class="hljs-title">testScope</span><span class="hljs-params">()</span> </span>&#123;        <span class="hljs-keyword">new</span> Thread(() -&gt; &#123;            a =<span class="hljs-number">20</span>;            System.out.println(a);            field = <span class="hljs-string">"Test Scope"</span>;            System.out.println(field);        &#125;).start();    &#125;    <span class="hljs-function"><span class="hljs-keyword">public</span> <span class="hljs-keyword">static</span> <span class="hljs-keyword">void</span> <span class="hljs-title">main</span><span class="hljs-params">(String[] args)</span> </span>&#123;        <span class="hljs-keyword">new</span> TestField().testScope();    &#125;&#125;</code></pre><h4 id="5-3、接口的默认方法"><a href="#5-3、接口的默认方法" class="headerlink" title="5.3、接口的默认方法"></a>5.3、接口的默认方法</h4><p>在普通的接口中，JDK8后新增的 default 关键字可实现让接口有其默认实现方法，外部接口对象不必实现，且可直接调用，然而在 Lambda 表达式中，不支持直接调用这些默认方法，如下：</p><pre><code class="hljs java"><span class="hljs-keyword">public</span> <span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">TestDefaultMethod</span> </span>&#123;    <span class="hljs-function"><span class="hljs-keyword">public</span> <span class="hljs-keyword">static</span> <span class="hljs-keyword">void</span> <span class="hljs-title">main</span><span class="hljs-params">(String[] args)</span> </span>&#123;        LambdaInterface lambdaInterface = () -&gt; System.out.println(<span class="hljs-string">"Hello"</span>);        <span class="hljs-comment">// 以下代码无法通过编译</span>        <span class="hljs-comment">// LambdaInterface lambdaInterface = () -&gt; testDeafaultMethod();</span>    &#125;&#125;</code></pre><h4 id="5-4、this-关键字"><a href="#5-4、this-关键字" class="headerlink" title="5.4、this 关键字"></a>5.4、this 关键字</h4><p>在内部类中，this 关键字指向当前内部类对象自己，而在 Lambda 表达式中，this 关键字指向的是 Lambda 表达式外部的类对象。</p><h3 id="6、Lambda-例子"><a href="#6、Lambda-例子" class="headerlink" title="6、Lambda 例子"></a>6、Lambda 例子</h3><p>以下为一个利用 Lambda 表达式简写 forEach 的例子：</p><pre><code class="hljs java"><span class="hljs-keyword">public</span> <span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">TestLambdaForEach</span> </span>&#123;    <span class="hljs-function"><span class="hljs-keyword">public</span> <span class="hljs-keyword">static</span> <span class="hljs-keyword">void</span> <span class="hljs-title">main</span><span class="hljs-params">(String[] args)</span> </span>&#123;        <span class="hljs-comment">// 首先创建一个待排序的 list</span>        String str [] = &#123;<span class="hljs-string">"a"</span>,<span class="hljs-string">"b"</span>,<span class="hljs-string">"c"</span>,<span class="hljs-string">"d"</span>,<span class="hljs-string">"e"</span>&#125;;        List&lt;String&gt; list = Arrays.asList(str);        <span class="hljs-comment">// 原始的 forEach 遍历</span>        <span class="hljs-keyword">for</span> (String strTmp:list) &#123;            System.out.println(strTmp);        &#125;        <span class="hljs-comment">// Lambda 表达式遍历</span>        list.forEach((String strTmp) -&gt; System.out.println(strTmp));    &#125;&#125;</code></pre><p>除了 forEach 外还可以简写 Thread，如下：</p><pre><code class="hljs java"><span class="hljs-keyword">public</span> <span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">TestLambdaThread</span> </span>&#123;    <span class="hljs-function"><span class="hljs-keyword">public</span> <span class="hljs-keyword">static</span> <span class="hljs-keyword">void</span> <span class="hljs-title">main</span><span class="hljs-params">(String[] args)</span> </span>&#123;        <span class="hljs-keyword">new</span> Thread(() -&gt; System.out.println(<span class="hljs-string">"Lambda Thread Test!"</span>)).start();    &#125;&#125;</code></pre><h2 id="四、方法引用"><a href="#四、方法引用" class="headerlink" title="四、方法引用"></a>四、方法引用</h2><p>Lambda 表达式提供了一种匿名方法实现，并允许我么以函数接口的形式使用它；对于已有的方法，JDK8 提供了 <strong>方法引用(Method references)</strong> 来实现同样的特性；方法引用并不需要提供方法体，而只需使用已有的方法名即可直接引用已存在的方法。</p><h3 id="1、方法引用替代-Lambda-表达式"><a href="#1、方法引用替代-Lambda-表达式" class="headerlink" title="1、方法引用替代 Lambda 表达式"></a>1、方法引用替代 Lambda 表达式</h3><p>以下为一个使用方法引用来替代 Lambda 表达式的例子：</p><p><strong>POJO</strong></p><pre><code class="hljs java"><span class="hljs-keyword">public</span> <span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">Persion</span> </span>&#123;    <span class="hljs-keyword">private</span> String name;    <span class="hljs-comment">//省略 SET GET</span>&#125;</code></pre><p><strong>一个函数接口</strong></p><pre><code class="hljs java"><span class="hljs-meta">@FunctionalInterface</span><span class="hljs-keyword">public</span> <span class="hljs-class"><span class="hljs-keyword">interface</span> <span class="hljs-title">TestInterface</span> </span>&#123;    <span class="hljs-function"><span class="hljs-keyword">public</span> String <span class="hljs-title">test</span><span class="hljs-params">(Persion persion)</span></span>;&#125;</code></pre><p><strong>测试类</strong></p><pre><code class="hljs java"><span class="hljs-keyword">public</span> <span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">TestMethodReferences</span> </span>&#123;    <span class="hljs-function"><span class="hljs-keyword">public</span> <span class="hljs-keyword">static</span> <span class="hljs-keyword">void</span> <span class="hljs-title">main</span><span class="hljs-params">(String[] args)</span> </span>&#123;        <span class="hljs-comment">// 使用 Lambda 表达式</span>        TestInterface testInterface = (persion) -&gt; persion.getName();        <span class="hljs-comment">// 使用方法引用</span>        TestInterface testInterface1 = Persion::getName;    &#125;&#125;</code></pre><h3 id="2、方法引用的种类"><a href="#2、方法引用的种类" class="headerlink" title="2、方法引用的种类"></a>2、方法引用的种类</h3><p>JDK 8中方法引用大致分为以下6种：</p><ul><li>静态方法引用 <code>ClassName::methodName</code></li><li>实例上的实例方法引用 <code>instanceReference::methodName</code></li><li>超类上的实例方法引用 <code>super::methodName</code></li><li>类型上的实例方法引用 <code>ClassName::methodName</code></li><li>构造方法引用 <code>Class::new</code></li><li>数组构造方法引用 <code>TypeName[]::new</code></li></ul><h3 id="3、方法引用与-Lambda-表达式"><a href="#3、方法引用与-Lambda-表达式" class="headerlink" title="3、方法引用与 Lambda 表达式"></a>3、方法引用与 Lambda 表达式</h3><p><strong>无论是方法引用还是 Lambda 表达式，其最终原理和所要实现的就是当某一个类中，或者接口中的某一方法，其入参为一个接口类型时，使用方法引用或者 Lambda 表达式可以快速而简洁的实现这个接口，而不必繁琐的创建一个这个接口的对象或者直接实现。</strong></p><p>参考文章：</p><ul><li><a href="http://news.oneapm.com/java-8-oneapm-lambda/" target="_blank" rel="noopener">http://news.oneapm.com/java-8-oneapm-lambda/</a></li><li><a href="https://blog.chou.it/2014/03/java-8-new-features/" target="_blank" rel="noopener">https://blog.chou.it/2014/03/java-8-new-features/</a></li><li><a href="http://www.cnblogs.com/figure9/p/java-8-lambdas-insideout-language-features.html" target="_blank" rel="noopener">http://www.cnblogs.com/figure9/p/java-8-lambdas-insideout-language-features.html</a></li></ul><h2 id="五、Stream-API"><a href="#五、Stream-API" class="headerlink" title="五、Stream API"></a>五、Stream API</h2><p>JDK 8 带来了强大的流式 API，用于简化集合操作；流式(Stream) API 配合 Lambda 表达式和方法引用 “可变身超级英雄”。</p><h3 id="1、Strean-API-简介"><a href="#1、Strean-API-简介" class="headerlink" title="1、Strean API 简介"></a>1、Strean API 简介</h3><p>新增的 <code>Stream API</code> 主要围绕 <code>java.util.stream</code> 包下的 <code>Stream</code> 类展开，Stream API 并非其语义上的 “流”，它不同于任何 I/O 流，实质上 <strong>Stream 可以看做是一个加强版 Iterator，不同的是 Stream 可以并行并高效的对集合进行 “神的” 奇操作</strong>，如下一个小例子：</p><pre><code class="hljs java"><span class="hljs-keyword">public</span> <span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">Test1</span> </span>&#123;    <span class="hljs-function"><span class="hljs-keyword">public</span> <span class="hljs-keyword">static</span> <span class="hljs-keyword">void</span> <span class="hljs-title">main</span><span class="hljs-params">(String[] args)</span> </span>&#123;        <span class="hljs-comment">// 首先创建一个待处理的 List</span>        List&lt;Integer&gt; list = <span class="hljs-keyword">new</span> ArrayList&lt;&gt;();        list.add(<span class="hljs-number">1</span>);        list.add(<span class="hljs-number">2</span>);        list.add(<span class="hljs-number">3</span>);        list.add(<span class="hljs-number">4</span>);        <span class="hljs-comment">// 传统的过滤掉元素</span>        <span class="hljs-comment">// 定义一个 ArrayList 用于存放结果</span>        List resultList1 = <span class="hljs-keyword">new</span> ArrayList&lt;&gt;();        <span class="hljs-comment">// 循环遍历</span>        <span class="hljs-keyword">for</span> (<span class="hljs-keyword">int</span> tmp:list) &#123;            <span class="hljs-keyword">if</span> (tmp&lt;<span class="hljs-number">4</span>)&#123;                resultList1.add(tmp);            &#125;        &#125;        System.out.println(resultList1);        <span class="hljs-comment">// Stream API + Lambda 过滤</span>        <span class="hljs-comment">// 定义结果 List</span>        List resultList2 = <span class="hljs-keyword">new</span> ArrayList&lt;&gt;();        <span class="hljs-comment">// Stream+Lambda 操作</span>        list.stream().filter((tmp) -&gt; tmp &lt;<span class="hljs-number">4</span>).forEach((result)-&gt; resultList2.add(result));        System.out.println(resultList2);    &#125;&#125;</code></pre><h3 id="2、Stream-语法"><a href="#2、Stream-语法" class="headerlink" title="2、Stream 语法"></a>2、Stream 语法</h3><p>Stream 的大体语法如下：</p><p><img src="https://cdn.oss.link/markdown/hexo_java8_stream1.jpg" srcset="/img/loading.gif" alt="hexo_java8_stream1"></p><p>如上图所示，红色部分主要是创建一个 Stream，可以使从集合中创建或者选择其它方式，绿色部分代表对这个流的处理操作，可能是一个也可能是多个，比如连续 filter，<strong>每次操作后都可能返回处理后的流对象以便下次重复操作；</strong>最后的蓝色部分代表最终的 “汇聚” 动作，<strong>实质上中间部分对流做的各种动作不会立即执行，因为流不可逆性，无法重复操作，所以只有到最后的汇聚动作时才会一次执行到底。≈</strong></p><h3 id="3、创建-Stream"><a href="#3、创建-Stream" class="headerlink" title="3、创建 Stream"></a>3、创建 Stream</h3><p>创建一个 Stream 可以通过两种方式，一种是通过其静态工厂方法创建，另一种通过 <code>Collection</code> 接口默认方法创建；</p><h4 id="3-1、静态工厂创建-Stream"><a href="#3-1、静态工厂创建-Stream" class="headerlink" title="3.1、静态工厂创建 Stream"></a>3.1、静态工厂创建 Stream</h4><p>Stream 类中提供了三个静态工厂方法 <code>Stream.of()</code> 、 <code>Stream.generate()</code> 和 <code>Stream.iterate()</code> 来创建 Stream，其中 <code>Stream.of()</code> 有两个重载，一个接受固定长度的参数，一个接受可变长度的参数.</p><p><strong>Stream.of() 创建 Stream</strong></p><pre><code class="hljs java"><span class="hljs-comment">// 固定长度参数</span>Stream&lt;String&gt; stream1 = Stream.of(<span class="hljs-string">"aa"</span>);<span class="hljs-comment">// 可变长度参数</span>Stream&lt;Integer&gt; stream2 = Stream.of(<span class="hljs-number">1</span>,<span class="hljs-number">2</span>,<span class="hljs-number">3</span>,<span class="hljs-number">4</span>,<span class="hljs-number">5</span>);</code></pre><p><strong>Stream.generate() 创建 Stream</strong></p><p><code>Stream.generate()</code> 方法用于生成一个无限长度的 Stream，其接受一个 <code>Supplier</code> 接口作为入参，该接口可看作是 Stream 内部元素的对象工厂，每次调用必须返回一个对象实例，测试如下：</p><pre><code class="hljs java">Stream stream3 = Stream.generate(() -&gt; <span class="hljs-keyword">new</span> Object());</code></pre><p><strong>Stream.iterate() 创建 Stream</strong></p><p>该方法同样用于创建一个无限长度的 Stream，不同的是 <code>iterate()</code> 方法接受两个参数，第一个参数可看作是一个种子，第二个参数可看作是一个方法，每次都会使用种子重复的执行方法，如下：</p><pre><code class="hljs java"><span class="hljs-comment">// 此代码将一直执行</span>Stream.iterate(<span class="hljs-number">1</span>,(i) -&gt; i++).forEach((i)-&gt; System.out.println(i));</code></pre><h4 id="3-2、Collection-创建-Stream"><a href="#3-2、Collection-创建-Stream" class="headerlink" title="3.2、Collection 创建 Stream"></a>3.2、Collection 创建 Stream</h4><p>Collection 集合类的父接口提供了一个默认的方法 <code>ConllectionInstance.stream()</code> 来实现创建 Stream，所以直接使用或使用任意的 Collection 实现类即可创建 Stream，样例代码如下：</p><pre><code class="hljs java"><span class="hljs-comment">// 使用 ArrayList 创建</span>Stream stream = <span class="hljs-keyword">new</span> ArrayList().stream();</code></pre><h3 id="4、转换Stream"><a href="#4、转换Stream" class="headerlink" title="4、转换Stream"></a>4、转换Stream</h3><p>对 Stream 每次操作后其实都会对其进行转换，从而返回一个新的 Stream，Stream 中定义了几个常用的转换方法如下：</p><h4 id="4-1、distinct-方法"><a href="#4-1、distinct-方法" class="headerlink" title="4.1、distinct 方法"></a>4.1、distinct 方法</h4><p>顾名思义，此方法将对 Stream 中的内容进行去重操作，<strong>注意，此操作依赖于 元素的 equals 方法，所以在对象去重时请重写 equals 和 hashcode 方法。</strong>此操作后新生成的 Stream 没有重复的元素，以下为测试代码：</p><pre><code class="hljs java"><span class="hljs-comment">// 创建一个 存放元素的 ArrayList</span>List&lt;String&gt; list = <span class="hljs-keyword">new</span> ArrayList&lt;&gt;();list.add(<span class="hljs-string">"a"</span>);list.add(<span class="hljs-string">"a"</span>);list.add(<span class="hljs-string">"b"</span>);<span class="hljs-comment">// 去重后的 list</span>List list1 = list.stream().distinct().collect(Collectors.toList());System.out.println(list1);</code></pre><h4 id="4-2、filter-方法"><a href="#4-2、filter-方法" class="headerlink" title="4.2、filter 方法"></a>4.2、filter 方法</h4><p>同样，从名字可以看出这是个过滤操作，通过 filter 方法可以过滤流，并返回一个新的 Stream，其中 filter 方法入参接受一个 Predicate 函数接口，可使用 Lambda 实现该函数接口，该接口主要作用是需要我们实现一个过滤条件；测试如下：</p><pre><code class="hljs java"><span class="hljs-comment">//创建一个元素 List</span>List&lt;Integer&gt; list = <span class="hljs-keyword">new</span> ArrayList&lt;&gt;();list.add(<span class="hljs-number">1</span>);list.add(<span class="hljs-number">2</span>);list.add(<span class="hljs-number">3</span>);<span class="hljs-comment">// 创建 Stream 并进行过滤</span>List&lt;Integer&gt; list1 = list.stream().filter((i) -&gt; i&gt;<span class="hljs-number">1</span>).collect(Collectors.toList());System.out.println(list1);</code></pre><h4 id="4-3、map-方法"><a href="#4-3、map-方法" class="headerlink" title="4.3、map 方法"></a>4.3、map 方法</h4><p>该方法从名字上看似乎与 Map 有关，实质上该方法主要作用是将 Stream 中的数据按照某种给定的转换策略进行转换，比如将 Stream 中的 字符串全部转化为 Integer 类型，map 方法同样接受一个函数接口，该接口需要我们实现为具体的转换策略，测试代码如下：</p><pre><code class="hljs java"><span class="hljs-comment">// 创建一个待转换的 List</span>List&lt;String&gt; list = <span class="hljs-keyword">new</span> ArrayList&lt;&gt;();list.add(<span class="hljs-string">"1"</span>);list.add(<span class="hljs-string">"2"</span>);list.add(<span class="hljs-string">"3"</span>);<span class="hljs-comment">// 获取 Stream 并转换</span>List&lt;Integer&gt; list1 = list.stream().map((str) -&gt; Integer.parseInt(str)).collect(Collectors.toList());System.out.println(list1);</code></pre><p><strong>map 方法有其精简版本，如 mapToInt，mapToLong和mapToDouble，从名字可以知道其作用。</strong></p><h4 id="4-4、flatMap-方法"><a href="#4-4、flatMap-方法" class="headerlink" title="4.4、flatMap 方法"></a>4.4、flatMap 方法</h4><p>flatMap 方法 和 map 方法类似，同样用于结果转换，但不同的是 flatMap 可以处理返回值为 Stream 的处理，如下面的样例：</p><p><strong>使用 map 的处理情况</strong></p><pre><code class="hljs java"><span class="hljs-keyword">public</span> <span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">TestFlatMap</span> </span>&#123;    <span class="hljs-function"><span class="hljs-keyword">public</span> <span class="hljs-keyword">static</span> <span class="hljs-keyword">void</span> <span class="hljs-title">main</span><span class="hljs-params">(String[] args)</span> </span>&#123;        <span class="hljs-comment">// 创建元素集合</span>        List&lt;String&gt; list = <span class="hljs-keyword">new</span> ArrayList&lt;&gt;();        list.add(<span class="hljs-string">"123"</span>);        list.add(<span class="hljs-string">"456"</span>);        list.add(<span class="hljs-string">"789"</span>);        <span class="hljs-comment">// 创建流并转换   此处将返回一个套嵌流</span>        Stream&lt;Stream&lt;Character&gt;&gt; stream = list.stream().map(TestFlatMap::toCharterStream);        <span class="hljs-comment">// 获取这个 套嵌流中所有内容</span>        stream.forEach((streamTmp) -&gt; streamTmp.forEach((c) -&gt; System.out.println(c)));    &#125;    <span class="hljs-comment">// 字符串分解 并返回 Charcater 的 Stream</span>    <span class="hljs-function"><span class="hljs-keyword">public</span> <span class="hljs-keyword">static</span> Stream&lt;Character&gt; <span class="hljs-title">toCharterStream</span><span class="hljs-params">(String s)</span></span>&#123;        <span class="hljs-comment">// 缓存结果</span>        List&lt;Character&gt; list = <span class="hljs-keyword">new</span> ArrayList&lt;&gt;();        <span class="hljs-keyword">for</span> (<span class="hljs-keyword">char</span> c: s.toCharArray()) &#123;            <span class="hljs-comment">// 将每一个字符串分解为 char 放入缓存结果中</span>            list.add(Character.valueOf(c));        &#125;        <span class="hljs-comment">// 返回 一个 Character 的 Stream</span>        <span class="hljs-keyword">return</span> list.stream();    &#125;&#125;</code></pre><p><strong>从上面可以看出，当转换方法返回另一个流即套嵌流 <code>Stream&lt;Stream&gt;</code> 时，我们想获取 Stream 中所有内容极其困难和复杂，而 flatMap 方法很好的解决了这个问题：</strong></p><p><strong>flatmap 实现</strong></p><pre><code class="hljs java"><span class="hljs-keyword">public</span> <span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">TestFlatMap1</span> </span>&#123;    <span class="hljs-function"><span class="hljs-keyword">public</span> <span class="hljs-keyword">static</span> <span class="hljs-keyword">void</span> <span class="hljs-title">main</span><span class="hljs-params">(String[] args)</span> </span>&#123;        <span class="hljs-comment">// 创建元素集合</span>        List&lt;String&gt; list = <span class="hljs-keyword">new</span> ArrayList&lt;&gt;();        list.add(<span class="hljs-string">"123"</span>);        list.add(<span class="hljs-string">"456"</span>);        list.add(<span class="hljs-string">"789"</span>);        <span class="hljs-comment">// 注意 此处套嵌流自动被合并</span>        Stream&lt;Character&gt; stream = list.stream().flatMap(TestFlatMap1::toCharterStream);        stream.forEach((c) -&gt; System.out.println(c));    &#125;    <span class="hljs-comment">// 字符串分解 并返回 Charcater 的 Stream</span>    <span class="hljs-function"><span class="hljs-keyword">public</span> <span class="hljs-keyword">static</span> Stream&lt;Character&gt; <span class="hljs-title">toCharterStream</span><span class="hljs-params">(String s)</span> </span>&#123;        <span class="hljs-comment">// 缓存结果</span>        List&lt;Character&gt; list = <span class="hljs-keyword">new</span> ArrayList&lt;&gt;();        <span class="hljs-keyword">for</span> (<span class="hljs-keyword">char</span> c : s.toCharArray()) &#123;            <span class="hljs-comment">// 将每一个字符串分解为 char 放入缓存结果中</span>            list.add(Character.valueOf(c));        &#125;        <span class="hljs-comment">// 返回 一个 Character 的 Stream</span>        <span class="hljs-keyword">return</span> list.stream();    &#125;&#125;</code></pre><p><strong>总结：map 方法在实现的转换方法返回 Stream 时，会形成套嵌 <code>Stream&lt;Stream&gt;</code>，当要遍历时极其困难，而 flatMap 方法则会自动合并套嵌流。</strong></p><h4 id="4-5、peek-方法"><a href="#4-5、peek-方法" class="headerlink" title="4.5、peek 方法"></a>4.5、peek 方法</h4><p>peek 方法用于对 Stream 内元素进行包装，即在外面包装一层我们自定义的方法，当 Stream 内元素被消费(处理/转化)时，则触发该方法执行，如下测试例子：</p><pre><code class="hljs java"><span class="hljs-keyword">public</span> <span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">TestPeek</span> </span>&#123;    <span class="hljs-function"><span class="hljs-keyword">public</span> <span class="hljs-keyword">static</span> <span class="hljs-keyword">void</span> <span class="hljs-title">main</span><span class="hljs-params">(String[] args)</span> </span>&#123;        <span class="hljs-comment">// peek 包装时自定义了一个 输出方法,当每次 map 方法对元素转换(消费)时,都会触发 该方法执行</span>        Stream.of(<span class="hljs-string">"1"</span>,<span class="hljs-string">"2"</span>,<span class="hljs-string">"3"</span>)                .peek((s) -&gt; System.out.println(<span class="hljs-string">"元素被消费: "</span>+s))                .map((s) -&gt; Integer.parseInt(s))                .collect(Collectors.toList());    &#125;&#125;</code></pre><h4 id="4-6、limit-方法"><a href="#4-6、limit-方法" class="headerlink" title="4.6、limit 方法"></a>4.6、limit 方法</h4><p>顾名思义，此方法将对 Stream 内元素进行截断操作，返回的 Stream 中包含截断后的元素，<strong>注意此方法向后截断，即语义为 “保留前面的 N 个元素”；</strong>测试如下：</p><pre><code class="hljs java"><span class="hljs-comment">// 截断操作</span>List&lt;Integer&gt; list = Stream.of(<span class="hljs-number">1</span>,<span class="hljs-number">2</span>,<span class="hljs-number">3</span>,<span class="hljs-number">4</span>,<span class="hljs-number">5</span>).limit(<span class="hljs-number">3</span>).collect(Collectors.toList());<span class="hljs-comment">// 打印 [1, 2, 3]</span>System.out.println(list);</code></pre><h4 id="4-7、skip-方法"><a href="#4-7、skip-方法" class="headerlink" title="4.7、skip 方法"></a>4.7、skip 方法</h4><p>skip 方法与 limit 正好相反，<strong>语义为：丢弃前 N 个元素</strong>；测试如下：</p><pre><code class="hljs java"><span class="hljs-comment">// 打印 3,4,5</span>Stream.of(<span class="hljs-number">1</span>,<span class="hljs-number">2</span>,<span class="hljs-number">3</span>,<span class="hljs-number">4</span>,<span class="hljs-number">5</span>).skip(<span class="hljs-number">2</span>).forEach((i)-&gt; System.out.println(i));</code></pre><h3 id="5、汇聚-折叠"><a href="#5、汇聚-折叠" class="headerlink" title="5、汇聚(折叠)"></a>5、汇聚(折叠)</h3><p>汇聚也成折叠，当对一个流进行各种操作后(filter、map等)，实际上该流并不会立即被处理，因为流只能流向性的一次走完，只有在最后进行汇聚(折叠)动作时，该流才会一直被转换/处理完成并最终返回一个新的流。</p><p><strong>汇聚一般分为两种：</strong></p><p><strong>可变汇聚：</strong>把输入的元素累积到一个可变容器中，比如 Collection或者 StringBuilder。<br><strong>其他汇聚：</strong>其他汇聚即除了可变汇聚之外的都称作其他汇聚，一般这些汇聚都是反复操作结果流，并通过前一次的结果流当做下一次的入参，最终形成一个结果，如 reduce，count，allMatch等。</p><h4 id="5-1、可变汇聚"><a href="#5-1、可变汇聚" class="headerlink" title="5.1、可变汇聚"></a>5.1、可变汇聚</h4><p>可变汇聚对应的只有一个方法 <code>Stream.collect()</code>，该方法将流中处理后的元素汇聚到一个 容器中，比如 <code>Collection</code>；<strong>注意，Stream API 中对流处理操作并不会改变原有容器，比图 List.stream() 返回的流无论怎么处理，原来的 List 都不会受影响。</strong></p><h5 id="5-1-1、默认的-collect-方法"><a href="#5-1-1、默认的-collect-方法" class="headerlink" title="5.1.1、默认的 collect 方法"></a>5.1.1、默认的 collect 方法</h5><p>查看 collect 方法如下：</p><pre><code class="hljs java">&lt;R&gt; <span class="hljs-function">R <span class="hljs-title">collect</span><span class="hljs-params">(Supplier&lt;R&gt; supplier,</span></span><span class="hljs-function"><span class="hljs-params">      BiConsumer&lt;R, ? <span class="hljs-keyword">super</span> T&gt; accumulator,</span></span><span class="hljs-function"><span class="hljs-params">      BiConsumer&lt;R, R&gt; combiner)</span></span>;</code></pre><p>Supplier supplier是一个工厂函数，用来生成一个新的容器；BiConsumer accumulator也是一个函数，用来把Stream中的元素添加到结果容器中；BiConsumer combiner还是一个函数，用来把中间状态的多个结果容器合并成为一个（并发的时候会用到），下面举个栗子：</p><pre><code class="hljs java">List result = Stream.of(<span class="hljs-number">1</span>,<span class="hljs-number">2</span>,<span class="hljs-number">3</span>,<span class="hljs-number">4</span>).filter((i) -&gt; i&gt;<span class="hljs-number">1</span>).                collect(() -&gt; <span class="hljs-keyword">new</span> ArrayList(),                        (list,i) -&gt; list.add(i),                        (listAll,list) -&gt; listAll.addAll(list));System.out.println(result);</code></pre><p><strong>第一个参数用于生成一个容器(ArrayList),第二个参数用于将 Stream 中的元素加入到前面生成的容器(list)中，第三个参数在并发时，用于将</strong></p><h5 id="5-1-2、collect-简化版本"><a href="#5-1-2、collect-简化版本" class="headerlink" title="5.1.2、collect 简化版本"></a>5.1.2、collect 简化版本</h5><p>clooect 还有一个 override 版本，如下：</p><pre><code class="hljs java">&lt;R, A&gt; <span class="hljs-function">R <span class="hljs-title">collect</span><span class="hljs-params">(Collector&lt;? <span class="hljs-keyword">super</span> T, A, R&gt; collector)</span></span>;</code></pre><p>从上面可以看出，该方法接受一个 <code>Collector</code> 接口，JDK 8 为我们提供了快速创建该接口的工具类 <code>java.util.stream.Collectors</code>，其中调用该工具类的方法可以快速创建 <code>Collector</code> 接口的实力，典型的方法有 <code>Collectors.toList()</code>、<code>Collectors.toSet()</code>、<code>Collectors.toCollection()</code> 等等，具体可自行查看源码，从名字也很容易看出其用途，如下是一个汇聚到 List 的代码示例：</p><pre><code class="hljs java">List result = Stream.of(<span class="hljs-number">1</span>,<span class="hljs-number">2</span>,<span class="hljs-number">3</span>,<span class="hljs-number">4</span>).filter((i) -&gt; i&gt;<span class="hljs-number">1</span>).collect(Collectors.toList());</code></pre><h4 id="5-2、其他汇聚"><a href="#5-2、其他汇聚" class="headerlink" title="5.2、其他汇聚"></a>5.2、其他汇聚</h4><h5 id="5-2-1、reduce-方法"><a href="#5-2-1、reduce-方法" class="headerlink" title="5.2.1、reduce 方法"></a>5.2.1、reduce 方法</h5><p>reduce 方法用于迭代累加操作，该方法有三种 override 版本，第一种结构如下：</p><pre><code class="hljs java"><span class="hljs-function">Optional&lt;T&gt; <span class="hljs-title">reduce</span><span class="hljs-params">(BinaryOperator&lt;T&gt; accumulator)</span></span>;</code></pre><p>同样该方法接受一个 BinaryOperator 接口对象，BinaryOperator 接口对象用于实现 “累加动作”，即以何种方式累加；该接口接受两个参数，第一个参数为上次操作结果的临时变量，第二个参数为 Stream 中的元素，测试代码如下：</p><pre><code class="hljs java"><span class="hljs-comment">// int 累加</span><span class="hljs-keyword">int</span> test1 = Stream.of(<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>, <span class="hljs-number">4</span>).filter((i) -&gt; i &gt; <span class="hljs-number">1</span>).reduce((result, filed) -&gt; result + filed).get();<span class="hljs-comment">// String 累加</span>String test2 = Stream.of(<span class="hljs-string">"He"</span>,<span class="hljs-string">"llo"</span>,<span class="hljs-string">" "</span>,<span class="hljs-string">"Wo"</span>,<span class="hljs-string">"rld"</span>).reduce((result,filed) -&gt; result+filed).get();System.out.println(test1);System.out.println(test2);</code></pre><p>该方法的第二种结构如下：</p><pre><code class="hljs java"><span class="hljs-function">T <span class="hljs-title">reduce</span><span class="hljs-params">(T identity, BinaryOperator&lt;T&gt; accumulator)</span></span>;</code></pre><p><strong>注意，该版本返回值直接就是一个具体的数据类型，这个数据类型可以根据传入的初始值 <code>identity</code> 进行推断出来。</strong></p><p>此时第一个参数为一个初始值，后面的累加操作基于这个初始值，如果 Stream 为空那么将直接返回初始值，第二个参数同样为累加算法，测试代码如下：</p><pre><code class="hljs java"><span class="hljs-comment">// 带有初始值的累加</span>String test3 = Stream.of(<span class="hljs-string">"llo"</span>,<span class="hljs-string">" "</span>,<span class="hljs-string">"Wo"</span>,<span class="hljs-string">"rld"</span>).reduce(<span class="hljs-string">"He"</span>,(result,filed) -&gt; result+filed);System.out.println(test3);</code></pre><h5 id="5-2-2、count-方法"><a href="#5-2-2、count-方法" class="headerlink" title="5.2.2、count 方法"></a>5.2.2、count 方法</h5><p>count 方法用于统计元素个数，测试代码如下：</p><pre><code class="hljs java"><span class="hljs-comment">// 注意返回值为long 类型</span><span class="hljs-keyword">long</span> count = Stream.of(<span class="hljs-number">1</span>,<span class="hljs-number">2</span>,<span class="hljs-number">3</span>,<span class="hljs-number">4</span>,<span class="hljs-number">5</span>).count();System.out.println(count);</code></pre><h5 id="5-2-3、其他方法"><a href="#5-2-3、其他方法" class="headerlink" title="5.2.3、其他方法"></a>5.2.3、其他方法</h5><ul><li>allMatch 方法用于检测结果中是否都满足某一条件，测试如下：</li></ul><pre><code class="hljs java"><span class="hljs-keyword">boolean</span> flag = Stream.of(<span class="hljs-number">1</span>,<span class="hljs-number">2</span>,<span class="hljs-number">3</span>,<span class="hljs-number">4</span>,<span class="hljs-number">5</span>).allMatch((i) -&gt; i&gt;<span class="hljs-number">0</span>);System.out.println(flag);</code></pre><p><strong>其他方法基本一致，代码就不写了</strong></p><ul><li>anyMatch：Stream中是否存在任何一个元素满足匹配条件</li><li>findFirst：返回Stream中的第一个元素，如果Stream为空，返回空Optional</li><li>noneMatch：是不是Stream中的所有元素都不满足给定的匹配条件</li><li>max和min：使用给定的比较器(Operator)，返回Stream中的最大/最小值。</li></ul><p>Stream API  参考自 <a href="http://ifeve.com/stream/" target="_blank" rel="noopener">Java8初体验（二）Stream语法详解</a></p>]]></content>
    
    
    <categories>
      
      <category>Java</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Java</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>JVM 对象判定和可回收算法</title>
    <link href="/2006/01/02/jvm-object-determination-and-reclaimable-algorithm/"/>
    <url>/2006/01/02/jvm-object-determination-and-reclaimable-algorithm/</url>
    
    <content type="html"><![CDATA[<h2 id="一、可回收对象判定方法"><a href="#一、可回收对象判定方法" class="headerlink" title="一、可回收对象判定方法"></a>一、可回收对象判定方法</h2><h3 id="1、引用计数算法"><a href="#1、引用计数算法" class="headerlink" title="1、引用计数算法"></a>1、引用计数算法</h3><p><strong>基本原理：</strong></p><p>给对象添加一个引用计数器，当有新的地方引用他的时候就将其+1，引用失效则对其-1，知道为0时，说明该对象不被任何引用，可被 GC 回收，如下图所示：</p><p><img src="https://cdn.oss.link/markdown/hexo_jvm_obj_recovery1.png" srcset="/img/loading.gif" alt="引用计数器算法示意图"></p><p><strong>但是这种算法是有缺陷的，比如当 A、B 对象都没有使用时，其应该被 GC 回收，然后根据引用计数器算法，A、B 对象互相持有对方引用，导致两者引用计数器都为1，所以不会被 GC 回收，图例如下：</strong></p><p><img src="https://cdn.oss.link/markdown/hexo_jvm_obj_recovery2.png" srcset="/img/loading.gif" alt="引用计数器算法缺陷"></p><p><strong>一般要想解决此问题需要在语言层面作支持，比如 ObjectC 语言，其引用中会带有明确的关键字(wait)等，来说明当前引用状态；比如 wait的引用不会导致引用计数器+1等。</strong></p><p><strong>由于 Java 并未在语言层面对此算法进行支持，所以主流 JVM 采用的是 科达信分析算法。</strong></p><h3 id="2、可达性分析算法"><a href="#2、可达性分析算法" class="headerlink" title="2、可达性分析算法"></a>2、可达性分析算法</h3><p><strong>基本原来：</strong></p><p>通过一系列称之为 “GC Roots” 的根对象作为起点，从这些对象开始向下搜索遍历，所有搜索过的路径称之为引用链(Reference Chain)，当一个对象到 GC Roots 没有任何一个引用链相连(对象不可达)时，证明该对象已不再被引用(失效)，此时该对象将被 GC 回收，如下图所示：</p><p><img src="https://cdn.oss.link/markdown/hexo_jvm_obj_recovery3.png" srcset="/img/loading.gif" alt="可达性分析算法"></p><p><strong>可作为 GC Roots 的条件：</strong></p><ul><li>在虚拟机栈(栈帧中的本地变量表)中引用的对象</li><li>在方法区中静态属性引用的对象</li><li>在方法区中常量引用的对象</li><li>在本地方法栈中JNI(Native方法)引用的对象</li></ul><h2 id="二、垃圾收集算法"><a href="#二、垃圾收集算法" class="headerlink" title="二、垃圾收集算法"></a>二、垃圾收集算法</h2><h3 id="1、标记-清除算法"><a href="#1、标记-清除算法" class="headerlink" title="1、标记-清除算法"></a>1、标记-清除算法</h3><p><strong>原理：算法分为 “标记” 和 “清除” 两个阶段，首先标记出所有需要回收的对象，在标记完成后，统一回收所有被标记的对象；示意图如下：</strong></p><p><img src="https://cdn.oss.link/markdown/hexo_jvm_gc_biaojimm.png" srcset="/img/loading.gif" alt="标记清除算法"></p><p><strong>缺陷：标记清除之后会产生大量的内存碎片，从而在后续的大对象内存分配需要大的连续空间时没有足够的连续空间来分配；从而导致不得不触发第二次垃圾回收，增加垃圾回收的频度；另外，标记和清除算法也并不高效。</strong></p><h3 id="2、复制算法"><a href="#2、复制算法" class="headerlink" title="2、复制算法"></a>2、复制算法</h3><p><strong>原理：首先将可用内存划分为2部分，在首次内存使用时，只使用其中一部分，当发生内存回收时，首先将存活对象在另一部分未使用空间中复制一份，然后移动栈顶指针，最后将待回收部分内存完全清空；示意图如下：</strong></p><p><img src="https://cdn.oss.link/markdown/hexo_jvm_gc_copymm.png" srcset="/img/loading.gif" alt="复制算法"></p><p><strong>缺陷：内存利用率低，只能使用其中一半的内存。</strong></p><p>现在的商用虚拟机普遍都用这种方法 <strong>对新生代</strong> 进行内存回收，<strong>由于新生代中的对象都是朝生夕死的，所以实际内存比例不必1:1，一半将内存划分为一块较大的使用空间和2块较小的拯救空间，在 JVM 运行时，只会使用一块较大的内存空间和一块较小的拯救空间，当 GC 时，复制两块内存中的存活对象到另一块拯救空间，然后释放两块空间即可。</strong></p><p><strong>HotSpot 默认使用空间和拯救空间比例是8:1，这样最大可使用内存将是 90%，但是并不是每次 GC 时存活下来的对象都不会超过剩下的 10%，这是可能就需要比如老年代中的内存做内存担保。</strong></p><h3 id="3、标记-整理算法"><a href="#3、标记-整理算法" class="headerlink" title="3、标记-整理算法"></a>3、标记-整理算法</h3><p><strong>原理：其原理与标记-清除算法基本一致，只不过标记后并不直接回收对象，而是移动对象到统一的一侧，然后释放边界以外的内存，这种方法在老年代中应用较广；示意图如下：</strong></p><p><img src="https://cdn.oss.link/markdown/hexo_jvm_gc_biaojizhenglimm.png" srcset="/img/loading.gif" alt="标记整理算法"></p><p><strong>缺陷：对 JVM 停顿时间较长</strong></p><h3 id="4、分代收集算法"><a href="#4、分代收集算法" class="headerlink" title="4、分代收集算法"></a>4、分代收集算法</h3><p><strong>原理：根据对象存货周期不同，将内存划分为几块进行回收；一般把 Java 堆划分为新生代和老年代，然后根据各个周期特点进行回收；该算法目前是商用 JVM 普遍采用的。</strong></p><blockquote><p>本文参考：<a href="http://www.jikexueyuan.com/course/2098_2.html?ss=2" target="_blank" rel="noopener">JVM 自动内存管理：对象判定和回收算法</a></p></blockquote>]]></content>
    
    
    <categories>
      
      <category>Java</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Java</tag>
      
      <tag>JMM</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>kubernetes 双向 TLS 配置</title>
    <link href="/2006/01/02/kubernetes-tls-config/"/>
    <url>/2006/01/02/kubernetes-tls-config/</url>
    
    <content type="html"><![CDATA[<blockquote><p>本文参考 《kubernetes 实战》、《kubernetes 权威指南》、<a href="https://coreos.com/kubernetes/docs/latest/getting-started.html" target="_blank" rel="noopener">CoreOS Getting-Started</a></p></blockquote><h2 id="一、简介"><a href="#一、简介" class="headerlink" title="一、简介"></a>一、简介</h2><p>kubernetes 提供了多种安全认证机制，其中对于集群通讯间可采用 TLS(https) 双向认证机制，也可采用基于 Token 或用户名密码的单向 tls 认证，由于 kubernetes 某些组件只支持双向 TLS 认证，所以本文主要记录 kubernetes 双向认证配置。</p><h2 id="二、签发证书"><a href="#二、签发证书" class="headerlink" title="二、签发证书"></a>二、签发证书</h2><p>其中 TLS 双向认证需要预先自建 CA 签发证书，权威 CA 机构的证书应该不可用，因为大部分 kubernetes 应该基于内网部署，而内网应该都会采用私有 IP 地址通讯，权威 CA 好像只能签署域名证书，对于签署到 IP 可能无法实现。</p><h3 id="2-1、自签-CA"><a href="#2-1、自签-CA" class="headerlink" title="2.1、自签 CA"></a>2.1、自签 CA</h3><p>对于私有证书签发首先要自签署 一个 CA 根证书，关于 OpenSSL 使用等相关可参考 <a href="http://mritd.me/2016/07/02/%E4%BA%92%E8%81%94%E7%BD%91%E5%8A%A0%E5%AF%86%E5%8F%8AOpenSSL%E4%BB%8B%E7%BB%8D%E5%92%8C%E7%AE%80%E5%8D%95%E4%BD%BF%E7%94%A8/" target="_blank" rel="noopener">互联网加密及OpenSSL介绍和简单使用</a></p><pre><code class="hljs sh"><span class="hljs-comment"># 创建证书存放目录</span>mkdir cert &amp;&amp; <span class="hljs-built_in">cd</span> cert<span class="hljs-comment"># 创建 CA 私钥</span>openssl genrsa -out ca-key.pem 2048<span class="hljs-comment"># 自签 CA</span>openssl req -x509 -new -nodes -key ca-key.pem -days 10000 -out ca.pem -subj <span class="hljs-string">"/CN=kube-ca"</span></code></pre><h3 id="2-2、签署-apiserver-证书"><a href="#2-2、签署-apiserver-证书" class="headerlink" title="2.2、签署 apiserver 证书"></a>2.2、签署 apiserver 证书</h3><p>自签 CA 后就需要使用这个根 CA 签署 apiserver 相关的证书了，首先先修改 openssl 的配置</p><pre><code class="hljs sh"><span class="hljs-comment"># 复制 openssl 配置文件</span>cp /etc/pki/tls/openssl.cnf .<span class="hljs-comment"># 编辑 openssl 配置使其支持 IP 认证</span>vim openssl.cnf<span class="hljs-comment"># 主要修改内容如下</span>[req]req_extensions = v3_reqdistinguished_name = req_distinguished_name[req_distinguished_name][ v3_req ]basicConstraints = CA:FALSEkeyUsage = nonRepudiation, digitalSignature, keyEnciphermentsubjectAltName = @alt_names[alt_names]DNS.1 = kubernetesDNS.2 = kubernetes.defaultDNS.3 = kubernetes.default.svcDNS.4 = kubernetes.default.svc.cluster.localIP.1 = <span class="hljs-variable">$&#123;K8S_SERVICE_IP&#125;</span>  <span class="hljs-comment"># kubernetes server ip</span>IP.2 = <span class="hljs-variable">$&#123;MASTER_HOST&#125;</span>     <span class="hljs-comment"># master ip(如果都在一台机器上写一个就行)</span></code></pre><p>然后开始签署 apiserver 相关的证书</p><pre><code class="hljs sh"><span class="hljs-comment"># 生成 apiserver 私钥</span>openssl genrsa -out apiserver-key.pem 2048<span class="hljs-comment"># 生成签署请求</span>openssl req -new -key apiserver-key.pem -out apiserver.csr -subj <span class="hljs-string">"/CN=kube-apiserver"</span> -config openssl.cnf<span class="hljs-comment"># 使用自建 CA 签署</span>openssl x509 -req -<span class="hljs-keyword">in</span> apiserver.csr -CA ca.pem -CAkey ca-key.pem -CAcreateserial -out apiserver.pem -days 365 -extensions v3_req -extfile openssl.cnf</code></pre><h3 id="2-3、签署-node-证书"><a href="#2-3、签署-node-证书" class="headerlink" title="2.3、签署 node 证书"></a>2.3、签署 node 证书</h3><p>apiserver 证书签署完成后还需要签署每个节点 node 的证书，同样需要先修改一下 openssl 配置</p><pre><code class="hljs sh"><span class="hljs-comment"># copy master 的 openssl 配置</span>cp openssl.cnf worker-openssl.cnf<span class="hljs-comment"># 修改 worker-openssl 配置</span>vim worker-openssl.cnf<span class="hljs-comment"># 修改内容如下，主要是去掉 DNS 同时增加节点 IP</span>[req]req_extensions = v3_reqdistinguished_name = req_distinguished_name[req_distinguished_name][ v3_req ]basicConstraints = CA:FALSEkeyUsage = nonRepudiation, digitalSignature, keyEnciphermentsubjectAltName = @alt_names[alt_names]IP.1 = NODE1-IP <span class="hljs-comment"># 此处填写 node 的内网 ip，多个 node ip 地址以此类推 IP.2 = NODE2-IP</span></code></pre><p>接下来签署 node 的证书，以 node1 IP为 192.168.1.142 为例</p><pre><code class="hljs sh"><span class="hljs-comment"># 先声明两个变量方便引用</span>WORKER_FQDN=node1          <span class="hljs-comment"># node 昵称</span>WORKER_IP=192.168.1.142    <span class="hljs-comment"># node IP</span><span class="hljs-comment"># 生成 node 私钥</span>openssl genrsa -out <span class="hljs-variable">$&#123;WORKER_FQDN&#125;</span>-worker-key.pem 2048<span class="hljs-comment"># 生成 签署请求</span>openssl req -new -key <span class="hljs-variable">$&#123;WORKER_FQDN&#125;</span>-worker-key.pem -out <span class="hljs-variable">$&#123;WORKER_FQDN&#125;</span>-worker.csr -subj <span class="hljs-string">"/CN=<span class="hljs-variable">$&#123;WORKER_FQDN&#125;</span>"</span> -config worker-openssl.cnf<span class="hljs-comment"># 使用自建 CA 签署</span>openssl x509 -req -<span class="hljs-keyword">in</span> <span class="hljs-variable">$&#123;WORKER_FQDN&#125;</span>-worker.csr -CA ca.pem -CAkey ca-key.pem -CAcreateserial -out <span class="hljs-variable">$&#123;WORKER_FQDN&#125;</span>-worker.pem -days 365 -extensions v3_req -extfile worker-openssl.cnf</code></pre><h3 id="2-4、生成集群管理证书"><a href="#2-4、生成集群管理证书" class="headerlink" title="2.4、生成集群管理证书"></a>2.4、生成集群管理证书</h3><p>在 master(apiserver) 和 node 的证书签署完成后还需要签署一个集群管理证书</p><pre><code class="hljs sh">openssl genrsa -out admin-key.pem 2048openssl req -new -key admin-key.pem -out admin.csr -subj <span class="hljs-string">"/CN=kube-admin"</span>openssl x509 -req -<span class="hljs-keyword">in</span> admin.csr -CA ca.pem -CAkey ca-key.pem -CAcreateserial -out admin.pem -days 365</code></pre><p><strong>最终生成的文件清单如下</strong></p><p><img src="https://cdn.oss.link/markdown/hexo_kubernetes_tls_certs.png" srcset="/img/loading.gif" alt="hexo_kubernetes_tls_certs"></p><h2 id="三、配置-kubernetes"><a href="#三、配置-kubernetes" class="headerlink" title="三、配置 kubernetes"></a>三、配置 kubernetes</h2><h3 id="3-1、配置-master"><a href="#3-1、配置-master" class="headerlink" title="3.1、配置 master"></a>3.1、配置 master</h3><p>相关证书全部准备好以后，开始配置 master，首先复制证书</p><pre><code class="hljs sh"><span class="hljs-comment"># 先把证书 copy 到配置目录</span>mkdir -p /etc/kubernetes/sslcp cert/ca.pem cert/apiserver.pem cert/apiserver-key.pem /etc/kubernetes/ssl<span class="hljs-comment"># rpm 安装的 kubernetes 默认使用 kube 用户，需要更改权限</span>chown kube:kube -R /etc/kubernetes/ssl</code></pre><p><strong>然后编辑 master 的 apiserver 配置</strong></p><pre><code class="hljs sh"><span class="hljs-comment"># 编辑 master apiserver 配置文件</span>vim /etc/kubernetes/apiserver<span class="hljs-comment"># 配置如下</span>KUBE_API_ADDRESS=<span class="hljs-string">"--bind-address=192.168.1.142 --insecure-bind-address=127.0.0.1 "</span>KUBE_API_PORT=<span class="hljs-string">"--secure-port=6443 --insecure-port=8080"</span>KUBE_ETCD_SERVERS=<span class="hljs-string">"--etcd-servers=http://192.168.1.100:2379"</span>KUBE_SERVICE_ADDRESSES=<span class="hljs-string">"--service-cluster-ip-range=10.254.0.0/16"</span>KUBE_ADMISSION_CONTROL=<span class="hljs-string">"--admission-control=NamespaceLifecycle,NamespaceExists,LimitRanger,SecurityContextDeny,ServiceAccount,ResourceQuota"</span>KUBE_API_ARGS=<span class="hljs-string">"--tls-cert-file=/etc/kubernetes/ssl/apiserver.pem --tls-private-key-file=/etc/kubernetes/ssl/apiserver-key.pem --client-ca-file=/etc/kubernetes/ssl/ca.pem --service-account-key-file=/etc/kubernetes/ssl/apiserver-key.pem "</span></code></pre><p><strong>接着编辑 controller manager 的配置</strong></p><pre><code class="hljs sh"><span class="hljs-comment"># 编辑 controller manager 配置</span>vim /etc/kubernetes/controller-manager<span class="hljs-comment"># 配置如下</span>KUBE_CONTROLLER_MANAGER_ARGS=<span class="hljs-string">"--service-account-private-key-file=/etc/kubernetes/ssl/apiserver-key.pem  --root-ca-file=/etc/kubernetes/ssl/ca.pem --master=http://127.0.0.1:8080"</span></code></pre><p><strong>最后启动 apiserver 、controller manager 和 scheduler</strong></p><pre><code class="hljs sh">systemctl start kube-apiserversystemctl start kube-controller-managersystemctl start kube-schedulersystemctl <span class="hljs-built_in">enable</span> kube-apiserversystemctl <span class="hljs-built_in">enable</span> kube-controller-managersystemctl <span class="hljs-built_in">enable</span> kube-schedulersystemctl status kube-apiserversystemctl status kube-controller-managersystemctl status kube-scheduler</code></pre><h3 id="3-2、配置-node"><a href="#3-2、配置-node" class="headerlink" title="3.2、配置 node"></a>3.2、配置 node</h3><p><strong>由于是测试，所以 node1 和 master 启动在同一台机器上，配置时同样先 copy 配置文件</strong></p><pre><code class="hljs sh">cp cert/node1-worker-key.pem cert/node1-worker.pem /etc/kubernetes/sslchown kube:kube -R /etc/kubernetes/ssl</code></pre><p><strong>修改 kubelet 配置</strong></p><pre><code class="hljs sh">vim /etc/kubernetes/kubelet<span class="hljs-comment"># 配置如下</span>KUBELET_ADDRESS=<span class="hljs-string">"--address=192.168.1.142"</span>KUBELET_HOSTNAME=<span class="hljs-string">"--hostname-override=192-168-1-142"</span>KUBELET_API_SERVER=<span class="hljs-string">"--api-servers=https://192.168.1.142:6443"</span>KUBELET_ARGS=<span class="hljs-string">"--tls-cert-file=/etc/kubernetes/ssl/node1-worker.pem --tls-private-key-file=/etc/kubernetes/ssl/node1-worker-key.pem --kubeconfig=/etc/kubernetes/worker-kubeconfig.yaml"</span></code></pre><p><strong>如果使用了 <code>KUBELET_HOSTNAME</code>，那么 hostname 必须在本地 hosts 存在，所以还需要修改一下 hosts 文件</strong></p><pre><code class="hljs sh"><span class="hljs-built_in">echo</span> <span class="hljs-string">"127.0.0.1 192-168-1-142"</span> &gt;&gt; /etc/hosts</code></pre><p><strong>修改 config 配置</strong></p><pre><code class="hljs sh">vim /etc/kubernetes/config<span class="hljs-comment"># 配置如下</span>KUBE_LOGTOSTDERR=<span class="hljs-string">"--logtostderr=true"</span>KUBE_LOG_LEVEL=<span class="hljs-string">"--v=0"</span>KUBE_ALLOW_PRIV=<span class="hljs-string">"--allow-privileged=false"</span>apiserverKUBE_MASTER=<span class="hljs-string">"--master=https://192.168.1.142:6443"</span></code></pre><p><strong>创建 kube-proxy 配置文件</strong></p><pre><code class="hljs sh">vim /etc/kubernetes/worker-kubeconfig.yaml<span class="hljs-comment"># 内容如下</span>apiVersion: v1kind: Configclusters:- name: <span class="hljs-built_in">local</span>  cluster:    certificate-authority: /etc/kubernetes/ssl/ca.pemusers:- name: kubelet  user:    client-certificate: /etc/kubernetes/ssl/node1-worker.pem    client-key: /etc/kubernetes/ssl/node1-worker-key.pemcontexts:- context:    cluster: <span class="hljs-built_in">local</span>    user: kubelet  name: kubelet-contextcurrent-context: kubelet-context</code></pre><p><strong>配置 kube-proxy 使其使用证书</strong></p><pre><code class="hljs sh">vim /etc/kubernetes/proxy<span class="hljs-comment"># 配置如下</span>KUBE_PROXY_ARGS=<span class="hljs-string">"--master=https://192.168.1.100:6443 --kubeconfig=/etc/kubernetes/worker-kubeconfig.yaml"</span></code></pre><p><strong>最后启动并测试</strong></p><pre><code class="hljs sh"><span class="hljs-comment"># 启动</span>systemctl start kubeletsystemctl start kube-proxysystemctl <span class="hljs-built_in">enable</span> kubeletsystemctl <span class="hljs-built_in">enable</span> kube-proxysystemctl status kubeletsystemctl status kube-proxy<span class="hljs-comment"># 测试</span>kubectl get node<span class="hljs-comment"># 显示如下</span>NAME            STATUS    AGE192-168-1-142   Ready     13s</code></pre><h2 id="四、其他相关"><a href="#四、其他相关" class="headerlink" title="四、其他相关"></a>四、其他相关</h2><p>master 启动后发现一个错误，大致意思是内核版本过低，但是 CentOS 已经 upgrade 到官方最新稳定版了。。。无奈换了下内核好了，以下为记录升级到最新内核的方法</p><pre><code class="hljs rpm"># 导入 elrepo 的keyrpm --import https:&#x2F;&#x2F;www.elrepo.org&#x2F;RPM-GPG-KEY-elrepo.org# 安装 elrepo 源rpm -Uvh http:&#x2F;&#x2F;www.elrepo.org&#x2F;elrepo-release-7.0-2.el7.elrepo.noarch.rpm# 在yum的ELRepo源中，mainline 为最新版本的内核，so 安装 ml 的内核yum --enablerepo&#x3D;elrepo-kernel install  kernel-ml-devel kernel-ml -y# 切换 grub 引导，默认启动的顺序应该为1,升级以后内核是往前面插入，为0grub2-set-default 0# 最后重启reboot# 再看下检查下内核版本已经是 4.7.3-1.el7.elrepo.x86_64uname -r</code></pre>]]></content>
    
    
    <categories>
      
      <category>Kubernetes</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Linux</tag>
      
      <tag>Docker</tag>
      
      <tag>Kubernetes</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Linux 笔记</title>
    <link href="/2006/01/02/linux-note/"/>
    <url>/2006/01/02/linux-note/</url>
    
    <content type="html"><![CDATA[<h2 id="一、基础"><a href="#一、基础" class="headerlink" title="一、基础"></a>一、基础</h2><h3 id="1、基础命令"><a href="#1、基础命令" class="headerlink" title="1、基础命令"></a>1、基础命令</h3><ul><li><code>tty</code> : 查看当前终端类型</li></ul><table><thead><tr><th>返回值</th><th>终端类型</th></tr></thead><tbody><tr><td>/dev/pst/#</td><td>伪终端</td></tr><tr><td>/dev/tty#</td><td>虚拟终端</td></tr><tr><td>/dev/console</td><td>物理终端</td></tr><tr><td>/dev/ttys#</td><td>串行终端</td></tr></tbody></table><ul><li><code>who</code> : 查看登录用户</li><li><code>bashname</code> : 查看目录基名</li><li><code>dirname</code> : 查看目录名</li><li><code>type</code> : 查看命令类型</li><li><code>where</code> : 命令在哪(zsh)</li><li><code>hash</code> : 查看命令缓存</li><li><code>which</code> : 查看命令所在位置</li><li><code>man</code> : 查看命令帮助，可为 命令、程序配置文件格式、系统调用、库调用、游戏及其他不便归类的文件提供操作手册，以下为 man 下的快捷键:<ul><li>space : 向下翻屏</li><li>b : 向上翻屏</li><li>Enter : 向下翻一行</li><li>k : 向上翻一行</li><li>/:keyword : 向下查找文字</li><li>?:keyword : 向上查找文字</li><li>n : 向下循环查找</li><li>N : 向上循环查找</li><li>q : 退出</li></ul></li><li>man 分段机制，<code>man [num] commond</code> 查看指定章节的帮助手册<ul><li>1、用户命令</li><li>2、系统调用</li><li>3、库调用</li><li>4、设备文件</li><li>5、文件格式</li><li>6、游戏</li><li>7、杂项</li><li>8、管理命令</li></ul></li><li><code>man -k</code> : 查看命令章节信息(模糊查找)<ul><li>whatis : 查看命令章节信息(精确查找)</li><li>通过 makewhatis 手动生成数据库信息</li></ul></li><li><code>info</code> : 获取在线文档(/usr/share/doc/ 离线文档)</li><li><code>shutdown TIME</code> : 关机<ul><li><code>-r</code> : 重启</li><li><code>-h</code> : 关机</li><li><code>-c</code> : 取消重启或关机</li></ul></li><li><code>shutdown</code> 时间格式 :<ul><li><code>now</code> : 立即关机</li><li><code>+#</code> : n 分钟后关机</li><li><code>hh:mm</code> : 指定时间关机</li></ul></li><li><code>date</code> : 时间管理<ul><li><code>date MMDDhhmm[[CC]YY][.ss]</code> : 设置时间</li><li><code>date [+FORMAT]</code> : 指定格式显示时间，具体 man 查看格式</li></ul></li><li><code>hwclock</code> : 硬件时间管理<ul><li><code>-s</code> : 把软件时钟设置为硬件时钟</li><li><code>-w</code> : 把硬件时钟设置为软件始终</li></ul></li><li><code>hitory</code> : 查看命令历史<ul><li><code>-c</code> : 清除命令历史</li><li><code>-d #</code> : 删除指定命令</li><li><code>-a</code> : 追加当前命令到命令历史</li></ul></li><li><code>ls</code> : 列出目录中内容<ul><li><code>-a</code> : 显示所有文件，包含隐藏文件</li><li><code>-A</code> : 显示所有文件，但不现实 . .. 目录</li><li><code>--color</code> : 设置文件颜色配置，默认 –color=always</li><li><code>-d</code> : 如果是目录，则显示目录本身属性，并非展示其目录下文件内容</li><li><code>-r</code> : 逆序显示</li><li><code>-R</code> : 递归现实</li><li><code>-h</code> : 格式化文件大小</li><li><code>-l</code> : 长格式显示每个文件具体信息</li><li><code>-i</code> : 显示 索引节点号</li></ul></li><li><code>file</code> : 查看文件内容格式</li><li><code>echo</code> : 回显(打印)内容<ul><li><code>-e</code> : 支持控制符显示</li><li><code>-e &quot;\033[##m文本\033[0m&quot;</code> : 设置文本颜色，## 第一个事前景色，3前景色4背景色，第二个是颜色，1~7，多控制用 ; 分隔</li><li><code>-n</code> : 不为显示的内容自动换行</li></ul></li><li><code>cat</code> : 文件连接显示命令<ul><li><code>-E</code> : 显示结束符 <code>$</code>，在 Linux 下 <code>$</code> 为换行符，Windows 下为 <code>\n$</code></li><li><code>-v</code> : 显示非打印字符</li><li><code>-e</code> : 显示全部非打印符，包括换行符，即全字符显示</li><li><code>-n</code> : 显示行号</li><li><code>-s</code> : 压缩显示空白行，多个连续空白行将显示为一个空白行</li><li><code>cat file1 file2</code> : 连接并显示两个文件</li></ul></li><li><code>tac</code> : 与 cat 相同，只不过按行逆序显示</li><li><code>more</code> : 分页查看文件内容，翻到文件尾部以后则不可以向前翻页，自动退出</li><li><code>less</code> : 与 man 相同，支持前后翻页，q 退出</li><li><code>head</code> : 查看文件头部内容，默认 10 行<ul><li><code>-n #</code> : 显示头部 N 行</li><li><code>#</code> : 与 -n # 相同</li></ul></li><li><code>tail</code> : 查看文件尾部内容,默认 10行<ul><li><code>-f</code> : 持续监测并显示文件内容</li></ul></li><li><code>cp</code> : 复制文件<ul><li><code>-r</code> : 递归复制</li><li><code>-i</code> : 交互式</li><li><code>-a</code> : 归档模式，相当于 <code>-dr</code>，链接文件不追溯源文件，cp 默认追溯源文件</li><li><code>-p</code> : 保持原有属性</li><li><code>-f</code> : 强制覆盖</li></ul></li><li><code>touch</code> : 更改时间戳<ul><li><code>-c</code> : 不创建文件</li><li><code>-t</code> : 明确指定时间</li><li><code>-a</code> : 改变访问时间</li><li><code>-m</code> : 改变修改时间</li></ul></li><li><code>stat</code> : 查看文件状态元数据</li><li><code>wc</code> : 文本统计命令<ul><li><code>-l</code> : 查看文件有多少行</li><li><code>-c</code> : 统计文本有多少字节</li><li><code>-w</code> : 统计文本有多少单词</li></ul></li><li><code>tr</code> : 从标准输入到标准输出的字符替换，如 <code>cat /etc/passwd | tr &#39;a-z&#39; &#39;A-Z&#39;</code><ul><li><code>-d</code> : 删除指定字符</li></ul></li><li><code>cut</code> : 切割字符<ul><li><code>-d&#39;字符&#39;</code> : 指定分段字符</li><li><code>-f#</code> : 指定要显示 的列，如 <code>cut -d&#39;:&#39; -f1 /etc/passwd</code></li></ul></li><li><code>tee</code> : 将标准输入内容重定向到标准输出并在此发出标准输出，常用于同时要显示/保存标准输入的内容并且再将其通过管道转发至另一个程序的标准输入</li><li><code>sort</code> : 文本排序<ul><li><code>-f</code> : 忽略大小写</li><li><code>-n</code> : 对数值排序</li><li><code>-t</code> : 指定分隔符</li><li><code>-k</code> : 指定排序字段</li><li><code>-u</code> : 重复行只显示一次</li></ul></li><li><code>uniq</code> : 去重统计<ul><li><code>-c</code> : 显示每行的重复次数</li><li><code>-d</code> : 仅显示重复的行(&gt;1)</li><li><code>-u</code> : 显示从未重复的行</li></ul></li><li><code>md5sum</code> : 获取 md5 摘要</li><li><code>sha1sum</code> : 获取 sha1 加密信息</li><li><code>read VARNAME</code> : 从标准输入读取一个输入值并赋值给变量<ul><li><code>-p</code> : 显示提示信息</li><li><code>-t</code> : 设置超时时间</li></ul></li><li><code>watch</code> : 持续运行命令<ul><li><code>-n #</code> : 每隔 # 秒运行其后面的命令</li></ul></li></ul><h3 id="2、Linux-哲学"><a href="#2、Linux-哲学" class="headerlink" title="2、Linux 哲学"></a>2、Linux 哲学</h3><ul><li>一切皆文件</li><li>没有返回消息就是最好的消息，不要打扰用户</li><li>由众多目的单一的小应用程序组成，每个程序完成单一功能</li><li>组合目的单一的小程序完成复杂任务</li><li>使用文本保存配置文件</li><li>提供机制，而非策略</li></ul><h3 id="3、bash-特性"><a href="#3、bash-特性" class="headerlink" title="3、bash 特性"></a>3、bash 特性</h3><h4 id="3-1、引用"><a href="#3-1、引用" class="headerlink" title="3.1、引用"></a>3.1、引用</h4><ul><li><code>&#39;&#39;</code> : 强引用，不会出现变量替换</li><li><code>&quot;&quot;</code> : 弱引用，会产生变量替换</li><li><code>``</code> : 命令引用，用于替换成命令执行结果</li><li><code>${}</code> : 引用变量，同上<ul><li><code>${VARNAME:-VALUE}</code> : 如果 VARNAME 不为空，则返回其本身，否则返回 VALUE 的值，常用于设置变量默认值，<strong>注意 VARNAME 并未被改变</strong></li></ul></li></ul><h4 id="3-2、命令行展开"><a href="#3-2、命令行展开" class="headerlink" title="3.2、命令行展开"></a>3.2、命令行展开</h4><ul><li><code>{}</code> 代表命令行展开，最简单的应用就是创建多层目录，如 <code>mkdir -p a/{b,c}</code>，最终将创建 <code>a/b</code> 和 <code>a/c</code> 文件夹。</li></ul><h4 id="3-3、命令历史"><a href="#3-3、命令历史" class="headerlink" title="3.3、命令历史"></a>3.3、命令历史</h4><p>bash 默认保存以前执行过的命令，使用 <code>history</code> 命令查看，默认保存1000行在 <code>~/.bash_history</code> 中。</p><ul><li><code>!#</code> : 快速执行命令历史中#号命令</li><li><code>!!</code> : 快速执行上一条命令</li><li><code>!-#</code> : 快速执行命令历史中倒数#号命令</li><li><code>!$</code> : 引用上条命令的参数</li><li><code>!str</code> : 快速执行历史中最近的以str 开始的命令</li></ul><h4 id="3-4、命令别名"><a href="#3-4、命令别名" class="headerlink" title="3.4、命令别名"></a>3.4、命令别名</h4><p>使用 <code>alias</code> 命令可显示系统上所有创建的别名。<br>在命令前加反斜杠代表使用命令本身，如 <code>\ls</code>。<br>使用 <code>alias 命令别名=&#39;原始命令&#39;</code> 定义命令别名。<br>使用 <code>unalias 命令别名</code> 撤销一个命令的别名。</p><h4 id="3-5、globbing-文件名通配符"><a href="#3-5、globbing-文件名通配符" class="headerlink" title="3.5、globbing 文件名通配符"></a>3.5、globbing 文件名通配符</h4><ul><li><code>*</code> : 表示任意长度的任意字符</li><li><code>?</code> : 任意单个字符</li><li><code>[]</code> : 匹配指定字符范围内的任意单个字符<ul><li>[a-z] : 所有英文字符，不区分大小写</li><li>[0-9] : 所有数字</li><li>[[:upper:]] : 所有大写字母</li><li>[[:lower:]] : 所有小写字母</li><li>[[:alpha:]] : 所有字母</li><li>[[:digit:]] : 所有数字</li><li>[[:alnum:]] : 数字加字母</li><li>[[:space:]] : 空白符</li><li>[[:punct:]] : 所有标点符号</li></ul></li><li><code>[^]</code> : 指定范围以外，取反操作</li></ul><h4 id="3-6、bash-快捷键"><a href="#3-6、bash-快捷键" class="headerlink" title="3.6、bash 快捷键"></a>3.6、bash 快捷键</h4><ul><li><code>Ctrl+a</code> : 跳转到行首</li><li><code>Ctrl+e</code> : 跳转到行尾</li><li><code>Ctrl+u</code> : 删除光标到行首</li><li><code>Ctrl+k</code> : 删除光标到行尾</li><li><code>Ctrl+z</code> : 后台当前程序</li></ul><h4 id="3-7、bash-补全"><a href="#3-7、bash-补全" class="headerlink" title="3.7、bash 补全"></a>3.7、bash 补全</h4><ul><li>命令补全 : tab 执行命令补全，默认从 PATH 变量从左向右补全</li><li>路径补全 : tab 执行路径不全</li></ul><h4 id="3-8、bash-变量"><a href="#3-8、bash-变量" class="headerlink" title="3.8、bash 变量"></a>3.8、bash 变量</h4><ul><li>本地变量 : 只对当前 shell 进程有效，对其子 shell 其他 shell 都无效，使用 <code>[set] var=Value</code> 声明</li><li>局部变量 : 只对指定代码块有效，使用 <code>local var=value</code> 声明</li><li>环境变量 : 对当前 shell 和其子 shell 有效，使用 <code>export var=value</code> 声明</li><li>位置变量 : 用于在脚本中引用传递的参数，如 <code>$1</code></li><li>特殊变量 :<ul><li><code>$0</code> : 脚本名称本身</li><li><code>$?</code> : 上一条命令执行状态，范围是 <code>0~255</code>；<code>0</code> 成功，<code>1~255</code> 失败</li><li><code>$$</code> :</li><li><code>$!</code> :</li><li><code>$#</code> :</li><li><code>$*</code> :</li></ul></li></ul><h3 id="4、FHS-文件目录层级标准"><a href="#4、FHS-文件目录层级标准" class="headerlink" title="4、FHS 文件目录层级标准"></a>4、FHS 文件目录层级标准</h3><table><thead><tr><th>目录</th><th>功能</th></tr></thead><tbody><tr><td>/boot</td><td>系统启动文件以及grub(引导加载器bootloader)、vmlinuz(内核)、initrd(完整的小Linux系统)、ramfs文件</td></tr><tr><td>/dev</td><td>设备文件：块设备(随机访问无顺序)、字符设备(线性访问有顺序)、设备号(主设备号(major)和次设备号(minor))</td></tr><tr><td>/etc</td><td>配置文件</td></tr><tr><td>/etc/sysconfig</td><td>系统级别的软件配置</td></tr><tr><td>/etc/init.d</td><td>系统运行级别脚本</td></tr><tr><td>/home</td><td>普通用户家目录</td></tr><tr><td>/lib</td><td>库文件和内核模块文件(.a 静态库、.so 动态库)</td></tr><tr><td>/lost+found</td><td>存储断电等情况造成的尚未保存的文件</td></tr><tr><td>/media</td><td>挂载移动设备目录</td></tr><tr><td>/mnt</td><td>挂载硬盘等设备目录</td></tr><tr><td>/misc</td><td>杂项</td></tr><tr><td>/opt</td><td>早起安装第三方软件目录</td></tr><tr><td>/proc</td><td>伪文件系统，默认是空，系统启动后则不为空，存放内核映射文件，一般为内核可调参数、内核工作数据；主要用于内核调优</td></tr><tr><td>/sys</td><td>伪文件系统；硬件设备相关属性 映射文件  比如修改磁盘I/O调度</td></tr><tr><td>/srv</td><td>为服务提供数据存储位置</td></tr><tr><td>/tmp</td><td>临时文件 任何人都可以在里面创建文件 但只能删除自己的  drwxrwxrwt</td></tr><tr><td>/var</td><td>可变化文件</td></tr><tr><td>/bin</td><td>二进制可执行文件(任意用户可执行)</td></tr><tr><td>/sbin</td><td>管理员二进制可执行程序</td></tr><tr><td>/usr</td><td>shared read-only 全局共享只读文件</td></tr><tr><td>/usr/include</td><td>头文件，编译安装软件时会使用</td></tr><tr><td>/usr/bin</td><td>全局只读共享二进制程序</td></tr><tr><td>/usr/sbin</td><td>全局只读共享管理员二进制程序</td></tr><tr><td>/usr/local</td><td>第三方软件安装目录</td></tr><tr><td>/usr/local/bin</td><td>第三方软件二进制可执行文件</td></tr><tr><td>/usr/local/sbin</td><td>第三方软件管理员命令</td></tr><tr><td>/usr/local/lib</td><td>第三方软件库文件</td></tr></tbody></table><h3 id="5、Linux-文件类型"><a href="#5、Linux-文件类型" class="headerlink" title="5、Linux 文件类型"></a>5、Linux 文件类型</h3><h4 id="5-1、Linux-文件类型"><a href="#5-1、Linux-文件类型" class="headerlink" title="5.1、Linux 文件类型"></a>5.1、Linux 文件类型</h4><ul><li>普通文件 : <code>-/f</code> 普通存储的文件，Linux 并不根据文件后缀区分文件类型</li><li>目录文件 : <code>d</code> 实现路径映射</li><li>链接文件 : <code>l</code> 软连接用于映射具体文件；硬链接实际上与源文件的 <strong>索引节点号</strong> 相同，可视为同源文件一样操作，参考 <a href="https://www.ibm.com/developerworks/cn/linux/l-cn-hardandsymb-links/" target="_blank" rel="noopener">Linux软连接与硬连接</a>。</li><li>设备文件 <code>c/b</code> : 鼠标键盘等为 <strong>字符设备</strong>，有存取顺序，c 标识；硬盘等称之为 <strong>块设备</strong>，可实现随机存取，b 标识。</li><li>管道文件 <code>p</code> : 用于链接两个命令的文件</li><li>套接字文件 <code>s</code> : 用于网络传输时进程间通讯</li></ul><h4 id="5-2、Linux-时间戳"><a href="#5-2、Linux-时间戳" class="headerlink" title="5.2、Linux 时间戳"></a>5.2、Linux 时间戳</h4><ul><li>访问时间: 最近一次访问文件的时间。</li><li>修改时间: 修改文件内容的最近时间，即文件真实内容的写时间。</li><li>改变时间: 文件元数据改变时间，即文件的实际类型、权限等改变时间，注意修改时间、访问时间也算是文件的元数据，当两者改变时，改变时间同样会更新。</li></ul><h3 id="6、用户管理"><a href="#6、用户管理" class="headerlink" title="6、用户管理"></a>6、用户管理</h3><h4 id="6-1、用户类别"><a href="#6-1、用户类别" class="headerlink" title="6.1、用户类别"></a>6.1、用户类别</h4><ul><li>管理员 : 0</li><li>普通用户 : 1-65535<ul><li>系统用户 : 1-499</li><li>登录用户 : 500-65535</li></ul></li></ul><h4 id="6-2、相关配置文件"><a href="#6-2、相关配置文件" class="headerlink" title="6.2、相关配置文件"></a>6.2、相关配置文件</h4><ul><li><code>/etc/passwd</code> : 用户主信息存储文件<ul><li>文件格式 : <code>用户名:x:UID:GID:finger(备注信息):HOME:SHELL</code></li></ul></li><li><code>/etc/shadow</code> : 用户密码存储文件<ul><li>文件格式 : <code>用户名:加密后的密码:最近一次密码修改时间:最短使用期限:最长使用期限:警告区间:非活动区间:账号过期时间:预留</code></li></ul></li><li><code>/etc/group</code> : 用户组信息存储文件</li><li><code>/etc/gshadow</code> : 用户组密码存储文件</li></ul><h4 id="6-3、创建用户及用户组"><a href="#6-3、创建用户及用户组" class="headerlink" title="6.3、创建用户及用户组"></a>6.3、创建用户及用户组</h4><ul><li><code>useradd USERNAME</code> : 用于创建用户<ul><li><code>-u</code> : 指定用户 UID</li><li><code>-g</code> : 指定用户的基本组 GID，但是GID必须预先存在</li><li><code>-G</code> : 指定用户的额外组 GID，组必须事先存在</li><li><code>-d</code> : 设置用户的家目录，不能实现复制</li><li><code>-c</code> : 备注信息</li><li><code>-s</code> : 指定用户 shell，应使用 <code>/etc/shells</code> 文件中指定的安全 shell，否则可能无法登录系统</li><li><code>-r</code> : 指定用户为系统用户，UID 在 1~499 之间，不会给用户创建家目录</li><li><code>-p</code> : 直接指定密码</li><li><code>-m</code> : 创建用户时自动创建家目录(默认)</li><li><code>-M</code> : 创建用户时不创建家目录</li><li><code>-D</code> : 为 useradd 命令指定创建用户时的默认值</li></ul></li><li><code>groupadd GRPNAME</code> : 用于创建用户组<ul><li><code>-g</code> : 创建组并为其指定 GID</li></ul></li></ul><h4 id="6-5、删除用户及用户组"><a href="#6-5、删除用户及用户组" class="headerlink" title="6.5、删除用户及用户组"></a>6.5、删除用户及用户组</h4><ul><li><code>userdel USERNAME</code> : 删除指定用户，默认不移除家目录<ul><li><code>-f</code>, <code>--force</code> : 即使不属于此用户，也强制删除文件</li><li><code>-h</code>, <code>--help</code> : 显示此帮助信息并推出</li><li><code>-r</code>, <code>--remove</code> : 删除主目录和邮件池</li><li><code>-R</code>, <code>--root CHROOT_DIR</code> : chroot 到的目录</li><li><code>-Z</code>, <code>--selinux-user</code> : 为用户删除所有的 SELinux 用户映射</li></ul></li><li><code>groupdel GRPNAME</code> : 删除用户组，删除用户基本组，用户未删除时无法操作</li></ul><h4 id="6-6、用户修改"><a href="#6-6、用户修改" class="headerlink" title="6.6、用户修改"></a>6.6、用户修改</h4><ul><li><code>chsh USERNAME</code> : 改变用户默认 shell</li><li><code>chfn USERNAME</code> : 更改用户描述信息</li><li><code>usermod USERNAME</code> : 改变用户信息<ul><li><code>-u</code> : 改变用户 UID</li><li><code>-g</code> : 改变用户 GID</li><li><code>-G</code> : 改变用户附加组，默认会覆盖原来的附加组，通常和 <code>-a</code> 一起使用</li><li><code>-a</code> : 修改附加组时保持追加，不覆盖原有附加组</li><li><code>-c</code> : 修改用户注释</li><li><code>-d</code> : 修改用户家目录，默认不会迁移用户的家目录，如果要迁移需要配合 <code>-m</code></li><li><code>-m</code> : 修改时家目录时迁移文件</li><li><code>-s</code> : 指定新的 shell</li><li><code>-l</code> : 更改用户名</li><li><code>-L</code> : 锁定用户账号</li><li><code>-U</code> : 解锁用户账号</li></ul></li><li><code>chage</code> : 设置密码期限<ul><li><code>-d</code> : 设置最近密码修改时间</li><li><code>-E</code> : 设置密码过期时间</li><li><code>-I</code> : 设置密码失效时间</li><li><code>-m</code> : 设置密码最短使用期限</li><li><code>-M</code> : 设置密码最长使用期限</li><li><code>-W</code> : 设置警告时间</li></ul></li><li><code>groupmod GRPNAME</code> : 用户组信息修改<ul><li><code>-g</code> : 改变组的 GID</li><li><code>-n</code> : 改变组名</li></ul></li><li><code>newgrp GRPNAME</code> : 切换用户组</li></ul><h3 id="7、管道与重定向"><a href="#7、管道与重定向" class="headerlink" title="7、管道与重定向"></a>7、管道与重定向</h3><h4 id="7-1、输入重定向"><a href="#7-1、输入重定向" class="headerlink" title="7.1、输入重定向"></a>7.1、输入重定向</h4><ul><li><code>&lt;</code> : 用于输入重定向，比如讲一个文本内容重定向到一个命令，如<code>tr &#39;a-z&#39; &#39;A-Z&#39; &lt; /etc/passwd</code></li><li><code>&lt;&lt;</code> : 此处文档，常用于生成菜单，同 <code>EOF</code> 结束符一起使用，如下所示 :</li></ul><pre><code class="hljs sh">cat &lt;&lt; EOF1、test12、test23、test44、test4EOF</code></pre><h4 id="7-2、输出重定向"><a href="#7-2、输出重定向" class="headerlink" title="7.2、输出重定向"></a>7.2、输出重定向</h4><ul><li><code>&gt;</code> : 覆盖输出重定向，该重定向会覆盖文件上一次的内容，重新写入新内容。</li><li><code>&gt;&gt;</code> : 追加输出重定向，该重定向会将新内容追加到文件末尾，不会覆盖原来的文件内容</li><li><code>set -C</code> : 禁止使用覆盖重定向到已存在的文件</li><li><code>set +C</code> : 与上面的相反</li><li><code>&gt;|</code> : 在 <code>set -C</code> 下强制覆盖重定向</li><li><code>2&gt;</code> : 错误输出覆盖重定向</li><li><code>2&gt;&gt;</code> : 错误输出追加重定向</li><li><code>&amp;&gt;</code> : 同时重定向错误输出和标准输出</li></ul><h4 id="7-3、管道"><a href="#7-3、管道" class="headerlink" title="7.3、管道"></a>7.3、管道</h4><ul><li><code>|</code> : 管道用于将前一个命令的输出重定向为第二个命令的输入，配合 <code>tee</code> 命令可实现 T 形管道</li></ul><h2 id="二、bash-编程"><a href="#二、bash-编程" class="headerlink" title="二、bash 编程"></a>二、bash 编程</h2><ul><li><code>bash -n FILE</code> : 测试 bash 脚本语法</li></ul><h3 id="1、基础语法"><a href="#1、基础语法" class="headerlink" title="1、基础语法"></a>1、基础语法</h3><h4 id="1-1、列表生成"><a href="#1-1、列表生成" class="headerlink" title="1.1、列表生成"></a>1.1、列表生成</h4><ul><li><code>{startNum..endNum}</code> : 生成数字列表，如 <code>{1..10}</code> 展开为1~10</li><li><code>seq</code> : 生成列表命令<ul><li><code>seq start end</code> : 从一个指定的数值开始，依次生成到另一个数直接薇</li><li><code>seq start Steps end</code> : 从第一个数值开始，以指定步长生成列表，如 <code>seq 1 3 10</code> 表示从 1 开始，每次步长为 3 生成列表，到 10 截止。</li></ul></li></ul><h4 id="1-2、for-循环"><a href="#1-2、for-循环" class="headerlink" title="1.2、for 循环"></a>1.2、for 循环</h4><pre><code class="hljs sh"><span class="hljs-keyword">for</span> varName <span class="hljs-keyword">in</span> arry;<span class="hljs-keyword">do</span>  somrthing<span class="hljs-keyword">done</span></code></pre><h4 id="1-3、bash-算术运算"><a href="#1-3、bash-算术运算" class="headerlink" title="1.3、bash 算术运算"></a>1.3、bash 算术运算</h4><ul><li><code>declare</code> : 指定变量声明类型<ul><li><code>i</code> : 声明变量为整型</li><li><code>x</code> : 声明变量为环境变量，与 <code>export</code> 类似</li></ul></li><li><code>let COMMOND</code> : 将后面的表达式视为算是运算</li><li><code>$[COMMOND]</code> : 同 <code>let</code> 相同</li><li><code>$((COMMOND))</code> : 同 <code>let</code> 相同</li><li><code>`expr COMMOND`</code> : 同 <code>let</code> 相同</li></ul><p><strong>bash 支持常见的算术运算，如 <code>+、-、*、/、++、--、%</code> 等，但是 bash <code>/</code> 运算时进行圆整操作，不会产生小数。</strong></p><h4 id="1-4、位置变量"><a href="#1-4、位置变量" class="headerlink" title="1.4、位置变量"></a>1.4、位置变量</h4><ul><li><code>$0</code> : 脚本自身</li><li><code>$1 ...</code> : 脚本的第 N 个参数</li><li><code>$#</code> : 位置参数的个数</li><li><code>$*</code> : 显示所有位置参数</li><li><code>$@</code> : 显示所有位置参数</li></ul><h3 id="2、grep-与-正则"><a href="#2、grep-与-正则" class="headerlink" title="2、grep 与 正则"></a>2、grep 与 正则</h3><p>grep 用于根据 <strong>指定的模式</strong> 在文本中搜索内容，其语法如下：</p><pre><code class="hljs sh">grep [选项]... PATTERN [FILE]...</code></pre><ul><li>v : 反向过滤，显示所有非命中字符的行</li><li>o : 只显示行当中被模式匹配到的字符，而非整行</li><li>i : 不区分字符大小写匹配</li><li>E : 支持扩展正则表达式</li><li>A # : 显示被模式匹配到的行，极其后面的 # 行</li><li>B # : 显示被模式匹配到的行，极其前面的 # 行</li><li>C # : 显示被模式匹配到的行，极其前后的 # 行</li></ul><h4 id="2-1、基本正则表达式"><a href="#2-1、基本正则表达式" class="headerlink" title="2.1、基本正则表达式"></a>2.1、基本正则表达式</h4><p><strong>默认的 grep 正则工作在贪婪模式下，即尽可能多的匹配更多的字符。</strong></p><ul><li>字符匹配<ul><li><code>.</code> : 匹配任意单个字符</li><li><code>[]</code> : 指定范围内的单个字符，如 <code>[a-z]</code></li><li><code>[^]</code> : 指定范围外的单个字符，如 <code>[^A-Z]</code></li></ul></li><li>常用字符匹配规则<ul><li><code>[0-9]</code>，<code>[[:digit:]]</code> : 所有数字</li><li><code>[a-z]</code>，<code>[[:lower:]]</code> : 所有小写字母</li><li><code>[A-Z]</code>，<code>[[:upper:]]</code> : 所有大写字母</li><li><code>[a-zA-Z]</code>，<code>[[:alpha:]]</code> : 所有大小写字母</li><li><code>[a-zA-Z0-9]</code>，<code>[[:alnum:]]</code> : 所有大小写字母加数字</li><li><code>[[:space:]]</code> : 所有空白字符</li><li><code>[[:punct:]]</code> : 所有标点符号</li></ul></li><li>次数匹配<ul><li><code>*</code> : 匹配其前面的字符任意次，<strong><code>.*</code> 代表匹配任意长度任意字符</strong></li><li><code>\?</code> : 匹配其前面的字符0次或1次,<strong><code>\</code> 代表转义</strong></li><li><code>\{m\}</code> : 匹配其前面的字符m次</li><li><code>\{m,n\}</code> : 匹配其前面的字符至少每次最多n次</li><li><code>\{m,\}</code> : 匹配其前面的字符至少m次</li><li><code>\{0,n\}</code> : 匹配其前面的字符最多n次</li></ul></li><li>位置锚定<ul><li><code>^</code> : 锚定行首，必须出现在模式行首</li><li><code>$</code>: 锚定行尾，必须写在模式行尾</li><li><code>^$</code> : <strong>表示空白行</strong></li><li><code>\&lt;</code> : 锚定词首</li><li><code>\&gt;</code> : 锚定词尾</li></ul></li><li>分组<ul><li><code>\(\)</code> : 分组，括号内内容当做一个整体对待</li></ul></li><li>引用<ul><li><code>\1</code> : 后向引用，引用前面第一个分组模式所匹配的内容，其中数字可根据需要变动，如 <code>\2</code></li></ul></li></ul><h4 id="2-2、扩展正则表达式"><a href="#2-2、扩展正则表达式" class="headerlink" title="2.2、扩展正则表达式"></a>2.2、扩展正则表达式</h4><ul><li><p>字符匹配</p><ul><li><code>.</code> : 任意单个字符</li><li><code>[]</code> : 指定范围单个字符</li><li><code>[^]</code> : 指定范围外单个字符</li></ul></li><li><p>次数匹配</p><ul><li><code>*</code> : 任意次</li><li><code>?</code> : 0次或1次</li><li><code>+</code> : 至少1次</li><li><code>{m}</code> : m次</li><li><code>{m,n}</code> : 至少m次，最多n次</li><li><code>{m,}</code> : 至少m次</li><li><code>{0,n}</code> : 最多n次</li></ul></li><li><p>位置锚定</p><ul><li><code>^</code> : 锚定行首</li><li><code>$</code> : 锚定行尾</li><li><code>\&lt;</code>、<code>\b</code> : 锚定词首</li><li><code>\&gt;</code>、<code>\b</code> : 锚定词尾</li></ul></li><li><p>分组匹配</p><ul><li><code>()</code> : 分组</li><li><code>|</code> : 或者</li></ul></li></ul><h3 id="3、bash-条件判断"><a href="#3、bash-条件判断" class="headerlink" title="3、bash 条件判断"></a>3、bash 条件判断</h3><h4 id="3-1、bash-测试"><a href="#3-1、bash-测试" class="headerlink" title="3.1、bash 测试"></a>3.1、bash 测试</h4><p>bash 中使用如下格式进行测试</p><pre><code class="hljs sh"><span class="hljs-built_in">test</span> EXPRESSION[ EXPRESSION ][[ EXPREXXION ]]</code></pre><h4 id="3-2、bash-判断"><a href="#3-2、bash-判断" class="headerlink" title="3.2、bash 判断"></a>3.2、bash 判断</h4><p>bash 一般有以下3种判断</p><h5 id="3-2-1、单分支判断"><a href="#3-2-1、单分支判断" class="headerlink" title="3.2.1、单分支判断"></a>3.2.1、单分支判断</h5><pre><code class="hljs sh"><span class="hljs-keyword">if</span> 条件测试; <span class="hljs-keyword">then</span>    分支语句<span class="hljs-keyword">fi</span></code></pre><h5 id="3-2-2、双分支判断"><a href="#3-2-2、双分支判断" class="headerlink" title="3.2.2、双分支判断"></a>3.2.2、双分支判断</h5><pre><code class="hljs sh"><span class="hljs-keyword">if</span> 条件测试; <span class="hljs-keyword">then</span>    分支语句1<span class="hljs-keyword">else</span>    分支语句2<span class="hljs-keyword">fi</span></code></pre><h5 id="3-2-3、多分支判断"><a href="#3-2-3、多分支判断" class="headerlink" title="3.2.3、多分支判断"></a>3.2.3、多分支判断</h5><pre><code class="hljs sh"><span class="hljs-keyword">if</span> 条件测试1; <span class="hljs-keyword">then</span>    分支语句1<span class="hljs-keyword">elif</span> 条件测试2; <span class="hljs-keyword">then</span>    分支语句2<span class="hljs-keyword">elif</span> 条件测试3; <span class="hljs-keyword">then</span>    分支语句3...<span class="hljs-keyword">else</span>    分支语句n<span class="hljs-keyword">fi</span></code></pre><h4 id="3-3、整数测试"><a href="#3-3、整数测试" class="headerlink" title="3.3、整数测试"></a>3.3、整数测试</h4><p>整数测试为二元表达式，其格式大致为 <code>[ num1 OPRAND num2 ]</code>，基本操作如下</p><ul><li><code>-gt</code> : 大于</li><li><code>-lt</code> : 小于</li><li><code>-ge</code> : 大于等于</li><li><code>-le</code> : 小于等于</li><li><code>eq</code> : 等于</li></ul><h4 id="3-4、字符测试"><a href="#3-4、字符测试" class="headerlink" title="3.4、字符测试"></a>3.4、字符测试</h4><ul><li><code>&gt;</code> : 大于</li><li><code>&lt;</code> : 小于</li><li><code>==</code> : 等于，在 bash 中，事实上由于变量赋值时变量不加 <code>$</code>，而等值比较时则会加上 <code>$</code>，所以使用一个 <code>=</code> 也是可以做等值判断的，不过鉴于规范一般不这么写</li><li><code>=~</code> : 左侧是一个字符串，右侧是一个模式，用于判断某个字符串是否满足给定的模式，通常在 <code>[[]]</code> 中使用；<strong>模式不要加引号</strong></li><li><code>-n</code> : 测试字符串是否为空，不空为真，空为假</li><li><code>-z</code> : 测试字符串是否为空，空为真，不空为假</li></ul><p><strong>注意，测试时操作符两边要有空格，如 <code>[ &quot;str1&quot; == &quot;str2&quot; ]</code></strong></p><h4 id="3-5、文件测试"><a href="#3-5、文件测试" class="headerlink" title="3.5、文件测试"></a>3.5、文件测试</h4><p><code>-e</code>、<code>-a</code> : 测试文件是否存在，如果存在则返回 true<br><code>-f</code> : 测试文件是否存在并且为普通文件<br><code>-d</code> : 测试文件是否存在并且为目录文件<br><code>-b</code> : 测试文件是否存在并且为块设备<br><code>-c</code> : 测试文件是否存在并且为字符设备文件<br><code>-h</code>、<code>-L</code> : 测试文件是否存在并且为符号链接文件<br><code>-p</code> : 测试文件是否存在并且为管道文件<br><code>-r</code> : 测试文件是否存在并且对当前用户具有读权限<br><code>-w</code> : 测试文件是否存在并且对当前用户具有写权限<br><code>-x</code> : 测试文件是否存在并且对当前用户具有执行权限<br><code>-S</code> : 测试文件是否存在并且为套接字文件<br><code>-s</code> : 测试文件是否存在并且文件不为空<br><code>file1 -nt file2</code> : 测试 file1 是否比 file2 更新(最近修改时间)<br><code>file1 -ot file2</code> : 测试 file1 是否比 file2 更老(最近修改时间)<br><code>file1 -ef file2</code> : 测试 file1 和 file2 是否是相同的设备以及 inode 是否相同</p><h4 id="3-6、脚本自定义退出"><a href="#3-6、脚本自定义退出" class="headerlink" title="3.6、脚本自定义退出"></a>3.6、脚本自定义退出</h4><h5 id="3-6-1、脚本退出状态码"><a href="#3-6-1、脚本退出状态码" class="headerlink" title="3.6.1、脚本退出状态码"></a>3.6.1、脚本退出状态码</h5><p><strong>默认 bash 脚本执行完成后，使用 <code>$?</code> 获取脚本执行状态码，其中 <code>0</code> 表示成功，<code>1~255</code> 表示失败。</strong></p><h5 id="3-6-2、自定义退出状态码"><a href="#3-6-2、自定义退出状态码" class="headerlink" title="3.6.2、自定义退出状态码"></a>3.6.2、自定义退出状态码</h5><p><strong>bash 中使用 <code>exit NUM</code> 命令自定义脚本的退出状态码，如果不指定，那么 bash 默认采用最后一条命令的执行状态作为整个脚本执行状态码；也就是说不论前面的命令执行是否失败，只返回最后一条命令的执行状态码。</strong></p><h4 id="3-7、shift"><a href="#3-7、shift" class="headerlink" title="3.7、shift"></a>3.7、shift</h4><p><code>shift</code> : 位置参数轮替</p><p>shift 用于动态替换位置参数，如下所示，当该脚本有多个参数是，使用 shift 将自动替换 $1 为后面的参数，类似于 java 的迭代器</p><pre><code class="hljs sh"><span class="hljs-meta">#!/bin/bash</span>sum=0<span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> `seq 1 <span class="hljs-variable">$#</span>`;<span class="hljs-keyword">do</span>  <span class="hljs-built_in">let</span> sum+=<span class="hljs-variable">$1</span>  <span class="hljs-built_in">shift</span><span class="hljs-keyword">done</span><span class="hljs-built_in">echo</span> <span class="hljs-variable">$sum</span></code></pre><h2 id="三、磁盘管理"><a href="#三、磁盘管理" class="headerlink" title="三、磁盘管理"></a>三、磁盘管理</h2><h3 id="1、文件系统"><a href="#1、文件系统" class="headerlink" title="1、文件系统"></a>1、文件系统</h3><ul><li>按名称存取是文件系统存在的一个主要目的</li><li>文件系统是一个软件，对磁盘上存在的二进制进行管理</li><li>为了能在一个磁盘上安装多个系统，引入了分区的概念</li><li>track: 磁道是有厂商划分好的</li><li>任意磁盘上的任意扇区，读取的平均时间称为平均寻道时间</li></ul><h3 id="2、MBR-Master-Boot-Record"><a href="#2、MBR-Master-Boot-Record" class="headerlink" title="2、MBR(Master Boot Record)"></a>2、MBR(Master Boot Record)</h3><p>MBR(Master Boot Record) 被称为主引导记录，通常存放与磁盘的第0个扇区，其保存着磁盘分区、引导信息。</p><p><strong>通常 MBR 为 512 byte，其中 bootloader(引导加载器) 占用 446 byte，剩下的每16个字节引导一个分区，2个字节被填充了2个5A，称之为MBR有效性标记，所以 MBR 分区表状态下的磁盘最多有 4 个主分区。</strong></p><h3 id="3、硬盘接口"><a href="#3、硬盘接口" class="headerlink" title="3、硬盘接口"></a>3、硬盘接口</h3><ul><li>IDE(ATA) : 并口，每个控制器可接两个硬盘，master/slave，133MB/S</li><li>/dev/hd[a-z]<ul><li>/dev/hda[1-4]</li><li>/dev/hda[5+] 逻辑分区5开始</li></ul></li><li>SCSI : Small Computer System Interface 小型计算机接口，理论速率320mb/s</li><li>SATA(Serial) : 300Mbps,600Mbps,6Gbps</li><li>SAS : 6Gbps</li><li>从 CentOS6 开始磁盘全部识别为 sda</li></ul><h3 id="4、常用命令"><a href="#4、常用命令" class="headerlink" title="4、常用命令"></a>4、常用命令</h3><ul><li><code>fdisk</code> : 磁盘分区命令<ul><li><code>-l</code> : 列出当前系统所有磁盘和分区</li><li><code>-d</code> : 删除分区</li><li><code>-n</code> : 新建一个分区</li><li><code>-p</code> : 列出已有分区</li><li><code>-t</code> : 调至分区ID</li><li><code>-l</code> : 列出内核支持的分区id</li><li><code>-w</code> : 保存退出</li><li><code>-q</code> : 不保存退</li></ul></li><li><code>partprobe</code> : 重读系统磁盘信息(CentOS5)</li><li><code>partx</code>、<code>kpartx</code> : 重读系统磁盘信息(CentOS6)</li></ul><p><strong>一般 CentOS6 后两个命令有时不生效，所以规律是按照以下执行可能会生效，实在不行重启…</strong></p><pre><code class="hljs sh">kpartx -l /dev/sdakpartx -af /dev/sdapartx -a /dev/sda</code></pre><ul><li><p><strong>显示内核当前读取的硬盘信息 <code>cat /proc/partitions</code></strong></p></li><li><p><strong>显示内核当前读取的内存信息 <code>cat /proc/meminfo</code></strong></p></li><li><p><strong>显示内核当前已识别的文件系统 <code>cat /proc/filesystem</code></strong></p></li><li><p><strong>查看当前系统识别了那些磁盘设备 <code>ls /dev/sda*</code></strong></p></li><li><p><code>mkfs DEV</code> : 创建文件系统(格式化)</p><ul><li><code>-t FSTYPE DEV</code> : 将指定设备格式化成指定文件系统</li></ul></li><li><p><code>mke2fs DEV</code> : ext2 文件系统创建的快捷命令，可用于创建其他文件系统，<strong>其配置文件位于 <code>/etc/mke2fs</code>，定义了命令默认的创建文件系统行为</strong></p><ul><li><code>-t</code> : 指定创建的文件系统，同 <code>mkfs -t</code></li><li><code>-j</code> : 快速创建 ext3 文件系统，同 <code>mkfs -t ext3</code></li><li><code>-L</code> : 指定卷标</li><li><code>-b</code> : 指定块大小，一般可选值 <code>1024|2018|4096</code>，默认4k</li><li><code>-i</code> : 设置 inode 值(多少字节预留一个 inode)，配置文件中有默认值</li><li><code>-N</code> : 直接指定预留多少 inode</li><li><code>-I</code> : 直接指定 inode 大小</li><li><code>-m</code> : 预留管理员空间百分比(用于磁盘沾满时操作数据使用)，默认 5%</li></ul></li><li><p><code>e2label DEV</code> : 查看卷标</p></li><li><p><code>e2label DEV LABEL_NAME</code> : 设置卷标</p></li><li><p><code>blkid DEV</code> : 查看磁盘信息，包括全局唯一标示 UUID 和 文件系统类型 TYPE</p></li><li><p><code>dumpe2fs DEV</code> : 查看磁盘块信息，包括超级块、块组、块位图、inode 位图、inode 表、空闲块、空闲 inode 等</p><ul><li><code>-h</code> : 仅显示 超级块信息</li><li><code>-o</code> : 指定显示超级块信息</li></ul></li><li><p><code>tune2fs</code> : 调整磁盘信息，<strong>块大小不能调整</strong></p><ul><li><code>-l DEV</code> : 显示超级块信息</li><li><code>-L LABEL</code> : 设置卷标</li><li><code>-m #</code> : 调整预留百分比</li><li><code>-j</code> : 调整文件系统，如果原来的文件系统为 ext2，则此选项将无损数据将其转换为 ext3，ext3 转 ext2 直接在挂载时挂载为 ext2 即可</li><li><code>-c #</code> : 每挂载 # 次检查文件系统</li><li><code>-o [^]mount-options[,...]</code> : 指定默认挂载选项(<code>^</code>代表关闭，不加代表启动)，如 acl 文件访问控制列表等</li><li><code>-O [^]feature[,...]</code> : 调整其分区特性</li></ul></li><li><p><code>fsck</code> : 文件系统检测命令</p><ul><li><code>-t FSTYPE DEV</code> : 以指定文件系统类型检测磁盘</li><li><code>-f DEV</code> : 强行检测</li><li><code>-r</code> : 交互式提醒修复错误</li><li><code>-a</code> : 自动修复错误</li></ul></li><li><p><code>e2fsck</code> : 快速检测文件系统(相当于快捷命令，文件系统自动判定)</p><ul><li><code>-f</code> : 强制检测</li><li><code>-t</code> : 指定执行时间计数</li><li><code>-y</code> : 自动回答所有问题，默认全部 yes</li></ul></li><li><p><code>交换分区</code> : 虚拟内存</p></li><li><p><code>mkswap DEV</code> : 格式化为交换分区</p><ul><li><code>-L</code> : 指定卷标</li></ul></li><li><p><code>swapon</code>、<code>swapoff</code> : 启动、禁用交换区</p><ul><li><code>-a</code> : 启用所有交换区</li><li><code>-p #</code> : 指定优先级(仅 on)</li></ul></li><li><p><code>hdparm</code> : 读取磁盘硬件信息</p><ul><li><code>i</code> : 显示磁盘磁头信息(通过内核)</li><li><code>I</code> : 显示磁盘磁头信息(通过硬件)</li><li><code>g</code> : 显示磁盘硬件布局信息</li><li><code>t</code> : 测试磁盘 buffer cache 信息</li><li><code>T</code> : 测试磁盘直接读取写入信息</li></ul></li></ul><h3 id="5、磁盘挂载"><a href="#5、磁盘挂载" class="headerlink" title="5、磁盘挂载"></a>5、磁盘挂载</h3><ul><li><strong>挂载:</strong><ul><li><strong>手动挂载</strong></li><li><strong>自动挂载</strong></li><li><strong>按需挂载</strong> : 进程访问时挂载，访问结束卸载</li></ul></li><li><code>挂载点</code> : 一旦某个目录被作为挂载点，则原目录下所有文件将被暂时隐藏，挂载点空闲时才可被卸载，也可以强制卸载</li><li><code>mount [options] [-t fstype] [-o options] DEV DIR</code> : 挂载命令，不带任何选项默认显示当前系统所有已挂在设备，实质是读取了 <code>/proc/mounts</code>，<strong><code>/etc/mtab</code> 文件存放了所有 mount 命令的挂载点信息</strong><ul><li><code>-t</code> : 指定文件系统类型</li><li><code>-r</code> : 只读方式挂载</li><li><code>-w</code> : 读写挂载</li><li><code>-o</code> : 指定额外的 options</li><li><code>-L</code> : 根据卷边挂载</li><li><code>LABLE=&#39;XXXX&#39;</code> : 根据卷标挂载</li><li><code>-U</code> : 以 UUID 方式挂载</li><li><code>UUID=&#39;xxxx&#39;</code> : 以 UUID 方式挂载</li><li><code>-a</code> : 自动挂载所有的支持自动挂载的设备(需先在/etc/fstab中定义)</li></ul></li><li><code>mount -o</code> : 设置磁盘硬件属性<ul><li><code>async</code> : 启用异步 I/O</li><li><code>sync</code> : 启用同步 I/O</li><li><code>atime</code> : 启用实时更新文件时间戳</li><li><code>noatime</code> : 禁用实时更新文件时间戳</li><li><code>auto</code> : 启用自动挂载(-a 支持设备)</li><li><code>noauto</code> : 禁用自动挂载(-a 支持设备)</li><li><code>exec</code> : 开启自动执行</li><li><code>noexec</code> : 禁止自动执行</li><li><code>group</code> : 此组内用户可挂载该设备</li><li><code>_netdev</code> : 禁止网络设备未初始化前挂载，在需要挂载网络硬盘时，如果设置了开机自动挂载，当网络设备未完全初始化前挂载网络硬盘，则可能造成卡死，此选项可确保网络谁被初始化后才被允许挂载</li><li><code>remount</code> : 自动重新挂载</li><li><code>ro</code> : 挂载为只读</li><li><code>rw</code> : 挂载为读写</li><li><code>acl</code> : 启用文件访问控制列表</li></ul></li><li><code>umount DEV|MOUNT_POINT</code> : 卸载命令</li><li><code>fuser OPTIONS DEV</code> : 查看磁盘访问占用信息<ul><li><code>-v</code> : 查看哪些用户正在访问目标磁盘</li><li><code>-k</code> : 杀死正在访问磁盘进程</li><li><code>-m</code> : 指明挂载点(一般km一起使用踢出占用磁盘的用户)</li></ul></li><li><code>df DEV</code> : 查看磁盘占用情况，不加设备显示所有<ul><li><code>-h</code> : 以以人易读的格式进行显示</li><li><code>-i</code> : 显示 inode 信息</li></ul></li><li><code>du</code> : 评估文件占用磁盘空间情况<ul><li><code>-s</code> : 显示目录大小</li></ul></li></ul><h3 id="6、开机自动挂载"><a href="#6、开机自动挂载" class="headerlink" title="6、开机自动挂载"></a>6、开机自动挂载</h3><ul><li><code>/etc/rc.d/rc.sysinit</code> : 系统初始化脚本，包括挂载 <code>/etc/fstab</code> 文件中定义的文件系统挂载点等</li><li><code>/etc/fstab</code> : 系统自动挂载配置文件，格式如下 :</li></ul><table><thead><tr><th>待挂载设备</th><th>挂载点</th><th>文件系统格式</th><th>挂载参数</th><th>转储频率</th><th>自检顺序</th></tr></thead><tbody><tr><td>proc</td><td>/proc</td><td>proc</td><td>defaults</td><td>0</td><td>0</td></tr><tr><td>/dev/mapper/vg0-usr</td><td>/usr</td><td>ext4</td><td>defaults</td><td>1</td><td>2</td></tr></tbody></table><ul><li>待挂载设备 : 通常为设备文件，也支持卷标 LABEL=xxxx 或 UUID=xxxx 方式</li><li>挂载点 : 通常是一个目录，但有些文件系统没有挂载点，如 swap 分区，挂载点即为 swap</li><li>挂载选项 : 一般为 defaults，自定义多个的话用逗号隔开</li><li>转储频率 : 系统对其进行 dump 频率，0是从不备份，1为每天备份，2为2天备份一次</li><li>自检顺序 : 系统开机自检文件系统顺序，1为首先自检，通常是根目录独有，2~9为先后顺序，0为不自检</li></ul><h3 id="7、交换分区"><a href="#7、交换分区" class="headerlink" title="7、交换分区"></a>7、交换分区</h3><p><strong>交换分区用于在系统内存不够用时将其转储到硬盘，在 <code>/proc/sys/vm/swapiness</code> 文件中有其使用倾向定义；一般当物理内存小于 2G 通常设置为 2*物理内存，2~4G 设置为4G，当大于4G时署名服务器对性能要求很高，基本没有什么要求</strong></p><ul><li><code>free</code> : 查看内存使用情况<ul><li><code>-h</code> : 以人类可读的形式显示</li><li><code>-s #</code> : 每隔 # 秒打印一次内存使用情况</li><li><code>-c #</code> : 与 -s 配合使用，表示打印 # 次后停止打印</li></ul></li><li><code>dd</code> : 低级复制命令，dd 命令会跨过文件系统直接复制磁盘块，相对于cp属于更低级的复制命令<ul><li><code>if</code> : input file，源文件</li><li><code>of</code> : out file，目标文件</li><li><code>bs</code> : 每次复制块大小，可选 <code>[b|kb|m|g]</code></li><li><code>count</code> : 复制块次数</li><li><code>oflag=FLAGS</code> : 创建稀疏文件</li><li><code>dd if=/dev/sda of=/dev/sdb</code> : 磁盘对拷</li><li><code>dd if=/dev/cdrom of=/tmp/linux.iso</code> : iso 镜像提取</li><li><code>dd if=/dev/zero of=/dev/sda bs=512 count=1</code> : 抹掉MBR</li><li><code>dd if=/dev/zero of=/swapfile bs=512M count=1</code> : 创建一个  512M 的本地回环设备文件</li></ul></li></ul><h3 id="8、软链接与硬链接"><a href="#8、软链接与硬链接" class="headerlink" title="8、软链接与硬链接"></a>8、软链接与硬链接</h3><p><strong>硬链接 :</strong> 硬链接实质上是对同一分区上多个路径指向了同一 inode 的文件，<strong>硬链接不可跨分区，目录无法创建硬链接。</strong></p><p><strong>软链接 :</strong> 软链接实质上与源文件是两个文件，且 inode 不同，但是软连接文件的 inode 中会存储源文件的路径信息，可跨分区，可对目录创建。</p><ul><li><code>ln SOURCE_FILE TARGET_FILE</code> : 创建硬链接文件<ul><li><code>-s</code> : 创建软链接</li><li><code>-v</code> : 显示创建过程</li></ul></li></ul><h3 id="9、压缩工具"><a href="#9、压缩工具" class="headerlink" title="9、压缩工具"></a>9、压缩工具</h3><ul><li><code>zip</code> : 归档压缩工具，需要指定输出文件，可压缩目录；但是想压缩目录下的文件时必须指定到文件，如 <code>zip tmp.zip /tmp/*</code></li><li><code>gzip</code>、<code>gunzip</code> : gzip SRC TARGET 压缩(删除源文件)<ul><li><code>-d</code> : 同 gunzip 解压缩(删除源文件)</li><li><code>-c</code> : 将压缩结果输出到 STDOUT，可使用管道等进行重定向操作</li><li><code>-r</code> : 递归压缩指定目录下的所有文件</li><li><code>-#</code> : 设置压缩率，默认6，取值1~9，其他压缩命令同样支持</li></ul></li><li><code>zcat</code> : 不解压查看 gzip 压缩包内容<ul><li><code>-l</code> : 列出压缩包内文件列表</li><li><code>-r</code> : 递归列出文件列表</li></ul></li><li><code>bzip2</code>、<code>bunzip2</code> : bzip2 压缩命令<ul><li><code>-d</code> : 解压文件</li><li><code>k</code> : 保留原文件压缩/解压</li></ul></li><li><code>bzcat</code> : 不解压查看 bzip2 压缩包内容</li><li><code>xz</code> : xz 压缩命令，更高级的压缩工具，压缩删除源文件<ul><li><code>-d</code> : 解压文件</li></ul></li><li><code>xzcat</code> : 不解压查看 xz 压缩包内容</li></ul><h3 id="10、归档工具"><a href="#10、归档工具" class="headerlink" title="10、归档工具"></a>10、归档工具</h3><ul><li><code>tar</code> : 创建归档命令，一般命令格式为 <code>tar [options] -f FILE.tar File1...</code>，其中源文件可以是文件也可以是目录<ul><li><code>-c</code> : 创建归档</li><li><code>-x</code> : 展开归档</li><li><code>-t</code> : 不展开查看归档文件列表</li><li><code>-z</code> : 调用 gzip 压缩</li><li><code>-j</code> : 调用 bzip2 压缩</li><li><code>-J</code> : 调用 xz 压缩</li><li><code>-v</code> : 展示过程</li></ul></li></ul>]]></content>
    
    
    <categories>
      
      <category>Linux</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Linux</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Maven 教程</title>
    <link href="/2006/01/02/maven-tutorial/"/>
    <url>/2006/01/02/maven-tutorial/</url>
    
    <content type="html"><![CDATA[<p><img src="https://cdn.oss.link/markdown/logo-maven.png" srcset="/img/loading.gif" alt="logo-maven"></p><h3 id="一、Maven简介"><a href="#一、Maven简介" class="headerlink" title="一、Maven简介"></a>一、Maven简介</h3><blockquote><p>Maven是一个开源的构建工具，它可以帮助我们管理项目的构建过程，管理项目的生命周期，jar包依赖关系等。Maven配合持续集成可以实现自动化的编译、测试、打包、发布等强大的功能，尤其在持续集成上有为我们带来了很大便利。</p></blockquote><h3 id="二、Maven的简单命令及配置"><a href="#二、Maven的简单命令及配置" class="headerlink" title="二、Maven的简单命令及配置"></a>二、Maven的简单命令及配置</h3><ul><li>下载Maven <a href="https://maven.apache.org" target="_blank" rel="noopener">https://maven.apache.org</a></li></ul><ul><li>解压并配置环境变量</li></ul><blockquote><p>1、首先解压Maven到任意目录：</p></blockquote><p><img src="https://cdn.oss.link/markdown/maven-unzip.png" srcset="/img/loading.gif" alt="maven-unzip"></p><blockquote><p>2、新建环境变量 <code>M2_HOME</code>，值为 <strong><em>Maven主目录</em></strong></p></blockquote><p><img src="https://cdn.oss.link/markdown/Maven-M2_HOME.png" srcset="/img/loading.gif" alt="Maven-M2_HOME"></p><blockquote><p>3、在 <code>Path</code> 变量中添加 Maven的bin目录</p></blockquote><p><img src="https://cdn.oss.link/markdown/Maven-Path.png" srcset="/img/loading.gif" alt="Maven-Path"></p><blockquote><p>4、测试 Maven 是否安装成功</p></blockquote><p><img src="https://cdn.oss.link/markdown/Maven-test.png" srcset="/img/loading.gif" alt="Maven-test"></p><ul><li>更改Maven 默认仓库地址</li></ul><blockquote><p>默认情况下Mavne 会将jar包插件等下载到 ${user.home}/.m2/repository 下，在Windows 系统下通常是用户目录下的.m2隐藏文件夹，这个文件夹在C盘，我们可以通过更改 Maven的配置文件 M2_HOME/conf/setting.xml 来更改默认的本地仓库位置</p></blockquote><p><img src="https://cdn.oss.link/markdown/Maven-Respository.png" srcset="/img/loading.gif" alt="Maven-Respository"></p><ul><li>Eclipse中设置maven</li></ul><blockquote><p>在Eclipse中设置使用自己下载的Maven <strong>*Windows-&gt;Preferences-&gt;Maven-&gt;Installations-&gt;add *</strong></p></blockquote><p><img src="https://cdn.oss.link/markdown/Maven-Eclipse-Setting1.png" srcset="/img/loading.gif" alt="Maven-Eclipse-Setting1"></p><blockquote><p>然后勾选新添加的 Mavne 并应用设置 在选择 <code>User Setting</code> 设置 配置文件</p></blockquote><p><img src="https://cdn.oss.link/markdown/Maven-Eclipse-Setting2.png" srcset="/img/loading.gif" alt="Maven-Eclipse-Setting2"></p><blockquote><p>第一个 <code>Global Setting</code> 为全局设置，一般选择 <code>M2_HOME/conf/setting.xml</code>；<code>User Setting</code> 是用户设置，一般copy 一份 <code>setting.xml</code> 到任意位置，然后根据自己需要改变一些内容；Maven的配置文件同样遵循”就近原则”；也就是说 <code>User Setting</code> 可以覆盖 <code>Global Setting</code>；如果仅仅自己使用的话，也可以懒一点 两个都选择 M2_HOME/conf/setting.xml，然后只更改 一个配置文件就行了。</p></blockquote><h3 id="三、Maven的使用"><a href="#三、Maven的使用" class="headerlink" title="三、Maven的使用"></a>三、Maven的使用</h3><ul><li>在Eclipse中建立Maven项目</li></ul><blockquote><p>首先 New 一个 <code>Maven Project</code></p></blockquote><p><img src="https://cdn.oss.link/markdown/Maven-create-project.png" srcset="/img/loading.gif" alt="Maven-create-project"></p><blockquote><p>然后直接下一步(让你选择工作空间啥的，默认就行)</p></blockquote><p><img src="https://cdn.oss.link/markdown/Maven-create-project1.png" srcset="/img/loading.gif" alt="Maven-create-project1"></p><blockquote><p>选择骨架类型，一般常用的 是 <code>quickstart</code> 和 <code>web app</code>；骨架的意思是项目的目录结构，maven采用约定优于配置的理念，项目结构已经约定好了，有兴趣可以自己打开工作空间看一下。</p></blockquote><p><img src="https://cdn.oss.link/markdown/Maven-create-project2.png" srcset="/img/loading.gif" alt="Maven-create-project2"></p><blockquote><p>填写项目坐标(如果为一个模块，写坐标打包后可被其他项目引用)；Artifact ID将作为项目名称。</p></blockquote><p><img src="https://cdn.oss.link/markdown/Maven-create-project3.png" srcset="/img/loading.gif" alt="Maven-create-project3"></p><ul><li>简单的 jar 包管理</li></ul><blockquote><p>Maven可以很好的为我们管理jar 包中的依赖关系，而我们不必进行像以前一样去下载 jar包，然后添加 <code>class path</code> 这种繁琐的操作，使用Maven后只要我们得知了该jar的仓库坐标，在 <code>POM文件</code> 中添加其坐标后，Maven将自动下载并将其加入 <code>class path</code> 中。假设现在我们需要使用 <code>SpringMVC</code>，首先到 <a href="http://mvnrepository.com/" target="_blank" rel="noopener">maven仓库</a> 搜索其坐标。</p></blockquote><p><img src="https://cdn.oss.link/markdown/Maven-repository-search.png" srcset="/img/loading.gif" alt="Maven-repository-search"></p><blockquote><p>然后打开选择一个版本</p></blockquote><p><img src="https://cdn.oss.link/markdown/Maven-repository-search2.png" srcset="/img/loading.gif" alt="Maven-repository-search2"></p><p><img src="https://cdn.oss.link/markdown/Maven-repository-search3.png" srcset="/img/loading.gif" alt="Maven-repository-search3"></p><blockquote><p>复制Maven坐标</p></blockquote><p><img src="https://cdn.oss.link/markdown/Maven-repository-search4.png" srcset="/img/loading.gif" alt="Maven-repository-search4"></p><blockquote><p>将其坐标加入到 <code>POM文件</code> 中</p></blockquote><p><img src="https://cdn.oss.link/markdown/Maven-repository-search5.png" srcset="/img/loading.gif" alt="Maven-repository-search5"></p><blockquote><p> 稍等片刻，Maven自动下载jar包并加入 <code>classpath</code> 中</p></blockquote><p><img src="https://cdn.oss.link/markdown/Maven-repository-search6.png" srcset="/img/loading.gif" alt="Maven-repository-search6"></p><ul><li>maven的请求下载jar包工作流程如下图所示(有点丑……)</li></ul><p><img src="https://cdn.oss.link/markdown/Maven-liucheng.png" srcset="/img/loading.gif" alt="Maven-liucheng"></p><ul><li>Maven的常用命令</li></ul><pre><code class="hljs bash"><span class="hljs-comment">#编译命令</span><span class="hljs-comment">#执行此命令 maven将自动编译src/main/java 下的java文件到</span><span class="hljs-comment">#target\classes目录</span>mvn compile<span class="hljs-comment">#测试命令</span><span class="hljs-comment">#执行此命令 maven将自动编译并执行src/test/java 下的java文件</span><span class="hljs-comment">#maven支持JUnit、TestNG等测试框架，@Test注解的方法将自动被执行</span><span class="hljs-comment">#具体可参考 http://seanzhou.iteye.com/blog/1393858</span>mvn <span class="hljs-built_in">test</span><span class="hljs-comment">#清理命令</span><span class="hljs-comment">#执行此命令将会清空 target目录，清空相关垃圾缓存文件</span>mvn clean<span class="hljs-comment">#打包命令</span><span class="hljs-comment">#执行此命令将对当前项目根据pom文件的描述进行打包</span><span class="hljs-comment">#可打包为 jar、war 等文件格式，便于其他项目使用</span>mvn package<span class="hljs-comment">#安装发布命令</span><span class="hljs-comment">#执行此命令 将自动将项目打包为jar包并根据pom文件内描述坐标</span><span class="hljs-comment">#发布到本地Respoitory中，其他项目可根据其坐标引用</span>mvn install<span class="hljs-comment">#创建骨架</span><span class="hljs-comment">#执行此命令可快速创建maven的项目骨架(文件目录结构和pom文件)</span>mvn archetype:generate</code></pre><blockquote><p>在 Eclipse 中可右键项目或 <code>POM文件</code>，选择 <code>Run AS</code> 中的 <code>Maven build…</code> 选项手动输入多个命令，Maven将依次执行，截图如下</p></blockquote><p><img src="https://cdn.oss.link/markdown/Maven-RunAs1.png" srcset="/img/loading.gif" alt="Maven-RunAs1"></p><p><img src="https://cdn.oss.link/markdown/Maven-RunAs2.png" srcset="/img/loading.gif" alt="Maven-RunAs2"></p><ul><li>项目模块间的引用</li></ul><blockquote><p>在实际开发中，通常我们将项目分为N多模块进行开发，我们可以使用Maven建立多个项目，然后如果项目模块关联时，比如A模块要调用B模块的方法，我们只需在B项目上运行 <code>mvn clean</code> 、<code>mvn install</code> 命令，将其发布到本地仓库，其他模块可根据其坐标直接引入即可。</p></blockquote><ul><li>Maven中的隐含变量</li></ul><blockquote><p>在项目多模块的配置时，groupid往往是重复的，某些高级设置时某些位置也可能是重复的，Maven为我们提供了通过变量引用的方式来动态拿到指定值.</p></blockquote><blockquote><p>Maven提供了三个隐式的变量可以用来访问 <code>环境变量</code>、 <code>POM信息</code> 和 <code>Maven Settings</code>：</p></blockquote><blockquote><ul><li>env变量:</li></ul></blockquote><blockquote><p>env变量，暴露了你操作系统或者shell的环境变量。如在Maven POM中一个对${env.PATH}的引用将会被${PATH}环境变量替换，在Windows中为%PATH%.</p></blockquote><blockquote><ul><li>project变量</li></ul></blockquote><blockquote><p> project变量暴露了POM，是使用最多的一个变量，可以使用点标记(.)的路径来引用POM元素的值；如下图</p></blockquote><p><img src="https://cdn.oss.link/markdown/Maven-var1.png" srcset="/img/loading.gif" alt="Maven-var1"></p><blockquote><p>具体值参考如下(eclipse一般都有提示，没有自行引入maven的dtd文件，此段内容引自 <a href="http://northc.iteye.com/blog/1507571" target="_blank" rel="noopener">iteye</a>)：</p></blockquote><pre><code class="hljs bash"><span class="hljs-comment">#项目根目录</span><span class="hljs-variable">$&#123;basedir&#125;</span><span class="hljs-comment">#构建目录，缺省为target</span><span class="hljs-variable">$&#123;project.build.directory&#125;</span><span class="hljs-comment">#构建过程输出目录，缺省为target/classes</span><span class="hljs-variable">$&#123;project.build.outputDirectory&#125;</span><span class="hljs-comment">#产出物名称，缺省为 $&#123;project.artifactId&#125;-$&#123;project.version&#125;</span><span class="hljs-variable">$&#123;project.build.finalName&#125;</span><span class="hljs-comment">#打包类型，缺省为jar</span><span class="hljs-variable">$&#123;project.packaging&#125;</span><span class="hljs-comment">#当前pom文件的任意节点的内容</span><span class="hljs-variable">$&#123;project.xxx&#125;</span></code></pre><h3 id="四、Maven的依赖"><a href="#四、Maven的依赖" class="headerlink" title="四、Maven的依赖"></a>四、Maven的依赖</h3><blockquote><p>每个项目都有其classpath，在Maven管理下的项目实际上有3个classpath(<code>编译classpath</code>、<code>测试classpath</code>、<code>运行classpath</code> );每个被maven管理的jar包在这三种 <code>classpath</code> 是否生效及如何选择称之为maven的依赖管理，Maven依赖管理可通过 <code>&lt;dependency&gt;</code> 下的 <code>&lt;scope&gt;</code> 标签设置，其值含义如下</p></blockquote><pre><code class="hljs bash">compile: 编译依赖范围。如果没有指定，就会默认使用该依赖范围。使用此依赖范围的Maven依赖，对于编译、测试、运行三种classpath都有效。<span class="hljs-built_in">test</span>: 测试依赖范围。使用此依赖范围的Maven依赖，只对于测试classpath有效，在编译主代码或者运行项目的使用时将无法使用此类依赖。典型的例子就是JUnit，它只有在编译测试代码及运行测试的时候才需要。provided: 已提供依赖范围。使用此依赖范围的Maven依赖，对于编译和测试classpath有效，但在运行时无效。典型的例子是servlet-api，编译和测试项目的时候需要该依赖，但在运行项目的时候，由于容器已经提供，就不需要Maven重复地引入一遍。runtime: 运行时依赖范围。使用此依赖范围的Maven依赖，对于测试和运行classpath有效，但在编译主代码时无效。典型的例子是JDBC驱动实现，项目主代码的编译只需要JDK提供的JDBC接口，只有在执行测试或者运行项目的时候才需要实现上述接口的具体JDBC驱动。system: 系统依赖范围。该依赖与三种classpath的关系，和provided依赖范围完全一致。但是，使用system范围依赖时必须通过systemPath元素显式地指定依赖文件的路径。由于此类依赖不是通过Maven仓库解析的，而且往往与本机系统绑定，可能造成构建的不可移植，因此应该谨慎使用。systemPath元素可以引用环境变量，如：&lt;systemPath&gt;<span class="hljs-variable">$&#123;java.home&#125;</span>/lib/rt.jar&lt;/systemPath&gt;import(Maven 2.0.9及以上): 导入依赖范围。该依赖范围不会对三种classpath产生实际的影响</code></pre><blockquote><p>例如我们设置JUint，只有测试时用到，scope 只需设置成 test即可</p></blockquote><p><img src="https://cdn.oss.link/markdown/Maven-scope1.png" srcset="/img/loading.gif" alt="Maven-scope1"></p><ul><li>Maven依赖传递性</li></ul><blockquote><p>当A项目依赖某一jar包，比如log4j 时，将其发布到本地仓库后，B项目进行引用时，则同样会依赖log4j；这种情况称之为 <code>Maven的依赖传递</code> ，当 <code>scope</code> 设置为 <code>test</code> 时，则依赖不会被传递。</p></blockquote><ul><li>Maven复杂依赖</li></ul><blockquote><p>单项目依赖多模块，多模块中jar包版本冲突，如下图</p></blockquote><p><img src="https://cdn.oss.link/markdown/Maven-yilai1.png" srcset="/img/loading.gif" alt="Maven-yilai1"></p><blockquote><p>在此种情况下，Maven无法自动判断jar包x的版本；模块C中的jar包x版本取决于C模块的pom文件中 模块A、B的书写位置，也就是说谁先写在上面，那么就先使用谁的,如下</p></blockquote><p><img src="https://cdn.oss.link/markdown/Maven-yilai2.png" srcset="/img/loading.gif" alt="Maven-yilai2"></p><blockquote><p>单项目依赖多模块，多模块中jar包版本冲突，但单项目(模块C)中jar包x版本却并非取决于书写顺序，情况如下图</p></blockquote><p><img src="https://cdn.oss.link/markdown/Maven-yilai21.png" srcset="/img/loading.gif" alt="Maven-yilai21"></p><blockquote><p>假设模块C的pom文件中仍是先写得模块A；但此时在 模块C中 jar包x的版本仍是2.0；原因是模块A依赖了jar包K，而jar包K又依赖了jar包x；相当于jar包x到模块C之间存在二级关系，此时Maven的依赖选择以最近的为主，如果距离(层级)相同，那么再根据书写顺序判定选择那个，否则优先选择距离(层级)最近的，在Eclipse中可通过视图查看依赖层级关系，如下</p></blockquote><p><img src="https://cdn.oss.link/markdown/Maven-yilai3.png" srcset="/img/loading.gif" alt="Maven-yilai3"></p><ul><li>Maven依赖的排除</li></ul><blockquote><p>当项目模块中存在jar包冲突时，或想使用某个模块中指定的jar包版本，我们可以使用<exclusions>标签进行排除某个jar包，如下</p></blockquote><p><img src="https://cdn.oss.link/markdown/Maven-yilai4.png" srcset="/img/loading.gif" alt="Maven-yilai4"></p><h3 id="五、Maven的聚合与继承"><a href="#五、Maven的聚合与继承" class="headerlink" title="五、Maven的聚合与继承"></a>五、Maven的聚合与继承</h3><ul><li>Maven的聚合</li></ul><blockquote><p>我们已经知道Maven中多模块(项目)是可以互相引用的，但在实际开发中往往会遇到一些问题；当模块足够多时，每一次编译打包也就意味着我们要在N多个模块(项目)上执行 clean 、package 等命令，这显然是不能接受的；Maven为我们提供了项目(模块)的聚合功能，也就是说模块最终要统一到一个项目，每次打包编译，我们只需对 “总项目” 进行即可，而无需关心各个模块</p></blockquote><blockquote><p>1、首先我们创建一个没有骨架的项目(new 一个Maven项目)，在下面这步注意要勾选 “跳过骨架选择”</p></blockquote><p><img src="https://cdn.oss.link/markdown/Maven-juhe3.png" srcset="/img/loading.gif" alt="Maven-juhe3"></p><blockquote><p>2、填入相关信息，注意打包形式选择pom</p></blockquote><p><img src="https://cdn.oss.link/markdown/Maven-juhe4.png" srcset="/img/loading.gif" alt="Maven-juhe4"></p><blockquote><p>3、创建好项目以后我们编辑pom文件，使用<modules>标签指定该项目聚合哪些项目(模块)<br><strong>*<module>标签使用相对路径，指向其他项目(模块)，一般都是 ../xxxx*</strong></p></blockquote><p><img src="https://cdn.oss.link/markdown/Maven-juhe5.png" srcset="/img/loading.gif" alt="Maven-juhe5"></p><blockquote><p>4、此时对聚合项目执行一下 <code>clean compile</code> 测试一下，控制台打印如下，可以看出，Maven依次对3个项目执行了相关操作</p></blockquote><p><img src="https://cdn.oss.link/markdown/Maven-juhe6.png" srcset="/img/loading.gif" alt="Maven-juhe6"></p><ul><li>Maven的集成</li></ul><blockquote><p>当成功聚合了多个模块以后，我们还会发现在每个项目模块中存在大量的相同代码，这让我们感觉很不爽；有点 “造轮子” 的嫌疑，当然Maven同样支持继承的操作，也就是说将模块中的相同代码放到父类项目(模块)中，其他模块只需继承即可使用，同时也能很好地解决jar包版本冲突问题</p></blockquote><blockquote><p>1、首先看一下 各个模块的pom文件，这里面很多都是重复的</p></blockquote><p><img src="https://cdn.oss.link/markdown/Maven-juhe1.png" srcset="/img/loading.gif" alt="Maven-juhe1"></p><p><img src="https://cdn.oss.link/markdown/Maven-juhe2.png" srcset="/img/loading.gif" alt="Maven-juhe2"></p><blockquote><p>2、我们同样新建一个Maven项目，跳过骨架创建</p></blockquote><p><img src="https://cdn.oss.link/markdown/Maven-extends.png" srcset="/img/loading.gif" alt="Maven-extends"></p><blockquote><p>3、然后我们提取其他子模块pom文件中的共有部分放入父pom文件中(以下为父pom文件)，包括共有的定义以及每个项目的依赖jar包(依赖的jar包需要使用 <code>&lt;dependencyManagement&gt;</code> 标签来管理)</p></blockquote><p><img src="https://cdn.oss.link/markdown/Maven-extends1.png" srcset="/img/loading.gif" alt="Maven-extends1"></p><blockquote><p>4、此时我们可以在子项目模块中通过<parent>标签指定继承的Pom文件</p></blockquote><blockquote><p><strong><em>注意：聚合 <code>&lt;module&gt;</code> 标签指定的是相对的项目，而继承指定的则是相对的pom文件</em></strong></p></blockquote><p><img src="https://cdn.oss.link/markdown/Maven-extends2.png" srcset="/img/loading.gif" alt="Maven-extends2"></p><blockquote><p>5、当继承后我们便可以删除公有的相同内容，如 <url>等；同时依赖的jar包也可不必指定版本</p></blockquote><p><img src="https://cdn.oss.link/markdown/Maven-extends3.png" srcset="/img/loading.gif" alt="Maven-extends3"></p><blockquote><p>6、到此，继承完成，由于依赖jar包都来自父pom，同时子pom依赖的jar包无需书写版本，这样在一定程度上保证了jar包版本的统一性；同时我们可以发现，聚合和继承十分相似，其实我们可以将聚合也加入到这个pom中来，实现一个项目对多个项目(模块)的继承与聚合，如下为示例的pom文件。</p></blockquote><pre><code class="hljs xml"><span class="hljs-tag">&lt;<span class="hljs-name">project</span> <span class="hljs-attr">xmlns</span>=<span class="hljs-string">"http://maven.apache.org/POM/4.0.0"</span> <span class="hljs-attr">xmlns:xsi</span>=<span class="hljs-string">"http://www.w3.org/2001/XMLSchema-instance"</span></span><span class="hljs-tag"><span class="hljs-attr">xsi:schemaLocation</span>=<span class="hljs-string">"http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd"</span>&gt;</span><span class="hljs-tag">&lt;<span class="hljs-name">modelVersion</span>&gt;</span>4.0.0<span class="hljs-tag">&lt;/<span class="hljs-name">modelVersion</span>&gt;</span><span class="hljs-tag">&lt;<span class="hljs-name">groupId</span>&gt;</span>me.mritd.Test1<span class="hljs-tag">&lt;/<span class="hljs-name">groupId</span>&gt;</span><span class="hljs-tag">&lt;<span class="hljs-name">artifactId</span>&gt;</span>MavenParent<span class="hljs-tag">&lt;/<span class="hljs-name">artifactId</span>&gt;</span><span class="hljs-tag">&lt;<span class="hljs-name">version</span>&gt;</span>0.0.1-SNAPSHOT<span class="hljs-tag">&lt;/<span class="hljs-name">version</span>&gt;</span><span class="hljs-tag">&lt;<span class="hljs-name">packaging</span>&gt;</span>pom<span class="hljs-tag">&lt;/<span class="hljs-name">packaging</span>&gt;</span><span class="hljs-tag">&lt;<span class="hljs-name">modules</span>&gt;</span><span class="hljs-tag">&lt;<span class="hljs-name">module</span>&gt;</span>../MavenTest1<span class="hljs-tag">&lt;/<span class="hljs-name">module</span>&gt;</span><span class="hljs-tag">&lt;<span class="hljs-name">module</span>&gt;</span>../MavenTest2<span class="hljs-tag">&lt;/<span class="hljs-name">module</span>&gt;</span><span class="hljs-tag">&lt;<span class="hljs-name">module</span>&gt;</span>../MavenTest3<span class="hljs-tag">&lt;/<span class="hljs-name">module</span>&gt;</span><span class="hljs-tag">&lt;/<span class="hljs-name">modules</span>&gt;</span><span class="hljs-tag">&lt;<span class="hljs-name">url</span>&gt;</span>http://maven.apache.org<span class="hljs-tag">&lt;/<span class="hljs-name">url</span>&gt;</span><span class="hljs-tag">&lt;<span class="hljs-name">properties</span>&gt;</span><span class="hljs-tag">&lt;<span class="hljs-name">project.build.sourceEncoding</span>&gt;</span>UTF-8<span class="hljs-tag">&lt;/<span class="hljs-name">project.build.sourceEncoding</span>&gt;</span><span class="hljs-tag">&lt;/<span class="hljs-name">properties</span>&gt;</span><span class="hljs-tag">&lt;<span class="hljs-name">dependencyManagement</span>&gt;</span><span class="hljs-tag">&lt;<span class="hljs-name">dependencies</span>&gt;</span><span class="hljs-tag">&lt;<span class="hljs-name">dependency</span>&gt;</span><span class="hljs-tag">&lt;<span class="hljs-name">groupId</span>&gt;</span>junit<span class="hljs-tag">&lt;/<span class="hljs-name">groupId</span>&gt;</span><span class="hljs-tag">&lt;<span class="hljs-name">artifactId</span>&gt;</span>junit<span class="hljs-tag">&lt;/<span class="hljs-name">artifactId</span>&gt;</span><span class="hljs-tag">&lt;<span class="hljs-name">version</span>&gt;</span>4.11<span class="hljs-tag">&lt;/<span class="hljs-name">version</span>&gt;</span><span class="hljs-tag">&lt;<span class="hljs-name">scope</span>&gt;</span>test<span class="hljs-tag">&lt;/<span class="hljs-name">scope</span>&gt;</span><span class="hljs-tag">&lt;/<span class="hljs-name">dependency</span>&gt;</span><span class="hljs-tag">&lt;<span class="hljs-name">dependency</span>&gt;</span><span class="hljs-tag">&lt;<span class="hljs-name">groupId</span>&gt;</span>commons-dbutils<span class="hljs-tag">&lt;/<span class="hljs-name">groupId</span>&gt;</span><span class="hljs-tag">&lt;<span class="hljs-name">artifactId</span>&gt;</span>commons-dbutils<span class="hljs-tag">&lt;/<span class="hljs-name">artifactId</span>&gt;</span><span class="hljs-tag">&lt;<span class="hljs-name">version</span>&gt;</span>1.6<span class="hljs-tag">&lt;/<span class="hljs-name">version</span>&gt;</span><span class="hljs-tag">&lt;/<span class="hljs-name">dependency</span>&gt;</span><span class="hljs-tag">&lt;/<span class="hljs-name">dependencies</span>&gt;</span><span class="hljs-tag">&lt;/<span class="hljs-name">dependencyManagement</span>&gt;</span><span class="hljs-tag">&lt;/<span class="hljs-name">project</span>&gt;</span></code></pre>]]></content>
    
    
    <categories>
      
      <category>Java</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Java</tag>
      
      <tag>Maven</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Minio - AWS S3 存储开源实现</title>
    <link href="/2006/01/02/minio-aws-s3-storage-open-source-implementation/"/>
    <url>/2006/01/02/minio-aws-s3-storage-open-source-implementation/</url>
    
    <content type="html"><![CDATA[<blockquote><p>本文参考 <a href="https://docs.minio.io/" target="_blank" rel="noopener">Minio Quickstart Guide</a>、<a href="https://docs.minio.io/docs/minio-client-quickstart-guide" target="_blank" rel="noopener">Minio Client Quickstart Guide</a></p></blockquote><h2 id="一、简介"><a href="#一、简介" class="headerlink" title="一、简介"></a>一、简介</h2><p>Minio 是一个基于 golang 语言开发的 AWS S3 存储协议的开源实现，并附带 web ui 界面，可以通过 Minio 搭建私人的兼容 AWS S3 协议的存储服务器</p><h2 id="二、Minio-服务器搭建"><a href="#二、Minio-服务器搭建" class="headerlink" title="二、Minio 服务器搭建"></a>二、Minio 服务器搭建</h2><p>Minio 基于 golang 开发，所以编译后只有一个可执行文件，启动一个 Minio 服务器极其简单，只需要使用 <code>server</code> 参数，并附带一个或多个存储目录即可</p><pre><code class="hljs sh"><span class="hljs-comment"># 下载 Minio</span>wget https://dl.minio.io/server/minio/release/linux-amd64/minio<span class="hljs-comment"># 赋予可执行权限</span>chmod +x minio<span class="hljs-comment"># 创建一个目录用于存放 minio 文件</span>mkdir -p /data/aws_s3<span class="hljs-comment"># 以后台方式启动一个 minio 服务器</span>nohup ./minio server /data/aws_s3 &amp;</code></pre><p>minio 默认监听所有网卡的 9000 端口，此时直接访问 <code>http://ip:9000</code> 即可查看 web ui 界面，如下所示</p><p><img src="https://cdn.oss.link/markdown/hexo_minio_homepage.png" srcset="/img/loading.gif" alt="hexo_minio_homepage"></p><p>同时在启动 minio 后默认会输出当前 minio 服务器的相关登录参数，如 access_key 等，nohup 启动则默认重定向到了 nohup.out 文件中，如下所示</p><p><img src="https://cdn.oss.link/markdown/hexo_minio_token.png" srcset="/img/loading.gif" alt="hexo_minio_token"></p><h2 id="三、mc-客户端操作"><a href="#三、mc-客户端操作" class="headerlink" title="三、mc 客户端操作"></a>三、mc 客户端操作</h2><p>minio 服务器搭建好以后，就可以使用 mc 客户端进行上传下载操作</p><pre><code class="hljs sh"><span class="hljs-comment"># 首先下载 mc 客户端</span>wget https://dl.minio.io/client/mc/release/linux-amd64/mc<span class="hljs-comment"># 增加可执行权限</span>chmod +x mc</code></pre><p>mc 客户端通过 <code>config host add</code> 指令增加 minio 服务器，其实质操作是<strong>向 <code>~/.mc/config.json</code> 文件中加入新的 json 串</strong>，以下为增加一个 minio 服务器示例，相关参数可从服务端输出日志中找到</p><pre><code class="hljs sh">mc config host add aws_s3 http://192.168.1.100:9000 TP6T2AXYYVLI8D2CEHYQ /+4+KVa7p3suABEaJ/E1gv4KQra7GdwsxWNs0Nr7</code></pre><p><strong>默认 minio 服务端启动后目录为空，需要手动创建 bucket，然后才能向具体的 bucket 中存放文件</strong></p><pre><code class="hljs sh"><span class="hljs-comment"># 在 aws_s3 的服务器上创建一个叫 test 的 bucket</span>mc mb aws_s3/<span class="hljs-built_in">test</span></code></pre><p>返回创建成功后可登陆 ui 查看</p><p><img src="https://cdn.oss.link/markdown/hexo_minio_create_bucket.png" srcset="/img/loading.gif" alt="hexo_minio_token"></p><p>创建好 bucket 后就可以向里面增加或删除文件</p><pre><code class="hljs sh"><span class="hljs-comment"># 复制文件到 minio 服务器</span>mc cp test.tar.gz aws_s3/<span class="hljs-built_in">test</span><span class="hljs-comment"># 列出 minio 服务器上的文件</span>mc ls aws_s3/<span class="hljs-built_in">test</span><span class="hljs-comment"># 删除 minio 服务器上的文件</span>mc rm aws_s3/<span class="hljs-built_in">test</span>/test.tar.gz</code></pre><h2 id="四、其他相关"><a href="#四、其他相关" class="headerlink" title="四、其他相关"></a>四、其他相关</h2><p>minio 提供了生成文件分享连接、创建上传到指定 bucket 文件的连接等高级功能，并且官方提供 docker 镜像，同时由于其 server 创建时只是指定了一个或多个目录作为存储位置，便可以很方便的搭配 GlusterFS 等分布式文件系统，更多玩法需要自己实验</p>]]></content>
    
    
    <categories>
      
      <category>Linux</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Linux</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>MySQL 常用操作</title>
    <link href="/2006/01/02/mysql-common-command/"/>
    <url>/2006/01/02/mysql-common-command/</url>
    
    <content type="html"><![CDATA[<blockquote><p>真心不是搞运维的，命令几天就忘，哎，岁数大了……<br>常用的一些MySQL命令在这里记录一下撒</p></blockquote><pre><code class="hljs bash"><span class="hljs-comment">#开启远程登录</span>grant all privileges on *.* to <span class="hljs-string">'user'</span>@<span class="hljs-string">'%'</span> identified by <span class="hljs-string">'passwd'</span> with grant option;<span class="hljs-comment">#创建数据库</span>create database DB;<span class="hljs-comment">#创建用户</span>insert into mysql.user(Host,User,Password) values(<span class="hljs-string">"localhost"</span>,<span class="hljs-string">"user"</span>,password(<span class="hljs-string">"passwd"</span>));<span class="hljs-comment">#删除用户</span>DELETE FROM user WHERE user=<span class="hljs-string">"username"</span> and HOST=<span class="hljs-string">"localhost"</span>;<span class="hljs-comment">#修改指定用户密码</span>update mysql.user <span class="hljs-built_in">set</span> password=password(<span class="hljs-string">'new passwd'</span>) <span class="hljs-built_in">where</span> user=<span class="hljs-string">"username"</span> and host=<span class="hljs-string">"localhost"</span>;<span class="hljs-comment">#用户授权</span>grant all privileges on DB.* to <span class="hljs-string">'user'</span>@<span class="hljs-string">'localhost'</span> identified by <span class="hljs-string">'passwd'</span>;grant select,update on DB.* to <span class="hljs-string">'user'</span>@<span class="hljs-string">'localhost'</span> identified by <span class="hljs-string">'passwd'</span>;<span class="hljs-comment">#刷新权限</span>flush privileges;<span class="hljs-comment">#数据库导出</span>mysqldump -uUSRENAME -pPASSWD DATABASE &gt; DATABASE.sql<span class="hljs-comment">#数据库导出(只导出表结构 -d)</span>mysqldump -uUSRENAME -pPASSWD -d DATABASE &gt; DATABASE.sql<span class="hljs-comment">#数据库导入</span><span class="hljs-comment">#1.切换数据库</span>use DATABASE;<span class="hljs-comment">#2.设置编码</span><span class="hljs-built_in">set</span> names utf8;<span class="hljs-comment">#3.执行导入操作</span><span class="hljs-built_in">source</span> /home/abc/abc.sql;<span class="hljs-comment">#直接导入</span>mysql -uUSERNAME -p DATABASE &lt; DATABASE.sql</code></pre>]]></content>
    
    
    <categories>
      
      <category>Linux</category>
      
    </categories>
    
    
    <tags>
      
      <tag>MySQL</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>程序猿的自我修养</title>
    <link href="/2006/01/02/programmer&#39;s-self-cultivation/"/>
    <url>/2006/01/02/programmer&#39;s-self-cultivation/</url>
    
    <content type="html"><![CDATA[<p><img src="https://cdn.oss.link/markdown/hexo_programer.png" srcset="/img/loading.gif" alt="程序猿"></p><blockquote><p>最近在看到很多人问一些基本的东西，或者说一些资源、一些简单问题等，其实很好找到答案，在这里总结一下自己1年工作经验的学习经历。</p></blockquote><h2 id="一、善用搜索引擎"><a href="#一、善用搜索引擎" class="headerlink" title="一、善用搜索引擎"></a>一、善用搜索引擎</h2><blockquote><p>搜索引擎对于程序猿屌丝来说最熟悉不过了；但在实际使用中，你会发现经常出现 “我擦，我怎么没搜到” 这种问题，以下做了几点建议：</p></blockquote><h3 id="1、摒弃百度"><a href="#1、摒弃百度" class="headerlink" title="1、摒弃百度"></a>1、摒弃百度</h3><p>对于习惯了从小从教科书开始学习的我们来说，”有问题找度娘” 这似乎是铁律；而对于谁对谁错漠然在此不想加以评论，我只能说根据经验和实际使用，百度真心不如Google。</p><p>其实每个搜索引擎并没有绝对的好与坏，只是相对于使用场景，<strong>对于搜索代码错误、框架教程等强烈建议选择 Google，因为百度你会发现10条里9条是 copy 过来并且没有加以验证的。</strong>而对于本土化资源，如搜索周边公园、小吃、公交等等，Google绝对鞭长莫及。</p><p><strong>关于如何使用 Google，请看<a href="http://mritd.me" target="_blank" rel="noopener">我博客</a>教程，至少我认为程序猿不会爬墙是种耻辱，因为我们应该是最容易接触世界的人！</strong></p><h3 id="2、合理组织关键词"><a href="#2、合理组织关键词" class="headerlink" title="2、合理组织关键词"></a>2、合理组织关键词</h3><p>虽然我们使用同样的搜索引擎或者百度，但你会发现有些东西，别人能搜到你还是搜不到。。。你感觉这世界不怎么友好。。。</p><p>其实以程序员的角度理解搜索引擎，问题就简单了；搜索引擎在搜索东西时也是先建立网页索引和关键字，然后去匹配；而且有些人往往忽略这个问题，搜索问题时往往是这样的 <strong>“XXXX在XXXX时候为什么会出现XXXX错误？”</strong> ；可想而知这基本是没什么结果的，因为你的关键字(搜索内容)太长了，<strong>搜索引擎会通过空格做分词操作，为了精准匹配，如搜索 “XXXX(某个技术名词如框架) (错误关键字/技术关键字)” 效果会好得多</strong>；简单的例子如下：</p><p><strong>错误搜索案例：”Spring怎么在XML里配置事务切面？”</strong><br><strong>正确搜索案例：”Spring XML 事务切面”</strong></p><h3 id="3、中英文切换"><a href="#3、中英文切换" class="headerlink" title="3、中英文切换"></a>3、中英文切换</h3><p>百度不经常用，至少 Google 是这样的：当你的搜索词语中出现中文的时候，并且你的 Google 账户也是设置的语言中文，那么 Google 会优先索引中文网页，如果中文并没有很好地解释时，我们可以尝试全部使用英文来搜索，这样你会看到很多老外的回答。这个技巧也适用于你不想看英文内容时，可以试着在关键词里加入中文。</p><h3 id="4、善用-Google-翻译"><a href="#4、善用-Google-翻译" class="headerlink" title="4、善用 Google 翻译"></a>4、善用 Google 翻译</h3><p>Google 翻译其实还是很强大的，当我们在中文索引中无法找到答案时，可以搜索英文，并使用强大的 Google 翻译，进行全文翻译，如下：</p><p><strong>关键词搜索：</strong></p><p><img src="https://cdn.oss.link/markdown/hexo_programer_google1.png" srcset="/img/loading.gif" alt="关键词搜索"></p><p><strong>Google 全文翻译</strong></p><p><img src="https://cdn.oss.link/markdown/hexo_programer_google2.png" srcset="/img/loading.gif" alt="Google 全文翻译"></p><h2 id="二、善于搜索资源"><a href="#二、善于搜索资源" class="headerlink" title="二、善于搜索资源"></a>二、善于搜索资源</h2><blockquote><p>伟大的互联网给予了我们强大而丰富的分享资源；而获取方式对于程序猿来说，也是个必修课。</p></blockquote><h3 id="1、百度云分享搜索"><a href="#1、百度云分享搜索" class="headerlink" title="1、百度云分享搜索"></a>1、百度云分享搜索</h3><p>普遍的，我们大多是资料存放在百度云中(至少我)；而对于云盘，其实我们是可以加以利用的，比如张三给李四分享了资料(公开方式分享)，其实我们也是可以借机看看的：</p><h4 id="Chrome-下载油猴插件-Tampermonkey"><a href="#Chrome-下载油猴插件-Tampermonkey" class="headerlink" title="Chrome 下载油猴插件(Tampermonkey)"></a>Chrome 下载油猴插件(Tampermonkey)</h4><p><img src="https://cdn.oss.link/markdown/hexo_programer_install_Tampermonkey%20.png" srcset="/img/loading.gif" alt="下载油猴插件"></p><h4 id="安装百度云脚本-脚本仓库地址"><a href="#安装百度云脚本-脚本仓库地址" class="headerlink" title="安装百度云脚本 脚本仓库地址"></a>安装百度云脚本 <a href="https://greasyfork.org/zh-CN/scripts" target="_blank" rel="noopener">脚本仓库地址</a></h4><p>搜索脚本<br><img src="https://cdn.oss.link/markdown/hexo_programer_install_Tampermonkey_baiduyunplugin1.png" srcset="/img/loading.gif" alt="搜索脚本"></p><p>安装脚本<br><img src="https://cdn.oss.link/markdown/hexo_programer_install_Tampermonkey_baiduyunplugin2.png" srcset="/img/loading.gif" alt="安装脚本"></p><p>百度云搜索资源<br><img src="https://cdn.oss.link/markdown/hexo_programer_install_Tampermonkey_baiduyunplugin3.png" srcset="/img/loading.gif" alt="百度云搜索资源"></p><h3 id="2、网站搜索"><a href="#2、网站搜索" class="headerlink" title="2、网站搜索"></a>2、网站搜索</h3><p>当云盘搜索无法满足我们的实际需求时，可以尝试再资源分享网站搜索，有很多资源分享的网站放置大部分共享的教程等，以下列出了一些：</p><ul><li><a href="http://www.java1234.com/" target="_blank" rel="noopener">Java 1234</a> 主要分享各种视频资料</li><li><a href="https://github.com/" target="_blank" rel="noopener">Github</a> 众多分享项目在此</li><li><a href="http://git.oschina.net/" target="_blank" rel="noopener">OSC@Git</a> 国内的开源平台，有很多教程 Demo</li><li><a href="https://coding.net/" target="_blank" rel="noopener">Coding</a> 没怎么用过</li></ul><p>当然，更多的资源网站不一一列举，还有很多 比如 极客学院(推广免费体验会员，IDM可抓取视频下载)、慕课网(不怎么去)、北风网(收费且离谱)等等，自己发掘。</p><h3 id="3、万能X宝"><a href="#3、万能X宝" class="headerlink" title="3、万能X宝"></a>3、万能X宝</h3><p>当你穷途末路找不到好的资源时，千万不要忘了 “马云爸爸”；万能的X宝除了枪支弹剩下的一般都有，当然包括我们的学习视频：</p><p><img src="https://cdn.oss.link/markdown/hexo_programer_taobao.png" srcset="/img/loading.gif" alt="淘宝搜索"></p><h2 id="三、保持进取"><a href="#三、保持进取" class="headerlink" title="三、保持进取"></a>三、保持进取</h2><h3 id="1、保持好奇心"><a href="#1、保持好奇心" class="headerlink" title="1、保持好奇心"></a>1、保持好奇心</h3><p>虽说好奇心害死猫，但是猿类一般不会死，反而会变得更聪明。要保持良好的好奇心，多加入一些 Java 交流群，平时尽量把看电视剧的时间放在看看 OSCina、CSDN、Github 之类的IT网站上；每当别人讨论技术或者 CSDN等新闻提到新技术的时候，没听过的一定要 Google 一下，因为技术总归不会去主动找你，<strong>努力增加自己的知识广度</strong>。</p><h3 id="2、学会自我优化"><a href="#2、学会自我优化" class="headerlink" title="2、学会自我优化"></a>2、学会自我优化</h3><p>在工作中，每当我们敲完一段代码时，如果时间允许；那么就不要抱着以完成任务为目标的心态去写代码，多思考怎么实现更优雅；经常对自己的代码进行 review；做一个 “完美主义者”(量力而行)。</p><h3 id="3、专注偷懒"><a href="#3、专注偷懒" class="headerlink" title="3、专注偷懒"></a>3、专注偷懒</h3><p>不管大家怎么说，我们必须承认，程序猿每天都在试图偷懒；对于冗长的代码，尽量思考怎么去 “偷懒” 完成它，不要造轮子，做重复工作，因为程序员天生就擅长 “偷懒”，否则不会有 for 循环这种玩意。</p><h3 id="4、合理工作"><a href="#4、合理工作" class="headerlink" title="4、合理工作"></a>4、合理工作</h3><p><strong>永远记住：不是你做的越多就越好，也不是努力就一定成功！</strong></p><p>工作固然是重要的，但是从面试的角度来说，我们常常会关注一个很重要的因素：”你有几年工作经验？”；我们是否真切的思考过面试官为何问这个问题？</p><p>答案很简单，但是做到的不多。当然问你工作时间就是想评估你工作中学到了多少。但是工作时候并不会让你学习的，毕竟 “<strong>公司 不是 学校</strong>“；所以合理的安排时间就显得尤为重要，如何每天空出一部分时间让自己 “拿着工资学习” 这就取决于你的时间把控；这并不是要你1天干完的活拖到2天，那只能证明你无能，而不是你聪明，努力权衡好修改方案和最优的时间规划才是王道。</p><p>最后一点就是努力尝试，当工作中一个功能需要你来完成时，首先你应该考虑用到的技术，同时必须关注当前是否有解决这个问题更好的技术；如果有，那么评估好工作量，大胆去尝试吧，毕竟你为公司带来了价值，公司除了给你钱以外，还得让你成长。。。你的工资远远小于你对公司的价值。</p><h2 id="四、学会忘记"><a href="#四、学会忘记" class="headerlink" title="四、学会忘记"></a>四、学会忘记</h2><p>当你了解的很多的时候，你会发现 “知识无止境，根本学不完” 这个道理。当然有两个选择，一是啥也不学了，二是学了然后忘掉它。</p><p>我只能说我的计策是学了，然后 “忘掉” 它；在忘掉之前一定做好记录。。。</p><p>人脑子不是计算机，所以根据 艾宾浩斯遗忘曲线 来说，忘的速度跟吃饭喝凉水差不多，这时候做好笔记显得更重要；多锻炼写笔记总结，或者写一写博客，当然不要抱着给别人看装X的心理去写；写的时候要想着怎么让自己忘了时候快速的想起来，当你写完以后反复看几遍，如果感觉这个技术短期用不到，那就勇敢地 “忘了” 吧；作为标准猿类，<strong>很多技术我们不必全部都会，但是有人跟你提起的时候必须知道它是干什么的，如果现在要用，那么必须能快速找到资料并学会它，这是一种能力。</strong></p><h2 id="未完待续，欢迎补充……"><a href="#未完待续，欢迎补充……" class="headerlink" title="未完待续，欢迎补充……"></a>未完待续，欢迎补充……</h2>]]></content>
    
    
    <categories>
      
      <category>随笔</category>
      
    </categories>
    
    
    <tags>
      
      <tag>随笔</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>基于官方 rpm 快速创建自定义 rpm</title>
    <link href="/2006/01/02/quickly-create-custom-rpms-based-on-official-rpm/"/>
    <url>/2006/01/02/quickly-create-custom-rpms-based-on-official-rpm/</url>
    
    <content type="html"><![CDATA[<h2 id="一、扯淡"><a href="#一、扯淡" class="headerlink" title="一、扯淡"></a>一、扯淡</h2><p>由于工作中常常需要使用 yum 安装一些软件，而当需求特定版本(一般是比较新的版本)时，往往 CentOS 官方提供的都不是最新的，这时候一般解决方案是先 rpm 安装，然后再用高版本的 二进制文件进行替换，当机器多的时候这种方法很蛋疼，所以决定研究了一下 rpm 制作方法，以下为一些实践。</p><h2 id="二、spec-简介"><a href="#二、spec-简介" class="headerlink" title="二、spec 简介"></a>二、spec 简介</h2><blockquote><p>关于 spec 文件的具体作用和详细的介绍网上已经有很多文章，具体可参考 <a href="https://www.ibm.com/developerworks/cn/linux/l-rpm/" target="_blank" rel="noopener">RPM 打包技术与典型 SPEC 文件分析</a>，本文主要主要以概述的形式来大致介绍以下 spec 文件</p></blockquote><h3 id="2-1、spec-文件作用"><a href="#2-1、spec-文件作用" class="headerlink" title="2.1、spec 文件作用"></a>2.1、spec 文件作用</h3><p>在从0开始创建 rpm 时，使用 rpm build 工具基于一个预定义好的 spec 文件来创建 rpm；<strong>也就是说 spec 文件里定义了 rpm 如何创建以及创建过程，比如 rpm 内包含的资源文件、配置信息、安装卸载前后执行脚本、rpm 说明信息等；</strong>如果能完全了解了 sepc 文件，那么理论上就可以自己从 0 开始创建一个 rpm 包，然而对于书写 spec 就像写代码一样，有些时候自己写的也能跑起来…但是考虑的地方对比官方的总有些不足，所以如果可以最好的做法是基于官方 spec 修改，<strong>CentOS 官方的 rpm spec 可在 <a href="https://git.centos.org/project/rpms" target="_blank" rel="noopener">CentOS 官方仓库</a> 中获取</strong></p><h3 id="2-2、spec-文件结构"><a href="#2-2、spec-文件结构" class="headerlink" title="2.2、spec 文件结构"></a>2.2、spec 文件结构</h3><p>参考 <a href="https://www.ibm.com/developerworks/cn/linux/l-rpm/" target="_blank" rel="noopener">RPM 打包技术与典型 SPEC 文件分析</a> 这篇文章可知，spec 文件其实就是个模板文件，里面包含各种宏定义，其大之分为六大段:</p><h4 id="2-2-1、-文件头"><a href="#2-2-1、-文件头" class="headerlink" title="2.2.1、 文件头"></a>2.2.1、 文件头</h4><p>文件头部分主要是对即将编译出的 rpm 包做一些声明，包括但不限于 rpm 名称、编译平台、适用平台、版权信息、资源文件位置、说明信息 等等</p><h4 id="2-2-2、-prep段"><a href="#2-2-2、-prep段" class="headerlink" title="2.2.2、 %prep段"></a>2.2.2、 %prep段</h4><p>prep 顾名思义意为预处理段，预处理段主要是一些预处理脚本，比如 安装前执行哪些动作(创建用户什么的)、安装后执行那些动作，还有卸载前卸载后等</p><h4 id="2-2-3、-build段"><a href="#2-2-3、-build段" class="headerlink" title="2.2.3、 %build段"></a>2.2.3、 %build段</h4><p>对于需要编译的软件，比如 nginx，可在此段写好编译命令，然后在打包 rpm 的时候就可以直接编译成二进制文件被打入 rpm 包</p><h4 id="2-2-4、-install段"><a href="#2-2-4、-install段" class="headerlink" title="2.2.4、 %install段"></a>2.2.4、 %install段</h4><p>一般编译阶段就会执行 install 命令，但这个时候 install 的并非系统目录，而是指定一个临时目录，把他当成系统的根目录进行安装，此时编译好的二进制安装文件全部在这个临时目录下；<strong>install 段的作用就是定义临时目录这些文件将来在安装 rpm 包的时候究竟释放到系统的那些位置，包括权限是什么等等</strong></p><h4 id="2-2-5、-files段"><a href="#2-2-5、-files段" class="headerlink" title="2.2.5、 %files段"></a>2.2.5、 %files段</h4><p>在 install 段定义好临时目录下的文件的释放位置后，为防止无意外发生，还需要在 files 段中重新定义一些到底释放了那些文件，文件类型是什么；比如某某文件时配置文件，某某文件是可执行文件等，<strong>files 段相当于一个文件清单，且必须和 install 段定义的文件相匹配，也就是说你 install 了10个文件，那么 files 段必须声明这 10 个文件，当然可以使用通配符，但不能有缺失，否则 rpm 编译不过</strong></p><h4 id="2-2-6、-changelog段"><a href="#2-2-6、-changelog段" class="headerlink" title="2.2.6、 %changelog段"></a>2.2.6、 %changelog段</h4><p>顾名思义，这段主要记录 spec 文件的修改历史和修改原因</p><h2 id="三、创建自定义-rpm"><a href="#三、创建自定义-rpm" class="headerlink" title="三、创建自定义 rpm"></a>三、创建自定义 rpm</h2><h3 id="3-1、基于官方-spec-的不可行分析"><a href="#3-1、基于官方-spec-的不可行分析" class="headerlink" title="3.1、基于官方 spec 的不可行分析"></a>3.1、基于官方 spec 的不可行分析</h3><p>对于创建一个自定义的 rpm 一半有两种可选方案，一种是从 0 开始，自己参考官方的 spec 文件自己写一个 spec，然后自己编译；但是根据个人实践经验来看，这招很坑爹；主要有以下几个原因导致不可行:</p><ul><li>文件缺失: 官方 spec 文件放在 git 仓库中，其中有些本地的资源文件全部加入到 git 的忽略文件中，那些奇奇怪怪名字的文件你也不知道是啥玩意，所以没法准备环境</li><li>理解困难: 一般官方的 spec 都会非常吊…吊到你根本看不懂，小说上百行多的上千行，里面各种复杂的环境配置和宏替换，对于非运维专业的童鞋来说，甚至你想改个编译版本都极其困难</li><li>网络环境: 官方的 spec 一般都是从源码开始重新编译，而源码常常是直接从 github 拉取，众所周知的原因 github 存放在亚马逊 S3 上的文件在国内下载是以 b/s 的速度计算的，一个G的源码能 wget 1年</li><li>性能不足: 官方 sepc 不管什么玩意都是从源码编译的，比如 kubernetes，这东西官方有提供编译后的可执行二进制文件，而且跨平台；根本无须自己重新编译；<strong>我测试过在一台国外的 16G 8核心的 vps 上编译一次 kubernetes 需要半小时时间</strong></li><li>spec 无缓存: 使用 spec 文件编译 rpm 不像 docker build image，rpm 编译是没有缓存的，<strong>也就是说你写了 100 行的 spec 文件，你的第 100 行写错了，那么你修改以后前面 99 行还是得重来一遍</strong>，试想一下编译 kubernetes 的 rpm 时用自己写的 spec 是件多么恐怖的事情…轻轻松松一上午时间没了…对…没了，然后还啥也没干…</li></ul><h3 id="3-2、基于已有-rpm-的实践"><a href="#3-2、基于已有-rpm-的实践" class="headerlink" title="3.2、基于已有 rpm 的实践"></a>3.2、基于已有 rpm 的实践</h3><p>综上所述，最好的做法就是基于已有的 rpm 进行制作，比如说官方提供了 kubernetes 1.2 版本的 rpm，我们需要 1.3.6 的，那么就可以下载官 1.2 的 rpm 然后替换一些 1.3.6 的可执行文件再打包成新的 rpm 即可，这个过程涉及到几个问题:</p><ul><li>如何下载官方的 rpm : 借助 yumdownloader 工具可直接通过 yum 下载</li><li>如何解包 rpm : 使用 <code>rpm2cpio package.rpm | cpio -idmv</code> 可接包</li><li>rpm prep 段怎么处理: 可以使用 <code>rpm -qp --scripts package.rpm</code> 从 rpm 包中提取</li><li>没有 spec 如何封包: 使用 fpm 工具即可</li></ul><h3 id="3-3、撸一个-etcd-的-rpm"><a href="#3-3、撸一个-etcd-的-rpm" class="headerlink" title="3.3、撸一个 etcd 的 rpm"></a>3.3、撸一个 etcd 的 rpm</h3><p>以下以创建一个 etcd 的 rpm 为例，创建基于官方提供的 rpm 并替换掉可执行文件的方式；<strong>其实对于官方的 rpm 我们实际只关心两样东西: 第一个就是里面的文件，第二个就是 prep 段的脚本，</strong>以下是实际的制作过程</p><h4 id="3-3-1、下载官方-rpm"><a href="#3-3-1、下载官方-rpm" class="headerlink" title="3.3.1、下载官方 rpm"></a>3.3.1、下载官方 rpm</h4><pre><code class="hljs sh"><span class="hljs-comment"># 先安装 yum 工具</span>yum install -y yum-utils<span class="hljs-comment"># 使用 yumdownloader 下载</span>yumdownloader etcd</code></pre><h4 id="3-3-2、提取-prep-段"><a href="#3-3-2、提取-prep-段" class="headerlink" title="3.3.2、提取 prep 段"></a>3.3.2、提取 prep 段</h4><p><strong>一般官方的 rpm 主要包含 4 个部分的 prep 脚本，分别为: 安装前处理(preinstall)、安装后处理(postinstall)、卸载前处理(preuninstall)、卸载后处理(postuninstall)；</strong>我们可以将其提取后放到 4 个文件中，在使用 fpm 重新封包时指定一下即可</p><pre><code class="hljs sh"><span class="hljs-comment"># 首先看一下官方 rpm 的预处理脚本</span>rpm -qp --scripts etcd-2.3.7-2.el7.x86_64.rpm</code></pre><p>显示的脚本信息如下</p><p><img src="https://cdn.oss.link/markdown/hexo_makerpm_prep.png" srcset="/img/loading.gif" alt="hexo_makerpm_prep"></p><p><strong>将这四段脚本分别保存为四个文件即可，比如保存为 preinstall.sh、postinstall.sh…</strong></p><h4 id="3-3-3、解包-rpm"><a href="#3-3-3、解包-rpm" class="headerlink" title="3.3.3、解包 rpm"></a>3.3.3、解包 rpm</h4><p>首先创建一个临时目录，用于存放解包后的 rpm 文件，这个目录相当于 rpm 安装后的系统根目录，然后将 rpm 解包即可</p><pre><code class="hljs sh"><span class="hljs-comment"># 创建临时目录</span>mkdir rpmfiles<span class="hljs-comment"># 解包</span>cp etcd-2.3.7-2.el7.x86_64.rpm rpmfiles &amp;&amp; <span class="hljs-built_in">cd</span> rpmfilesrpm2cpio etcd-2.3.7-2.el7.x86_64.rpm | cpio -idmv</code></pre><h4 id="3-3-5、替换可执行文件"><a href="#3-3-5、替换可执行文件" class="headerlink" title="3.3.5、替换可执行文件"></a>3.3.5、替换可执行文件</h4><p>首先自己想办法下载一个高版本的可执行文件，然后在解包后的临时目录(rpmfiles)中找到旧的文件，删掉替换以下就行，不做演示了</p><h4 id="3-3-6、使用-fpm-打包"><a href="#3-3-6、使用-fpm-打包" class="headerlink" title="3.3.6、使用 fpm 打包"></a>3.3.6、使用 fpm 打包</h4><p>fpm 工具是 ruby 写的，又是由于众所周知的原因他么的国内 gem 不能用… 所以需要先搞好一个 ruby 环境，并安装上 fpm，ruby 安装参考 <a href="https://ruby-china.org/wiki/install_ruby_guide" target="_blank" rel="noopener">如何快速正确的安装 Ruby, Rails 运行环境</a>，以下是一段安装 rvm，借助 rvm 安装 ruby 和 fpm 的脚本</p><pre><code class="hljs sh"><span class="hljs-comment"># 直接从我原来的脚本里 copy 的</span><span class="hljs-built_in">echo</span> -e <span class="hljs-string">"\033[32minstall rvm...\033[0m"</span>PATH=<span class="hljs-variable">$PATH</span>:/usr/<span class="hljs-built_in">local</span>/rvm/bin:/usr/<span class="hljs-built_in">local</span>/rvm/rubies/ruby-2.3.0/bingpg --keyserver hkp://keys.gnupg.net --recv-keys 409B6B1796C275462A1703113804BB82D39DC0E3 &gt; /dev/null 2&gt;&amp;1curl -sSL https://get.rvm.io | bash -s stable &gt; /dev/null 2&gt;&amp;1<span class="hljs-built_in">echo</span> <span class="hljs-string">"ruby_url=https://cache.ruby-china.org/pub/ruby"</span> &gt;&gt; /usr/<span class="hljs-built_in">local</span>/rvm/user/dbrvm requirements &gt; /dev/null 2&gt;&amp;1<span class="hljs-built_in">echo</span> -e  <span class="hljs-string">"\033[32minstall ruby...\033[0m"</span>rvm install 2.3.0 &gt; /dev/null 2&gt;&amp;1rvm use 2.3.0 --default &gt; /dev/null 2&gt;&amp;1<span class="hljs-built_in">echo</span> -e  <span class="hljs-string">"\033[32minstall bundler...\033[0m"</span>gem install bundler &gt; /dev/null 2&gt;&amp;1<span class="hljs-built_in">echo</span> -e  <span class="hljs-string">"\033[32minstall fpm...\033[0m"</span>gem install fpm &gt; /dev/null 2&gt;&amp;1</code></pre><p><strong>最后一步开始封包，封包之前确认好两件事: rpm 解包后已经替换好了相关文件、prep 段脚本已经提取好了并保存至文件中；然后就一条命令封包</strong></p><pre><code class="hljs sh"><span class="hljs-comment"># $version 指定新的版本，后面的 etc、usr 等目录就是官方 rpm 解包的文件夹</span>fpm -s dir -t rpm -n <span class="hljs-string">"etcd"</span> -v <span class="hljs-variable">$version</span> --pre-install preinstall.sh --post-install postinstall.sh --pre-uninstall preuninstall.sh --post-uninstall postuninstall.sh etc usr var</code></pre><h2 id="四、其他相关"><a href="#四、其他相关" class="headerlink" title="四、其他相关"></a>四、其他相关</h2><p>关于 fpm 的命令参数以及使用可以参考 github 上的 wiki，地址 <a href="https://github.com/jordansissel/fpm" target="_blank" rel="noopener">点这里</a></p><p>以上制作过程可以自己写一个编译脚本，方便以后使用，可以参考我的 <a href="https://github.com/mritd/shell_scripts/blob/master/build_rpm_tool.sh" target="_blank" rel="noopener">k8s、etcd、flannel rpm 制作脚本</a>；<strong>在自己写脚本时候有些坑，比如 prep 段如果官方包的 prep 脚本中包含 $ 等特殊字符，那么在自己写的编译脚本中需要将他转义处理，否则会造成引用当前编译脚本的变量，最终在 rpm 中体现不正确的情况；自己编译好 rpm 后最好同样提取下 prep 脚本并跟官方的对比一下；同时如果自己编译时是用的 root 用户那么还要注意一下权限问题，比如 k8s 的 rpm 安装后创建 kube 用户，其可执行文件都以 kube 用户执行，这时候你 root 封包的一些文件 kube 用户可能没法读取，解决办法是在 prep 段中自己加入更改权限的 shell 脚本</strong></p>]]></content>
    
    
    <categories>
      
      <category>Linux</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Linux</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Raspberry Pi2 折腾</title>
    <link href="/2006/01/02/raspberry-pi2-build/"/>
    <url>/2006/01/02/raspberry-pi2-build/</url>
    
    <content type="html"><![CDATA[<h3 id="开启Root用户"><a href="#开启Root用户" class="headerlink" title="开启Root用户"></a>开启Root用户</h3><blockquote><p>初次使用pi用户登录后，默认密码为 raspberry，然后应该搞开Root用户，和Debian系列一样(我特么只用过基于Debian的，如Ubuntu，还没玩过纯血统Debian)，默认树莓派Root用户是未启用的，密码每次开机都变，执行以下命令开启Root用户，并更改密码:</p></blockquote><pre><code class="hljs sh">sudo passwd root<span class="hljs-comment"># 然后输两次密码，可以su过去了，不过这时你会发现</span><span class="hljs-comment"># 仍然无法远程以root登录，修改如下配置中的 PermitRootLogin 参数即可</span>vi /etc/ssh/sshd_config<span class="hljs-comment"># 该参数码值如下</span><span class="hljs-comment"># 允许root用户以任何认证方式登录</span>PermitRootLogin yes  <span class="hljs-comment"># 只允许root用public key认证方式登录</span>PermitRootLogin without-password      <span class="hljs-comment"># 不允许root远程登录</span>PermitRootLogin no</code></pre><hr><blockquote><p>以下相关设置大部分在root权限下进行</p></blockquote><hr><h3 id="解决vi方向-删除失灵"><a href="#解决vi方向-删除失灵" class="headerlink" title="解决vi方向/删除失灵"></a>解决vi方向/删除失灵</h3><pre><code class="hljs sh"><span class="hljs-comment"># 直接编辑vim配置文件，注意先调整到要改的位置，再编辑</span><span class="hljs-comment"># 因为一旦进入Insert模式 方向键是不好使的</span>vi /etc/vim/vimrc.tiny<span class="hljs-comment"># 修改后如下</span><span class="hljs-built_in">set</span> nocompatible<span class="hljs-built_in">set</span> backspace=2</code></pre><h3 id="less语法高亮"><a href="#less语法高亮" class="headerlink" title="less语法高亮"></a>less语法高亮</h3><blockquote><ol><li>执行命令 <code>apt-get install source-highlight</code></li><li>加入环境变量(更详细参考 <a href="http://leonyoung.sinaapp.com/blog/2013/10/syntax-highlight-in-less/" target="_blank" rel="noopener">这里</a>)</li></ol></blockquote><pre><code class="hljs sh"><span class="hljs-built_in">export</span> LESSOPEN=<span class="hljs-string">'| /usr/share/source-highlight/src-hilite-lesspipe.sh %s'</span><span class="hljs-built_in">export</span> LESS=<span class="hljs-string">' -R -N '</span></code></pre><h3 id="设置自动连接-WiFi"><a href="#设置自动连接-WiFi" class="headerlink" title="设置自动连接 WiFi"></a>设置自动连接 WiFi</h3><ul><li>编辑 /etc/network/interfaces 文件</li></ul><blockquote><p>一般只有一个无线网卡的话默认为 wlan0，修改 <code>iface wlan0 inet manual</code> 为 <code>iface wlan0 inet dhcp</code>，因为考虑到网络环境，频繁切换网络不适合设置静态IP，所以使用 DHCP 分配，样例配置文件如下:</p></blockquote><pre><code class="hljs sh"><span class="hljs-comment"># Please note that this file is written to be used with dhcpcd.</span><span class="hljs-comment"># For static IP, consult /etc/dhcpcd.conf and 'man dhcpcd.conf'.</span>auto loiface lo inet loopback<span class="hljs-comment"># 自动连接有线网卡</span>auto eth0allow-hotplug eth0iface eth0 inet manual<span class="hljs-comment"># 自动连接 无线网卡</span>auto wlan0<span class="hljs-comment"># 允许热插拔</span>allow-hotplug wlan0<span class="hljs-comment"># IP采用 DHCP 分配</span>iface wlan0 inet dhcp<span class="hljs-comment"># SSID 等相关设置(wifi密码啥的)</span>wpa-conf /etc/wpa_supplicant/wpa_supplicant.confauto wlan1allow-hotplug wlan1iface wlan1 inet dhcpwpa-conf /etc/wpa_supplicant/wpa_supplicant.conf</code></pre><ul><li>编辑 /etc/wpa_supplicant/wpa_supplicant.conf 文件</li></ul><blockquote><p>该文件主要存储无线网络连接的相关设置，包括SSID、密码、加密方式等，配置样例如下</p></blockquote><pre><code class="hljs sh">ctrl_interface=DIR=/var/run/wpa_supplicant GROUP=netdevupdate_config=1network=&#123;  <span class="hljs-comment"># Wifi SSID</span>  ssid=<span class="hljs-string">"mritd"</span>  <span class="hljs-comment"># Wifi Password</span>  psk=<span class="hljs-string">"12345678"</span>  <span class="hljs-comment"># 网络优先级， network块可以有多个，也就是可以设置多个无线链接及密码，</span>  <span class="hljs-comment"># 当有多个Wifi都存在时，根据这个值选择优先链接，越大优先级越高，不可为负数</span>  priority=5&#125;</code></pre><p><strong>其中 network 部分最好使用工具生成，命令如下 <code>wpa_passphrase SSID PASSWD</code></strong></p><ul><li>启用网卡链接</li></ul><blockquote><p>执行 <code>ifup wlan0</code> 启动 wlan0 并连接，执行 <code>ifdown wlan0</code> 关闭并断开链接，如下图:<br><img src="http://7xoixd.com1.z0.glb.clouddn.com/raspberry_ifup_wlan0.png" srcset="/img/loading.gif" alt="ifup_wlan0"></p></blockquote><p>链接成功后可执行 <code>ifconfig</code> 或 <code>iwconfig</code>查看网络连接状况，如下图:<br><img src="http://7xoixd.com1.z0.glb.clouddn.com/raspberry_iwconfig_wlan0.png" srcset="/img/loading.gif" alt="iwconfig"></p><blockquote></blockquote><p><img src="http://7xoixd.com1.z0.glb.clouddn.com/raspberry_ifconfig_wlan0.png" srcset="/img/loading.gif" alt="ifconfig"></p><h3 id="关闭-WiFi-休眠"><a href="#关闭-WiFi-休眠" class="headerlink" title="关闭 WiFi 休眠"></a>关闭 WiFi 休眠</h3><p>默认情况下树莓派的 wifi 时会自动休眠的，使用如下命令关闭网卡休眠</p><pre><code class="hljs sh"><span class="hljs-comment"># 检测网卡休眠状态 1 代表会自动休眠 0 反之</span>cat /sys/module/8192cu/parameters/rtw_power_mgnt<span class="hljs-comment"># 如果为1 编辑 8192cu.conf 文件关闭休眠(此文件没有可新建)</span>vim /etc/modprobe.d/8192cu.conf<span class="hljs-comment"># 内容如下</span><span class="hljs-comment"># Disable power saving</span>options 8192cu rtw_power_mgnt=0</code></pre><p>修改配置后重启即可</p><h3 id="设置命令提示符风格"><a href="#设置命令提示符风格" class="headerlink" title="设置命令提示符风格"></a>设置命令提示符风格</h3><ul><li>命令提示相关设置</li></ul><blockquote><p>Linxu命令提示符由 PS1变量控制，所以更改命令提示符样式也就是更改PS1变量，以下为相关码值:</p></blockquote><pre><code class="hljs livescript"><span class="hljs-string">\d</span> ：<span class="hljs-comment">#代表日期，格式为weekday month date，例如："Mon Aug 1"   </span><span class="hljs-string">\H</span> ：<span class="hljs-comment">#完整的主机名称。   </span><span class="hljs-string">\h</span> ：<span class="hljs-comment">#仅取主机的第一个名字，如上例，则为fc4，.linux则被省略   </span><span class="hljs-string">\t</span> ：<span class="hljs-comment">#显示时间为24小时格式，如：HH：MM：SS   </span><span class="hljs-string">\T</span> ：<span class="hljs-comment">#显示时间为12小时格式   </span><span class="hljs-string">\A</span> ：<span class="hljs-comment">#显示时间为24小时格式：HH：MM   </span><span class="hljs-string">\u</span> ：<span class="hljs-comment">#当前用户的账号名称   </span><span class="hljs-string">\v</span> ：<span class="hljs-comment">#BASH的版本信息   </span><span class="hljs-string">\w</span> ：<span class="hljs-comment">#完整的工作目录名称。家目录会以 ~代替   </span><span class="hljs-string">\W</span> ：<span class="hljs-comment">#利用basename取得工作目录名称，所以只会列出最后一个目录   </span><span class="hljs-string">\#</span> ：<span class="hljs-comment">#下达的第几个命令   </span><span class="hljs-string">\$</span> ：<span class="hljs-comment">#提示字符，如果是root时，提示符为：# ，普通用户则为：$</span></code></pre><blockquote><p>颜色及效果控制码值</p></blockquote><pre><code class="hljs angelscript"> 前景    背景     颜色------------------------  <span class="hljs-number">30</span>      <span class="hljs-number">40</span>      黑色     <span class="hljs-number">31</span>      <span class="hljs-number">41</span>      红色   　<span class="hljs-number">32</span>      <span class="hljs-number">42</span>      绿色   　<span class="hljs-number">33</span>      <span class="hljs-number">43</span>      黄色   　<span class="hljs-number">34</span>      <span class="hljs-number">44</span>      蓝色   　<span class="hljs-number">35</span>      <span class="hljs-number">45</span>      紫红色   　<span class="hljs-number">36</span>      <span class="hljs-number">46</span>      青蓝色   　<span class="hljs-number">37</span>      <span class="hljs-number">47</span>      白色  　 代码          意义   -------------------------     <span class="hljs-number">0</span>            OFF     <span class="hljs-number">1</span>            高亮显示     <span class="hljs-number">4</span>            underline     <span class="hljs-number">5</span>            闪烁     <span class="hljs-number">7</span>            反白显示     <span class="hljs-number">8</span>            不可见</code></pre><blockquote><p>设置PS1变量时，<code>\[\e[F;Bm]</code> 代表颜色开始，F为前景色，B为背景色，<code>\e[m]</code> 为颜色结束符，不写的话会造成整个命令行都是最后一种颜色，以下为我的PS1样式</p></blockquote><pre><code class="hljs sh">PS1=<span class="hljs-string">'\[\e[1;32m\][\u@\h\[\e[m\] \[\e[1;34m\]\W\[\e[m\]\[\e[1;32m\]]$\[\e[m\] '</span></code></pre><h3 id="设置开机自动同步时间"><a href="#设置开机自动同步时间" class="headerlink" title="设置开机自动同步时间"></a>设置开机自动同步时间</h3><blockquote><p>众所周知树莓派不加扩展板的情况下 没有硬件RTC时钟，也就意味着每次开机都要设置时钟，这特么可万万不能够啊，以下为设置开机自动同步时间的方法：</p></blockquote><ul><li>安装ntp</li></ul><pre><code class="hljs sh">apt-get install ntpdate</code></pre><ul><li>设置时区</li></ul><pre><code class="hljs sh"><span class="hljs-comment"># 执行如下命令</span>dpkg-reconfigure tzdata<span class="hljs-comment"># 选择 Asia(亚洲) 然后选择 ShanHai(上海)</span></code></pre><ul><li>手动校对时间</li></ul><pre><code class="hljs sh"><span class="hljs-comment"># 一般上面设置完市区就应该已经自动同步时间了</span><span class="hljs-comment"># 执行以下命令可能会报socket占用，可忽略</span><span class="hljs-comment"># 210.72.145.44 国家授时中心服务器IP</span>ntpdate 210.72.145.44</code></pre><ul><li>设置开机自动校对时间</li></ul><blockquote><p>编辑 <code>/ect/rc.local</code> 文件 执行 <code>vim /etc/rc.local</code>，加入 <code>htpdate -t -s 210.72.145.44</code> 这条命令，注意要放在 <code>exit 0</code> 前面，样例配置如下:</p></blockquote><pre><code class="hljs sh"><span class="hljs-comment">#!/bin/sh -e</span><span class="hljs-comment"># 此处省略1000行注释.....</span><span class="hljs-comment"># Sync Time</span>htpdate -t -s 210.72.145.44<span class="hljs-comment"># Print the IP address</span>_IP=$(hostname -I) || <span class="hljs-literal">true</span><span class="hljs-keyword">if</span> [ <span class="hljs-string">"<span class="hljs-variable">$_IP</span>"</span> ]; <span class="hljs-keyword">then</span>  <span class="hljs-built_in">printf</span> <span class="hljs-string">"My IP address is %s\n"</span> <span class="hljs-string">"<span class="hljs-variable">$_IP</span>"</span><span class="hljs-keyword">fi</span><span class="hljs-built_in">exit</span> 0</code></pre><h3 id="合并剩余分区空间"><a href="#合并剩余分区空间" class="headerlink" title="合并剩余分区空间"></a>合并剩余分区空间</h3><blockquote><p>默认树莓派安装系统后并不会占用所有SD卡空间，一般只会使用4G左右，对于大内存卡来说剩下的空间属于未分配状态，即未分区无法使用，我们可以使用 <code>fdisk</code> 来合并后面的分区加以利用</p></blockquote><h4 id="使用-raspi-config-2016-08-13-更新"><a href="#使用-raspi-config-2016-08-13-更新" class="headerlink" title="使用 raspi-config(2016-08-13 更新)"></a>使用 raspi-config(2016-08-13 更新)</h4><p>推荐直接使用树莓派提供的工具，执行 <code>raspi-config</code>，然后选择第一项，提示选择 OK ，最终 Finish 即可。</p><h4 id="使用-分区工具"><a href="#使用-分区工具" class="headerlink" title="使用 分区工具"></a>使用 分区工具</h4><ul><li>查看分区及起始参数</li></ul><pre><code class="hljs sh"><span class="hljs-comment"># 进入fdisk</span>fdisk /dev/mmcblk0<span class="hljs-comment"># 按 P 显示分区信息，并记录 Type 为Linux的分区起始柱面(122880)，打印如下:</span>Device         Boot  Start     End Sectors Size Id Type/dev/mmcblk0p1        8192  122879  114688  56M  c W95 FAT32 (LBA)/dev/mmcblk0p2      122880 8447999 8325120   4G 83 Linux</code></pre><ul><li>删除 主分区(Linux分区)</li></ul><pre><code class="hljs sh"><span class="hljs-comment"># 按D 并选择删除分区2，再按P 查看分区信息，打印如下:</span>Command (m <span class="hljs-keyword">for</span> <span class="hljs-built_in">help</span>): dPartition number (1,2, default 2): 2Partition 2 has been deleted.Command (m <span class="hljs-keyword">for</span> <span class="hljs-built_in">help</span>): pDisk /dev/mmcblk0: 28.8 GiB, 30908350464 bytes, 60367872 sectorsUnits: sectors of 1 * 512 = 512 bytesSector size (logical/physical): 512 bytes / 512 bytesI/O size (minimum/optimal): 512 bytes / 512 bytesDisklabel <span class="hljs-built_in">type</span>: dosDisk identifier: 0xba2edfb9Device         Boot Start    End Sectors Size Id Type/dev/mmcblk0p1       8192 122879  114688  56M  c W95 FAT32 (LBA)</code></pre><ul><li>增加新分区</li></ul><pre><code class="hljs sh"><span class="hljs-comment"># 按N 执行新建分区，再按P 选择增加主分区，并输入起始柱面(122880)，打印如下:</span>Command (m <span class="hljs-keyword">for</span> <span class="hljs-built_in">help</span>): nPartition <span class="hljs-built_in">type</span>   p   primary (1 primary, 0 extended, 3 free)   e   extended (container <span class="hljs-keyword">for</span> logical partitions)Select (default p): pPartition number (2-4, default 2): 2First sector (2048-60367871, default 2048): 122880Last sector, +sectors or +size&#123;K,M,G,T,P&#125; (122880-60367871, default 60367871):Created a new partition 2 of <span class="hljs-built_in">type</span> <span class="hljs-string">'Linux'</span> and of size 28.7 GiB.</code></pre><ul><li>执行增加分区操作</li></ul><pre><code class="hljs sh"><span class="hljs-comment"># 首先按P 查看一下，确认分区增加无误，然后按W 执行分区操作，打印如下:</span>Command (m <span class="hljs-keyword">for</span> <span class="hljs-built_in">help</span>): pDisk /dev/mmcblk0: 28.8 GiB, 30908350464 bytes, 60367872 sectorsUnits: sectors of 1 * 512 = 512 bytesSector size (logical/physical): 512 bytes / 512 bytesI/O size (minimum/optimal): 512 bytes / 512 bytesDisklabel <span class="hljs-built_in">type</span>: dosDisk identifier: 0xba2edfb9Device         Boot  Start      End  Sectors  Size Id Type/dev/mmcblk0p1        8192   122879   114688   56M  c W95 FAT32 (LBA)/dev/mmcblk0p2      122880 60367871 60244992 28.7G 83 LinuxCommand (m <span class="hljs-keyword">for</span> <span class="hljs-built_in">help</span>): wThe partition table has been altered.<span class="hljs-comment"># 此处说明 设备忙碌，新的分区表将在下次重启生效</span>Calling ioctl() to re-read partition table.Re-reading the partition table failed.: Device or resource busyThe kernel still uses the old table. The new table will be used at the next reboot or after you run partprobe(8) or kpartx(8).</code></pre><ul><li>重启并执行分区修复</li></ul><pre><code class="hljs sh"><span class="hljs-comment"># 由于上面已经提示了重启生效，所以执行重启命令</span>reboot<span class="hljs-comment"># 重启后需要进行一次分区修复，否则df查看磁盘占用是不变的，执行如下命令修复分区</span>resize2fs /dev/mmcblk0p2<span class="hljs-comment"># 执行成功后使用df查看分区占用情况，打印如下:</span>[root@raspberrypi ~]<span class="hljs-comment"># df</span>Filesystem     1K-blocks    Used Available Use% Mounted on/dev/root       29586708 3255252  25069844  12% /devtmpfs          469756       0    469756   0% /devtmpfs             474060       0    474060   0% /dev/shmtmpfs             474060    6412    467648   2% /runtmpfs               5120       4      5116   1% /run/locktmpfs             474060       0    474060   0% /sys/fs/cgroup/dev/mmcblk0p1     57288   20232     37056  36% /boottmpfs              94812       0     94812   0% /run/user/1000</code></pre><h3 id="安装lrzsz-快捷上传下载"><a href="#安装lrzsz-快捷上传下载" class="headerlink" title="安装lrzsz(快捷上传下载)"></a>安装lrzsz(快捷上传下载)</h3><blockquote><p>操作简单，但大有用处，执行一条命令 <code>apt-get install lrzsz</code> 就安装成功了；作用就是在使用xShell通过ssh连接到树莓派后，上传文件只需要敲 <code>rz</code> 命令就会弹出文件选择对话框，选择文件后就直接上传到当前shell显示的目录下了，也可以直接将文件拖向命令行，也会直接将文件上传到当前目录；下载的话直接敲 <code>sz FILENAME</code> 就会马上弹出下载选择框，选择到哪就会下载到那个目录，奏是这么吊！</p></blockquote><h3 id="切换国内源"><a href="#切换国内源" class="headerlink" title="切换国内源"></a>切换国内源</h3><p>默认树莓派连接的源是官方源，其服务器在美国，然后你懂得……编辑 <code>/etc/apt/sources.list</code> 注释掉其他源，从 <a href="http://www.raspbian.org/RaspbianMirrors" target="_blank" rel="noopener">树莓派源列表</a> 中选择一个填入，然后 <code>apt-get update</code> 即可，以下为 <code>/etc/apt/sources.lis</code> 文件参考(清华大学源)</p><pre><code class="hljs sh"><span class="hljs-comment">#deb http://mirrordirector.raspbian.org/raspbian/ jessie main contrib non-free rpi</span><span class="hljs-comment"># Uncomment line below then 'apt-get update' to enable 'apt-get source'</span><span class="hljs-comment">#deb-src http://archive.raspbian.org/raspbian/ jessie main contrib non-free rpi</span>deb http://mirrors.tuna.tsinghua.edu.cn/raspbian/raspbian/ jessie main non-free contribdeb-src http://mirrors.tuna.tsinghua.edu.cn/raspbian/raspbian/ jessie main non-free contrib</code></pre><h3 id="编译并安装Nginx"><a href="#编译并安装Nginx" class="headerlink" title="编译并安装Nginx"></a>编译并安装Nginx</h3><blockquote><p>由于做J2EE开发，想搞个Nginx，so 鼓捣吧……</p></blockquote><ul><li>下载相关组件源码</li></ul><blockquote><p>Nginx 编译安装需要 如下支持<br>gcc pcre pcre-devel zlib zlib-devel openssl openssl-devel<br>默认gcc树莓派已经自带了，剩下的 pcre、zlib、openssl 需要自己下载，下载地址: <a href="http://www.pcre.org/" target="_blank" rel="noopener">pcre</a>、<a href="http://www.zlib.net/" target="_blank" rel="noopener">zlib</a>、<a href="https://www.openssl.org/source/" target="_blank" rel="noopener">openssl</a>、<a href="http://nginx.org/en/download.html" target="_blank" rel="noopener">Nginx</a></p></blockquote><ul><li>解压相关组件</li></ul><pre><code class="hljs sh"><span class="hljs-comment"># 下载下来想办法搞到树莓派上，然后解压</span>tar -zxvf openssl-1.0.2d.tar.gztar -zxvf pcre-8.37.tar.gztar -zxvf zlib-1.2.8.tar.gztar -zxvf nginx-1.8.0.tar.gz</code></pre><ul><li>执行编译安装</li></ul><pre><code class="hljs sh"><span class="hljs-comment"># 首先进入到 nginx目录</span><span class="hljs-built_in">cd</span> nginx-1.8.0<span class="hljs-comment"># 执行编译，编译参数如下:</span>./configure--sbin-path=/usr/<span class="hljs-built_in">local</span>/nginx/nginx--conf-path=/usr/<span class="hljs-built_in">local</span>/nginx/nginx.conf--pid-path=/usr/<span class="hljs-built_in">local</span>/nginx/nginx.pid--with-http_ssl_module--with-pcre=../pcre-8.37--with-zlib=../zlib-1.2.8--with-openssl=../openssl-1.0.2d<span class="hljs-comment"># 最后安装(时间有点长......)</span>make &amp;&amp; make install</code></pre><h3 id="安装花生壳-内网穿透"><a href="#安装花生壳-内网穿透" class="headerlink" title="安装花生壳(内网穿透)"></a>安装花生壳(内网穿透)</h3><blockquote><p>作为一名 Java Web狗，Nginx、Tomcat已经搞起了，只能内网访问怎么可以；所以必须搞一个 花生壳做内网映射，使之通过域名可直接从外网访问到内网下的树莓派中部署的项目。</p></blockquote><ul><li>下载花生壳并解压</li></ul><blockquote><p>花生壳树莓派版下载地址: <a href="http://hsk.oray.com/download/#type=http|shumeipai" target="_blank" rel="noopener">点击下载</a><br>下载后上传到树莓派，执行 <code>tar -zxvf phddns_raspberry.tgz</code> 解压文件</p></blockquote><ul><li>安装并运行花生壳</li></ul><blockquote><p>解压后会生成 <code>phddns2</code> 目录，<code>cd</code> 进去，执行 <code>./oraynewph start</code> 命令进行安装，安装完成后屏幕上会显示几行信息，其中一个是 <strong>SN码</strong>，记录下这个 <strong>SN码</strong>，一般是一串复杂的类似于md5的字符串<br>然后进入 <a href="http://b.oray.com" target="_blank" rel="noopener">b.oray.com</a> ，选择使用 <strong>SN码</strong> 登录，默认密码是admin，<strong><em>注意：此时树莓派必须成功连接外网，否则无法登陆</em></strong>，登录如下图所示:</p></blockquote><blockquote><p><img src="http://7xoixd.com1.z0.glb.clouddn.com/raspberry_oray_login.png" srcset="/img/loading.gif" alt="登录"></p></blockquote><blockquote><p>登陆后首先选择 <strong><em>动态域名解析</em></strong> ，然后 <strong><em>注册一个壳域名</em></strong> ，再回到首页选择 <strong><em>内网映射</em></strong> ，配置一个域名内网映射即可，映射配置如下图:</p></blockquote><blockquote><p><img src="http://7xoixd.com1.z0.glb.clouddn.com/raspberry_oray_netmapper.png" srcset="/img/loading.gif" alt="内网映射配置"></p></blockquote><blockquote><p>其他关于更详细的花生壳使用教程请谷歌，安装教程参考 <a href="http://service.oray.com/question/2680.html" target="_blank" rel="noopener">这里</a>，最后附上一张装好逼的截图:</p></blockquote><blockquote><p><img src="http://7xoixd.com1.z0.glb.clouddn.com/raspberry_oray_netmapperopen.png" srcset="/img/loading.gif" alt="动态域名访问树莓派Nginx"></p></blockquote><h3 id="编译安装Nexus"><a href="#编译安装Nexus" class="headerlink" title="编译安装Nexus"></a>编译安装Nexus</h3><blockquote><p>由于现在项目都使用Maven，所以也想搞一个私服Nexus，无奈Nexus官方二进制包中并不提供 arm平台支持，主要是wrapper 没有arm平台的可执行文件和动态链接库，故需要自己编译wrapper，所以有了 “编译安装Nexus” 这一说法。</p></blockquote><ul><li>下载 Nexus</li></ul><blockquote><p>登录 <a href="http://www.sonatype.org/nexus/go/" target="_blank" rel="noopener">Nexus官网</a> 选择 TGZ 格式下载，上传到 树莓派并解压，然后 mv 到你想要放置的目录。配置一个环境变量，样例如下 : <code>export NEXUS_HOME=&#39;/usr/local/java/nexus/nexus-2.11.4-01&#39;</code>，并将 <code>$NEXUS_HOME/bin</code> 加入到 <code>PATH</code> 中。</p></blockquote><ul><li>下载wrapper</li></ul><blockquote><p>为何需要下载 <code>wrapper</code> : <code>nexus</code> 本身确实自带了 <code>wrapper</code>，但 <code>wrapper</code> 这玩意是跟平台结合的，目前 <code>Nexus</code> 不支持 <code>arm</code> 平台，所以需要我们手动编译一个替换 <code>Nexu</code> 中的 <code>wrapper</code>。</p><hr><ol><li>目前下载的最新版 Nexus-2.11.4-01 依赖的是 wrapper3.2.3，下载地址 <a href="http://wrapper.tanukisoftware.com/download/3.5.9/wrapper_3.5.9_src.tar.gz?mode=html" target="_blank" rel="noopener">点这里</a> <strong><em>(注意我们下载的事3.5.9版本)</em></strong>。</li><li>下载完 src 源码包后上传到树莓派并解压，在开始编译前，需要正确的配置<code>JAVA_HOME</code> 和 <code>PATH</code>，这里有个小问题，树莓派2自带了JDK8，但 <code>JAVA_HOME</code> 啥的没配置，所以会有问题；但直接卸载的话 <code>apt-get</code> 会自动给你安装 <code>open-jdk7</code>，可执行 <code>apt-cache rdepends oracle-java8-jdk</code> 查看依赖 <code>jdk</code> 的相关软件包，并执行 <code>apt-get purge xxxxx</code> 卸载他们，基本这些软件包都是教学用的，可以删掉；然后自己下载 <code>arm</code> 平台的 <code>jdk7</code> (感觉8太新了怕不稳定) 安装、配置环境。</li></ol></blockquote><ul><li>编译 wrapper</li></ul><blockquote><ol><li>配置完 <code>JAVA_HOME</code>、<code>PATH</code> 变量以后还需要下载一个 <code>Ant</code>，因为 <code>wrapper</code> 是基于 <code>Ant</code> 构建的，基本步骤也是。 <a href="http://ant.apache.org/bindownload.cgi" target="_blank" rel="noopener">下载Ant</a> 然后解压到指定目录，配置一下 <code>ANT_HOME</code>，方法自查。</li><li>在正式编译前需要 <code>cp src/c/Makefile-linux-x86-32.make</code> to <code>src/c/Makefile-linux-arm-32.make</code> (老外的原文，说来了就是 <code>copy</code> 一份到当前目录并重命名一下)。</li><li>进入到 <code>wrapper</code> 解压后的目录执行 <code>./build32.sh</code> 进行编译，如果 <code>JAVA_HOME</code>、<code>PATH</code>、<code>Ant</code>、<code>.make 文件</code> 没问题的话编译一般不会出错。</li><li>编译完成后在 <code>nexus-2.11.4-01/bin/jsw</code> 下新建一个 <code>linux-armv7l-32</code> 文件夹，复制编译好的 <code>wrapper_3.5.9_src/bin/wrapper</code> 文件到 刚刚新建的 <code>linux-armv7l-32</code> 目录下，由于<strong><em>使用了高版本</em></strong> <code>wrapper</code>，<code>wrapper.jar</code> 复制过去后需要先删掉原来的 <code>wrapper-3.2.3.jar</code> 并将 <code>wrapper.jar</code> 重命名为 <code>wrapper-3.2.3.jar</code>。</li></ol></blockquote><ul><li>配置并启动 Nexus</li></ul><blockquote><ul><li>新建用户 <code>nexus</code> : <code>adduser nexus</code> (别用 <code>useradd</code> 我一直以为这两个命令一样，但你在树莓派2下可以试试)</li><li>改密码 : <code>passwd nexus</code></li><li>改两个配置文件 : <code>nexus-2.11.4-01/bin/nexus</code>、<code>nexus-2.11.4-01/bin/jsw/conf/wrapper.conf</code>，两个配置要改的地方贴出来如下 (更详细的参见 <a href="http://mritd.me/archives/2024" target="_blank" rel="noopener">Nexus 2.11 CentOS搭建教程</a>) :</li></ul></blockquote><pre><code class="hljs sh"><span class="hljs-comment">###############################</span><span class="hljs-comment">## nexus-2.11.4-01/bin/nexus ##</span><span class="hljs-comment">###############################</span><span class="hljs-comment">#</span><span class="hljs-comment"># Set the JVM executable</span><span class="hljs-comment"># (modify this to absolute path if you need a Java that is not on the OS path)</span><span class="hljs-comment"># 配置 jdk中 java 可执行文件的位置(其实我感觉jre就可以，没测试，有兴趣的测试一下)</span>wrapper.java.command=/usr/<span class="hljs-built_in">local</span>/java/jdk1.7.0_79/bin/java</code></pre><pre><code class="hljs sh"><span class="hljs-comment">##############################################</span><span class="hljs-comment">## nexus-2.11.4-01/bin/jsw/conf/wrapper.con ##</span><span class="hljs-comment">##############################################</span><span class="hljs-comment">#</span><span class="hljs-comment"># Set this to the root of the Nexus installation</span><span class="hljs-comment"># 设置 nexus 主目录，就是解压后的那个 nexus目录绝对路径</span>NEXUS_HOME=<span class="hljs-string">"/usr/local/java/nexus-2.11.4-01"</span></code></pre><blockquote><ul><li>先切换到 <code>nexus</code> 用户，因为官方不推荐以 <code>root</code> 用户运行，执行 : <code>su - nexus</code>，然后启动 <code>nexus</code>，执行 <code>nexus start</code> 启动，时间比较长，大约2分钟，使用 <code>tail -f nexus-2.11.4-01/logs/wrapper.log</code> 查看进度，启动成功后访问 <code>IP:8081/nexus</code> 即可，默认用户 <code>admin</code>，密码 <code>admin123</code>；到此结束。</li></ul></blockquote><h3 id="编译安装MySQL"><a href="#编译安装MySQL" class="headerlink" title="编译安装MySQL"></a>编译安装MySQL</h3><ul><li>安装Screen</li></ul><blockquote><p>执行 <code>apt-get install screen</code> 安装screen，用于后台运行编译任务，防止断网等原因造成的编译失败。</p></blockquote><ul><li>下载MySQL5.6源码</li></ul><blockquote><p>可去官网下载，百度云分享 <a href="http://pan.baidu.com/s/1dDwPxRV" target="_blank" rel="noopener">点击这里</a> 密码: g2ab<br>下载完成后上传到树莓派并解压</p></blockquote><ul><li>系统设置初始化</li></ul><blockquote><p>编译前需要做以下操作:</p></blockquote><pre><code class="hljs sh"><span class="hljs-comment"># 新建mysql用户</span>adduser mysql<span class="hljs-comment"># 创建MySQL安装目录</span><span class="hljs-built_in">cd</span> /usr/<span class="hljs-built_in">local</span>mkdir mysql<span class="hljs-built_in">cd</span> /usr/<span class="hljs-built_in">local</span>/mysqlmkdir data</code></pre><ul><li>安装依赖包</li></ul><pre><code class="hljs sh"><span class="hljs-comment"># 首先升级软件源(如果改过非官方源必须改回来)</span>apt-get update<span class="hljs-comment"># 升级已安装软件包</span>apt-get upgrade<span class="hljs-comment"># 安装mysql编译时依赖</span>apt-get isntall cmake make bison bzr libncurses5-dev g++ libtinfo5 ncurses-bin libncurses5 libtinfo-dev</code></pre><ul><li>开始编译MySQL</li></ul><pre><code class="hljs sh"><span class="hljs-comment"># 进入源码目录</span><span class="hljs-built_in">cd</span> mysql-5.6.27/<span class="hljs-comment"># 开启screen session 防止断网等造成的免疫中断</span>screen -S mysqlinstall<span class="hljs-comment"># 执行 cmake 预编译(最少一小时)</span>cmake -DCMAKE_INSTALL_PREFIX=/usr/<span class="hljs-built_in">local</span>/mysql -DMYSQL_UNIX_ADDR=/usr/<span class="hljs-built_in">local</span>/mysql/data/mysql.sock -DDEFAULT_CHARSET=utf8 -DDEFAULT_COLLATION=utf8_general_ci -DEXTRA_CHARSETS=all -DWITH_MYISAM_STORAGE_ENGINE=1 -DWITH_INNOBASE_STORAGE_ENGINE=1 -DWITH_MEMORY_STORAGE_ENGINE=1 -DWITH_READLINE=1 -DENABLED_LOCAL_INFILE=1 -DMYSQL_DATADIR=/usr/<span class="hljs-built_in">local</span>/mysql/data -DMYSQL_USER=mysql -DWITH_DEBUG=0</code></pre><blockquote><p>关于cmake预编译参数设置参考 <a href="http://ohgenlong16300.blog.51cto.com/499130/1264096" target="_blank" rel="noopener">这里</a></p></blockquote><ul><li>执行安装</li></ul><pre><code class="hljs sh">make &amp;&amp; make install</code></pre><ul><li>后续操作</li></ul><pre><code class="hljs sh"><span class="hljs-comment"># 更改所有者</span><span class="hljs-built_in">cd</span> /usr/<span class="hljs-built_in">local</span>/chown -R mysql.mysql mysql<span class="hljs-comment"># 创建配置文件</span><span class="hljs-built_in">cd</span> /usr/<span class="hljs-built_in">local</span>/mysql/support-files  cp my-default.cnf  /etc/my.cnf<span class="hljs-comment"># 初始化数据库</span><span class="hljs-built_in">cd</span> /usr/<span class="hljs-built_in">local</span>/mysql./scripts/mysql_install_db --user=mysql<span class="hljs-comment"># 安全启动(后台)</span>/usr/<span class="hljs-built_in">local</span>/mysql/bin/mysqld_safe --user=mysql --port=3306 --sock=/usr/<span class="hljs-built_in">local</span>/mysql/data/mysql.sock &amp;<span class="hljs-comment"># 开机自启动</span>cp /usr/<span class="hljs-built_in">local</span>/mysql/support-files/mysql.server /etc/init.d/mysqlchkconfig --add mysql</code></pre><blockquote><p>mysql5.6的默认参数设置问题，更改my.cnf，调整以下参数<br><code>performance_schema_max_table_instances=600</code><br><code>table_definition_cache=400</code><br><code>table_open_cache=256</code><br>这时mysql启动后内存就只占用40–60M内存了</p></blockquote><h3 id="安装-xrdp远程桌面"><a href="#安装-xrdp远程桌面" class="headerlink" title="安装 xrdp远程桌面"></a>安装 xrdp远程桌面</h3><blockquote><p>其实树莓派感觉没必要买显示器，因为直接可以安装远程桌面，执行 <code>apt-get install xrdp</code> 安装，在 <code>Windows</code> 下可直接使用远程桌面连接，按 <code>Win+R</code> 键输入 <code>mstsc</code>，再输入树莓派地址和用户名密码   就可以，截图如下:<br><img src="http://7xoixd.com1.z0.glb.clouddn.com/raspberry_xrdp.png" srcset="/img/loading.gif" alt="远程桌面"></p></blockquote>]]></content>
    
    
    <categories>
      
      <category>Linux</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Linux</tag>
      
      <tag>Raspberry Pi2</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Redis Key 常用命令</title>
    <link href="/2006/01/02/redis-key-common-command/"/>
    <url>/2006/01/02/redis-key-common-command/</url>
    
    <content type="html"><![CDATA[<p><img src="https://cdn.oss.link/markdown/hexo_redis_logo1.jpg" srcset="/img/loading.gif" alt="hexo_redis_logo"></p><blockquote><p><strong>参考自 <a href="http://www.redis.cn/commands.html" target="_blank" rel="noopener">Redis官方文档(中文版)</a></strong></p></blockquote><h2 id="KEYS"><a href="#KEYS" class="headerlink" title="KEYS"></a>KEYS</h2><blockquote><p>加入版本 1.0.0。<br>时间复杂度： O(N)，N 为数据库中 key 的数量。</p><p><strong>查找所有符合给定模式 <code>pattern</code> 的 <code>key</code>。特殊符号用 <code>\</code> 隔开。</strong></p></blockquote><pre><code class="hljs sh">KEYS *          <span class="hljs-comment"># 匹配数据库中所有 key 。</span>KEYS h?llo      <span class="hljs-comment"># 匹配 hello ， hallo 和 hxllo 等。</span>KEYS h*llo      <span class="hljs-comment"># 匹配 hllo 和 heeeeello 等。</span>KEYS h[ae]llo   <span class="hljs-comment"># 匹配 hello 和 hallo ，但不匹配 hillo 。</span></code></pre><blockquote><p><strong>Warning：KEYS 的速度非常快，但在一个大的数据库中使用它仍然可能造成性能问题，如果你需要从一个数据集中查找特定的 KEYS， 你最好还是用 Redis 的集合结构 SETS 来代替。</strong></p></blockquote><h2 id="DEL"><a href="#DEL" class="headerlink" title="DEL"></a>DEL</h2><blockquote><p>加入版本 1.0.0。<br>时间复杂度： O(n)N为要移除的key的数量。移除单个字符串类型的key，时间复杂度为O(1)。移除单个列表、集合、有序集合或哈希表类型的key，时间复杂度为O(M)，M为以上数据结构内的元素数量。</p><p><strong>如果删除的key不存在，则直接忽略。返回被删除的keys的数量，Key 不存在返回0</strong></p></blockquote><p><strong>语法</strong></p><pre><code class="hljs sh">DEL KEY [KEY ...]</code></pre><h2 id="EXISTS"><a href="#EXISTS" class="headerlink" title="EXISTS"></a>EXISTS</h2><blockquote><p>加入版本 1.0.0。<br>时间复杂度： O(1)。<br><strong>返回key是否存在。如果key存在返回 1 ；如果key不存在 返回 0。</strong></p></blockquote><p><strong>语法</strong></p><pre><code class="hljs sh">EXISTS KEY</code></pre><h2 id="MOVE"><a href="#MOVE" class="headerlink" title="MOVE"></a>MOVE</h2><blockquote><p>加入版本 1.0.0。<br>时间复杂度： O(1)。<br>将当前数据库的 key 移动到给定的数据库 db 当中。<br>如果当前数据库(源数据库)和给定数据库(目标数据库)有相同名字的给定 key ，或者 key 不存在于当前数据库，那么 MOVE 没有任何效果。<strong>因此，也可以利用这一特性，将 MOVE 当作锁(locking)原语(primitive)。</strong></p></blockquote><p><strong>语法</strong></p><pre><code class="hljs sh">MOVE KEY DBNAME</code></pre><h2 id="RENAME"><a href="#RENAME" class="headerlink" title="RENAME"></a>RENAME</h2><blockquote><p>加入版本 1.0.0。<br>时间复杂度： O(1)。<br>将key重命名为newkey，如果key与newkey相同，将返回一个错误。如果newkey已经存在，则值将被覆盖。</p></blockquote><p><strong>语法</strong></p><pre><code class="hljs sh">RENAME OLDKEYNAME NEWKEYNAME</code></pre><h2 id="RENAMENX"><a href="#RENAMENX" class="headerlink" title="RENAMENX"></a>RENAMENX</h2><blockquote><p>加入版本 1.0.0。<br>时间复杂度： O(1)。<br><strong>当且仅当 newkey 不存在时，将 key 改名为 newkey 。当 key 不存在时，返回一个错误。</strong></p></blockquote><p><strong>语法</strong></p><pre><code class="hljs sh">RENAMENX OLDKEYNAME NEWKEYNAME</code></pre><h2 id="EXPIRE"><a href="#EXPIRE" class="headerlink" title="EXPIRE"></a>EXPIRE</h2><blockquote><p>加入版本 1.0.0。<br>时间复杂度： O(1)。<br>设置key的过期时间。如果key已过期，将会被自动删除。设置了过期时间的key被称之为volatile。<strong>在key过期之前可以重新更新他的过期时间，也可以使用PERSIST命令删除key的过期时间。</strong><br>在Redis&lt; 2.1.3之前的版本,key的生存时间可以被更新</p></blockquote><p><strong>语法</strong></p><pre><code class="hljs sh">EXPIRE KEY SECONDS</code></pre><h2 id="PERSIST"><a href="#PERSIST" class="headerlink" title="PERSIST"></a>PERSIST</h2><blockquote><p>加入版本 2.2.0。<br>时间复杂度： O(1)。<br>移除给定key的生存时间，将这个 key 从『易失的』(带生存时间 key )转换成『持久的』(一个不带生存时间、永不过期的 key )。</p></blockquote><p><strong>语法</strong></p><pre><code class="hljs sh">PERSIST KEY</code></pre><h2 id="TTL"><a href="#TTL" class="headerlink" title="TTL"></a>TTL</h2><blockquote><p>加入版本 1.0.0。<br>时间复杂度： O(1)。<br>以秒为单位，返回给定 key 的剩余生存时间(TTL, time to live)。</p></blockquote><p><strong>返回值</strong></p><blockquote><p>当 key 不存在时，返回 -2 。<br>当 key 存在但没有设置剩余生存时间时，返回 -1 。<br>否则，以秒为单位，返回 key 的剩余生存时间。</p></blockquote><p><strong>语法</strong></p><pre><code class="hljs sh">TTL KEY</code></pre><h2 id="TYPE"><a href="#TYPE" class="headerlink" title="TYPE"></a>TYPE</h2><blockquote><p>加入版本 1.0.0。<br>时间复杂度： O(1)。<br>返回 key 所储存的值的类型。</p></blockquote><p><strong>返回值</strong></p><ul><li>none (key不存在)</li><li>string (字符串)</li><li>list (列表)</li><li>set (集合)</li><li>zset (有序集)</li><li>hash (哈希表)</li></ul><p><strong>语法</strong></p><pre><code class="hljs sh">TYPE KEY</code></pre><h2 id="RANDOMKEY"><a href="#RANDOMKEY" class="headerlink" title="RANDOMKEY"></a>RANDOMKEY</h2><blockquote><p>加入版本 1.0.0。<br>时间复杂度： O(1)。<br>从当前数据库返回一个随机的key。</p></blockquote><p><strong>语法</strong></p><pre><code class="hljs sh">RANDOMKEY</code></pre>]]></content>
    
    
    <categories>
      
      <category>Linux</category>
      
      <category>Database</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Linux</tag>
      
      <tag>Redis</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Redis学习笔记</title>
    <link href="/2006/01/02/redis-note/"/>
    <url>/2006/01/02/redis-note/</url>
    
    <content type="html"><![CDATA[<p><img src="https://cdn.oss.link/markdown/hexo_redis_logo1.jpg" srcset="/img/loading.gif" alt="hexo_redis_logo"></p><h2 id="一、简介"><a href="#一、简介" class="headerlink" title="一、简介"></a>一、简介</h2><blockquote><p>redis是一个key-value存储系统。和Memcached类似，它支持存储的value类型相对更多，包括string(字符串)、list(链表)、set(集合)、zset(有序集合)和hashs（哈希类型）。这些数据类型都支持push/pop、add/remove及取交集并集和差集及更丰富的操作，而且这些操作都是原子性的。在此基础上，redis支持各种不同方式的排序。与memcached一样，为了保证效率，数据都是缓存在内存中。</p></blockquote><blockquote><p>redis会周期性的把更新的数据写入磁盘或者把修改操作写入追加的记录文件，并且在此基础上实现了master-slave(主从)同步。Redis 是一个高性能的key-value数据库。 redis的出现，很大程度补偿了memcached这类key/value存储的不足，在部 分场合可以对关系数据库起到很好的补充作用。它提供了Python，Ruby，Erlang，PHP客户端，使用很方便。</p></blockquote><h2 id="二、Redis功能"><a href="#二、Redis功能" class="headerlink" title="二、Redis功能"></a>二、Redis功能</h2><ol><li><p>Redis 的Sharding</p><blockquote><p>Redis 支持客户端的Sharding 功能，通过一致性hash 算法实现，当前Redis 不支持故障冗余，在集群中不能在线增加或删除Redis。</p></blockquote></li><li><p>Redis 的master/slave 复制</p><blockquote><p> 一个master 支持多个slave；Slave 可以接受其他slave 的连接来替代他连接master；复制在master 是非阻塞的，而在slave 是阻塞的；复制被利用来提供可扩展性，在slave 端只提供查询功能及数据的冗余。</p></blockquote></li><li><p>Redis 的Virtual Memory 功能(VM)</p><blockquote><p>vm 是Redis2.0 新增的一个非常稳定和可靠的功能，vm 的引入是为了提高Redis 的性能，也就是把很少使用的value 保存到disk，而key 保存在内存中。实际上就是如果你有10w 的keys 在内存中，而只有仅仅10%左右的key 经常使用，那么Redis 可以通过开启VM 尝试将不经常使用的Value 转换到disk 上保存。</p></blockquote></li><li><p>Redis 的附加档案（AOF）功能</p><blockquote><p>Redis 通过配置的策略将数据集保存到aof 中，当Redis 挂掉后能够通过aof 恢复到挂掉前的状态</p></blockquote></li></ol><h2 id="三、Redis-数据类型"><a href="#三、Redis-数据类型" class="headerlink" title="三、Redis 数据类型"></a>三、Redis 数据类型</h2><ul><li><p>字符串(Strings)</p><blockquote><p>字符串是一种最基本的Redis值类型。<strong>Redis字符串是二进制安全的，这意味着一个Redis字符串能包含任意类型的数据</strong>，例如： 一张JPEG格式的图片或者一个序列化的Ruby对象。<strong>一个字符串类型的值最多能存储512M字节的内容。</strong>与Memcached一样，一个key对应一个value，其上支持的操作与Memcached的操作类似；但它的功能更丰富。</p></blockquote></li><li><p>列表(Lists)</p><blockquote><p>Redis列表是简单的字符串列表，<strong>按照插入顺序排序</strong>。 你可以添加一个元素到列表的头部（左边）或者尾部（右边）。Lists是一个链表结构，主要功能是push、pop、获取一个范围的所有值等等。操作中<strong>key理解为链表的名字</strong>。LPUSH命令插入一个新元素到列表头部，而RPUSH命令 插入一个新元素到列表的尾部。<strong>当对一个空key执行其中某个命令时，将会创建一个新表</strong>。 类似的，<strong>如果一个操作要清空列表，那么key会从对应的key空间删除</strong>。这是个非常便利的语义，因为如果使用一个不存在的key作为参数，所有的列表命令都会像在对一个空表操作一样。</p></blockquote></li><li><p>集合(Sets)</p><blockquote><p>Redis集合是一个无序的字符串合集。你可以以 <strong>O(1)</strong> 的时间复杂度完成 <strong>添加、删除以及测试元素是否存在</strong> 的操作。Redis集合有着 <strong>不允许相同成员存在</strong> 的优秀特性。向集合中多次添加同一元素，在集合中最终只会存在一个此元素。实际上这就意味着，在添加元素前，你并不需要事先进行检验此元素是否已经存在的操作。Redis的集合支持集合运算，<strong>可以进行 合集、交集、差集运算。一个集合最多可以包含232-1个元素（4294967295，每个集合超过40亿个元素）。</strong></p></blockquote></li><li><p>有序集合(Sorted sets)</p><blockquote><p>Redis有序集合和Redis集合类似，是不包含 相同字符串的合集；他在set的基础上 <strong>增加了一个顺序属性</strong>，这一属性 <strong>在添加修改元素的时候可以指定，每次指定后自动重新按新的值调整顺序</strong>。可以理解了有两列的mysql表，一列存value，一列存顺序。操作中key理解为 Sorted sets 的名字。</p></blockquote></li><li><p>哈希(Hashes)</p><blockquote><p>Redis hash是一个string类型的field和value的映射表.一个key可对应多个field，一个field对应一个value。将一个对象存储为hash类型，较于每个字段都存储成string类型更能节省内存。新建一个hash对象时开始是用zipmap(又称为small hash)来存储的。这个zipmap其实并不是hash table，但是zipmap相比正常的hash实现可以节省不少hash本身需要的一些元数据存储开销。尽管zipmap的添加，删除，查找都是O(n)，但是由于一般对象的field数量都不太多。所以使用zipmap也是很快的,也就是说添加删除平均还是O(1)。如果field或者value的大小超出一定限制后，Redis会在内部自动将zipmap替换成正常的hash实现。</p></blockquote></li><li><p>Hashes相关</p></li></ul><blockquote><p>Hashes 相关方法如下</p></blockquote><pre><code class="hljs stylus"><span class="hljs-function"><span class="hljs-title">hset</span><span class="hljs-params">(key, field, value)</span></span>                             向名称为key的hash中添加元素field&lt;—&gt;value<span class="hljs-function"><span class="hljs-title">hget</span><span class="hljs-params">(key, field)</span></span>                                    返回名称为key的hash中field对应的value<span class="hljs-function"><span class="hljs-title">hmget</span><span class="hljs-params">(key, field1,...,field N)</span></span>                      返回名称为key的hash中field i对应的value<span class="hljs-function"><span class="hljs-title">hmset</span><span class="hljs-params">(key, field1, value1,...,field N, value N)</span></span>     向名称为key的hash中添加元素field i&lt;—&gt;value i<span class="hljs-function"><span class="hljs-title">hincrby</span><span class="hljs-params">(key, field, integer)</span></span>                        将名称为key的hash中field的value增加integer<span class="hljs-function"><span class="hljs-title">hexists</span><span class="hljs-params">(key, field)</span></span>                                 名称为key的hash中是否存在键为field的域<span class="hljs-function"><span class="hljs-title">hdel</span><span class="hljs-params">(key, field)</span></span>                                    删除名称为key的hash中键为field的域<span class="hljs-function"><span class="hljs-title">hlen</span><span class="hljs-params">(key)</span></span>                                           返回名称为key的hash中元素个数<span class="hljs-function"><span class="hljs-title">hkeys</span><span class="hljs-params">(key)</span></span>                                          返回名称为key的hash中所有键<span class="hljs-function"><span class="hljs-title">hvals</span><span class="hljs-params">(key)</span></span>                                          返回名称为key的hash中所有键对应的value<span class="hljs-function"><span class="hljs-title">hgetall</span><span class="hljs-params">(key)</span></span>                                        返回名称为key的hash中所有的键（field）及其对应的value</code></pre><blockquote><p>示例操作如下</p></blockquote><pre><code class="hljs sh">redis&gt; HMSET user:1000 username antirez password P1pp0 age 34OKredis&gt; HGETALL user:10001) <span class="hljs-string">"username"</span>2) <span class="hljs-string">"antirez"</span>3) <span class="hljs-string">"password"</span>4) <span class="hljs-string">"P1pp0"</span>5) <span class="hljs-string">"age"</span>6) <span class="hljs-string">"34"</span>redis&gt; HSET user:1000 password 12345(<span class="hljs-built_in">integer</span>) 0redis&gt; HGETALL user:10001) <span class="hljs-string">"username"</span>2) <span class="hljs-string">"antirez"</span>3) <span class="hljs-string">"password"</span>4) <span class="hljs-string">"12345"</span>5) <span class="hljs-string">"age"</span>6) <span class="hljs-string">"34"</span>redis&gt;</code></pre><h2 id="四、Redis-数据类型的常用操作"><a href="#四、Redis-数据类型的常用操作" class="headerlink" title="四、Redis 数据类型的常用操作"></a>四、Redis 数据类型的常用操作</h2><blockquote><p>由于命令太多，所以不一一列举，具体参考 <a href="http://redisdoc.com/" target="_blank" rel="noopener">Redis命令参考</a>，文档已进行离线缓存 <a href="http://pan.baidu.com/s/1ntPny6d" target="_blank" rel="noopener">Redis命令手册下载</a></p></blockquote><h3 id="字符串"><a href="#字符串" class="headerlink" title="字符串"></a>字符串</h3><h4 id="SET"><a href="#SET" class="headerlink" title="SET"></a>SET</h4><blockquote><p>将字符串值 <code>value</code> 关联到 <code>key</code> (设置值)。<br>如果 key 已经持有其他值，<code>SET</code> 就覆写旧值，无视类型。<br>对于某个原本带有生存时间（TTL）的键来说，当 <code>SET</code> 命令成功在这个键上执行时，这个键原有的 TTL 将被清除。</p></blockquote><p><strong>可选参数</strong></p><ul><li>EX<br>格式为 <code>EX second</code>；设置键的过期时间为 second 秒。 SET key value EX second 效果等同于 SETEX key second value 。  </li><li>PX<br>格式为 <code>PX millisecond</code>；设置键的过期时间为 millisecond 毫秒。 SET key value PX millisecond 效果等同于 PSETEX key millisecond value 。</li><li>NX<br>只在键不存在时，才对键进行设置操作。 SET key value NX 效果等同于 SETNX key value 。</li><li>XX<br>只在键已经存在时，才对键进行设置操作。</li></ul><p><strong>可接受的语法</strong></p><pre><code class="hljs sh">SET KEY VALUE [OPTION] [OPTION_VALUE]</code></pre><p><strong>代码示例</strong></p><pre><code class="hljs sh">SET a 10             <span class="hljs-comment"># 设置 key a value 10</span>SET a 10 EX 10       <span class="hljs-comment"># 同上，但有效期为10秒，过期后再 get key 则返回nil</span>SET a 10 PX 10000    <span class="hljs-comment"># 同上，有效期为10000毫秒</span>SET a 10 NX          <span class="hljs-comment"># 仅当 a 不存在时才进行设置，否则返回错误</span>SET a 10 XX          <span class="hljs-comment"># 仅当 a 存在时进行设置</span></code></pre><h4 id="GET"><a href="#GET" class="headerlink" title="GET"></a>GET</h4><blockquote><p>返回 <code>key</code> 所关联的字符串值。(获取值)<br>如果 <code>key</code> 不存在那么返回特殊值 <code>nil</code>。<br>假如 <code>key</code> 储存的值不是字符串类型，返回一个错误，因为 <code>GET</code> 只能用于处理字符串值。</p></blockquote><p><strong>可接受的语法</strong></p><pre><code class="hljs sh">GET KEY</code></pre><p><strong>代码示例</strong></p><pre><code class="hljs sh">SET a 10         <span class="hljs-comment"># 先设置一个值</span>GET a            <span class="hljs-comment"># 获取 a 的值，此时 a 必须为String，否则返回错误</span>GET b            <span class="hljs-comment"># 尝试获取一个不存在的值时 返回nil</span></code></pre><h4 id="MSET"><a href="#MSET" class="headerlink" title="MSET"></a>MSET</h4><blockquote><p>同时设置一个或多个 <code>key-value</code> 对。<br>如果某个给定 <code>key</code> 已经存在，那么 <code>MSET</code> 会用新值覆盖原来的旧值，如果这不是你所希望的效果，请考虑使用 <code>MSETNX</code> 命令：它只会在所有给定 <code>key</code> 都不存在的情况下进行设置操作。<br><strong><code>MSET</code> 是一个原子性(atomic)操作，所有给定 <code>key</code> 都会在同一时间内被设置，某些给定 <code>key</code> 被更新而另一些给定 <code>key</code> 没有改变的情况，不可能发生。</strong></p></blockquote><p><strong>可接受的语法</strong></p><pre><code class="hljs sh">MSET KEY1 VALUE1 KEY2 VALUE2 ...</code></pre><p><strong>代码示例</strong></p><pre><code class="hljs sh">MSET a 10 b 11 c 12       <span class="hljs-comment"># 同时设置 a、b、c的值，要么全部成功要么全部失败</span></code></pre><h4 id="MGET"><a href="#MGET" class="headerlink" title="MGET"></a>MGET</h4><blockquote><p>返回所有(一个或多个)给定 <code>key</code> 的值。<br>如果给定的 <code>key</code> 里面，<strong>有某个 <code>key</code> 不存在，那么这个 <code>key</code> 返回特殊值 <code>nil</code></strong>。因此，该命令永不失败。</p></blockquote><p><strong>可接受的语法</strong></p><pre><code class="hljs sh">MGET KEY1 KEY2 KEY3 ...</code></pre><p><strong>代码示例</strong></p><pre><code class="hljs sh">localhost:6379&gt; MSET a 10 b 11 c 12OKlocalhost:6379&gt; MGET a b c d              <span class="hljs-comment"># d 不存在 则返回 nil</span>1) <span class="hljs-string">"10"</span>2) <span class="hljs-string">"11"</span>3) <span class="hljs-string">"12"</span>4) (nil)localhost:6379&gt;</code></pre><h4 id="SETRANGE"><a href="#SETRANGE" class="headerlink" title="SETRANGE"></a>SETRANGE</h4><blockquote><p>用 <code>value</code> 参数覆写(overwrite)给定 <code>key</code> 所储存的字符串值，从偏移量 <code>offset</code> 开始。不存在的 <code>key</code> 当作空白字符串处理。<strong>可以理解为对指定位置字符串覆盖</strong><br><code>SETRANGE</code> 命令会确保字符串足够长以便将 <code>value</code> 设置在指定的偏移量上，<strong>如果给定 <code>key</code> 原来储存的字符串长度比偏移量小(比如字符串只有5个字符长，但你设置的 <code>offset</code> 是10)，那么原字符和偏移量之间的空白将用零字节<code>(zerobytes,&quot;\x00&quot;)</code>来填充。</strong><br>注意你能使用的最大偏移量是 2^29-1(536870911) ，因为 Redis 字符串的大小被限制在 512 兆(megabytes)以内。如果你需要使用比这更大的空间，你可以使用多个 key 。</p><p>当生成一个很长的字符串时，Redis 需要分配内存空间，该操作有时候可能会造成服务器阻塞(block)。在2010年的Macbook Pro上，设置偏移量为 536870911(512MB 内存分配)，耗费约 300 毫秒，设置偏移量为 134217728(128MB 内存分配)，耗费约 80 毫秒，设置偏移量 33554432(32MB 内存分配)，耗费约 30 毫秒，设置偏移量为 8388608(8MB 内存分配)，耗费约 8 毫秒。<strong>注意若首次内存分配成功之后，再对同一个 key 调用 SETRANGE 操作，无须再重新内存。</strong></p></blockquote><p><strong>可接受的语法</strong></p><pre><code class="hljs sh">SETRANGE KEY STR_VALUE</code></pre><p><strong>代码示例</strong></p><pre><code class="hljs sh">localhost:6379&gt; SET a <span class="hljs-string">"123456789"</span>OKlocalhost:6379&gt; SETRANGE a 5 <span class="hljs-string">"XX"</span>(<span class="hljs-built_in">integer</span>) 9localhost:6379&gt; GET a<span class="hljs-string">"12345XX89"</span>localhost:6379&gt; SETRANGE a 5 <span class="hljs-string">"XXXXXXXXXXXXXX"</span>     <span class="hljs-comment"># 超过原有长度自动拼接</span>(<span class="hljs-built_in">integer</span>) 19localhost:6379&gt; GET a<span class="hljs-string">"12345XXXXXXXXXXXXXX"</span>localhost:6379&gt; SETRANGE a 7 <span class="hljs-string">"XXXX"</span>               <span class="hljs-comment"># 不够的话自动用 \x00 占位</span>(<span class="hljs-built_in">integer</span>) 11localhost:6379&gt; GET a<span class="hljs-string">"123\x00\x00\x00\x00XXXX"</span></code></pre><h4 id="GETRANGE"><a href="#GETRANGE" class="headerlink" title="GETRANGE"></a>GETRANGE</h4><blockquote><p>返回 <code>key</code> 中字符串值的子字符串，字符串的截取范围由 <code>start</code> 和 <code>end</code> 两个偏移量决定(包括 start 和 end 在内)。负数偏移量表示从字符串最后开始计数， -1 表示最后一个字符， -2 表示倒数第二个，以此类推。<code>GETRANGE</code> 通过保证子字符串的值域(range)不超过实际字符串的值域来处理超出范围的值域请求。</p><p><strong>在 &lt;= 2.0 的版本里，GETRANGE 被叫作 SUBSTR。</strong></p></blockquote><p><strong>可接受的语法</strong></p><pre><code class="hljs sh">GETRANGE KEY START_INDEX END_INDEX</code></pre><p><strong>代码示例</strong></p><pre><code class="hljs sh">localhost:6379&gt; SET a <span class="hljs-string">"test GETRANGE"</span>OKlocalhost:6379&gt; GETRANGE a 0 3          <span class="hljs-comment"># 获取0~3位置字符</span><span class="hljs-string">"test"</span>localhost:6379&gt; GETRANGE a 0 100        <span class="hljs-comment"># 超过长度则表示获取全部</span><span class="hljs-string">"test GETRANGE"</span>localhost:6379&gt; GETRANGE a -3 -1        <span class="hljs-comment"># 获取只能从左到右获取</span><span class="hljs-string">"NGE"</span>localhost:6379&gt; GETRANGE a -1 -3        <span class="hljs-comment"># 回绕操作将返回空串</span><span class="hljs-string">""</span>localhost:6379&gt; GETRANGE a 0 -1         <span class="hljs-comment"># 正确的获取全部字符串</span><span class="hljs-string">"test GETRANGE"</span>localhost:6379&gt;</code></pre><h4 id="APPEND"><a href="#APPEND" class="headerlink" title="APPEND"></a>APPEND</h4><blockquote><p>如果 <code>key</code> 已经存在并且是一个字符串， <code>APPEND</code> 命令将 <code>value</code> 追加到 <code>key</code> 原来的值的末尾。<strong>如果 <code>key</code> 不存在，<code>APPEND</code> 就简单地将给定 key 设为 <code>value</code> ，就像执行 <code>SET key value</code> 一样。</strong></p></blockquote><p><strong>可接受的语法</strong></p><pre><code class="hljs sh">APPEND KEY STRING_VALUE</code></pre><p><strong>代码示例</strong></p><pre><code class="hljs sh">localhost:6379&gt; EXISTS b                      <span class="hljs-comment"># b 不存在</span>(<span class="hljs-built_in">integer</span>) 0localhost:6379&gt; APPEND b <span class="hljs-string">"test not exists"</span>    <span class="hljs-comment"># 此时进行 APPEND 操作等同于 SET</span>(<span class="hljs-built_in">integer</span>) 15localhost:6379&gt; GET b                         <span class="hljs-string">"test not exists"</span>localhost:6379&gt; APPEND b <span class="hljs-string">"XXXXXXX"</span>            <span class="hljs-comment"># 拼接字符串</span>(<span class="hljs-built_in">integer</span>) 22localhost:6379&gt; GET b<span class="hljs-string">"test not existsXXXXXXX"</span>localhost:6379&gt;</code></pre><h3 id="List类型"><a href="#List类型" class="headerlink" title="List类型"></a>List类型</h3><blockquote><p>List类型是按照插入顺序排序的字符串链表。和普通链表一样，可以在其头部和尾部添加新的元素。<strong>插入时，如果该键不存在，将为该键创建一个新的链表。与此相反，如果链表中所有的元素均被移除，那么该键也将会被从数据库中删除</strong>。List中可以包含的<strong>最大元素数量是4294967295</strong>。</p><p>从元素插入和删除的效率来看，<strong>如果我们是在链表的两头插入或删除元素，这将会是非常高效的操作</strong>，即使链表中已经存储了百万条记录，该操作也可以在常量时间内完成。<strong>如果元素插入或删除操作是作用于链表中间，那将会是非常低效的</strong>。</p></blockquote><h4 id="RPUSH-LPUSH"><a href="#RPUSH-LPUSH" class="headerlink" title="RPUSH/LPUSH"></a>RPUSH/LPUSH</h4><blockquote><p>将一个或多个值放入 <code>List</code>中，<code>RPUSH</code> 新加入的元素将放在 <code>List</code> 链表尾部(右)，<code>LPUSH</code> 同理放在左边，<strong>如果对应的 <code>List</code> 的 <code>key</code> 不存在，将自动创建一个 <code>List</code>，并将值放入 <code>List</code> 中</strong>；<strong>当 <code>key</code> 存在，且不是 <code>List</code> 类型时，将返回错误</strong>。</p></blockquote><p><strong>可接受的语法</strong></p><pre><code class="hljs sh">RPUSH/LPUSH KEY VALUE1 VALUE2 VALUE3    <span class="hljs-comment"># 同时放入多个值</span>RPUSH/LPUSH KEY VALUE                   <span class="hljs-comment"># 每次只放入一个值</span></code></pre><p><strong>代码示例</strong></p><pre><code class="hljs sh">localhost:6379&gt; RPUSH list1 1 2 3 4(<span class="hljs-built_in">integer</span>) 4localhost:6379&gt; LRANGE list1 0 -1       <span class="hljs-comment"># 获取List所有值</span>1) <span class="hljs-string">"1"</span>2) <span class="hljs-string">"2"</span>3) <span class="hljs-string">"3"</span>4) <span class="hljs-string">"4"</span>localhost:6379&gt;</code></pre><h4 id="LLEN"><a href="#LLEN" class="headerlink" title="LLEN"></a>LLEN</h4><blockquote><p>返回指定 <code>List</code> 的长度，<strong>如果给定的 <code>KEY</code> 不存在则返回0，如果给定的 <code>KEY</code> 不是 <code>List</code> 类型将返回错误</strong>。</p></blockquote><p><strong>可接受的语法</strong></p><pre><code class="hljs sh">LLEN KEY</code></pre><p><strong>代码示例</strong></p><pre><code class="hljs sh">localhost:6379&gt; LLEN list1(<span class="hljs-built_in">integer</span>) 4localhost:6379&gt; LLEN L(<span class="hljs-built_in">integer</span>) 0localhost:6379&gt;</code></pre><h4 id="LRANGE"><a href="#LRANGE" class="headerlink" title="LRANGE"></a>LRANGE</h4><blockquote><p>返回指定返回的 <code>List</code> 的值，<strong>如果开始值大于 <code>List</code> 的长度，那么将返回空列表，如果结束值大于 <code>List</code> 长度，那么将示结束值为 <code>List</code> 最大长度</strong>。</p></blockquote><p><strong>可接受的语法</strong></p><pre><code class="hljs sh">LRANGE KEY START END</code></pre><p><strong>代码示例</strong></p><pre><code class="hljs sh">localhost:6379&gt; LRANGE list1 0 31) <span class="hljs-string">"1"</span>2) <span class="hljs-string">"2"</span>3) <span class="hljs-string">"3"</span>4) <span class="hljs-string">"4"</span>localhost:6379&gt; LRANGE list1 10 11(empty list or <span class="hljs-built_in">set</span>)localhost:6379&gt; LRANGE list1 0 10001) <span class="hljs-string">"1"</span>2) <span class="hljs-string">"2"</span>3) <span class="hljs-string">"3"</span>4) <span class="hljs-string">"4"</span></code></pre><h4 id="LTRIM"><a href="#LTRIM" class="headerlink" title="LTRIM"></a>LTRIM</h4><blockquote><p>截取列表指定位置的元素，<strong>当Start值大于 <code>List</code> 长度时，将返回空列表；当End值大于 <code>List</code> 长度时，示End值为 <code>List</code> 长度</strong>。</p></blockquote><p><strong>可接受的语法</strong></p><pre><code class="hljs sh">LTRIM KEY START END</code></pre><p><strong>代码示例</strong></p><pre><code class="hljs sh">localhost:6379&gt; RPUSH list 1 2 3 4 5 6 7 8 9 10(<span class="hljs-built_in">integer</span>) 10localhost:6379&gt; LTRIM list 1 9OKlocalhost:6379&gt; LRANGE list 0 -11) <span class="hljs-string">"2"</span>2) <span class="hljs-string">"3"</span>3) <span class="hljs-string">"4"</span>4) <span class="hljs-string">"5"</span>5) <span class="hljs-string">"6"</span>6) <span class="hljs-string">"7"</span>7) <span class="hljs-string">"8"</span>8) <span class="hljs-string">"9"</span>9) <span class="hljs-string">"10"</span>localhost:6379&gt; LTRIM list 5 100OKlocalhost:6379&gt; LRANGE list 0 -11) <span class="hljs-string">"7"</span>2) <span class="hljs-string">"8"</span>3) <span class="hljs-string">"9"</span>4) <span class="hljs-string">"10"</span>localhost:6379&gt; LTRIM list 50 100OKlocalhost:6379&gt; LRANGE list 0 -1(empty list or <span class="hljs-built_in">set</span>)localhost:6379&gt;</code></pre><h4 id="LINDEX"><a href="#LINDEX" class="headerlink" title="LINDEX"></a>LINDEX</h4><blockquote><p>获取指定位置的值，<strong>超出位置(不在 <code>List</code> 范围内)返回nil</strong>。</p></blockquote><p><strong>可接受的语法</strong></p><pre><code class="hljs sh">LINDEX KEY INDEX</code></pre><p><strong>代码示例</strong></p><pre><code class="hljs sh">localhost:6379&gt; RPUSH list 1 2 3 4 5 6 7 8 9(<span class="hljs-built_in">integer</span>) 9localhost:6379&gt; LINDEX list 3<span class="hljs-string">"4"</span>localhost:6379&gt; LINDEX list 100(nil)localhost:6379&gt;</code></pre><h4 id="LREM"><a href="#LREM" class="headerlink" title="LREM"></a>LREM</h4><blockquote><p>移除列表中与指定值相等的元素，</p><p><strong>在最新版本 3.0.6测试，无论下面的 count等于多少，都会移除所有目标值，只是搜索方向问题，尚不清楚原因</strong></p></blockquote><ul><li>count &gt; 0 : 从表头开始向表尾搜索，移除与 value 相等的元素，数量为 count 。</li><li>count &lt; 0 : 从表尾开始向表头搜索，移除与 value 相等的元素，数量为 count 的绝对值。</li><li>count = 0 : 移除表中所有与 value 相等的值。</li></ul><p><strong>可接受的语法</strong></p><pre><code class="hljs sh">LREM KEY COUNT VALUE</code></pre><p><strong>代码示例</strong></p><pre><code class="hljs sh">localhost:6379&gt; RPUSH list 0 0 xxx 0 xxx xxx 0 0 0 xxx 0(<span class="hljs-built_in">integer</span>) 11localhost:6379&gt; LRANGE list 0 -1 1) <span class="hljs-string">"0"</span> 2) <span class="hljs-string">"0"</span> 3) <span class="hljs-string">"xxx"</span> 4) <span class="hljs-string">"0"</span> 5) <span class="hljs-string">"xxx"</span> 6) <span class="hljs-string">"xxx"</span> 7) <span class="hljs-string">"0"</span> 8) <span class="hljs-string">"0"</span> 9) <span class="hljs-string">"0"</span>10) <span class="hljs-string">"xxx"</span>11) <span class="hljs-string">"0"</span>localhost:6379&gt; LREM list 5 xxx(<span class="hljs-built_in">integer</span>) 4localhost:6379&gt; LRANGE list 0 -11) <span class="hljs-string">"0"</span>2) <span class="hljs-string">"0"</span>3) <span class="hljs-string">"0"</span>4) <span class="hljs-string">"0"</span>5) <span class="hljs-string">"0"</span>6) <span class="hljs-string">"0"</span>7) <span class="hljs-string">"0"</span>localhost:6379&gt; del list(<span class="hljs-built_in">integer</span>) 1localhost:6379&gt; RPUSH list 0 0 xxx 0 xxx xxx 0 0 0 xxx 0(<span class="hljs-built_in">integer</span>) 11localhost:6379&gt; LRANGE list 0 -1 1) <span class="hljs-string">"0"</span> 2) <span class="hljs-string">"0"</span> 3) <span class="hljs-string">"xxx"</span> 4) <span class="hljs-string">"0"</span> 5) <span class="hljs-string">"xxx"</span> 6) <span class="hljs-string">"xxx"</span> 7) <span class="hljs-string">"0"</span> 8) <span class="hljs-string">"0"</span> 9) <span class="hljs-string">"0"</span>10) <span class="hljs-string">"xxx"</span>11) <span class="hljs-string">"0"</span>localhost:6379&gt; LREM list 6 <span class="hljs-string">"xxx"</span>(<span class="hljs-built_in">integer</span>) 4localhost:6379&gt; LRANGE list 0 -11) <span class="hljs-string">"0"</span>2) <span class="hljs-string">"0"</span>3) <span class="hljs-string">"0"</span>4) <span class="hljs-string">"0"</span>5) <span class="hljs-string">"0"</span>6) <span class="hljs-string">"0"</span>7) <span class="hljs-string">"0"</span>localhost:6379&gt; del list(<span class="hljs-built_in">integer</span>) 1localhost:6379&gt; RPUSH list 0 0 xxx 0 xxx xxx 0 0 0 xxx 0(<span class="hljs-built_in">integer</span>) 11localhost:6379&gt; LREM list -6 <span class="hljs-string">"xxx"</span>(<span class="hljs-built_in">integer</span>) 4localhost:6379&gt; LRANGE list 0 -11) <span class="hljs-string">"0"</span>2) <span class="hljs-string">"0"</span>3) <span class="hljs-string">"0"</span>4) <span class="hljs-string">"0"</span>5) <span class="hljs-string">"0"</span>6) <span class="hljs-string">"0"</span>7) <span class="hljs-string">"0"</span>localhost:6379&gt; del list(<span class="hljs-built_in">integer</span>) 1localhost:6379&gt; RPUSH list 0 0 xxx 0 xxx xxx 0 0 0 xxx 0(<span class="hljs-built_in">integer</span>) 11localhost:6379&gt; LREM list 0 <span class="hljs-string">"xxx"</span>(<span class="hljs-built_in">integer</span>) 4localhost:6379&gt; del list(<span class="hljs-built_in">integer</span>) 1localhost:6379&gt; RPUSH list 0 0 xxx 0 xxx xxx 0 0 0 xxx 0(<span class="hljs-built_in">integer</span>) 11localhost:6379&gt;</code></pre><h4 id="LSET"><a href="#LSET" class="headerlink" title="LSET"></a>LSET</h4><blockquote><p>向指定位置设置值，<strong>列表 <code>KEY</code> 不存在将返回错误，坐标不对返回对应错误信息</strong>。</p></blockquote><p><strong>可接受的语法</strong></p><pre><code class="hljs sh">LSET KEY INDEX VALUE</code></pre><p><strong>代码示例</strong></p><pre><code class="hljs sh">localhost:6379&gt; RPUSH list 1 1 1 1 1(<span class="hljs-built_in">integer</span>) 5localhost:6379&gt; LSET list 2 xxxxOKlocalhost:6379&gt; LRANGE list 0 -11) <span class="hljs-string">"1"</span>2) <span class="hljs-string">"1"</span>3) <span class="hljs-string">"xxxx"</span>4) <span class="hljs-string">"1"</span>5) <span class="hljs-string">"1"</span>localhost:6379&gt;</code></pre><h4 id="RPOPLPUSH-LPOPRPUSH"><a href="#RPOPLPUSH-LPOPRPUSH" class="headerlink" title="RPOPLPUSH/LPOPRPUSH"></a>RPOPLPUSH/LPOPRPUSH</h4><blockquote><p>简单地说用于倒腾两个链表，<strong>RPOPLPUSH 将 A 链表最右侧一个元素删除并返回，同时将其放入到 B 链表最左侧</strong>，LPOPRPUSH 同理；<strong>测试最新版本 3.0.6 已经移除</strong>。</p></blockquote><p><strong>可接受的语法</strong></p><pre><code class="hljs sh">LPOPRPUSH/RPOPLPUSH KEY1 SOURCE KEY2</code></pre><h3 id="SET类型"><a href="#SET类型" class="headerlink" title="SET类型"></a>SET类型</h3><blockquote><p><code>SET</code> 是一个无序的字符串合集，可以以O(1) 的时间复杂度完成 添加，删除以及测试元素是否存在的操作；<code>SET</code> 中不允许相同成员存在，<strong>向集合中多次添加同一元素，在集合中最终只会存在一个此元素。</strong>它支持一些服务端的命令进行集合运算。 我们可以在很短的时间内完成合并(<code>union</code>),求交(<code>intersection</code>)， 找出不同元素的操作。</p></blockquote><h4 id="SADD"><a href="#SADD" class="headerlink" title="SADD"></a>SADD</h4><blockquote><p>将一个或多个 <code>member(value)</code> 元素加入到集合 <code>key</code> 当中，已经存在于集合的 <code>member</code> 元素将被忽略。假如 <code>key</code> 不存在，则创建一个只包含 <code>member</code> 元素作成员的集合。当 <code>key</code> 不是集合类型时，返回一个错误。</p><p><strong>在Redis2.4版本以前， SADD 只接受单个 member 值。</strong></p></blockquote><p><strong>可接受的语法</strong></p><pre><code class="hljs sh">SADD KEY MEMBER [MEMBER...]</code></pre><p><strong>代码示例</strong></p><pre><code class="hljs sh">localhost:6379&gt; SADD set1 1 2 3 4(<span class="hljs-built_in">integer</span>) 4localhost:6379&gt; SMEMBERS set11) <span class="hljs-string">"1"</span>2) <span class="hljs-string">"2"</span>3) <span class="hljs-string">"3"</span>4) <span class="hljs-string">"4"</span>localhost:6379&gt;</code></pre><h4 id="SREM"><a href="#SREM" class="headerlink" title="SREM"></a>SREM</h4><blockquote><p>移除集合 <code>key</code> 中的一个或多个 <code>member</code> 元素，不存在的 <code>member</code> 元素会被忽略。当 <code>key</code> 不是集合类型，返回一个错误。</p><p><strong>在 Redis 2.4 版本以前， SREM 只接受单个 member 值。</strong></p></blockquote><p><strong>可接受的语法</strong></p><pre><code class="hljs sh">SREM KEY MEMBER [MEMBER...]</code></pre><p><strong>代码示例</strong></p><pre><code class="hljs sh">localhost:6379&gt; SREM set1 1 2(<span class="hljs-built_in">integer</span>) 2localhost:6379&gt; SMEMBERS set11) <span class="hljs-string">"3"</span>2) <span class="hljs-string">"4"</span>localhost:6379&gt;</code></pre><h4 id="SPOP"><a href="#SPOP" class="headerlink" title="SPOP"></a>SPOP</h4><blockquote><p><strong>移除并返回集合中的一个随机元素。</strong>如果只想获取一个随机元素，但不想该元素从集合中被移除的话，可以使用 <code>SRANDMEMBER</code> 命令。被移除的随机元素当 <code>key</code> 不存在或 <code>key</code> 是空集时，返回 <code>nil</code> 。</p></blockquote><p><strong>可接受的语法</strong></p><pre><code class="hljs sh">SPOP KEY</code></pre><p><strong>代码示例</strong></p><pre><code class="hljs sh">localhost:6379&gt; SPOP set1<span class="hljs-string">"3"</span>localhost:6379&gt; SMEMBERS set11) <span class="hljs-string">"4"</span>localhost:6379&gt; SPOP set2(nil)localhost:6379&gt;</code></pre><h4 id="SMOVE"><a href="#SMOVE" class="headerlink" title="SMOVE"></a>SMOVE</h4><blockquote><p>将 <code>member</code> 元素从 <code>source</code> 集合移动到 <code>destination</code> 集合；<strong><code>SMOVE</code> 是原子性操作。</strong><br>如果 <code>source</code> 集合不存在或不包含指定的 <code>member</code> 元素，则 <code>SMOVE</code> 命令不执行任何操作，仅返回 0 。否则， <code>member</code> 元素从 <code>source</code> 集合中被移除，并添加到 <code>destination</code> 集合中去。当 <code>destination</code> 集合已经包含 <code>member</code> 元素时， <code>SMOVE</code> 命令只是简单地将 <code>source</code> 集合中的 <code>member</code> 元素删除。当 <code>source</code> 或 <code>destination</code> 不是集合类型时，返回一个错误。</p></blockquote><p><strong>可接受的语法</strong></p><pre><code class="hljs sh">SMOVE SOURCE DESTINATION MEMBER</code></pre><p><strong>代码示例</strong></p><pre><code class="hljs sh">localhost:6379&gt; SADD set1 1 2 3 4(<span class="hljs-built_in">integer</span>) 4localhost:6379&gt; SADD set2 5 6 7 8(<span class="hljs-built_in">integer</span>) 4localhost:6379&gt; SMOVE set1 set2 4(<span class="hljs-built_in">integer</span>) 1localhost:6379&gt; SMEMBERS set21) <span class="hljs-string">"4"</span>2) <span class="hljs-string">"5"</span>3) <span class="hljs-string">"6"</span>4) <span class="hljs-string">"7"</span>5) <span class="hljs-string">"8"</span>localhost:6379&gt; SMEMBERS set11) <span class="hljs-string">"1"</span>2) <span class="hljs-string">"2"</span>3) <span class="hljs-string">"3"</span>localhost:6379&gt;</code></pre><h4 id="SCARD"><a href="#SCARD" class="headerlink" title="SCARD"></a>SCARD</h4><blockquote><p>返回指定结合的元素数量</p></blockquote><p><strong>可接受的语法</strong></p><pre><code class="hljs sh">SCARD KEY</code></pre><p><strong>代码示例</strong></p><pre><code class="hljs sh">localhost:6379&gt; SCARD set2(<span class="hljs-built_in">integer</span>) 5localhost:6379&gt;</code></pre><h4 id="SISMEMBER"><a href="#SISMEMBER" class="headerlink" title="SISMEMBER"></a>SISMEMBER</h4><blockquote><p>判断指定元素是否存在与集合中；如果 <code>member</code> 元素是集合的成员，返回 1 。如果 <code>member</code> 元素不是集合的成员，或 <code>key</code> 不存在，返回 0 。</p></blockquote><p><strong>可接受的语法</strong></p><pre><code class="hljs sh">SISMEMBER KEY</code></pre><p><strong>代码示例</strong></p><pre><code class="hljs sh">localhost:6379&gt; SISMEMBER set2 5(<span class="hljs-built_in">integer</span>) 1localhost:6379&gt; SISMEMBER set2 999(<span class="hljs-built_in">integer</span>) 0localhost:6379&gt;</code></pre><h4 id="SINTER"><a href="#SINTER" class="headerlink" title="SINTER"></a>SINTER</h4><blockquote><p>返回多个集合的交集，<strong>不存在的KEY将视为空集，与空集进行取交集操作将返回空集。</strong></p></blockquote><p><strong>可接受的语法</strong></p><pre><code class="hljs sh">SINTER KEY1 KEY2 ...</code></pre><p><strong>代码示例</strong></p><pre><code class="hljs sh">localhost:6379&gt;localhost:6379&gt; SADD set1 1 2 3(<span class="hljs-built_in">integer</span>) 3localhost:6379&gt; SADD set2 2 3 4(<span class="hljs-built_in">integer</span>) 3localhost:6379&gt; SINTER set1 set21) <span class="hljs-string">"2"</span>2) <span class="hljs-string">"3"</span>localhost:6379&gt; SINTER set1 set44444(empty list or <span class="hljs-built_in">set</span>)localhost:6379&gt; SADD set3 2 5 6(<span class="hljs-built_in">integer</span>) 3localhost:6379&gt; SINTER set1 set2 set31) <span class="hljs-string">"2"</span>localhost:6379&gt;</code></pre><h4 id="SINTERSTORE"><a href="#SINTERSTORE" class="headerlink" title="SINTERSTORE"></a>SINTERSTORE</h4><blockquote><p>基本同上，但是 <code>SINTERSTORE</code>可以将取出的交集保存到指定集合中。</p></blockquote><p><strong>可接受的语法</strong></p><pre><code class="hljs sh">SINTERSTORE DESTINATION SOURCE1 SOURCE2 ...</code></pre><p><strong>代码示例</strong></p><pre><code class="hljs sh">localhost:6379&gt; SADD SET1 1 2 3(<span class="hljs-built_in">integer</span>) 3localhost:6379&gt; SADD SET2 2 3 4(<span class="hljs-built_in">integer</span>) 3localhost:6379&gt; SADD SET3 3 4 5(<span class="hljs-built_in">integer</span>) 3localhost:6379&gt; SINTERSTORE SET4 SET1 SET2 SET3(<span class="hljs-built_in">integer</span>) 1localhost:6379&gt; SMEMBERS SET41) <span class="hljs-string">"3"</span>localhost:6379&gt;</code></pre><h4 id="SUNION"><a href="#SUNION" class="headerlink" title="SUNION"></a>SUNION</h4><blockquote><p>取指定集合的并集</p></blockquote><p><strong>可接受的语法</strong></p><pre><code class="hljs sh">SUNION KEY1 KEY2 ...</code></pre><p><strong>代码示例</strong></p><pre><code class="hljs sh">localhost:6379&gt; SADD SET1 1 2 3(<span class="hljs-built_in">integer</span>) 3localhost:6379&gt; SADD SET2 2 3 4(<span class="hljs-built_in">integer</span>) 3localhost:6379&gt; SADD SET3 3 4 5(<span class="hljs-built_in">integer</span>) 3localhost:6379&gt; SUNION SET1 SET2 SET31) <span class="hljs-string">"1"</span>2) <span class="hljs-string">"2"</span>3) <span class="hljs-string">"3"</span>4) <span class="hljs-string">"4"</span>5) <span class="hljs-string">"5"</span>localhost:6379&gt;</code></pre><h4 id="SUNIONSTORE"><a href="#SUNIONSTORE" class="headerlink" title="SUNIONSTORE"></a>SUNIONSTORE</h4><blockquote><p>同 <code>SINTERSTORE</code>；不做阐述</p></blockquote><h4 id="SDIFF"><a href="#SDIFF" class="headerlink" title="SDIFF"></a>SDIFF</h4><blockquote><p>取指定集合的差集，同 <code>SINTER</code>、<code>SUNION</code></p></blockquote><p><strong>可接受的语法</strong></p><pre><code class="hljs sh">SDIFF KEY1 KEY2 ...</code></pre><p><strong>代码示例</strong></p><pre><code class="hljs sh">localhost:6379&gt; SADD SET1 1 2 3(<span class="hljs-built_in">integer</span>) 3localhost:6379&gt; SADD SET2 2 3 4(<span class="hljs-built_in">integer</span>) 3localhost:6379&gt; SADD SET3 3 4 5(<span class="hljs-built_in">integer</span>) 3localhost:6379&gt; SDIFF SET1 SET2 SET31) <span class="hljs-string">"1"</span>localhost:6379&gt;</code></pre><blockquote><p><strong>对于集合的 合集、差集、并集操作，中文API的解释是 <code>返回一个集合的全部成员</code>，我的理解是，以第一个集合为准，返回的总是这个结合依次与后续集合运算的结果，并忽略后面集合的元素；具体还是不算太理解……以后研究</strong></p></blockquote><h3 id="ZSET"><a href="#ZSET" class="headerlink" title="ZSET"></a>ZSET</h3><blockquote><p>和Set类型极为相似，它们都是字符串的集合，都不允许重复的成员出现在一个Set中。它们之间的主要差别是zset中的每一个成员都会有一个分数(score)与之关联，Redis正是通过分数来为集合中的成员进行从小到大的排序。然而需要额外指出的是，尽管zset中的成员必须是唯一的，但是分数(score)却是可以重复的。<br>在zset中添加、删除或更新一个成员都是非常快速的操作，其时间复杂度为集合中成员数量的对数。由于zset中的成员在集合中的位置是有序的，因此，即便是访问位于集合中部的成员也仍然是非常高效的。事实上，Redis所具有的这一特征在很多其它类型的数据库中是很难实现的，换句话说，在该点上要想达到和Redis同样的高效，在其它数据库中进行建模是非常困难的。</p></blockquote><h4 id="ZADD"><a href="#ZADD" class="headerlink" title="ZADD"></a>ZADD</h4><blockquote><p>将一个或多个 <code>member</code> 元素及其 <code>score</code> 值加入到有序集 key 当中。如果某个 <code>member</code> 已经是有序集的成员，那么更新这个 <code>member</code> 的 <code>score</code> 值，并通过重新插入这个 <code>member</code> 元素，来保证该 <code>member</code> 在正确的位置上，<strong>同时返回0</strong>。<code>score</code> 值可以是整数值或双精度浮点数。如果 <code>key</code> 不存在，则创建一个空的有序集并执行 <code>ZADD</code> 操作。当 <code>key</code> 存在但不是有序集类型时，返回一个错误。</p></blockquote><p><strong>可接受的语法</strong></p><pre><code class="hljs sh">ZADD KEY SCORE VALUE [SCORE VALUE ...]</code></pre><p><strong>代码示例</strong></p><pre><code class="hljs sh">localhost:6379&gt; ZADD ZSET1 1 <span class="hljs-string">"TEST1"</span>(<span class="hljs-built_in">integer</span>) 1localhost:6379&gt; ZADD ZSET1 2 <span class="hljs-string">"TEST2"</span> 3 <span class="hljs-string">"TEST3"</span>(<span class="hljs-built_in">integer</span>) 2localhost:6379&gt; ZRANGE ZSET1 0 -1 WITHSCORES1) <span class="hljs-string">"TEST1"</span>2) <span class="hljs-string">"1"</span>3) <span class="hljs-string">"TEST2"</span>4) <span class="hljs-string">"2"</span>5) <span class="hljs-string">"TEST3"</span>6) <span class="hljs-string">"3"</span>localhost:6379&gt; ZADD ZSET1 1 <span class="hljs-string">"TEST10000"</span>(<span class="hljs-built_in">integer</span>) 1localhost:6379&gt; ZADD ZSET1 1 <span class="hljs-string">"TEST10000"</span>(<span class="hljs-built_in">integer</span>) 0localhost:6379&gt;</code></pre><h4 id="ZCARD"><a href="#ZCARD" class="headerlink" title="ZCARD"></a>ZCARD</h4><blockquote><p>返回集合的长度</p></blockquote><p><strong>可接受的语法</strong></p><pre><code class="hljs sh">ZCARD KEY</code></pre><p><strong>代码示例</strong></p><pre><code class="hljs sh">localhost:6379&gt; ZCARD ZSET1(<span class="hljs-built_in">integer</span>) 4localhost:6379&gt;</code></pre><h4 id="ZCOUNT"><a href="#ZCOUNT" class="headerlink" title="ZCOUNT"></a>ZCOUNT</h4><blockquote><p>返回指定 <code>SCORE</code> 之间的元素数量</p></blockquote><p><strong>可接受的语法</strong></p><pre><code class="hljs sh">ZCOUNT KEY MIN MAX</code></pre><p><strong>代码示例</strong></p><pre><code class="hljs sh">localhost:6379&gt; ZCOUNT ZSET1 1 3(<span class="hljs-built_in">integer</span>) 4localhost:6379&gt;</code></pre><h4 id="ZSCORE"><a href="#ZSCORE" class="headerlink" title="ZSCORE"></a>ZSCORE</h4><blockquote><p>查看指定成员的 <code>SCORE</code></p></blockquote><p><strong>可接受的语法</strong></p><pre><code class="hljs sh">ZSCORE KEY VALUE</code></pre><p><strong>代码示例</strong></p><pre><code class="hljs sh">localhost:6379&gt; ZSCORE ZSET1 <span class="hljs-string">"TEST2"</span><span class="hljs-string">"2"</span>localhost:6379&gt;</code></pre><h4 id="ZINCRBY"><a href="#ZINCRBY" class="headerlink" title="ZINCRBY"></a>ZINCRBY</h4><blockquote><p>对集合中指定 <code>VALUE</code> 的 <code>SROCE</code> 加上指定值；当集合 <code>KEY</code> 不存在时，相当于 <code>ZADD</code>。</p></blockquote><p><strong>可接受的语法</strong></p><pre><code class="hljs sh">ZINCRBY KEY INCREMENT VALUE</code></pre><p><strong>代码示例</strong></p><pre><code class="hljs sh">localhost:6379&gt; ZINCRBY ZSET1 -1 <span class="hljs-string">"TEST3"</span><span class="hljs-string">"2"</span>localhost:6379&gt; ZSCORE ZSET1 <span class="hljs-string">"TEST3"</span><span class="hljs-string">"2"</span>localhost:6379&gt;</code></pre><h4 id="ZRANGE"><a href="#ZRANGE" class="headerlink" title="ZRANGE"></a>ZRANGE</h4><blockquote><p>返回集合内指定区间的元素，其中成员的位置按 score 值递增(从小到大)来排序。具有相同 <code>score</code> 值的成员按字典序( <code>lexicographical order</code> )来排列。如果需要成员按 <code>score</code> 值递减(从大到小)来排列，请使用 <code>ZREVRANGE</code> 命令。</p></blockquote><p><strong>可接受的语法</strong></p><pre><code class="hljs sh">ZRANGE KEY START STOP [WITHSCORES]</code></pre><p><strong>代码示例</strong></p><pre><code class="hljs sh">localhost:6379&gt; ZADD ZSET1 1 <span class="hljs-string">"TEST1"</span> 2 <span class="hljs-string">"TEST2"</span> 3 <span class="hljs-string">"TEST3"</span> 4 <span class="hljs-string">"TEST4"</span> 5 <span class="hljs-string">"TEST5"</span>(<span class="hljs-built_in">integer</span>) 5localhost:6379&gt; ZRANGE ZSET1 0 -11) <span class="hljs-string">"TEST1"</span>2) <span class="hljs-string">"TEST2"</span>3) <span class="hljs-string">"TEST3"</span>4) <span class="hljs-string">"TEST4"</span>5) <span class="hljs-string">"TEST5"</span>localhost:6379&gt;</code></pre><h4 id="ZREVRANGE"><a href="#ZREVRANGE" class="headerlink" title="ZREVRANGE"></a>ZREVRANGE</h4><blockquote><p>同 <code>ZRANGE</code> 只不过 <code>SCORE</code> 从大到小排列，略过</p></blockquote><h4 id="ZRANGEBYSCORE"><a href="#ZRANGEBYSCORE" class="headerlink" title="ZRANGEBYSCORE"></a>ZRANGEBYSCORE</h4><blockquote><p>基本同 <code>ZRANGE</code> 一样，但是返回分数在 <code>min</code> 和 <code>max</code> 之间的所有成员满足表达式 <code>min &lt;= score &lt;= max</code> 的成员，其中返回的成员是按照其分数从低到高的顺序返回，<strong>如果成员具有相同的分数，则按成员的字典顺序返回。</strong></p><p><code>(</code> 表示小于，默认 包含 <code>min</code> 和 <code>max</code>；极限 <code>min</code> 和 <code>max</code> 可以用 <code>-inf</code> 和 <code>+inf</code> 表示</p></blockquote><p><strong>可接受的语法</strong></p><pre><code class="hljs sh">ZRANGEBYSCORE KEY [(]MIN [(]MAX [WITHSCORES]</code></pre><p><strong>代码示例</strong></p><pre><code class="hljs sh">localhost:6379&gt; ZADD ZSET1 1 <span class="hljs-string">"TEST1"</span> 2 <span class="hljs-string">"TEST2"</span> 3 <span class="hljs-string">"TEST3"</span> 4 <span class="hljs-string">"TEST4"</span> 5 <span class="hljs-string">"TEST5"</span> 6 <span class="hljs-string">"TEST6"</span>(<span class="hljs-built_in">integer</span>) 6localhost:6379&gt; ZRANGEBYSCORE ZSET1 (2 5 WITHSCORES1) <span class="hljs-string">"TEST3"</span>2) <span class="hljs-string">"3"</span>3) <span class="hljs-string">"TEST4"</span>4) <span class="hljs-string">"4"</span>5) <span class="hljs-string">"TEST5"</span>6) <span class="hljs-string">"5"</span>localhost:6379&gt; ZRANGEBYSCORE ZSET1 2 +INF WITHSCORES 1) <span class="hljs-string">"TEST2"</span> 2) <span class="hljs-string">"2"</span> 3) <span class="hljs-string">"TEST3"</span> 4) <span class="hljs-string">"3"</span> 5) <span class="hljs-string">"TEST4"</span> 6) <span class="hljs-string">"4"</span> 7) <span class="hljs-string">"TEST5"</span> 8) <span class="hljs-string">"5"</span> 9) <span class="hljs-string">"TEST6"</span>10) <span class="hljs-string">"6"</span>localhost:6379&gt;</code></pre><h4 id="ZRANK"><a href="#ZRANK" class="headerlink" title="ZRANK"></a>ZRANK</h4><blockquote><p>返回指定成员位置</p></blockquote><p><strong>可接受的语法</strong></p><pre><code class="hljs sh">ZRANK KEY VALUE</code></pre><p><strong>代码示例</strong></p><pre><code class="hljs sh">localhost:6379&gt; ZRANK ZSET1 <span class="hljs-string">"TEST3"</span>(<span class="hljs-built_in">integer</span>) 2localhost:6379&gt;</code></pre><h3 id="ZREM"><a href="#ZREM" class="headerlink" title="ZREM"></a>ZREM</h3><blockquote><p>删除指定元素</p></blockquote><p><strong>可接受的语法</strong></p><pre><code class="hljs sh">ZREM KEY VALUE</code></pre><p><strong>代码示例</strong></p><pre><code class="hljs sh">localhost:6379&gt; ZREM ZSET1 <span class="hljs-string">"TEST5"</span>(<span class="hljs-built_in">integer</span>) 1localhost:6379&gt; ZRANGE ZSET1 1 -1 WITHSCORES1) <span class="hljs-string">"TEST2"</span>2) <span class="hljs-string">"2"</span>3) <span class="hljs-string">"TEST3"</span>4) <span class="hljs-string">"3"</span>5) <span class="hljs-string">"TEST4"</span>6) <span class="hljs-string">"4"</span>7) <span class="hljs-string">"TEST6"</span>8) <span class="hljs-string">"6"</span>localhost:6379&gt;</code></pre><h4 id="ZSCORE-1"><a href="#ZSCORE-1" class="headerlink" title="ZSCORE"></a>ZSCORE</h4><blockquote><p>获取指定元素的 分数，如果该成员存在，以字符串的形式返回其分数，否则返回nil。</p></blockquote><p><strong>可接受的语法</strong></p><pre><code class="hljs sh">ZSCORE KEY VALUE</code></pre><p><strong>代码示例</strong></p><pre><code class="hljs sh">localhost:6379&gt; ZSCORE ZSET1 <span class="hljs-string">"TEST2"</span><span class="hljs-string">"2"</span>localhost:6379&gt;</code></pre><h4 id="ZREVRANGEBYSCORE"><a href="#ZREVRANGEBYSCORE" class="headerlink" title="ZREVRANGEBYSCORE"></a>ZREVRANGEBYSCORE</h4><blockquote><p>除了排序方式是基于从高到低的分数排序之外，其它功能和参数含义均与 <code>ZRANGEBYSCORE</code> 相同。</p></blockquote><h4 id="ZREMRANGEBYRANK"><a href="#ZREMRANGEBYRANK" class="headerlink" title="ZREMRANGEBYRANK"></a>ZREMRANGEBYRANK</h4><blockquote><p>删除索引位置位于 <code>start</code> 和 <code>stop</code> 之间的成员，<code>start</code> 和 <code>stop</code> 都是 <code>0-based</code>，即 <code>0</code> 表示分数最低的成员，<code>-1</code> 表示最后一个成员，即分数最高的成员。</p></blockquote><h4 id="ZREMRANGEBYSCORE"><a href="#ZREMRANGEBYSCORE" class="headerlink" title="ZREMRANGEBYSCORE"></a>ZREMRANGEBYSCORE</h4><blockquote><p>删除分数在 <code>min</code> 和 <code>max</code> 之间的所有成员，即满足表达式 <code>min &lt;= score &lt;= max</code> 的所有成员。</p></blockquote><h3 id="Hashes类型"><a href="#Hashes类型" class="headerlink" title="Hashes类型"></a>Hashes类型</h3><blockquote><p>可以将 <code>Redis</code> 中的 <code>Hashes</code> 类型看成具有 <code>String Key</code> 和 <code>String Value</code> 的 <code>map</code> 容器。所以该类型非常适合于存储值对象的信息。如 <code>Username</code>、<code>Password</code> 和 <code>Age</code> 等。如果 <code>Hash</code> 中包含很少的字段，那么该类型的数据也将仅占用很少的磁盘空间。每一个 <code>Hash</code> 可以存储 <code>4294967295</code> 个键值对。</p></blockquote><h4 id="HSET"><a href="#HSET" class="headerlink" title="HSET"></a>HSET</h4><blockquote><p>将哈希表 key 中的域 field 的值设为 value 。如果 key 不存在，一个新的哈希表被创建并进行 HSET 操作。如果域 field 已经存在于哈希表中，旧值将被覆盖。</p></blockquote><p><strong>可接受的语法</strong></p><pre><code class="hljs sh">HSET KEY FIELD1 VALUE1</code></pre><p><strong>代码示例</strong></p><pre><code class="hljs sh">localhost:6379&gt; HSET USER1 NAME <span class="hljs-string">"ZAHNGSAN"</span>(<span class="hljs-built_in">integer</span>) 1</code></pre><h4 id="HSETNX"><a href="#HSETNX" class="headerlink" title="HSETNX"></a>HSETNX</h4><blockquote><p>将哈希表 key 中的域 field 的值设置为 value ，当且仅当域 field 不存在。<strong>若域 field 已经存在，该操作无效。</strong>如果 key 不存在，一个新哈希表被创建并执行 HSETNX 命令。</p></blockquote><p><strong>可接受的语法</strong></p><pre><code class="hljs sh">HSETNX KEY FIELD VALUE</code></pre><p><strong>代码示例</strong></p><pre><code class="hljs sh">localhost:6379&gt; HSETNX USER1 AGE 10(<span class="hljs-built_in">integer</span>) 1localhost:6379&gt; HGET USER1 AGE<span class="hljs-string">"10"</span>localhost:6379&gt; HSETNX USER1 NAME <span class="hljs-string">"LISI"</span>(<span class="hljs-built_in">integer</span>) 0localhost:6379&gt; HGET USER1 NAME<span class="hljs-string">"ZAHNGSAN"</span>localhost:6379&gt;</code></pre><h4 id="HGET"><a href="#HGET" class="headerlink" title="HGET"></a>HGET</h4><blockquote><p>返回 key 中指定 field 的 value</p></blockquote><p><strong>可接受的语法</strong></p><pre><code class="hljs sh">HGET KEY FIELD</code></pre><p><strong>代码示例</strong></p><pre><code class="hljs sh">localhost:6379&gt; HGET USER1 NAME<span class="hljs-string">"ZAHNGSAN"</span>localhost:6379&gt;</code></pre><h4 id="HGETALL"><a href="#HGETALL" class="headerlink" title="HGETALL"></a>HGETALL</h4><blockquote><p>获取指定 key 中全部 field 和 value</p></blockquote><p><strong>可接受的语法</strong></p><pre><code class="hljs sh">HGETALL KEY</code></pre><p><strong>代码示例</strong></p><pre><code class="hljs sh">localhost:6379&gt; HGETALL USER11) <span class="hljs-string">"NAME"</span>2) <span class="hljs-string">"ZAHNGSAN"</span>3) <span class="hljs-string">"AGE"</span>4) <span class="hljs-string">"10"</span>localhost:6379&gt;</code></pre><h4 id="HEXISTS"><a href="#HEXISTS" class="headerlink" title="HEXISTS"></a>HEXISTS</h4><blockquote><p>判断指定的 key 中的 field 是否存在，1表示存在，0表示 <strong>参数中的Field或Key不存在。</strong></p></blockquote><p><strong>可接受的语法</strong></p><pre><code class="hljs sh">HEXISTS KEY FIELD</code></pre><p><strong>代码示例</strong></p><pre><code class="hljs sh">localhost:6379&gt; HEXISTS USER1 NAME(<span class="hljs-built_in">integer</span>) 1localhost:6379&gt; HEXISTS USER1 ADDRESS(<span class="hljs-built_in">integer</span>) 0localhost:6379&gt; HEXISTS USER2 NAME(<span class="hljs-built_in">integer</span>) 0localhost:6379&gt;</code></pre><h4 id="HLEN"><a href="#HLEN" class="headerlink" title="HLEN"></a>HLEN</h4><blockquote><p>获取指定 key 中的 field 数量，当 key 不存在时，返回 0 。</p></blockquote><p><strong>可接受的语法</strong></p><pre><code class="hljs sh">HLEN KEY</code></pre><p><strong>代码示例</strong></p><pre><code class="hljs sh">localhost:6379&gt; HLEN USER1(<span class="hljs-built_in">integer</span>) 2localhost:6379&gt;</code></pre><h4 id="HKEYS"><a href="#HKEYS" class="headerlink" title="HKEYS"></a>HKEYS</h4><blockquote><p>返回指定 key 中的 所有 field，当 key 不存在时，返回一个空表。</p></blockquote><p><strong>可接受的语法</strong></p><pre><code class="hljs sh">HKEYS KEY</code></pre><p><strong>代码示例</strong></p><pre><code class="hljs sh">localhost:6379&gt; HKEYS USER11) <span class="hljs-string">"NAME"</span>2) <span class="hljs-string">"AGE"</span>localhost:6379&gt;</code></pre><h4 id="HVALS"><a href="#HVALS" class="headerlink" title="HVALS"></a>HVALS</h4><blockquote><p>返回指定 key 中所有 field 的 value，当 key 不存在时，返回一个空表。</p></blockquote><p><strong>可接受的语法</strong></p><pre><code class="hljs sh">HVALS KEY</code></pre><p><strong>代码示例</strong></p><pre><code class="hljs sh">localhost:6379&gt; HVALS USER11) <span class="hljs-string">"ZAHNGSAN"</span>2) <span class="hljs-string">"10"</span>localhost:6379&gt;</code></pre><h4 id="HDEL"><a href="#HDEL" class="headerlink" title="HDEL"></a>HDEL</h4><blockquote><p>删除指定 key 中的一个或多个 field，返回被成功移除的域的数量，不包括被忽略的域。<strong>在Redis2.4以下的版本里， HDEL 每次只能删除单个域，如果你需要在一个原子时间内删除多个域，请将命令包含在 MULTI / EXEC 块内。</strong></p></blockquote><p><strong>可接受的语法</strong></p><pre><code class="hljs sh">HDEL KEY FIELD1 [FIELD2 ...]</code></pre><p><strong>代码示例</strong></p><pre><code class="hljs sh">localhost:6379&gt; HDEL USER1 NAME AGE(<span class="hljs-built_in">integer</span>) 2localhost:6379&gt; HGETALL USER11) <span class="hljs-string">"ADDRESS"</span>2) <span class="hljs-string">"CHINA"</span>localhost:6379&gt;</code></pre><h4 id="HMSET"><a href="#HMSET" class="headerlink" title="HMSET"></a>HMSET</h4><blockquote><p>设置指定 key 的多个 field 对应的 value；此命令会覆盖哈希表中已存在的域。如果 key 不存在，一个空哈希表被创建并执行 HMSET 操作。</p></blockquote><p><strong>可接受的语法</strong></p><pre><code class="hljs sh">HMSET KEY FIELD1 VALUE1 [FIELD2 VALUE2...]</code></pre><p><strong>代码示例</strong></p><pre><code class="hljs sh">localhost:6379&gt; HMSET USER NAME <span class="hljs-string">"ZHANGSAN"</span> AGE 10 ADDRESS <span class="hljs-string">"CHINA"</span>OKlocalhost:6379&gt; HGETALL USER1) <span class="hljs-string">"NAME"</span>2) <span class="hljs-string">"ZHANGSAN"</span>3) <span class="hljs-string">"AGE"</span>4) <span class="hljs-string">"10"</span>5) <span class="hljs-string">"ADDRESS"</span>6) <span class="hljs-string">"CHINA"</span>localhost:6379&gt;</code></pre><h4 id="HMGET"><a href="#HMGET" class="headerlink" title="HMGET"></a>HMGET</h4><blockquote><p>获取指定 key 中的 多个 field 的 value</p></blockquote><p><strong>可接受的语法</strong></p><pre><code class="hljs sh">HMGET KEY FIELD1 [FIELD2 ...]</code></pre><p><strong>代码示例</strong></p><pre><code class="hljs sh">localhost:6379&gt; HMGET USER NAME AGE1) <span class="hljs-string">"ZHANGSAN"</span>2) <span class="hljs-string">"10"</span>localhost:6379&gt;</code></pre>]]></content>
    
    
    <categories>
      
      <category>Database</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Linux</tag>
      
      <tag>Redis</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Redis3.0.6编译安装</title>
    <link href="/2006/01/02/redis3-compile-and-install/"/>
    <url>/2006/01/02/redis3-compile-and-install/</url>
    
    <content type="html"><![CDATA[<p><img src="https://cdn.oss.link/markdown/hexo_redis_logo1.jpg" srcset="/img/loading.gif" alt="hexo_redis_logo"></p><h2 id="扯淡"><a href="#扯淡" class="headerlink" title="扯淡"></a>扯淡</h2><blockquote><p>最近在学习Redis，准备在Linux下安装一下，现记录一下编译安装过程。<br>环境为 CentOS6.7、Redis3.0.6、Xshell5</p></blockquote><h2 id="一、准备工作"><a href="#一、准备工作" class="headerlink" title="一、准备工作"></a>一、准备工作</h2><ol><li>下载Redis安装包 <a href="http://download.redis.io/releases/redis-3.0.6.tar.gz" target="_blank" rel="noopener">下载地址</a></li><li>安装好CentOS6系统</li><li>Xshell5连接CentOS</li></ol><h2 id="二、开始安装"><a href="#二、开始安装" class="headerlink" title="二、开始安装"></a>二、开始安装</h2><ol><li>下载 Redis Linux安装包，或者先下载再上传</li></ol><pre><code class="hljs sh">wget http://download.redis.io/releases/redis-3.0.6.tar.gz</code></pre><ol start="2"><li>解压Redis安装包</li></ol><pre><code class="hljs sh">tar -zxvf redis-3.0.6.tar.gz</code></pre><ol start="3"><li>安装相关依赖</li></ol><pre><code class="hljs sh">yum install gccyum install gcc-c++</code></pre><ol start="4"><li>编译&amp;&amp;安装</li></ol><pre><code class="hljs sh">make &amp;&amp; make install</code></pre><h2 id="三、更改配置"><a href="#三、更改配置" class="headerlink" title="三、更改配置"></a>三、更改配置</h2><blockquote><p>Redis许多特性通过配置文件设置，编辑 <code>redis.conf</code> 修改相关配置，以下参考自 <a href="http://yijiebuyi.com/blog/bc2b3d3e010bf87ba55267f95ab3aa71.html" target="_blank" rel="noopener">一介布衣</a> 建议更改后将其放在 <code>/etc/redis.conf</code></p></blockquote><hr><ul><li>基本配置</li></ul><pre><code class="hljs sh"><span class="hljs-comment"># 当配置中需要配置内存大小时，可以使用 1k, 5GB, 4M 等类似的格式，其转换方式如下(不区分大小写)</span><span class="hljs-comment">#</span><span class="hljs-comment"># 1k =&gt; 1000 bytes</span><span class="hljs-comment"># 1kb =&gt; 1024 bytes</span><span class="hljs-comment"># 1m =&gt; 1000000 bytes</span><span class="hljs-comment"># 1mb =&gt; 1024*1024 bytes</span><span class="hljs-comment"># 1g =&gt; 1000000000 bytes</span><span class="hljs-comment"># 1gb =&gt; 1024*1024*1024 bytes</span><span class="hljs-comment">#</span><span class="hljs-comment"># 内存配置大小写是一样的.比如 1gb 1Gb 1GB 1gB</span><span class="hljs-comment"># daemonize no 默认情况下，redis不是在后台运行的，如果需要在后台运行，把该项的值更改为yes</span>daemonize yes<span class="hljs-comment"># 当redis在后台运行的时候，[Redis](http://yijiebuyi.com/so.html?k=redis)默认会把pid文件放在/var/run/redis.pid，你可以配置到其他地址。</span><span class="hljs-comment"># 当运行多个redis服务时，需要指定不同的pid文件和端口</span>pidfile /var/run/redis.pid<span class="hljs-comment"># 指定redis运行的端口，默认是6379</span>port 6379<span class="hljs-comment"># 指定redis只接收来自于该IP地址的请求，如果不进行设置，那么将处理所有请求，</span><span class="hljs-comment"># 在生产环境中最好设置该项</span><span class="hljs-comment"># bind 127.0.0.1</span><span class="hljs-comment"># Specify the path for the unix socket that will be used to listen for</span><span class="hljs-comment"># incoming connections. There is no default, so Redis will not listen</span><span class="hljs-comment"># on a unix socket when not specified.</span><span class="hljs-comment">#</span><span class="hljs-comment"># unixsocket /tmp/redis.sock</span><span class="hljs-comment"># unixsocketperm 755</span><span class="hljs-comment"># 设置客户端连接时的超时时间，单位为秒。当客户端在这段时间内没有发出任何指令，那么关闭该连接</span><span class="hljs-comment"># 0是关闭此设置</span>timeout 0<span class="hljs-comment"># 指定日志记录级别</span><span class="hljs-comment"># Redis总共支持四个级别：debug、verbose、notice、warning，默认为verbose</span><span class="hljs-comment"># debug 记录很多信息，用于开发和测试</span><span class="hljs-comment"># varbose 有用的信息，不像debug会记录那么多</span><span class="hljs-comment"># notice 普通的verbose，常用于生产环境</span><span class="hljs-comment"># warning 只有非常重要或者严重的信息会记录到日志</span>loglevel debug<span class="hljs-comment"># 配置log文件地址</span><span class="hljs-comment"># 默认值为stdout，标准输出，若后台模式会输出到/dev/null</span><span class="hljs-comment">#logfile stdout</span>logfile /var/<span class="hljs-built_in">log</span>/redis/redis.log<span class="hljs-comment"># To enable logging to the system logger, just set 'syslog-enabled' to yes,</span><span class="hljs-comment"># and optionally update the other syslog parameters to suit your needs.</span><span class="hljs-comment"># syslog-enabled no</span><span class="hljs-comment"># Specify the syslog identity.</span><span class="hljs-comment"># syslog-ident redis</span><span class="hljs-comment"># Specify the syslog facility.  Must be USER or between LOCAL0-LOCAL7.</span><span class="hljs-comment"># syslog-facility local0</span><span class="hljs-comment"># 可用数据库数</span><span class="hljs-comment"># 默认值为16，默认数据库为0，数据库范围在0-（database-1）之间</span>databases 16<span class="hljs-comment">################################ 快照  #################################</span><span class="hljs-comment">#</span><span class="hljs-comment"># 保存数据到磁盘，格式如下:</span><span class="hljs-comment">#</span><span class="hljs-comment">#   save &lt;seconds&gt; &lt;changes&gt;</span><span class="hljs-comment">#</span><span class="hljs-comment">#   指出在多长时间内，有多少次更新操作，就将数据同步到数据文件rdb。</span><span class="hljs-comment">#   相当于条件触发抓取快照，这个可以多个条件配合</span><span class="hljs-comment">#    </span><span class="hljs-comment">#   比如默认配置文件中的设置，就设置了三个条件</span><span class="hljs-comment">#</span><span class="hljs-comment">#   save 900 1  900秒内至少有1个key被改变</span><span class="hljs-comment">#   save 300 10  300秒内至少有300个key被改变</span><span class="hljs-comment">#   save 60 10000  60秒内至少有10000个key被改变</span>save 900 1save 300 10save 60 10000<span class="hljs-comment"># 存储至本地数据库时（持久化到rdb文件）是否压缩数据，默认为yes</span>rdbcompression yes<span class="hljs-comment"># 本地持久化数据库文件名，默认值为dump.rdb</span>dbfilename dump.rdb<span class="hljs-comment"># 工作目录</span><span class="hljs-comment">#</span><span class="hljs-comment"># 数据库镜像备份的文件放置的路径。</span><span class="hljs-comment"># 这里的路径跟文件名要分开配置是因为redis在进行备份时，先会将当前数据库的状态写入到一个临时文件中，等备份完成时，</span><span class="hljs-comment"># 再把该该临时文件替换为上面所指定的文件，而这里的临时文件和上面所配置的备份文件都会放在这个指定的路径当中。</span><span class="hljs-comment">#</span><span class="hljs-comment"># AOF文件也会存放在这个目录下面</span><span class="hljs-comment">#</span><span class="hljs-comment"># 注意这里必须制定一个目录而不是文件</span>dir ./<span class="hljs-comment">################################# 复制 #################################</span><span class="hljs-comment"># 主从复制. 设置该数据库为其他数据库的从数据库.</span><span class="hljs-comment"># 设置当本机为slav服务时，设置master服务的IP地址及端口，在Redis启动时，它会自动从master进行数据同步</span><span class="hljs-comment">#</span><span class="hljs-comment"># slaveof &lt;masterip&gt; &lt;masterport&gt;</span><span class="hljs-comment"># 当master服务设置了密码保护时(用requirepass制定的密码)</span><span class="hljs-comment"># slav服务连接master的密码</span><span class="hljs-comment">#</span><span class="hljs-comment"># masterauth &lt;master-password&gt;</span><span class="hljs-comment"># 当从库同主机失去连接或者复制正在进行，从机库有两种运行方式：</span><span class="hljs-comment">#</span><span class="hljs-comment"># 1) 如果slave-serve-stale-data设置为yes(默认设置)，从库会继续相应客户端的请求</span><span class="hljs-comment">#</span><span class="hljs-comment"># 2) 如果slave-serve-stale-data是指为no，出去INFO和SLAVOF命令之外的任何请求都会返回一个</span><span class="hljs-comment">#    错误"SYNC with master in progress"</span><span class="hljs-comment">#</span>slave-serve-stale-data yes<span class="hljs-comment"># 从库会按照一个时间间隔向主库发送PINGs.可以通过repl-ping-slave-period设置这个时间间隔，默认是10秒</span><span class="hljs-comment">#</span><span class="hljs-comment"># repl-ping-slave-period 10</span><span class="hljs-comment"># repl-timeout 设置主库批量数据传输时间或者ping回复时间间隔，默认值是60秒</span><span class="hljs-comment"># 一定要确保repl-timeout大于repl-ping-slave-period</span><span class="hljs-comment"># repl-timeout 60</span></code></pre><ul><li>安全配置</li></ul><pre><code class="hljs sh"><span class="hljs-comment">################################## 安全 ###################################</span><span class="hljs-comment"># 设置客户端连接后进行任何其他指定前需要使用的密码。</span><span class="hljs-comment"># 警告：因为redis速度相当快，所以在一台比较好的服务器下，一个外部的用户可以在一秒钟进行150K次的密码尝试，这意味着你需要指定非常非常强大的密码来防止暴力破解</span><span class="hljs-comment">#</span><span class="hljs-comment"># requirepass foobared</span><span class="hljs-comment"># 命令重命名.</span><span class="hljs-comment">#</span><span class="hljs-comment"># 在一个共享环境下可以重命名相对危险的命令。比如把CONFIG重名为一个不容易猜测的字符。</span><span class="hljs-comment">#</span><span class="hljs-comment"># 举例:</span><span class="hljs-comment">#</span><span class="hljs-comment"># rename-command CONFIG b840fc02d524045429941cc15f59e41cb7be6c52</span><span class="hljs-comment">#</span><span class="hljs-comment"># 如果想删除一个命令，直接把它重命名为一个空字符""即可，如下：</span><span class="hljs-comment">#</span><span class="hljs-comment"># rename-command CONFIG ""</span></code></pre><ul><li>Redis 约束</li></ul><pre><code class="hljs sh"><span class="hljs-comment">################################### 约束 ####################################</span><span class="hljs-comment"># 设置同一时间最大客户端连接数，默认无限制，Redis可以同时打开的客户端连接数为Redis进程可以打开的最大文件描述符数，</span><span class="hljs-comment"># 如果设置 maxclients 0，表示不作限制。</span><span class="hljs-comment"># 当客户端连接数到达限制时，Redis会关闭新的连接并向客户端返回max number of clients reached错误信息</span><span class="hljs-comment">#</span><span class="hljs-comment"># maxclients 128</span><span class="hljs-comment"># 指定Redis最大内存限制，Redis在启动时会把数据加载到内存中，达到最大内存后，Redis会先尝试清除已到期或即将到期的Key</span><span class="hljs-comment"># Redis同时也会移除空的list对象</span><span class="hljs-comment">#</span><span class="hljs-comment"># 当此方法处理后，仍然到达最大内存设置，将无法再进行写入操作，但仍然可以进行读取操作</span><span class="hljs-comment">#</span><span class="hljs-comment"># 注意：Redis新的vm机制，会把Key存放内存，Value会存放在swap区</span><span class="hljs-comment">#</span><span class="hljs-comment"># maxmemory的设置比较适合于把redis当作于类似memcached的缓存来使用，而不适合当做一个真实的DB。</span><span class="hljs-comment"># 当把Redis当做一个真实的数据库使用的时候，内存使用将是一个很大的开销</span><span class="hljs-comment"># maxmemory &lt;bytes&gt;</span><span class="hljs-comment"># 当内存达到最大值的时候Redis会选择删除哪些数据？有五种方式可供选择</span><span class="hljs-comment">#</span><span class="hljs-comment"># volatile-lru -&gt; 利用LRU算法移除设置过过期时间的key (LRU:最近使用 Least Recently Used )</span><span class="hljs-comment"># allkeys-lru -&gt; 利用LRU算法移除任何key</span><span class="hljs-comment"># volatile-random -&gt; 移除设置过过期时间的随机key</span><span class="hljs-comment"># allkeys-&gt;random -&gt; remove a random key, any key</span><span class="hljs-comment"># volatile-ttl -&gt; 移除即将过期的key(minor TTL)</span><span class="hljs-comment"># noeviction -&gt; 不移除任何可以，只是返回一个写错误</span><span class="hljs-comment">#</span><span class="hljs-comment"># 注意：对于上面的策略，如果没有合适的key可以移除，当写的时候Redis会返回一个错误</span><span class="hljs-comment">#</span><span class="hljs-comment">#       写命令包括: set setnx setex append</span><span class="hljs-comment">#       incr decr rpush lpush rpushx lpushx linsert lset rpoplpush sadd</span><span class="hljs-comment">#       sinter sinterstore sunion sunionstore sdiff sdiffstore zadd zincrby</span><span class="hljs-comment">#       zunionstore zinterstore hset hsetnx hmset hincrby incrby decrby</span><span class="hljs-comment">#       getset mset msetnx exec sort</span><span class="hljs-comment">#</span><span class="hljs-comment"># 默认是:</span><span class="hljs-comment">#</span><span class="hljs-comment"># maxmemory-policy volatile-lru</span><span class="hljs-comment"># LRU 和 minimal TTL 算法都不是精准的算法，但是相对精确的算法(为了节省内存)，随意你可以选择样本大小进行检测。</span><span class="hljs-comment"># Redis默认的灰选择3个样本进行检测，你可以通过maxmemory-samples进行设置</span><span class="hljs-comment">#</span><span class="hljs-comment"># maxmemory-samples 3</span></code></pre><ul><li>其他配置</li></ul><pre><code class="hljs sh"><span class="hljs-comment">############################## AOF ###############################</span><span class="hljs-comment"># 默认情况下，redis会在后台异步的把数据库镜像备份到磁盘，但是该备份是非常耗时的，而且备份也不能很频繁，如果发生诸如拉闸限电、拔插头等状况，那么将造成比较大范围的数据丢失。</span><span class="hljs-comment"># 所以redis提供了另外一种更加高效的数据库备份及灾难恢复方式。</span><span class="hljs-comment"># 开启append only模式之后，redis会把所接收到的每一次写操作请求都追加到appendonly.aof文件中，当redis重新启动时，会从该文件恢复出之前的状态。</span><span class="hljs-comment"># 但是这样会造成appendonly.aof文件过大，所以redis还支持了BGREWRITEAOF指令，对appendonly.aof 进行重新整理。</span><span class="hljs-comment"># 你可以同时开启asynchronous dumps 和 AOF</span>appendonly no<span class="hljs-comment"># AOF文件名称 (默认: "appendonly.aof")</span><span class="hljs-comment"># appendfilename appendonly.aof</span><span class="hljs-comment"># Redis支持三种同步AOF文件的策略:</span><span class="hljs-comment">#</span><span class="hljs-comment"># no: 不进行同步，系统去操作 . Faster.</span><span class="hljs-comment"># always: always表示每次有写操作都进行同步. Slow, Safest.</span><span class="hljs-comment"># everysec: 表示对写操作进行累积，每秒同步一次. Compromise.</span><span class="hljs-comment">#</span><span class="hljs-comment"># 默认是"everysec"，按照速度和安全折中这是最好的。</span><span class="hljs-comment"># 如果想让Redis能更高效的运行，你也可以设置为"no"，让操作系统决定什么时候去执行</span><span class="hljs-comment"># 或者相反想让数据更安全你也可以设置为"always"</span><span class="hljs-comment">#</span><span class="hljs-comment"># 如果不确定就用 "everysec".</span><span class="hljs-comment"># appendfsync always</span>appendfsync everysec<span class="hljs-comment"># appendfsync no</span><span class="hljs-comment"># AOF策略设置为always或者everysec时，后台处理进程(后台保存或者AOF日志重写)会执行大量的I/O操作</span><span class="hljs-comment"># 在某些Linux配置中会阻止过长的fsync()请求。注意现在没有任何修复，即使fsync在另外一个线程进行处理</span><span class="hljs-comment">#</span><span class="hljs-comment"># 为了减缓这个问题，可以设置下面这个参数no-appendfsync-on-rewrite</span><span class="hljs-comment">#</span><span class="hljs-comment"># This means that while another child is saving the durability of Redis is</span><span class="hljs-comment"># the same as "appendfsync none", that in pratical terms means that it is</span><span class="hljs-comment"># possible to lost up to 30 seconds of log in the worst scenario (with the</span><span class="hljs-comment"># default Linux settings).</span><span class="hljs-comment">#</span><span class="hljs-comment"># If you have latency problems turn this to "yes". Otherwise leave it as</span><span class="hljs-comment"># "no" that is the safest pick from the point of view of durability.</span>no-appendfsync-on-rewrite no<span class="hljs-comment"># Automatic rewrite of the append only file.</span><span class="hljs-comment"># AOF 自动重写</span><span class="hljs-comment"># 当AOF文件增长到一定大小的时候Redis能够调用 BGREWRITEAOF 对日志文件进行重写</span><span class="hljs-comment">#</span><span class="hljs-comment"># 它是这样工作的：Redis会记住上次进行些日志后文件的大小(如果从开机以来还没进行过重写，那日子大小在开机的时候确定)</span><span class="hljs-comment">#</span><span class="hljs-comment"># 基础大小会同现在的大小进行比较。如果现在的大小比基础大小大制定的百分比，重写功能将启动</span><span class="hljs-comment"># 同时需要指定一个最小大小用于AOF重写，这个用于阻止即使文件很小但是增长幅度很大也去重写AOF文件的情况</span><span class="hljs-comment"># 设置 percentage 为0就关闭这个特性</span>auto-aof-rewrite-percentage 100auto-aof-rewrite-min-size 64mb</code></pre><ul><li>Log 文件配置</li></ul><pre><code class="hljs sh"><span class="hljs-comment">################################## SLOW LOG ###################################</span><span class="hljs-comment"># Redis Slow Log 记录超过特定执行时间的命令。执行时间不包括I/O计算比如连接客户端，返回结果等，只是命令执行时间</span><span class="hljs-comment">#</span><span class="hljs-comment"># 可以通过两个参数设置slow log：一个是告诉Redis执行超过多少时间被记录的参数slowlog-log-slower-than(微妙)，</span><span class="hljs-comment"># 另一个是slow log 的长度。当一个新命令被记录的时候最早的命令将被从队列中移除</span><span class="hljs-comment"># 下面的时间以微妙微单位，因此1000000代表一分钟。</span><span class="hljs-comment"># 注意制定一个负数将关闭慢日志，而设置为0将强制每个命令都会记录</span>slowlog-log-slower-than 10000<span class="hljs-comment"># 对日志长度没有限制，只是要注意它会消耗内存</span><span class="hljs-comment"># 可以通过 SLOWLOG RESET 回收被慢日志消耗的内存</span>slowlog-max-len 1024</code></pre><ul><li>限制</li></ul><pre><code class="hljs sh"><span class="hljs-comment">############################### ADVANCED CONFIG ###############################</span><span class="hljs-comment"># 当hash中包含超过指定元素个数并且最大的元素没有超过临界时，</span><span class="hljs-comment"># hash将以一种特殊的编码方式（大大减少内存使用）来存储，这里可以设置这两个临界值</span><span class="hljs-comment"># Redis Hash对应Value内部实际就是一个HashMap，实际这里会有2种不同实现，</span><span class="hljs-comment"># 这个Hash的成员比较少时Redis为了节省内存会采用类似一维数组的方式来紧凑存储，而不会采用真正的HashMap结构，对应的value redisObject的encoding为zipmap,</span><span class="hljs-comment"># 当成员数量增大时会自动转成真正的HashMap,此时encoding为ht。</span><span class="hljs-built_in">hash</span>-max-zipmap-entries 512<span class="hljs-built_in">hash</span>-max-zipmap-value 64<span class="hljs-comment"># list数据类型多少节点以下会采用去指针的紧凑存储格式。</span><span class="hljs-comment"># list数据类型节点值大小小于多少字节会采用紧凑存储格式。</span>list-max-ziplist-entries 512list-max-ziplist-value 64<span class="hljs-comment"># set数据类型内部数据如果全部是数值型，且包含多少节点以下会采用紧凑格式存储。</span><span class="hljs-built_in">set</span>-max-intset-entries 512<span class="hljs-comment"># zsort数据类型多少节点以下会采用去指针的紧凑存储格式。</span><span class="hljs-comment"># zsort数据类型节点值大小小于多少字节会采用紧凑存储格式。</span>zset-max-ziplist-entries 128zset-max-ziplist-value 64<span class="hljs-comment"># Redis将在每100毫秒时使用1毫秒的CPU时间来对redis的hash表进行重新hash，可以降低内存的使用</span><span class="hljs-comment">#</span><span class="hljs-comment"># 当你的使用场景中，有非常严格的实时性需要，不能够接受Redis时不时的对请求有2毫秒的延迟的话，把这项配置为no。</span><span class="hljs-comment">#</span><span class="hljs-comment"># 如果没有这么严格的实时性要求，可以设置为yes，以便能够尽可能快的释放内存</span>activerehashing yes<span class="hljs-comment">################################## INCLUDES ###################################</span><span class="hljs-comment"># 指定包含其它的配置文件，可以在同一主机上多个Redis实例之间使用同一份配置文件，而同时各个实例又拥有自己的特定配置文件</span><span class="hljs-comment"># include /path/to/local.conf</span><span class="hljs-comment"># include /path/to/other.conf</span></code></pre><h2 id="五、连接测试"><a href="#五、连接测试" class="headerlink" title="五、连接测试"></a>五、连接测试</h2><p>1、启动 Redis 服务</p><pre><code class="hljs sh">redis-server /etc/redis.conf</code></pre><p>2、启动 Redis Cli 客户端</p><pre><code class="hljs sh">redis-cli -h localhost -p 6379</code></pre><blockquote><p>此时如果能正常启动并连接则代表安装成功。</p></blockquote><h2 id="六、其他异常信息"><a href="#六、其他异常信息" class="headerlink" title="六、其他异常信息"></a>六、其他异常信息</h2><blockquote><p>其他异常信息参考 <a href="http://blog.csdn.net/u012969732/article/details/45049065" target="_blank" rel="noopener">CentOS7下安装Redis3.0及安装时遇到的问题</a></p></blockquote>]]></content>
    
    
    <categories>
      
      <category>Linux</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Linux</tag>
      
      <tag>Redis</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Redundent Array of Inexpensive Disks</title>
    <link href="/2006/01/02/redundent-array-of-inexpensive-disks/"/>
    <url>/2006/01/02/redundent-array-of-inexpensive-disks/</url>
    
    <content type="html"><![CDATA[<h2 id="一、简介"><a href="#一、简介" class="headerlink" title="一、简介"></a>一、简介</h2><blockquote><p>Redundent Array of Inexpensive/Indepedent Disks 简称 RAID 廉价(独立)冗余磁盘技术，早起由美国加州大学伯克利分校发明，用于使用廉价磁盘替代 SCSI 硬盘而设计，后经过发展成为 Linux 服务器构建高可用磁盘阵列技术；RAID 能够将多块硬盘组合成磁盘阵列，使其具备高速读写、数据冗余备份功能，对于 Host OS 来说，RAID 相当于一块高性能并有高可靠数据存储的硬盘。</p></blockquote><h2 id="二、RAID-分级"><a href="#二、RAID-分级" class="headerlink" title="二、RAID 分级"></a>二、RAID 分级</h2><h3 id="1、RAID-级别"><a href="#1、RAID-级别" class="headerlink" title="1、RAID 级别"></a>1、RAID 级别</h3><p>常用的 RAID 级别分别为 <strong>RAID0、RAID1、RAID5、RAID10、RAID50、JBOD。</strong></p><h3 id="2、RAID0"><a href="#2、RAID0" class="headerlink" title="2、RAID0"></a>2、RAID0</h3><p>RAID0 至少需要两块硬盘，其主要工作逻辑是将一个文件分片存放到多块硬盘，<strong>无冗余备份</strong>，理论上此时相对于单个磁盘的读写性能将提升 2倍或者更高(取决于硬盘块数)，磁盘利用率为100%；当然由于没有冗余备份，此时<strong>将会放大数据损坏概率(每块磁盘损坏概率假设10%，多块相乘)</strong>；通常将需要高速读写，并且数据损坏无实际影响的数据存放与此，如 swap 交换分区、tmp缓存分区等。</p><h3 id="3、RAID1"><a href="#3、RAID1" class="headerlink" title="3、RAID1"></a>3、RAID1</h3><p>RAID1 通常称之为镜像盘，同样 RAID1 至少需要两块磁盘，其工作逻辑是将一个文件分片，但会同时存储到两块硬盘上，相当于一个文件存2份，对于高可靠性数据则需要使用 RAID1，RAID1会降低磁盘存储性能 n 倍，具体取决于磁盘数量，磁盘利用率为 1/n；但每块磁盘上都有完整的磁盘数据，此时相当于完全冗余，会提升读取性能。</p><h3 id="4、RAID4"><a href="#4、RAID4" class="headerlink" title="4、RAID4"></a>4、RAID4</h3><p>为了既能够冗余备份，又能提升性能，则需要 RAID4，RAID4至少需要3块硬盘，其工作逻辑为 <strong>对于一个文件，首先将其分片存储进前两块硬盘，然后对于两个数据片做亦或运算生成校验码存储到第三块硬盘，第三块硬盘只存校验码，不存储真正的文件数据；</strong>此时前两块硬盘相当于 RAID0 的性能，当其中一块损坏后，可通过第三块硬盘与第一块硬盘做反向运算得出第二块硬盘内的数据，达到数据冗余的效果；<strong>但当前两块硬盘某一硬盘损坏后，剩下的两块硬盘将产生巨大压力，同时半损巨大的性能降低，</strong>因为每次读取存储文件都需要两块硬盘参与并完成数据计算。RAID1 下一旦磁盘出现损坏必须第一时间更换，否则剩下的两块磁盘一旦在此损坏数据将完全丢失，此种存储方式磁盘利用率为 (n-1)/n，下图为 RAID4 示意图 :</p><p><img src="https://cdn.oss.link/markdown/hexo_raid4.png" srcset="/img/loading.gif" alt="hexo_raid4"></p><h3 id="5、RAID5"><a href="#5、RAID5" class="headerlink" title="5、RAID5"></a>5、RAID5</h3><p>RAID5 与 RAID4 基本一致，区别在于 <strong>RAID4 将所有数据的校验码存放于最后一块磁盘上，当有磁盘损坏时，虽然能通过计算恢复数据，但此时如果校验盘再出现问题那么将导致全部数据丢失，RAID5 在此基础上将校验码分别存储于每个磁盘之上，从而降低了数据损毁风险。</strong></p><h3 id="6、RAID6"><a href="#6、RAID6" class="headerlink" title="6、RAID6"></a>6、RAID6</h3><p>RAID6 同 RAID5 基本一致，不过 RAID6至少需要4块硬盘，并且允许同时有两块硬盘损坏；</p><h3 id="7、RAID10"><a href="#7、RAID10" class="headerlink" title="7、RAID10"></a>7、RAID10</h3><p>RAID10 基于 RAID1 和 RAID0，RAID10 需要至少4块磁盘，空间利用率为50%，首先使用2块磁盘一组做 RAID1 镜像盘，保证数据完全备份，然后使用2块磁盘作为一个磁盘组，用这些磁盘组组件 RAID0 提高读写性能，此阵列允许同一组 RAID1 内有一块硬盘损坏，但不能同时损坏，同时损坏则会造成数据丢失，其示意图如下 :</p><p><img src="https://cdn.oss.link/markdown/hexo_raid10.png" srcset="/img/loading.gif" alt="hexo_raid10"></p><h3 id="8、JBOD"><a href="#8、JBOD" class="headerlink" title="8、JBOD"></a>8、JBOD</h3><p>JBOD 并非为了高可用数据冗余而设计，其主要作用是提高空间利用率，工作逻辑只是简单地将多块硬盘连接为一块使用，也不会为文件做自动的分片处理，常常用于类似 Hadoop 集群等应用，因为 Hadoop HDFS 等有其本身的容错机制。</p><h2 id="三、RAID-实现方式"><a href="#三、RAID-实现方式" class="headerlink" title="三、RAID 实现方式"></a>三、RAID 实现方式</h2><h3 id="1、硬件实现"><a href="#1、硬件实现" class="headerlink" title="1、硬件实现"></a>1、硬件实现</h3><p>硬件一般有两种实现方案，一种是板载集成，其性能一般较差，另一种是采用 PCI-E 插槽等接口的外置设备，通常称为 HBA 卡；此种阵列卡要求 Linux 内核能够驱动，其 RAID 级别在 BIOS 中便可完成设置。</p><h3 id="2、软件实现"><a href="#2、软件实现" class="headerlink" title="2、软件实现"></a>2、软件实现</h3><p>在硬件成本无法负担的情况下，Linux 内核提供一种组织机制(md模块)，可以在内核级别将多块硬盘组合成一块硬盘来使用，此时组件的 RAID 基于软件实现，此种操作需要浪费大量 CPU 性能。</p><h3 id="3、mdadm-命令"><a href="#3、mdadm-命令" class="headerlink" title="3、mdadm 命令"></a>3、mdadm 命令</h3><p>mdadm 命令用于与内核的 md 模块通讯，设置软件模式的 RAID；mdadm 类似 vim 是一个模式化的工具，其配置文件在 <code>/etc/mdadm.conf</code>，具体模式如下(用的很少) :</p><ul><li><code>-A</code> : Assemble 装配模式</li><li><code>-C</code> : Create 创建模式<ul><li><code>-n #</code> : 用于创建RAID设备的个数</li><li><code>-x #</code> : 热备磁盘的个数</li><li><code>-l</code> : 指定RAID级别</li><li><code>-a</code> : =yes（自动为创建的RAID设备创建设备文件） md mdp part p 如何创建设备文件</li><li><code>-c</code> :指定块的大小，默认为512KB</li></ul></li><li><code>-F</code> : FOLLOW 监控</li><li><code>-S</code> : 停止RAID</li><li><code>-D</code>、<code>--detail</code> : 显示阵列详细信息</li><li>Manage 管理模式专用项<ul><li><code>-f</code> : 模拟损害</li><li><code>-r</code> : 模拟移除设备</li><li><code>-a</code> : 模拟添加新设备</li></ul></li><li><code>/proc/mdstat</code> : 内核 md 模块配置</li><li>创建一个大小为12G的RAID0：2<em>6G，3</em>4G 4<em>3G 6</em>2G : <code>mdadm -C /dev/md0 -a yes -l 0 -n 2 /dev/sdb{1,2}</code></li></ul><audio autoplay="autoplay"><source src="https://cdn.oss.link/markdown/Cake-By-The Ocean.mp3" type="audio/mpeg" />Your browser does not support the audio element.</audio>]]></content>
    
    
    <categories>
      
      <category>Linux</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Linux</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>10分钟 dnsmasq 搭建</title>
    <link href="/2006/01/02/set-up-dnsmasq-for-10-minutes/"/>
    <url>/2006/01/02/set-up-dnsmasq-for-10-minutes/</url>
    
    <content type="html"><![CDATA[<h2 id="一、安装"><a href="#一、安装" class="headerlink" title="一、安装"></a>一、安装</h2><pre><code class="hljs sh">yum install -y dnsmasq</code></pre><h2 id="二、配置"><a href="#二、配置" class="headerlink" title="二、配置"></a>二、配置</h2><pre><code class="hljs sh"><span class="hljs-comment"># 编辑配置文件</span>vim /etc/dnsmasq.conf</code></pre><p><strong>主要配置如下</strong></p><pre><code class="hljs sh"><span class="hljs-comment"># 上游 DNS 定义</span>resolv-file=/etc/resolv.dnsmasq.conf<span class="hljs-comment"># 取消从本地 hosts 读取</span>no-hosts<span class="hljs-comment"># 监听地址</span>listen-address=127.0.0.1,192.168.1.106<span class="hljs-comment"># 指定本地 dns host 配置</span>addn-hosts=/etc/dnsmasq.hosts<span class="hljs-comment"># 设置 dns 缓存大小</span>cache-size=150</code></pre><p><strong>配置 dns 解析</strong></p><pre><code class="hljs sh"><span class="hljs-comment"># 增加本地回环</span><span class="hljs-built_in">echo</span> <span class="hljs-string">'nameserver 127.0.0.1'</span> &gt;&gt; /etc/resolv.conf<span class="hljs-comment"># 增加本地 hosts</span>cp /etc/hosts /etc/dnsmasq.hosts<span class="hljs-comment"># 添加上游 DNS 服务器</span><span class="hljs-built_in">echo</span> <span class="hljs-string">'nameserver 8.8.8.8'</span> &gt;&gt; /etc/resolv.dnsmasq.conf<span class="hljs-built_in">echo</span> <span class="hljs-string">'nameserver 192.168.1.1'</span> &gt;&gt; /etc/resolv.dnsmasq.conf</code></pre><h2 id="三、测试"><a href="#三、测试" class="headerlink" title="三、测试"></a>三、测试</h2><p><strong>首先启动 dnsmasq</strong></p><pre><code class="hljs sh">systemctl <span class="hljs-built_in">enable</span> dnsmasqsystemctl start dnsmasqsystemctl status dnsmasq</code></pre><p><strong>使用 dig 命令测试即可</strong></p><pre><code class="hljs sh">dig @192.168.1.106 www.baidu.com; &lt;&lt;&gt;&gt; DiG 9.9.4-RedHat-9.9.4-29.el7_2.3 &lt;&lt;&gt;&gt; @192.168.1.106 www.baidu.com; (1 server found);; global options: +cmd;; Got answer:;; -&gt;&gt;HEADER&lt;&lt;- opcode: QUERY, status: NOERROR, id: 4980;; flags: qr rd ra; QUERY: 1, ANSWER: 3, AUTHORITY: 0, ADDITIONAL: 1;; OPT PSEUDOSECTION:; EDNS: version: 0, flags:; udp: 4096;; QUESTION SECTION:;www.baidu.com.                 IN      A;; ANSWER SECTION:www.baidu.com.          490     IN      CNAME   www.a.shifen.com.www.a.shifen.com.       201     IN      A       61.135.169.125www.a.shifen.com.       201     IN      A       61.135.169.121;; Query time: 5 msec;; SERVER: 192.168.1.106<span class="hljs-comment">#53(192.168.1.106)</span>;; WHEN: 二 8月 30 16:00:21 EDT 2016;; MSG SIZE  rcvd: 101</code></pre><h2 id="四、视频"><a href="#四、视频" class="headerlink" title="四、视频"></a>四、视频</h2><video src="https://cdn.oss.link/videos/dnsmasq-2016-09-01_16.27.12.mkv" width="320" height="240" controls="controls">Your browser does not support the video tag.</video>]]></content>
    
    
    <categories>
      
      <category>Linux</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Linux</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Etcd 集群搭建</title>
    <link href="/2006/01/02/set-up-etcd-ha-cluster/"/>
    <url>/2006/01/02/set-up-etcd-ha-cluster/</url>
    
    <content type="html"><![CDATA[<blockquote><p>本文参考 <a href="https://wiki.archlinux.org/index.php/Systemd_(%E7%AE%80%E4%BD%93%E4%B8%AD%E6%96%87)" target="_blank" rel="noopener">Systemd 教程</a>、<a href="https://coreos.com/etcd/docs/latest/configuration.html" target="_blank" rel="noopener">Etcd 配置说明</a></p></blockquote><h2 id="一、简介"><a href="#一、简介" class="headerlink" title="一、简介"></a>一、简介</h2><p>Etcd 是 CoreOS 推出的高可用的键值存储系统，主要用于k8s集群的服务发现等，而本身 Etcd 也支持集群模式部署，从而实现自身高可用；</p><p><strong>Etcd 构建自身高可用集群主要有三种形式:</strong></p><ul><li>静态发现: 预先已知 Etcd 集群中有哪些节点，在启动时直接指定好 Etcd 的各个 node 节点地址</li><li>Etcd 动态发现: 通过已有的 Etcd 集群作为数据交互点，然后在扩展新的集群时实现通过已有集群进行服务发现的机制</li><li>DNS 动态发现: 通过 DNS 查询方式获取其他节点地址信息</li></ul><h2 id="二、静态发现"><a href="#二、静态发现" class="headerlink" title="二、静态发现"></a>二、静态发现</h2><h3 id="2-1、环境准备"><a href="#2-1、环境准备" class="headerlink" title="2.1、环境准备"></a>2.1、环境准备</h3><p>以下在 3 台虚拟机上搭建，系统环境为 CentOS7</p><table><thead><tr><th>节点</th><th>地址</th></tr></thead><tbody><tr><td>etcd0</td><td>192.168.1.154</td></tr><tr><td>etcd1</td><td>192.168.1.156</td></tr><tr><td>etcd2</td><td>192.168.1.249</td></tr></tbody></table><h3 id="2-2、安装-Etcd"><a href="#2-2、安装-Etcd" class="headerlink" title="2.2、安装 Etcd"></a>2.2、安装 Etcd</h3><p>CentOS 官方提供了 Etcd 的rpm，可通过 yum 直接安装，目前 yum 上最新版本为 2.3.7，比较合适；官方最新版本更新到了 3.0.6，鉴于稳定因素，这里使用 2.3.7 搭建</p><pre><code class="hljs sh">yum install etcd -y</code></pre><h3 id="2-3、修改-Etcd-配置"><a href="#2-3、修改-Etcd-配置" class="headerlink" title="2.3、修改 Etcd 配置"></a>2.3、修改 Etcd 配置</h3><p>yum 安装的 Etcd 默认配置文件在 <code>/etc/etcd/etcd.conf</code>，以下为 <code>etcd0</code> 上的样例(etcd1、etcd2同理)：</p><pre><code class="hljs vim"># 编辑配置文件<span class="hljs-keyword">vim</span> /etc/etcd/etcd.<span class="hljs-keyword">conf</span># 样例配置如下# 节点名称ETCD_NAME=etcd0# 数据存放位置ETCD_DATA_DIR=<span class="hljs-string">"/var/lib/etcd/etcd0"</span># 监听其他 Etcd 实例的地址ETCD_LISTEN_PEER_URLS=<span class="hljs-string">"http://0.0.0.0:2380"</span># 监听客户端地址ETCD_LISTEN_CLIENT_URLS=<span class="hljs-string">"http://0.0.0.0:2379,http://0.0.0.0:4001"</span># 通知其他 Etcd 实例地址ETCD_INITIAL_ADVERTISE_PEER_URLS=<span class="hljs-string">"http://192.168.1.154:2380"</span># 初始化集群内节点地址ETCD_INITIAL_CLUSTER=<span class="hljs-string">"etcd0=http://192.168.1.154:2380,etcd1=http://192.168.1.156:2380,etcd2=http://192.168.1.249:2380"</span># 初始化集群状态，<span class="hljs-keyword">new</span> 表示新建ETCD_INITIAL_CLUSTER_STATE=<span class="hljs-string">"new"</span># 初始化集群 tokenETCD_INITIAL_CLUSTER_TOKEN=<span class="hljs-string">"mritd-etcd-cluster"</span># 通知 客户端地址ETCD_ADVERTISE_CLIENT_URLS=<span class="hljs-string">"http://192.168.1.154:2379,http://192.168.1.154:4001"</span></code></pre><h3 id="2-4、测试"><a href="#2-4、测试" class="headerlink" title="2.4、测试"></a>2.4、测试</h3><p>集群搭建好后，在任意节点执行 <code>etcdctl member list</code> 可列所有集群节点信息，如下所示</p><p><img src="https://cdn.oss.link/markdown/hexo_etcd_getallnodes.png" srcset="/img/loading.gif" alt="hexo_etcd_getallnodes"></p><p>同时可以使用 <code>etcdctl cluster-health</code> 检查集群健康状态</p><p><img src="https://cdn.oss.link/markdown/hexo_etcd_checkhealth.png" srcset="/img/loading.gif" alt="hexo_etcd_checkhealth"></p><h2 id="三、DNS-动态发现"><a href="#三、DNS-动态发现" class="headerlink" title="三、DNS 动态发现"></a>三、DNS 动态发现</h2><h3 id="3-1、创建-DNS-记录"><a href="#3-1、创建-DNS-记录" class="headerlink" title="3.1、创建 DNS 记录"></a>3.1、创建 DNS 记录</h3><p>Etcd 在基于 DNS 做服务发现时，实际上是利用 DNS 的 SRV 记录不断轮训查询实现的，所以首先要加入 DNS SRV 记录，以下采用 dnsmasq 作为 dns 服务器，dnsmasq 可参考本博客 <a href="http://mritd.me/2016/09/01/10%E5%88%86%E9%92%9F-dnsmasq-%E6%90%AD%E5%BB%BA/" target="_blank" rel="noopener">10分钟 dnsmasq 搭建</a></p><pre><code class="hljs sh"><span class="hljs-comment"># 增加 SRV 记录</span>vim /etc/dnsmasq.conf<span class="hljs-comment"># 增加内容如下</span>srv-host=_etcd-server._tcp.mritd.me,etcd1.mritd.me,2380,0,100srv-host=_etcd-server._tcp.mritd.me,etcd2.mritd.me,2380,0,100srv-host=_etcd-server._tcp.mritd.me,etcd3.mritd.me,2380,0,100<span class="hljs-comment"># 然后增加对应的域名解析</span>vim /etc/dnsmasq.hosts<span class="hljs-comment"># 增加内容如下</span>192.168.1.154 etcd1.mritd.me192.168.1.156 etcd2.mritd.me192.168.1.249 etcd3.mritd.me</code></pre><p><strong>重启 dnsmasq 测试是否成功</strong></p><pre><code class="hljs sh"><span class="hljs-comment"># 重启</span>systemctl restart dnsmasq<span class="hljs-comment"># 查询 SRV</span>dig @192.168.1.106 +noall +answer SRV _etcd-server._tcp.mritd.me_etcd-server._tcp.mritd.me. 0   IN      SRV     0 100 2380 etcd2.mritd.me._etcd-server._tcp.mritd.me. 0   IN      SRV     0 100 2380 etcd1.mritd.me._etcd-server._tcp.mritd.me. 0   IN      SRV     0 100 2380 etcd3.mritd.me.<span class="hljs-comment"># 查询域名解析</span>dig @192.168.1.106 +noall +answer etcd1.mritd.me etcd2.mritd.me etcd3.mritd.meetcd1.mritd.me.         0       IN      A       192.168.1.154etcd2.mritd.me.         0       IN      A       192.168.1.156etcd3.mritd.me.         0       IN      A       192.168.1.249</code></pre><h3 id="3-2、修改-DNS-服务器"><a href="#3-2、修改-DNS-服务器" class="headerlink" title="3.2、修改 DNS 服务器"></a>3.2、修改 DNS 服务器</h3><p>Linux 系统默认从 <code>/etc/resolv.conf</code> 配置文件读取 DNS 服务器，为了让 Etcd 能够从 dnsmasq 服务器获取自定义域名解析，<strong>要修改3台 Etcd 服务器的 <code>/etc/resolv.conf</code> 文件</strong></p><pre><code class="hljs sh"><span class="hljs-comment"># 编辑 resolv.conf 文件</span>vim /etc/resolv.conf<span class="hljs-comment"># 文件内容如下，保证我们自定义的 dnsmasq 服务器在第一位</span><span class="hljs-comment"># Generated by NetworkManager</span>search lan lan.nameserver 192.168.1.106</code></pre><h3 id="3-3、配置-Etcd"><a href="#3-3、配置-Etcd" class="headerlink" title="3.3、配置 Etcd"></a>3.3、配置 Etcd</h3><p>接下来修改 Etcd 配置文件，开启 DNS 服务发现，主要是删除掉 <code>ETCD_INITIAL_CLUSTER</code> 字段(用于静态服务发现)，并指定 DNS SRV 域名(<code>ETCD_DISCOVERY_SRV</code>)</p><pre><code class="hljs sh"><span class="hljs-comment"># 编辑 etcd 配置文件</span>vim /etc/etcd/etcd.conf<span class="hljs-comment"># 配置样例如下</span><span class="hljs-comment"># 节点名称</span>ETCD_NAME=etcd1<span class="hljs-comment"># 数据存放位置</span>ETCD_DATA_DIR=<span class="hljs-string">"/var/lib/etcd/etcd1"</span><span class="hljs-comment"># 监听其他 Etcd 实例的地址</span>ETCD_LISTEN_PEER_URLS=<span class="hljs-string">"http://etcd1.mritd.me:2380"</span><span class="hljs-comment"># 监听客户端地址</span>ETCD_LISTEN_CLIENT_URLS=<span class="hljs-string">"http://etcd1.mritd.me:2379,http://etcd1.mritd.me:4001"</span><span class="hljs-comment"># 通知其他 Etcd 实例地址</span>ETCD_INITIAL_ADVERTISE_PEER_URLS=<span class="hljs-string">"http://etcd1.mritd.me:2380"</span><span class="hljs-comment"># 初始化集群状态，new 表示新建</span>ETCD_INITIAL_CLUSTER_STATE=<span class="hljs-string">"new"</span><span class="hljs-comment"># 初始化集群 token</span>ETCD_INITIAL_CLUSTER_TOKEN=<span class="hljs-string">"mritd-etcd-cluster"</span><span class="hljs-comment"># 通知 客户端地址</span>ETCD_ADVERTISE_CLIENT_URLS=<span class="hljs-string">"http://etcd1.mritd.me:2379,http://etcd1.mritd.me:4001"</span><span class="hljs-comment"># 集群 DNS SRV 域名</span>ETCD_DISCOVERY_SRV=<span class="hljs-string">"mritd.me"</span></code></pre><h3 id="3-4、测试"><a href="#3-4、测试" class="headerlink" title="3.4、测试"></a>3.4、测试</h3><pre><code class="hljs sh"><span class="hljs-comment"># 由于端口并未绑定到 0.0.0.0，所以需要指定 etcd 服务器</span><span class="hljs-comment"># 静态服务发现是绑定了 0.0.0.0 只是因为懒.....</span><span class="hljs-comment"># 出于安全考虑最好只监听局域网</span>etcdctl --endpoints <span class="hljs-string">"http://etcd1.mritd.me:2379,http://etcd1.mritd.me:4001"</span> member list<span class="hljs-comment"># 显示如下</span>1e306cee69c3e859: name=etcd3 peerURLs=http://etcd3.mritd.me:2380 clientURLs=http://etcd3.mritd.me:2379,http://etcd3.mritd.me:4001 isLeader=<span class="hljs-literal">false</span>ee23db1083fd44ac: name=etcd1 peerURLs=http://etcd1.mritd.me:2380 clientURLs=http://etcd1.mritd.me:2379,http://etcd1.mritd.me:4001 isLeader=<span class="hljs-literal">true</span>f9de134820e6ec9e: name=etcd2 peerURLs=http://etcd2.mritd.me:2380 clientURLs=http://etcd2.mritd.me:2379,http://etcd2.mritd.me:4001 isLeader=<span class="hljs-literal">false</span></code></pre><h2 id="四、基于已有集群的服务发现"><a href="#四、基于已有集群的服务发现" class="headerlink" title="四、基于已有集群的服务发现"></a>四、基于已有集群的服务发现</h2><p>这种方法感觉和静态服务发现感觉并没有什么卵用……根本达不到所谓的 “服务发现”</p><h3 id="4-1、获取集群标识"><a href="#4-1、获取集群标识" class="headerlink" title="4.1、获取集群标识"></a>4.1、获取集群标识</h3><p>集群标识可以从已有的 Etcd 集群中创建，Etcd 官方提供了一个创建地址，一下以官方地址做演示，如果从私有集群创建集群标识请自行 Google</p><pre><code class="hljs sh"><span class="hljs-comment"># 获取集群标识 size 代表要创建的集群大小</span>curl -w <span class="hljs-string">"\n"</span> <span class="hljs-string">'https://discovery.etcd.io/new?size=3'</span><span class="hljs-comment"># 返回如下</span>https://discovery.etcd.io/f6a252c5240cc89b91fa00dac95d5732</code></pre><h3 id="4-2、修改配置"><a href="#4-2、修改配置" class="headerlink" title="4.2、修改配置"></a>4.2、修改配置</h3><p>依次编辑3个节点的 Etcd 配置文件，主要指定 <code>ETCD_DISCOVERY</code> 参数为获取的集群标识即可</p><pre><code class="hljs sh"><span class="hljs-comment"># 编辑 etcd 配置文件</span>vim /etc/etcd/etcd.conf<span class="hljs-comment"># 配置样例如下</span><span class="hljs-comment"># 节点名称</span>ETCD_NAME=etcd1<span class="hljs-comment"># 数据存放位置</span>ETCD_DATA_DIR=<span class="hljs-string">"/var/lib/etcd/etcd1"</span><span class="hljs-comment"># 监听其他 Etcd 实例的地址</span>ETCD_LISTEN_PEER_URLS=<span class="hljs-string">"http://etcd1.mritd.me:2380"</span><span class="hljs-comment"># 监听客户端地址</span>ETCD_LISTEN_CLIENT_URLS=<span class="hljs-string">"http://etcd1.mritd.me:2379,http://etcd1.mritd.me:4001"</span><span class="hljs-comment"># 通知其他 Etcd 实例地址</span>ETCD_INITIAL_ADVERTISE_PEER_URLS=<span class="hljs-string">"http://etcd1.mritd.me:2380"</span><span class="hljs-comment"># 初始化集群状态，new 表示新建</span>ETCD_INITIAL_CLUSTER_STATE=<span class="hljs-string">"new"</span><span class="hljs-comment"># 初始化集群 token</span>ETCD_INITIAL_CLUSTER_TOKEN=<span class="hljs-string">"mritd-etcd-cluster"</span><span class="hljs-comment"># 通知 客户端地址</span>ETCD_ADVERTISE_CLIENT_URLS=<span class="hljs-string">"http://etcd1.mritd.me:2379,http://etcd1.mritd.me:4001"</span><span class="hljs-comment"># 设置集群标识</span>ETCD_DISCOVERY=<span class="hljs-string">"https://discovery.etcd.io/f6a252c5240cc89b91fa00dac95d5732"</span></code></pre><h3 id="4-3、测试"><a href="#4-3、测试" class="headerlink" title="4.3、测试"></a>4.3、测试</h3><p>同 DNS 测试方法一样</p><pre><code class="hljs sh">etcdctl --endpoints <span class="hljs-string">"http://etcd1.mritd.me:2379,http://etcd1.mritd.me:4001"</span> member list1e306cee69c3e859: name=etcd3 peerURLs=http://etcd3.mritd.me:2380 clientURLs=http://etcd3.mritd.me:2379,http://etcd3.mritd.me:4001 isLeader=<span class="hljs-literal">true</span>ee23db1083fd44ac: name=etcd1 peerURLs=http://etcd1.mritd.me:2380 clientURLs=http://etcd1.mritd.me:2379,http://etcd1.mritd.me:4001 isLeader=<span class="hljs-literal">false</span>f9de134820e6ec9e: name=etcd2 peerURLs=http://etcd2.mritd.me:2380 clientURLs=http://etcd2.mritd.me:2379,http://etcd2.mritd.me:4001 isLeader=<span class="hljs-literal">false</span></code></pre>]]></content>
    
    
    <categories>
      
      <category>Linux</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Linux</tag>
      
      <tag>Docker</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>GlusterFS 笔记</title>
    <link href="/2006/01/02/set-up-glusterfs-cluster/"/>
    <url>/2006/01/02/set-up-glusterfs-cluster/</url>
    
    <content type="html"><![CDATA[<h2 id="一、简介"><a href="#一、简介" class="headerlink" title="一、简介"></a>一、简介</h2><p>GlusterFS 是近年兴起的一个高性能开源分布式文件系统，其目标是全局命名空间、分布式前端的高性能文件系统，目前已被 RedHat 看中，GlusterFS 具有高扩展、高可性、高性能、可横向扩展等特点，并且 GlusterFS 没有元数据服务器的设计，使其没有单点故障问题。</p><h2 id="二、GlusterFS-集群搭建"><a href="#二、GlusterFS-集群搭建" class="headerlink" title="二、GlusterFS 集群搭建"></a>二、GlusterFS 集群搭建</h2><h3 id="2-1、环境准备"><a href="#2-1、环境准备" class="headerlink" title="2.1、环境准备"></a>2.1、环境准备</h3><p>由于资源有限，所以以虚拟机测试，故性能上无法体现，毕竟是一块硬盘，搭建环境如下</p><table><thead><tr><th>主机</th><th>域名</th><th>磁盘</th><th>大小</th></tr></thead><tbody><tr><td>192.168.1.100</td><td>gfs-server1</td><td>/dev/sdb</td><td>50G</td></tr><tr><td>192.168.1.107</td><td>gfs-server2</td><td>/dev/sdb</td><td>50G</td></tr><tr><td>192.168.1.126</td><td>gfs-server3</td><td>/dev/sdb</td><td>50G</td></tr><tr><td>192.168.1.217</td><td>gfs-server4</td><td>/dev/sdb</td><td>50G</td></tr></tbody></table><h3 id="2-2、安装-GlusterFS"><a href="#2-2、安装-GlusterFS" class="headerlink" title="2.2、安装 GlusterFS"></a>2.2、安装 GlusterFS</h3><p>CentOS7 默认官方 yum 源中有 GlusterFS 相关的 rpm 包，但是发现没有 server 端的，官方给出的安装命令如下</p><pre><code class="hljs sh"><span class="hljs-comment"># 安装 GlusterFS yum 源</span>yum install -y centos-release-gluster &amp;&amp; yum update -y<span class="hljs-comment"># 安装 GlusterFS 以及客户端</span>yum install -y glusterfs glusterfs-fuse glusterfs-cli glusterfs-server glusterfs-api</code></pre><h3 id="2-3、基础环境配置"><a href="#2-3、基础环境配置" class="headerlink" title="2.3、基础环境配置"></a>2.3、基础环境配置</h3><h4 id="2-3-1、ntp-时钟同步"><a href="#2-3-1、ntp-时钟同步" class="headerlink" title="2.3.1、ntp 时钟同步"></a>2.3.1、ntp 时钟同步</h4><p>由于 GlusterFS 需要进行节点间同步，所以各节点时间要保证一致性，故需要安装 ntp 时钟同步工具</p><pre><code class="hljs sh"><span class="hljs-comment"># 安装 ntp</span>yum install -y ntp</code></pre><p>安装完成后应进行时钟同步，这里暂时使用 windows 的时钟授权服务器，也可以选择其他时钟授权服务器，安装时测试国内的清华大等全部超时……</p><pre><code class="hljs sh"><span class="hljs-comment"># 同步时钟</span>ntpdate time.windows.com</code></pre><p>同步完成后最好设置定时同步，以下为每天 0 时开始没 3 小时进行一次时钟同步</p><pre><code class="hljs sh"><span class="hljs-comment"># 增加 当前用户 cron 任务</span>crontab -e<span class="hljs-comment"># 写入如下任务</span>* 0-23/3 * * * root  /usr/sbin/ntpdate time.windows.com &amp;&gt; /dev/null; /usr/sbin/clock -w</code></pre><h4 id="2-3-2、host-定义"><a href="#2-3-2、host-定义" class="headerlink" title="2.3.2、host 定义"></a>2.3.2、host 定义</h4><p>由于使用域名方式访问，所以需要修改本地 hosts 文件，也可以通过自建 dns 服务器来解决，不过据说 dns 服务器方式会有一点点延迟，所以推荐修改本地 hosts(没证明过)</p><pre><code class="hljs sh"><span class="hljs-comment"># 修改 hosts</span>vim /etc/hosts<span class="hljs-comment"># 增加如下</span>192.168.1.100 gfs-server1192.168.1.107 gfs-server2192.168.1.126 gfs-server3192.168.1.217 gfs-server4</code></pre><h3 id="2-4、启动并加入节点"><a href="#2-4、启动并加入节点" class="headerlink" title="2.4、启动并加入节点"></a>2.4、启动并加入节点</h3><p>GlusterFS 安装完成后可直接通过 systemd 启动</p><pre><code class="hljs sh">systemctl <span class="hljs-built_in">enable</span> glusterdsystemctl start glusterd</code></pre><p>启动完成后便可在任意节点上将其他节点加入进来，组建集群</p><pre><code class="hljs sh"><span class="hljs-keyword">for</span> gfs_host <span class="hljs-keyword">in</span> gfs-server1 gfs-server2 gfs-server3 gfs-server4;<span class="hljs-keyword">do</span>  gluster peer probe <span class="hljs-variable">$gfs_host</span><span class="hljs-keyword">done</span></code></pre><h3 id="2-5、磁盘预处理"><a href="#2-5、磁盘预处理" class="headerlink" title="2.5、磁盘预处理"></a>2.5、磁盘预处理</h3><p>GlusterFS 集群服务启动并加入其他节点后，就需要为下一步创建卷(volume)准备磁盘，以下以一块 50G 的磁盘为例，在每个节点上执行如下</p><pre><code class="hljs sh"><span class="hljs-comment"># 首先创建分区</span>fdisk /dev/sdb<span class="hljs-comment"># 然后输入 n 创建新分区，再输入 p 选择主分区</span><span class="hljs-comment"># 最后输入 w 保存分区表，如下所示</span>欢迎使用 fdisk (util-linux 2.23.2)。更改将停留在内存中，直到您决定将更改写入磁盘。使用写入命令前请三思。Device does not contain a recognized partition table使用磁盘标识符 0x445cdbb5 创建新的 DOS 磁盘标签。命令(输入 m 获取帮助)：nPartition <span class="hljs-built_in">type</span>:   p   primary (0 primary, 0 extended, 4 free)   e   extendedSelect (default p): p分区号 (1-4，默认 1)：起始 扇区 (2048-104857599，默认为 2048)：将使用默认值 2048Last 扇区, +扇区 or +size&#123;K,M,G&#125; (2048-104857599，默认为 104857599)：将使用默认值 104857599分区 1 已设置为 Linux 类型，大小设为 50 GiB命令(输入 m 获取帮助)：wThe partition table has been altered!Calling ioctl() to re-read partition table.正在同步磁盘。</code></pre><p>分区创建完成后需要对其进行格式化</p><pre><code class="hljs sh">mkfs.ext4 /dev/sdb</code></pre><p>最后准备接下来要使用的相关挂载目录</p><pre><code class="hljs sh"><span class="hljs-comment"># 创建磁盘挂载目录</span>mkdir -p /data/gfs<span class="hljs-comment"># 挂载硬盘</span>mount -t ext4 /dev/sdb /data/gfs<span class="hljs-comment"># 创建 GlusterFS 卷目录</span>mkdir -p /data/gfs/brick0</code></pre><p>为了保证磁盘一直挂载，最好设置一下开机自动挂载</p><pre><code class="hljs sh"><span class="hljs-comment"># 后面三个选项，defaults 表示使用默认挂载参数，</span><span class="hljs-comment"># 第一个 1 代表 允许 demp</span><span class="hljs-comment"># 第二个 1 代表 开机执行挂载分区检查</span><span class="hljs-built_in">echo</span> <span class="hljs-string">"/dev/sdb /data/gfs ext4 defaults 1 1"</span> &gt;&gt; /etc/fstab</code></pre><p>到此集群基本部署完成</p><h2 id="三、GlusterFS-存储卷设置"><a href="#三、GlusterFS-存储卷设置" class="headerlink" title="三、GlusterFS 存储卷设置"></a>三、GlusterFS 存储卷设置</h2><p>GlusterFS 集群搭建完成后，便需要创建存储卷(volume)来整合各个集群磁盘存储资源，以便后面进行挂载和使用</p><h3 id="3-1、创建分布式-Hash-卷"><a href="#3-1、创建分布式-Hash-卷" class="headerlink" title="3.1、创建分布式 Hash 卷"></a>3.1、创建分布式 Hash 卷</h3><p>分布式 Hash 卷其原理是当向 GlusterFS 写入一个文件时，GlusterFS 通过弹性 Hash 算法对文件名进行计算，然后将其均匀的分布到各个节点上，<strong>因此分布式 Hash 卷没有数据冗余</strong>，创建分布式 Hash 卷命令如下</p><pre><code class="hljs sh">gluster volume create gfs_disk\  gfs-server1:/data/gfs/brick0 \  gfs-server2:/data/gfs/brick0 \  gfs-server3:/data/gfs/brick0 \  gfs-server4:/data/gfs/brick0</code></pre><h3 id="3-2、创建复制卷"><a href="#3-2、创建复制卷" class="headerlink" title="3.2、创建复制卷"></a>3.2、创建复制卷</h3><p>复制卷相当于 RAID1，在向 GlusterFS 中存储文件时，GlusterFS 将其拷贝到所有节点，并且是同步的，这会极大降低磁盘性能，并呈线性降低，<strong>但是复制卷随着节点数量增加，起数据冗余能力也在增加，因为每个节点上都有一份数据的完全拷贝</strong>，创建复制卷命令如下</p><pre><code class="hljs sh"><span class="hljs-comment"># replica 参数用于指定数据拷贝有多少份</span><span class="hljs-comment"># 注意: replica 数量必须与指定的 GlusterFS 集群 brick 数量保持一致</span>gluster volume create gfs_disk replica 4 transport tcp \  gfs-server1:/data/gfs/brick0 \  gfs-server2:/data/gfs/brick0 \  gfs-server3:/data/gfs/brick0 \  gfs-server4:/data/gfs/brick0</code></pre><h3 id="3-3、创建分布式-Hash-复制卷"><a href="#3-3、创建分布式-Hash-复制卷" class="headerlink" title="3.3、创建分布式 Hash 复制卷"></a>3.3、创建分布式 Hash 复制卷</h3><p>顾名思义，分布式 Hash 复制卷就是将 Hash 卷与复制卷整合一下，通过 replica 参数指定复制份数，以下例子为使用4个节点创建，每两个节点组成一个复制卷，然后两对节点再组成 Hash 卷</p><pre><code class="hljs sh"><span class="hljs-comment"># 创建 分布式 Hash 复制卷时，只需要保证集群数量是 replica 数量的整数倍即可</span>gluster volume create gfs_disk replica 2 transport tcp \  gfs-server1:/data/gfs/brick0 \  gfs-server2:/data/gfs/brick0 \  gfs-server3:/data/gfs/brick0 \  gfs-server4:/data/gfs/brick0</code></pre><h3 id="3-4、其他卷"><a href="#3-4、其他卷" class="headerlink" title="3.4、其他卷"></a>3.4、其他卷</h3><ul><li>条带卷: 条带卷既将文件切分成块，分布存放在各个节点，将 <code>replica</code> 替换成 <code>stripe</code> 即可</li><li>分布式 Hash 条带卷: 同分布式 Hash 复制卷一样，保证节点数量是 <code>stripe</code> 的整数倍即可</li></ul><p>由于条带卷使用并不多，所以不做演示，一般条带卷适用于单个大文件超过磁盘大小时使用</p><h2 id="四、GlusterFS-挂载及测试"><a href="#四、GlusterFS-挂载及测试" class="headerlink" title="四、GlusterFS 挂载及测试"></a>四、GlusterFS 挂载及测试</h2><p>无论创建的哪种卷，最终创建完成后都可以通过如下命令查看</p><pre><code class="hljs sh">gluster volume info</code></pre><h3 id="4-1、GlusterFS-卷挂载"><a href="#4-1、GlusterFS-卷挂载" class="headerlink" title="4.1、GlusterFS 卷挂载"></a>4.1、GlusterFS 卷挂载</h3><p>卷创建完成后，只需要像正常磁盘一样挂载即可使用，<strong>不同的是文件系统类型指定为 <code>glusterfs</code> 而已</strong>，操作如下</p><pre><code class="hljs sh"><span class="hljs-comment"># mount 时指定任意一个节点即可</span>mount -t glusterfs gfs-server1:gfs_disk /mnt/gfs</code></pre><p>为保证一直可用，同样最好设置开机自动挂载</p><pre><code class="hljs sh"><span class="hljs-built_in">echo</span> <span class="hljs-string">"gfs-server1:gfs_disk /mnt/gfs glusterfs defaults 0 1"</span> &gt;&gt; /etc/fstab</code></pre><h3 id="4-2、测试-GlusterFS"><a href="#4-2、测试-GlusterFS" class="headerlink" title="4.2、测试 GlusterFS"></a>4.2、测试 GlusterFS</h3><p>以下以分布式 Hash 卷为例，由于准备了 4 各节点，并且 replica 为 2，所以理论上向 GlusterFS 写入一个文件应该会在任意两个节点上都有一份</p><pre><code class="hljs sh"><span class="hljs-comment"># 在 gfs-server1 上执行</span>cp test.tar.gz /mnt.gfs<span class="hljs-comment"># 最后可以在 gfs-server3、gfs-server4 的磁盘目录中看到</span>ls /data/gfs/brick0</code></pre><h2 id="五、其他相关"><a href="#五、其他相关" class="headerlink" title="五、其他相关"></a>五、其他相关</h2><p>没有任何通用的文件系统，通用意味着通通不能用，关于 GlusterFS 适用场景以及缺点可参考 <a href="http://blog.sae.sina.com.cn/archives/5141" target="_blank" rel="noopener">换个角度看GlusterFS分布式文件系统</a> 文章，关于 GlusterFS 性能与监控可参考 <a href="https://github.com/jiobxn/one/wiki/00086_GlusterFS%E6%80%A7%E8%83%BD%E7%9B%91%E6%8E%A7&%E9%85%8D%E9%A2%9D" target="_blank" rel="noopener">GlusterFS性能监控&amp;配额</a></p>]]></content>
    
    
    <categories>
      
      <category>Linux</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Linux</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Kubernetes 集群搭建</title>
    <link href="/2006/01/02/set-up-kubernetes/"/>
    <url>/2006/01/02/set-up-kubernetes/</url>
    
    <content type="html"><![CDATA[<h2 id="一、简介"><a href="#一、简介" class="headerlink" title="一、简介"></a>一、简介</h2><p>kubernetes 是 Google 内部使用的 Borg 容器调度框架的开源实现，其凝聚了 Google 十几年容器经验的最佳实践，其支持 Dokcer 和 Rkt 容器的编排功能，以下记录一下 Kubernetes 集群搭建过程。</p><h2 id="二、环境准备"><a href="#二、环境准备" class="headerlink" title="二、环境准备"></a>二、环境准备</h2><p>kubernetes 搭建集群环境推荐至少3个节点，1个 master 和2个 slave 节点，所以至少三台服务器(虚拟机)；容器间通讯采用 flannel 实现跨主机通讯，具体参考 <a href="http://mritd.me/2016/09/03/Dokcer-%E4%BD%BF%E7%94%A8-Flannel-%E8%B7%A8%E4%B8%BB%E6%9C%BA%E9%80%9A%E8%AE%AF/" target="_blank" rel="noopener">Dokcer 使用 Flannel 跨主机通讯</a> 文章，同时 kubernetes 本身的服务发现机制依赖于 etcd，etcd 可使用单机模式，也可以构建高可用集群，集群搭建可参考 <a href="http://mritd.me/2016/09/01/Etcd-%E9%9B%86%E7%BE%A4%E6%90%AD%E5%BB%BA/" target="_blank" rel="noopener">Etcd 集群搭建</a> 文章；最后总体环境如下:</p><ul><li>3台虚拟机</li><li>每台安装好 Docker</li><li>配置好 Docker 使用 Flannel 跨主机通讯</li></ul><p><strong>主机列表如下</strong></p><table><thead><tr><th>主机</th><th>环境配置</th></tr></thead><tbody><tr><td>192.168.1.108</td><td>k8s master、etcd</td></tr><tr><td>192.168.1.139</td><td>k8s node1</td></tr><tr><td>192.168.1.215</td><td>k8s node2</td></tr></tbody></table><h2 id="三、搭建示例"><a href="#三、搭建示例" class="headerlink" title="三、搭建示例"></a>三、搭建示例</h2><blockquote><p>以下安装全部基于 rpm 包方式，关于 falnnel、etcd、kubernetes 的 rpm 包可通过 <a href="https://github.com/mritd/shell_scripts/blob/master/build_rpm_tool.sh" target="_blank" rel="noopener">build_rpm_tool.sh</a> 脚本工具创建指定版本的 rpm 包，下载脚本使用 <code>./build_rpm_tool.sh k8s 1.3.6</code> 命令即可创建一个 k8s 的 rpm，<strong>如需多次使用，请将脚本第 181 行 k8s rpm 地址替换为已经自己编译好的 k8s rpm 地址(cdn 流量不多)</strong></p></blockquote><h3 id="3-1、安装-Etcd"><a href="#3-1、安装-Etcd" class="headerlink" title="3.1、安装 Etcd"></a>3.1、安装 Etcd</h3><p>首先安装 etcd rpm 包</p><pre><code class="hljs sh">rpm -ivh etcd-2.3.7-1.x86_64.rpm</code></pre><p>然后修改 etcd 配置</p><pre><code class="hljs sh"><span class="hljs-comment"># 编辑配置文件</span>vim /etc/etcd/etcd.conf<span class="hljs-comment"># 修改后内容如下</span>ETCD_NAME=defaultETCD_DATA_DIR=<span class="hljs-string">"/var/lib/etcd/default.etcd"</span>ETCD_LISTEN_CLIENT_URLS=<span class="hljs-string">"http://192.168.1.108:2379"</span>ETCD_ADVERTISE_CLIENT_URLS=<span class="hljs-string">"http://192.168.1.108:2379"</span></code></pre><p>最后启动并测试</p><pre><code class="hljs sh"><span class="hljs-comment"># 启动</span>systemctl <span class="hljs-built_in">enable</span> etcdsystemctl start etcd<span class="hljs-comment"># 连接测试</span>etcdctl --endpoints http://192.168.1.108:2379 member list<span class="hljs-comment"># 返回结果如下</span>ce2a822cea30bfca: name=default peerURLs=http://localhost:2380,http://localhost:7001 clientURLs=http://192.168.1.108:2379 isLeader=<span class="hljs-literal">true</span></code></pre><h3 id="3-2、安装-flannel"><a href="#3-2、安装-flannel" class="headerlink" title="3.2、安装 flannel"></a>3.2、安装 flannel</h3><p>安装过程如下，三台虚拟机都要安装</p><pre><code class="hljs sh"><span class="hljs-comment"># 安装</span>rpm -ivh flannel-0.6.1-1.x86_64.rpm<span class="hljs-comment"># 设置 IP 段</span>etcdctl --endpoints http://192.168.1.108:2379 <span class="hljs-built_in">set</span> /coreos.com/network/config <span class="hljs-string">'&#123;"NetWork":"10.0.0.0/16"&#125;'</span><span class="hljs-comment"># 修改配置</span>vim /etc/sysconfig/flanneld<span class="hljs-comment"># 配置如下 enp0s3 为监听网卡</span>FLANNEL_ETCD=<span class="hljs-string">"http://192.168.1.108:2379"</span>FLANNEL_ETCD_KEY=<span class="hljs-string">"/coreos.com/network"</span>FLANNEL_OPTIONS=<span class="hljs-string">"--iface=enp0s3"</span><span class="hljs-comment"># 启动</span>systemctl <span class="hljs-built_in">enable</span> flanneldsystemctl start flanneld<span class="hljs-comment"># 修改 docker 配置</span>vim /usr/lib/systemd/system/docker.service<span class="hljs-comment"># 在 ExecStart 后增加 $DOCKER_NETWORK_OPTIONS 参数</span>ExecStart=/usr/bin/dockerd <span class="hljs-variable">$DOCKER_NETWORK_OPTIONS</span><span class="hljs-comment"># 重启 docker</span>systemctl daemon-reloadsystemctl restart docker</code></pre><h3 id="3-3、安装-master"><a href="#3-3、安装-master" class="headerlink" title="3.3、安装 master"></a>3.3、安装 master</h3><p>kubernetes master 安装在 108 上，与etcd 在同一台主机，安装 kubernetes rpm 包命令如下</p><pre><code class="hljs sh">rpm -ivh kubernetes-1.3.6-1.x86_64.rpm</code></pre><p><strong>配置 apiserver</strong></p><pre><code class="hljs sh"><span class="hljs-comment"># 编辑配置文件</span>vim /etc/kubernetes/apiserver<span class="hljs-comment"># 配置信息如下</span>KUBE_API_ADDRESS=<span class="hljs-string">"--insecure-bind-address=192.168.1.108"</span><span class="hljs-comment"># The port on the local server to listen on.</span>KUBE_API_PORT=<span class="hljs-string">"--insecure-port=8080"</span><span class="hljs-comment"># Port minions listen on</span>KUBELET_PORT=<span class="hljs-string">"--kubelet_port=10250"</span><span class="hljs-comment"># Comma separated list of nodes in the etcd cluster</span>KUBE_ETCD_SERVERS=<span class="hljs-string">"--etcd_servers=http://192.168.1.108:2379"</span><span class="hljs-comment"># Address range to use for services</span>KUBE_SERVICE_ADDRESSES=<span class="hljs-string">"--service-cluster-ip-range=10.254.0.0/16"</span><span class="hljs-comment"># default admission control policies</span>KUBE_ADMISSION_CONTROL=<span class="hljs-string">"--admission_control=NamespaceLifecycle,NamespaceExists,LimitRanger,SecurityContextDeny,ServiceAccount,ResourceQuota"</span><span class="hljs-comment"># Add your own!</span>KUBE_API_ARGS=<span class="hljs-string">""</span></code></pre><p><strong>启动 master</strong></p><pre><code class="hljs sh">systemctl start kube-apiserversystemctl start kube-controller-managersystemctl start kube-schedulersystemctl <span class="hljs-built_in">enable</span> kube-apiserversystemctl <span class="hljs-built_in">enable</span> kube-controller-managersystemctl <span class="hljs-built_in">enable</span> kube-schedulersystemctl status kube-apiserversystemctl status kube-controller-managersystemctl status kube-scheduler</code></pre><h3 id="3-4、安装-slave"><a href="#3-4、安装-slave" class="headerlink" title="3.4、安装 slave"></a>3.4、安装 slave</h3><p>其余两台虚拟机需要安装成 slave 节点，首先配置好 docker 和flannel，参考 3.1，安装过程如下</p><p>首先安装 kubernetes</p><pre><code class="hljs sh">rpm -ivh kubernetes-1.3.6-1.x86_64.rpm</code></pre><p>配置 kubelet</p><pre><code class="hljs sh"><span class="hljs-comment"># 编辑配置文件</span>vim /etc/kubernetes/kubelet<span class="hljs-comment"># 配置如下</span>KUBELET_ADDRESS=<span class="hljs-string">"--address=192.168.1.139"</span><span class="hljs-comment"># The port for the info server to serve on</span>KUBELET_PORT=<span class="hljs-string">"--port=10250"</span><span class="hljs-comment"># You may leave this blank to use the actual hostname</span>KUBELET_HOSTNAME=<span class="hljs-string">"--hostname_override=192-168-1-139"</span><span class="hljs-comment"># location of the api-server</span>KUBELET_API_SERVER=<span class="hljs-string">"--api_servers=http://192.168.1.108:8080"</span><span class="hljs-comment"># Add your own!</span>KUBELET_ARGS=<span class="hljs-string">"--pod-infra-container-image=docker.io/kubernetes/pause:latest"</span></code></pre><p><strong><code>KUBELET_HOSTNAME</code> 参数用于指定 slave 在 master 中显示的名字，一般为了便于区分会自定义名字，但是自定义的名字必须在 hosts 文件中存在，所以还要修改 hosts 文件</strong></p><pre><code class="hljs sh"><span class="hljs-built_in">echo</span> <span class="hljs-string">"127.0.0.1 192-168-1-139"</span> &gt;&gt; /etc/hosts</code></pre><p>接着修改主配置文件</p><pre><code class="hljs sh"><span class="hljs-comment"># 编辑配置文件</span>vim /etc/kubernetes/config<span class="hljs-comment"># 配置样例如下</span>KUBE_LOGTOSTDERR=<span class="hljs-string">"--logtostderr=true"</span><span class="hljs-comment"># journal message level, 0 is debug</span>KUBE_LOG_LEVEL=<span class="hljs-string">"--v=0"</span><span class="hljs-comment"># Should this cluster be allowed to run privileged docker containers</span>KUBE_ALLOW_PRIV=<span class="hljs-string">"--allow_privileged=false"</span><span class="hljs-comment"># How the controller-manager, scheduler, and proxy find the apiserver</span>KUBE_MASTER=<span class="hljs-string">"--master=http://192.168.1.108:8080"</span></code></pre><p><strong>最后启动测试</strong></p><pre><code class="hljs sh"><span class="hljs-comment"># 启动</span>systemctl start kubeletsystemctl start kube-proxysystemctl <span class="hljs-built_in">enable</span> kubeletsystemctl <span class="hljs-built_in">enable</span> kube-proxysystemctl status kubeletsystemctl status kube-proxy<span class="hljs-comment"># 回到 108 master 测试</span>kubectl --server=<span class="hljs-string">"http://192.168.1.108:8080"</span> get node<span class="hljs-comment"># 显示如下</span>NAME            STATUS    AGE192-168-1-139   Ready     53s</code></pre><p>另外一台同理，到此 集群搭建完成</p><h2 id="四、其他相关"><a href="#四、其他相关" class="headerlink" title="四、其他相关"></a>四、其他相关</h2><ul><li>kubernetes、etcd 等监听端口最好指定为内网IP，尽量不要监听 <code>0.0.0.0</code>，监听公网很容易被黑</li><li>kubernetes 通讯最好开启 SSL，尤其是使用 “联邦模式” 时(跨级群调度)</li><li>etcd 最好搭建集群解决单点问题，同时最好准备2台dns服务器，用于服务发现等</li><li>kubernetes 如果启用了 skydns 则最好在 <code>KUBELET_ARGS</code> 参数中加入 <code>--cluster_dns=x.x.x.x</code> 为 skydns，这样在每个 pod 启动后默认 dns 都会指向 skydns</li></ul>]]></content>
    
    
    <categories>
      
      <category>Kubernetes</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Linux</tag>
      
      <tag>Docker</tag>
      
      <tag>Kubernetes</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Nexus 2.11 CentOS搭建教程</title>
    <link href="/2006/01/02/set-up-nexus-on-centos/"/>
    <url>/2006/01/02/set-up-nexus-on-centos/</url>
    
    <content type="html"><![CDATA[<p><img src="https://cdn.oss.link/markdown/Maven-Nexus-logo.png" srcset="/img/loading.gif" alt="Nexus Logo"></p><h3 id="一、简介"><a href="#一、简介" class="headerlink" title="一、简介"></a>一、简介</h3><blockquote><p>Nexus是一个高效的Maven私有服务器，用过Maven的都知道，一般公司开发都会搭建一个私服；然后项目中通过POM文件或者Maven的setting.xml指向私服地址，这样我们便可以通过私服管理jar包；可自定义jar包坐标，高效的构建项目。</p></blockquote><h3 id="二、前期准备"><a href="#二、前期准备" class="headerlink" title="二、前期准备"></a>二、前期准备</h3><ol><li>首先，你得有台能连接公网的CentOS系统服务器(可使用虚拟机，但网络必须桥接，保证内网独立IP)。</li><li>然后下载jdk和 nexus   <a href="http://apache.fayea.com/maven/maven-3/" target="_blank" rel="noopener">Nexus下载地址</a></li><li><strong>注意事项：</strong>Nexus 有其对应的 jdk版本，请参考官网，截至目前最高版本2.11.4-01 需要jdk7</li></ol><h3 id="三、搭建Nexus私服"><a href="#三、搭建Nexus私服" class="headerlink" title="三、搭建Nexus私服"></a>三、搭建Nexus私服</h3><blockquote><p>啰嗦一句，由于也是刚刚开始搞，以前虽然搞过，但版本比较低，配置已经变了；还有刚刚被一篇博客坑了；算了，不说了，都是眼泪，喝瓶养乐多压压惊……</p></blockquote><ol><li>配置jdk和nexus环境变量</li></ol><ul><li><p>首先上传jdk到Linux服务器(nexus顺带也传上去)，推荐xshell工具，先执行 <code>yum install lrzsz</code> ，然后把jdk往命令行一拖就上传成功，当然你有其他办法我也不阻止，反正搞上去就行。<br><img src="https://cdn.oss.link/markdown/Maven-Nexus-uploadfileo.png" srcset="/img/loading.gif" alt="Maven-Nexus-uploadfileo"></p></li><li><p>解压 jdk 和 nexus 执行以下命令</p></li></ul><pre><code class="hljs bash"><span class="hljs-comment">#解压 jdk</span>tar -zxvf jdk-7u79-linux-x64.tar.gz<span class="hljs-comment">#解压 nexus</span>tar -zxvf nexus-latest-bundle.tar.gz</code></pre><ul><li><p>解压后如下<br><img src="https://cdn.oss.link/markdown/Maven-Nexus-unzipfile1.png" srcset="/img/loading.gif" alt="Maven-Nexus-unzipfile1"></p></li><li><p>创建 并移动 解压后的目录到 /usr/local/java</p><pre><code class="hljs bash"><span class="hljs-built_in">cd</span> /usr/<span class="hljs-built_in">local</span>mkdir java<span class="hljs-built_in">cd</span> ~mv jdk1.7.0_79 /usr/<span class="hljs-built_in">local</span>/javamv nexus-2.11.4-01 /usr/loca/javamv sonatype-work /usr/<span class="hljs-built_in">local</span>/java</code></pre></li><li><p>最终效果如下<br><img src="https://cdn.oss.link/markdown/Maven-Nexus-mvfile.png" srcset="/img/loading.gif" alt="Maven-Nexus-mvfile"></p></li><li><p>编辑 /etc/profile，配置环境变量，在末尾添加以下 变量(vim不会用的下面不用看了)<br><img src="https://cdn.oss.link/markdown/Maven-Nexus-path.png" srcset="/img/loading.gif" alt="Maven-Nexus-path"></p></li><li><p>执行以下命令重新初始化环境变量，并测试是否成功</p></li></ul><pre><code class="hljs bash"><span class="hljs-built_in">source</span> /etc/profilejavac -version</code></pre><ul><li>返回 javac 版本未成功，若出现 “command not found” 说明配置不成功，检查环境变量位置。</li></ul><ol start="2"><li>配置nexus</li></ol><blockquote><p>上面的估计玩过Linux的基本都能轻松搞定，说白了Nexus主要是下面的配置</p></blockquote><ul><li>配置nexus启动文件</li></ul><blockquote><p>编辑 /usr/local/java/nexus-2.11.4-01/bin 下的 nexus 可执行文件，主要配置部分样例如下</p></blockquote><pre><code class="hljs bash"><span class="hljs-comment">#-----------------------------------------------------------------------------</span><span class="hljs-comment"># These settings can be modified to fit the needs of your application</span><span class="hljs-comment"># Set this to the root of the Nexus installation</span><span class="hljs-comment"># 设置 nexus 主目录，就是解压后的那个 nexus目录绝对路径</span>NEXUS_HOME=<span class="hljs-string">"/usr/local/java/nexus-2.11.4-01"</span><span class="hljs-comment"># If specified, the Wrapper will be run as the specified user.</span><span class="hljs-comment"># IMPORTANT - Make sure that the user has the required privileges to write into the Nexus installation directory.</span><span class="hljs-comment"># NOTE - This will set the user which is used to run the Wrapper as well as</span><span class="hljs-comment">#  the JVM and is not useful in situations where a privileged resource or</span><span class="hljs-comment">#  port needs to be allocated prior to the user being changed.</span><span class="hljs-comment"># nexus官方不推荐以root 用户运行，如果你非要这么做，下面注释去掉 后面填写root</span><span class="hljs-comment">#RUN_AS_USER=</span><span class="hljs-comment"># Application</span>APP_NAME=<span class="hljs-string">"nexus"</span>APP_LONG_NAME=<span class="hljs-string">"Nexus OSS"</span><span class="hljs-comment"># Priority at which to run the wrapper.  See "man nice" for valid priorities.</span><span class="hljs-comment">#  nice is only used if a priority is specified.</span><span class="hljs-comment"># 这块我查了一下 好像是设置运行优先级，默认10，没太搞懂，可不填</span>PRIORITY=<span class="hljs-comment">#Location of the pid file.</span><span class="hljs-comment"># nexus pid 文件存放位置 不用改</span>PIDDIR=<span class="hljs-string">"."</span><span class="hljs-comment"># If uncommented, causes the Wrapper to be shutdown using an anchor file.</span><span class="hljs-comment">#  When launched with the 'start' command, it will also ignore all INT and</span><span class="hljs-comment">#  TERM signals.</span><span class="hljs-comment">#IGNORE_SIGNALS=true</span><span class="hljs-comment"># The following two lines are used by the chkconfig command. Change as is</span><span class="hljs-comment">#  appropriate for your application.  They should remain commented.</span><span class="hljs-comment"># chkconfig: 2345 20 80</span><span class="hljs-comment"># description: Test Wrapper Sample Application</span></code></pre><ul><li>配置 /usr/local/java/nexus-2.11.4-01/bin/jsw/conf 下的 wrapper.conf 文件</li></ul><blockquote><p>妈蛋，最坑的就是这货，配置样例如下</p></blockquote><pre><code class="hljs bash"><span class="hljs-comment"># JSW Configuration file</span><span class="hljs-comment">#</span><span class="hljs-comment"># For complete coverage of available configuration, please go here:</span><span class="hljs-comment"># http://wrapper.tanukisoftware.org/doc/english/properties.html</span><span class="hljs-comment">#</span><span class="hljs-comment"># Some "most often used" keywords are added to end of this file</span><span class="hljs-comment"># to be used as a "guideline". They are NOT the complete list of</span><span class="hljs-comment"># keywords.</span><span class="hljs-comment"># Set the JSW working directory (used as base for resolving relative paths)</span><span class="hljs-comment"># 设置wrapper的工作目录 其实也是指向 nexus 主目录即可</span><span class="hljs-comment"># 妈蛋的地址我就不说了，刚刚一个博主告诉我把他指向 /usr/local/java/nexus-2.11.4-01/bin/jsw/linux-x86-64</span><span class="hljs-comment"># 我进去一看 里面确实有个wrapper 可执行文件，就特么信了，结果各种报错</span><span class="hljs-comment"># 不凑巧，我特么居然还看懂了，就是找不到java类，然后我就根据错误各种改底下配置的绝对路径</span><span class="hljs-comment"># 改了半天我特么感觉不对啊...怎么其实都是基于默认值往上 跳了几层，妈的瞬间领悟...</span>wrapper.working.dir=/usr/<span class="hljs-built_in">local</span>/java/nexus-2.11.4-01<span class="hljs-comment"># Set the JVM executable</span><span class="hljs-comment"># (modify this to absolute path if you need a Java that is not on the OS path)</span><span class="hljs-comment"># 配置 jdk中 java 可执行文件的位置(其实我感觉jre就可以，没测试，有兴趣的测试一下)</span>wrapper.java.command=/usr/<span class="hljs-built_in">local</span>/java/jdk1.7.0_79/bin/java<span class="hljs-comment"># The main class that JSW will execute within JVM</span><span class="hljs-comment"># 这货不要动，目测这个玩意应该在下面的 wrapper-3.2.3.jar 里面</span>wrapper.java.mainclass=org.sonatype.nexus.bootstrap.jsw.JswLauncher<span class="hljs-comment"># The JVM classpath</span><span class="hljs-comment"># 如果你上面的 wrapper.working.dir 没问题这块就没问题...再次吐槽，坑爹博主，害得我第一次把这都改成绝对路径了</span>wrapper.java.classpath.1=bin/jsw/lib/wrapper-3.2.3.jarwrapper.java.classpath.2=./lib/*.jarwrapper.java.classpath.3=./conf/<span class="hljs-comment"># The library path</span><span class="hljs-comment"># 不需要动，一些依赖jar包 下面有路径配置的，默认值其实都是基于 wrapper.working.dir 的</span><span class="hljs-comment"># 也就是说一但你 wrapper.working.dir 错了，其他的就都得改</span>wrapper.java.library.path.1=bin/jsw/lib<span class="hljs-comment"># Additional JVM parameters (tune if needed, but match the sequence of numbers!)</span><span class="hljs-comment"># 一些 jvm 参数，默认即可，能玩明白的 可以自己调优啥的，下面的有能力可以设置一下，</span><span class="hljs-comment"># 搞不定的 就不用往下看了......</span>wrapper.java.additional.1=-XX:MaxPermSize=192mwrapper.java.additional.2=-Djava.io.tmpdir=./tmpwrapper.java.additional.3=-Djava.net.preferIPv4Stack=<span class="hljs-literal">true</span>wrapper.java.additional.4=-Dcom.sun.jndi.ldap.connect.pool.protocol=<span class="hljs-string">"plain ssl"</span>wrapper.java.additional.4.stripquotes=TRUE<span class="hljs-comment">#wrapper.java.additional.5=-Xdebug</span><span class="hljs-comment">#wrapper.java.additional.6=-Xnoagent</span><span class="hljs-comment">#wrapper.java.additional.7=-Djava.compiler=NONE</span><span class="hljs-comment">#wrapper.java.additional.8=-Xrunjdwp:transport=dt_socket,server=y,suspend=y,address=8000</span><span class="hljs-comment">#wrapper.java.additional.9=-XX:+HeapDumpOnOutOfMemoryError</span>wrapper.app.parameter.1=./conf/jetty.xmlwrapper.app.parameter.2=./conf/jetty-requestlog.xml<span class="hljs-comment">#下面的都无需更改，暂且省略......</span></code></pre><ul><li>创建 nexus 用户</li></ul><blockquote><p>nexus 不推荐以root用户运行，所以我们创建一个 nexus 用户，安全性要求高的可以控制其是否可登录什么的，我这里只是自己玩玩，so 允许登录。</p></blockquote><pre><code class="hljs bash">useradd nexuspasswd nexus<span class="hljs-comment">#输入2次密码</span></code></pre><ol start="3"><li>启动nexus</li></ol><blockquote><p>首先切换到 nexus 用户，然后启动nexus</p></blockquote><pre><code class="hljs bash">su - nexus<span class="hljs-comment"># 由于配置了PATH变量，所以可在任意位置执行 nexus命令</span>nexus start</code></pre><ul><li>查看是否启动成功&amp;监测日志</li></ul><blockquote><p>访问 ip:8081/nexus ；启动成功的话会进入如下界面</p></blockquote><p><img src="https://cdn.oss.link/markdown/Maven-Nexus-home-1024x436.png" srcset="/img/loading.gif" alt="Maven-Nexus-home-1024x436"></p><blockquote><p>默认登陆账户密码为  admin/admin123 ；如启动不成功可查看日志</p></blockquote><pre><code class="hljs bash">less /usr/<span class="hljs-built_in">local</span>/java/nexus-2.11.4-01/logs/wrapper.log</code></pre><blockquote><p>到此结束，关于仓库使用啥的不管了…</p></blockquote>]]></content>
    
    
    <categories>
      
      <category>Java</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Java</tag>
      
      <tag>Maven</tag>
      
      <tag>Nexus</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Spring4 学习笔记</title>
    <link href="/2006/01/02/spring3-note/"/>
    <url>/2006/01/02/spring3-note/</url>
    
    <content type="html"><![CDATA[<p><img src="https://cdn.oss.link/markdown/Spring-logotouming.png" srcset="/img/loading.gif" alt="Spring-logo"></p><h3 id="一、Spring-Hello-World"><a href="#一、Spring-Hello-World" class="headerlink" title="一、Spring Hello World"></a>一、Spring Hello World</h3><ul><li>1、新建 <code>Mavne java</code> 项目，<code>POM</code> 加入<code>Spring Context</code></li></ul><pre><code class="hljs xml"><span class="hljs-tag">&lt;<span class="hljs-name">dependency</span>&gt;</span><span class="hljs-tag">&lt;<span class="hljs-name">groupId</span>&gt;</span>org.springframework<span class="hljs-tag">&lt;/<span class="hljs-name">groupId</span>&gt;</span><span class="hljs-tag">&lt;<span class="hljs-name">artifactId</span>&gt;</span>spring-context<span class="hljs-tag">&lt;/<span class="hljs-name">artifactId</span>&gt;</span><span class="hljs-tag">&lt;<span class="hljs-name">version</span>&gt;</span>4.2.4.RELEASE<span class="hljs-tag">&lt;/<span class="hljs-name">version</span>&gt;</span><span class="hljs-tag">&lt;/<span class="hljs-name">dependency</span>&gt;</span></code></pre><ul><li>2、新建一个 <code>Java Bean</code></li></ul><pre><code class="hljs java"><span class="hljs-keyword">package</span> me.mritd.beans;<span class="hljs-keyword">public</span> <span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">Persion</span> </span>&#123;<span class="hljs-keyword">private</span> String name;<span class="hljs-keyword">private</span> <span class="hljs-keyword">int</span> age;<span class="hljs-keyword">private</span> String address;<span class="hljs-function"><span class="hljs-keyword">public</span> String <span class="hljs-title">getName</span><span class="hljs-params">()</span> </span>&#123;<span class="hljs-keyword">return</span> name;&#125;<span class="hljs-function"><span class="hljs-keyword">public</span> <span class="hljs-keyword">void</span> <span class="hljs-title">setName</span><span class="hljs-params">(String name)</span> </span>&#123;<span class="hljs-keyword">this</span>.name = name;&#125;<span class="hljs-function"><span class="hljs-keyword">public</span> <span class="hljs-keyword">int</span> <span class="hljs-title">getAge</span><span class="hljs-params">()</span> </span>&#123;<span class="hljs-keyword">return</span> age;&#125;<span class="hljs-function"><span class="hljs-keyword">public</span> <span class="hljs-keyword">void</span> <span class="hljs-title">setAge</span><span class="hljs-params">(<span class="hljs-keyword">int</span> age)</span> </span>&#123;<span class="hljs-keyword">this</span>.age = age;&#125;<span class="hljs-function"><span class="hljs-keyword">public</span> String <span class="hljs-title">getAddress</span><span class="hljs-params">()</span> </span>&#123;<span class="hljs-keyword">return</span> address;&#125;<span class="hljs-function"><span class="hljs-keyword">public</span> <span class="hljs-keyword">void</span> <span class="hljs-title">setAddress</span><span class="hljs-params">(String address)</span> </span>&#123;<span class="hljs-keyword">this</span>.address = address;&#125;<span class="hljs-meta">@Override</span><span class="hljs-function"><span class="hljs-keyword">public</span> String <span class="hljs-title">toString</span><span class="hljs-params">()</span> </span>&#123;<span class="hljs-keyword">return</span> <span class="hljs-string">"Persion [name="</span> + name + <span class="hljs-string">", age="</span> + age + <span class="hljs-string">", address="</span> + address + <span class="hljs-string">"]"</span>;&#125;<span class="hljs-function"><span class="hljs-keyword">public</span> <span class="hljs-title">Persion</span><span class="hljs-params">()</span> </span>&#123;<span class="hljs-keyword">super</span>();<span class="hljs-comment">// TODO Auto-generated constructor stub</span>&#125;<span class="hljs-function"><span class="hljs-keyword">public</span> <span class="hljs-title">Persion</span><span class="hljs-params">(String name, <span class="hljs-keyword">int</span> age, String address)</span> </span>&#123;<span class="hljs-keyword">super</span>();<span class="hljs-keyword">this</span>.name = name;<span class="hljs-keyword">this</span>.age = age;<span class="hljs-keyword">this</span>.address = address;&#125;&#125;</code></pre><ul><li>3、创建 <code>Spring</code> 配置文件 <code>applicationContext.xml</code>，并配置 <code>Bean</code> 信息</li></ul><pre><code class="hljs xml"><span class="hljs-meta">&lt;?xml version="1.0" encoding="UTF-8"?&gt;</span><span class="hljs-tag">&lt;<span class="hljs-name">beans</span> <span class="hljs-attr">xmlns</span>=<span class="hljs-string">"http://www.springframework.org/schema/beans"</span></span><span class="hljs-tag"><span class="hljs-attr">xmlns:xsi</span>=<span class="hljs-string">"http://www.w3.org/2001/XMLSchema-instance"</span></span><span class="hljs-tag"><span class="hljs-attr">xsi:schemaLocation</span>=<span class="hljs-string">"http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd"</span>&gt;</span><span class="hljs-tag">&lt;<span class="hljs-name">bean</span> <span class="hljs-attr">id</span>=<span class="hljs-string">"persion"</span> <span class="hljs-attr">name</span>=<span class="hljs-string">"persion"</span> <span class="hljs-attr">class</span>=<span class="hljs-string">"me.mritd.beans.Persion"</span>&gt;</span><span class="hljs-tag">&lt;<span class="hljs-name">property</span> <span class="hljs-attr">name</span>=<span class="hljs-string">"name"</span> <span class="hljs-attr">value</span>=<span class="hljs-string">"Hello World"</span>&gt;</span><span class="hljs-tag">&lt;/<span class="hljs-name">property</span>&gt;</span><span class="hljs-tag">&lt;<span class="hljs-name">property</span> <span class="hljs-attr">name</span>=<span class="hljs-string">"age"</span> <span class="hljs-attr">value</span>=<span class="hljs-string">"20"</span>&gt;</span><span class="hljs-tag">&lt;/<span class="hljs-name">property</span>&gt;</span><span class="hljs-tag">&lt;<span class="hljs-name">property</span> <span class="hljs-attr">name</span>=<span class="hljs-string">"address"</span> <span class="hljs-attr">value</span>=<span class="hljs-string">"伊拉克"</span>&gt;</span><span class="hljs-tag">&lt;/<span class="hljs-name">property</span>&gt;</span><span class="hljs-tag">&lt;/<span class="hljs-name">bean</span>&gt;</span><span class="hljs-tag">&lt;/<span class="hljs-name">beans</span>&gt;</span></code></pre><ul><li>4、编写测试类测试 <strong>从Spring上下文获取一个Bean</strong></li></ul><pre><code class="hljs java"><span class="hljs-keyword">package</span> me.mritd.test;<span class="hljs-keyword">import</span> org.junit.Test;<span class="hljs-keyword">import</span> org.springframework.context.ApplicationContext;<span class="hljs-keyword">import</span> org.springframework.context.support.ClassPathXmlApplicationContext;<span class="hljs-keyword">import</span> me.mritd.beans.Persion;<span class="hljs-comment">/**</span><span class="hljs-comment"> *</span><span class="hljs-comment"> * Copyright © 2016 Mritd. All rights reserved.</span><span class="hljs-comment"> *</span><span class="hljs-comment"> * <span class="hljs-doctag">@ClassName</span>: Test1</span><span class="hljs-comment"> * <span class="hljs-doctag">@Description</span>: TODO</span><span class="hljs-comment"> * <span class="hljs-doctag">@author</span>: 漠然</span><span class="hljs-comment"> * <span class="hljs-doctag">@date</span>: 2016年1月1日 下午8:59:14</span><span class="hljs-comment"> */</span><span class="hljs-keyword">public</span> <span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">Test1</span> </span>&#123;<span class="hljs-meta">@Test</span><span class="hljs-function"><span class="hljs-keyword">public</span> <span class="hljs-keyword">void</span> <span class="hljs-title">testHelloWorld</span><span class="hljs-params">()</span></span>&#123;<span class="hljs-comment">// 创建ApplicationContext</span>ApplicationContext applicationContext = <span class="hljs-keyword">new</span> ClassPathXmlApplicationContext(<span class="hljs-string">"applicationContext.xml"</span>);<span class="hljs-comment">// 从上下文获取 persion Bean</span>Persion persion = (Persion) applicationContext.getBean(<span class="hljs-string">"persion"</span>);<span class="hljs-comment">// 查看获取的Persion</span>System.out.println(persion);&#125;&#125;</code></pre><h3 id="二、Spring-Bean配置"><a href="#二、Spring-Bean配置" class="headerlink" title="二、Spring Bean配置"></a>二、Spring Bean配置</h3><ul><li>1、IOC 和 DI 概述</li></ul><blockquote><p><code>IOC(Inversion of Control)</code>：其思想是反转资源获取的方向.传统的资源查找方式要求组件向容器发起请求查找资源. 作为回应, 容器适时的返回资源. 而应用了 <code>IOC</code> 之后,则是容器主动地将资源推送给它所管理的组件,组件所要做的仅是选择一种合适的方式来接受资源. 这种行为也被称为查找的被动形式 <code>DI(Dependency Injection)</code> - <code>IOC</code> 的另一种表述方式：即组件以一些预先定义好的方式(例如: setter 方法)接受来自如容器的资源注入. 相对于 <code>IOC</code> 而言，这种表述更直接。</p></blockquote><ul><li>2、配置Bean</li></ul><blockquote><p>Spring通过在xml配置文件中的 <code>&lt;bean&gt;</code> 节点来配置放入IOC容器中的bean；</p></blockquote><pre><code class="hljs xml"><span class="hljs-tag">&lt;<span class="hljs-name">bean</span> <span class="hljs-attr">id</span>=<span class="hljs-string">"persion"</span> <span class="hljs-attr">name</span>=<span class="hljs-string">"persion"</span> <span class="hljs-attr">class</span>=<span class="hljs-string">"me.mritd.beans.Persion"</span>&gt;</span><span class="hljs-tag">&lt;<span class="hljs-name">property</span> <span class="hljs-attr">name</span>=<span class="hljs-string">"name"</span> <span class="hljs-attr">value</span>=<span class="hljs-string">"Hello World"</span>&gt;</span><span class="hljs-tag">&lt;/<span class="hljs-name">property</span>&gt;</span><span class="hljs-tag">&lt;<span class="hljs-name">property</span> <span class="hljs-attr">name</span>=<span class="hljs-string">"age"</span> <span class="hljs-attr">value</span>=<span class="hljs-string">"20"</span>&gt;</span><span class="hljs-tag">&lt;/<span class="hljs-name">property</span>&gt;</span><span class="hljs-tag">&lt;<span class="hljs-name">property</span> <span class="hljs-attr">name</span>=<span class="hljs-string">"address"</span> <span class="hljs-attr">value</span>=<span class="hljs-string">"伊拉克"</span>&gt;</span><span class="hljs-tag">&lt;/<span class="hljs-name">property</span>&gt;</span><span class="hljs-tag">&lt;/<span class="hljs-name">bean</span>&gt;</span></code></pre><blockquote><ul><li>id: IOC容器中Bean的ID<ul><li>在 IOC 容器中必须是唯一的</li><li>若 id 没有指定，Spring 自动 <strong>将全限定性类名</strong> 作为 Bean 的名字</li><li>id 可以指定多个名字，名字之间可用逗号、分号、或空格分隔</li></ul></li></ul></blockquote><hr><blockquote><p>在 Spring IOC 容器读取 Bean 配置创建 Bean 实例之前,必须对它进行实例化. 只有在容器实例化后, 才可以从 IOC 容器里获取 Bean 实例并使用；Spring 提供了 <strong>两种类型</strong> 的 IOC 容器实现:</p></blockquote><blockquote><ul><li>BeanFactory: IOC 容器的基本实现.</li><li>ApplicationContext: 提供了更多的高级特性. 是 BeanFactory 的子接口.</li></ul></blockquote><blockquote><p><code>BeanFactory</code> 是 Spring 框架的基础设施，面向 Spring 本身；<code>ApplicationContext</code> 面向使用 Spring 框架的开发者，几乎所有的应用场合都直接使用 <code>ApplicationContext</code> 而非底层的 <code>BeanFactory</code>;无论使用何种方式,配置文件时相同的. ApplicationContext UML 图示例如下<br>  <img src="https://cdn.oss.link/markdown/Spring-applicationContext-UML.png" srcset="/img/loading.gif" alt="Spring-applicationContext-UML"></p></blockquote><blockquote><ul><li>ApplicationContext 的主要实现类：<ul><li>ClassPathXmlApplicationContext：从 类路径下加载配置文件</li><li>FileSystemXmlApplicationContext: 从文件系统中加载配置文件</li></ul></li></ul></blockquote><blockquote><ul><li>ConfigurableApplicationContext 扩展于 ApplicationContext，新增加两个主要方法：<code>refresh()</code> 和 <code>close()</code>， 让 ApplicationContext 具有启动、刷新和关闭上下文的能力</li></ul></blockquote><blockquote><ul><li>ApplicationContext <strong>在初始化上下文时就实例化所有单例的 Bean</strong></li></ul></blockquote><blockquote><ul><li>WebApplicationContext 是专门为 WEB 应用而准备的，它允许从相对于 WEB 根目录的路径中完成初始化工作</li></ul></blockquote><ul><li><p>3、Spring IOC/DI 注入方式:</p><ul><li>属性注入</li></ul><blockquote><p>通过Set方法注入，使用<property >标签设置，name属性指定属性名，value指定属性值</p></blockquote><ul><li>构造器注入</li></ul><blockquote><p>通过有参构造器进行注入，使用<constructor-arg> 标签，构造器注入既可以按照索引注入，也可是按照参数类型注入:</p></blockquote><blockquote><ul><li>按索引匹配注入<pre><code class="hljs xml"><span class="hljs-comment">&lt;!-- 构造器注入(索引方式) --&gt;</span><span class="hljs-tag">&lt;<span class="hljs-name">bean</span> <span class="hljs-attr">id</span>=<span class="hljs-string">"persion"</span> <span class="hljs-attr">name</span>=<span class="hljs-string">"persion"</span> <span class="hljs-attr">class</span>=<span class="hljs-string">"me.mritd.beans.Persion"</span>&gt;</span><span class="hljs-tag">&lt;<span class="hljs-name">constructor-arg</span> <span class="hljs-attr">value</span>=<span class="hljs-string">"zhangsan"</span> <span class="hljs-attr">index</span>=<span class="hljs-string">"0"</span>&gt;</span><span class="hljs-tag">&lt;/<span class="hljs-name">constructor-arg</span>&gt;</span><span class="hljs-tag">&lt;<span class="hljs-name">constructor-arg</span> <span class="hljs-attr">value</span>=<span class="hljs-string">"10"</span> <span class="hljs-attr">index</span>=<span class="hljs-string">"1"</span>&gt;</span><span class="hljs-tag">&lt;/<span class="hljs-name">constructor-arg</span>&gt;</span><span class="hljs-tag">&lt;<span class="hljs-name">constructor-arg</span> <span class="hljs-attr">value</span>=<span class="hljs-string">"伊拉克"</span> <span class="hljs-attr">index</span>=<span class="hljs-string">"2"</span>&gt;</span><span class="hljs-tag">&lt;/<span class="hljs-name">constructor-arg</span>&gt;</span><span class="hljs-tag">&lt;/<span class="hljs-name">bean</span>&gt;</span></code></pre></li><li>按类型匹配注入<pre><code class="hljs xml"><span class="hljs-comment">&lt;!-- 构造器注入(按照类型) --&gt;</span><span class="hljs-tag">&lt;<span class="hljs-name">bean</span> <span class="hljs-attr">id</span>=<span class="hljs-string">"persion1"</span> <span class="hljs-attr">name</span>=<span class="hljs-string">"persion1"</span> <span class="hljs-attr">class</span>=<span class="hljs-string">"me.mritd.beans.Persion"</span>&gt;</span><span class="hljs-tag">&lt;<span class="hljs-name">constructor-arg</span> <span class="hljs-attr">value</span>=<span class="hljs-string">"zhangsan1"</span> <span class="hljs-attr">type</span>=<span class="hljs-string">"java.lang.String"</span>&gt;</span><span class="hljs-tag">&lt;/<span class="hljs-name">constructor-arg</span>&gt;</span><span class="hljs-tag">&lt;<span class="hljs-name">constructor-arg</span> <span class="hljs-attr">value</span>=<span class="hljs-string">"11"</span> <span class="hljs-attr">type</span>=<span class="hljs-string">"java.lang.Integer"</span>&gt;</span><span class="hljs-tag">&lt;/<span class="hljs-name">constructor-arg</span>&gt;</span><span class="hljs-tag">&lt;<span class="hljs-name">constructor-arg</span> <span class="hljs-attr">value</span>=<span class="hljs-string">"伊拉克1"</span> <span class="hljs-attr">type</span>=<span class="hljs-string">"java.lang.String"</span>&gt;</span><span class="hljs-tag">&lt;/<span class="hljs-name">constructor-arg</span>&gt;</span><span class="hljs-tag">&lt;/<span class="hljs-name">bean</span>&gt;</span></code></pre></li><li>工厂方法注入（很少使用，不推荐）</li></ul></blockquote></li><li><p>4、Sping bean 引用其他 bean 配置</p></li></ul><blockquote><p>首先定义两个 <code>Java Bean</code></p></blockquote><pre><code class="hljs java"><span class="hljs-keyword">package</span> me.mritd.beans;<span class="hljs-comment">/**</span><span class="hljs-comment"> *</span><span class="hljs-comment"> * Copyright © 2016 Mritd. All rights reserved.</span><span class="hljs-comment"> *</span><span class="hljs-comment"> * <span class="hljs-doctag">@ClassName</span>: Car</span><span class="hljs-comment"> * <span class="hljs-doctag">@Description</span>: TODO</span><span class="hljs-comment"> * <span class="hljs-doctag">@author</span>: 漠然</span><span class="hljs-comment"> * <span class="hljs-doctag">@date</span>: 2016年1月2日 上午1:09:46</span><span class="hljs-comment"> */</span><span class="hljs-keyword">public</span> <span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">Car</span> </span>&#123;<span class="hljs-keyword">private</span> String carName;<span class="hljs-function"><span class="hljs-keyword">public</span> String <span class="hljs-title">getCarName</span><span class="hljs-params">()</span> </span>&#123;<span class="hljs-keyword">return</span> carName;&#125;<span class="hljs-function"><span class="hljs-keyword">public</span> <span class="hljs-keyword">void</span> <span class="hljs-title">setCarName</span><span class="hljs-params">(String carName)</span> </span>&#123;<span class="hljs-keyword">this</span>.carName = carName;&#125;<span class="hljs-meta">@Override</span><span class="hljs-function"><span class="hljs-keyword">public</span> String <span class="hljs-title">toString</span><span class="hljs-params">()</span> </span>&#123;<span class="hljs-keyword">return</span> <span class="hljs-string">"Car [carName="</span> + carName + <span class="hljs-string">"]"</span>;&#125;<span class="hljs-function"><span class="hljs-keyword">public</span> <span class="hljs-title">Car</span><span class="hljs-params">()</span> </span>&#123;<span class="hljs-keyword">super</span>();<span class="hljs-comment">// TODO Auto-generated constructor stub</span>&#125;<span class="hljs-function"><span class="hljs-keyword">public</span> <span class="hljs-title">Car</span><span class="hljs-params">(String carName)</span> </span>&#123;<span class="hljs-keyword">super</span>();<span class="hljs-keyword">this</span>.carName = carName;&#125;&#125;</code></pre><pre><code class="hljs java"><span class="hljs-keyword">package</span> me.mritd.beans;<span class="hljs-comment">/**</span><span class="hljs-comment"> *</span><span class="hljs-comment"> * Copyright © 2016 Mritd. All rights reserved.</span><span class="hljs-comment"> *</span><span class="hljs-comment"> * <span class="hljs-doctag">@ClassName</span>: Persion</span><span class="hljs-comment"> * <span class="hljs-doctag">@Description</span>: TODO</span><span class="hljs-comment"> * <span class="hljs-doctag">@author</span>: 漠然</span><span class="hljs-comment"> * <span class="hljs-doctag">@date</span>: 2016年1月2日 上午1:11:21</span><span class="hljs-comment"> */</span><span class="hljs-keyword">public</span> <span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">Persion</span> </span>&#123;<span class="hljs-keyword">private</span> String name;<span class="hljs-keyword">private</span> <span class="hljs-keyword">int</span> age;<span class="hljs-keyword">private</span> String address;<span class="hljs-keyword">private</span> Car car;<span class="hljs-meta">@Override</span><span class="hljs-function"><span class="hljs-keyword">public</span> String <span class="hljs-title">toString</span><span class="hljs-params">()</span> </span>&#123;<span class="hljs-keyword">return</span> <span class="hljs-string">"Persion [name="</span> + name + <span class="hljs-string">", age="</span> + age + <span class="hljs-string">", address="</span> + address + <span class="hljs-string">", car="</span> + car + <span class="hljs-string">"]"</span>;&#125;<span class="hljs-function"><span class="hljs-keyword">public</span> <span class="hljs-title">Persion</span><span class="hljs-params">()</span> </span>&#123;<span class="hljs-keyword">super</span>();<span class="hljs-comment">// TODO Auto-generated constructor stub</span>&#125;<span class="hljs-function"><span class="hljs-keyword">public</span> <span class="hljs-title">Persion</span><span class="hljs-params">(String name, <span class="hljs-keyword">int</span> age, String address, Car car)</span> </span>&#123;<span class="hljs-keyword">super</span>();<span class="hljs-keyword">this</span>.name = name;<span class="hljs-keyword">this</span>.age = age;<span class="hljs-keyword">this</span>.address = address;<span class="hljs-keyword">this</span>.car = car;&#125;<span class="hljs-function"><span class="hljs-keyword">public</span> String <span class="hljs-title">getName</span><span class="hljs-params">()</span> </span>&#123;<span class="hljs-keyword">return</span> name;&#125;<span class="hljs-function"><span class="hljs-keyword">public</span> <span class="hljs-keyword">void</span> <span class="hljs-title">setName</span><span class="hljs-params">(String name)</span> </span>&#123;<span class="hljs-keyword">this</span>.name = name;&#125;<span class="hljs-function"><span class="hljs-keyword">public</span> <span class="hljs-keyword">int</span> <span class="hljs-title">getAge</span><span class="hljs-params">()</span> </span>&#123;<span class="hljs-keyword">return</span> age;&#125;<span class="hljs-function"><span class="hljs-keyword">public</span> <span class="hljs-keyword">void</span> <span class="hljs-title">setAge</span><span class="hljs-params">(<span class="hljs-keyword">int</span> age)</span> </span>&#123;<span class="hljs-keyword">this</span>.age = age;&#125;<span class="hljs-function"><span class="hljs-keyword">public</span> String <span class="hljs-title">getAddress</span><span class="hljs-params">()</span> </span>&#123;<span class="hljs-keyword">return</span> address;&#125;<span class="hljs-function"><span class="hljs-keyword">public</span> <span class="hljs-keyword">void</span> <span class="hljs-title">setAddress</span><span class="hljs-params">(String address)</span> </span>&#123;<span class="hljs-keyword">this</span>.address = address;&#125;<span class="hljs-function"><span class="hljs-keyword">public</span> Car <span class="hljs-title">getCar</span><span class="hljs-params">()</span> </span>&#123;<span class="hljs-keyword">return</span> car;&#125;<span class="hljs-function"><span class="hljs-keyword">public</span> <span class="hljs-keyword">void</span> <span class="hljs-title">setCar</span><span class="hljs-params">(Car car)</span> </span>&#123;<span class="hljs-keyword">this</span>.car = car;&#125;&#125;</code></pre><blockquote><p>配置 Bean 引用</p></blockquote><pre><code class="hljs xml"><span class="hljs-meta">&lt;?xml version="1.0" encoding="UTF-8"?&gt;</span><span class="hljs-tag">&lt;<span class="hljs-name">beans</span> <span class="hljs-attr">xmlns</span>=<span class="hljs-string">"http://www.springframework.org/schema/beans"</span></span><span class="hljs-tag"><span class="hljs-attr">xmlns:xsi</span>=<span class="hljs-string">"http://www.w3.org/2001/XMLSchema-instance"</span></span><span class="hljs-tag"><span class="hljs-attr">xsi:schemaLocation</span>=<span class="hljs-string">"http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd"</span>&gt;</span><span class="hljs-comment">&lt;!-- 首先定义没有引用的 Bean --&gt;</span><span class="hljs-tag">&lt;<span class="hljs-name">bean</span> <span class="hljs-attr">id</span>=<span class="hljs-string">"car"</span> <span class="hljs-attr">class</span>=<span class="hljs-string">"me.mritd.beans.Car"</span>&gt;</span><span class="hljs-tag">&lt;<span class="hljs-name">property</span> <span class="hljs-attr">name</span>=<span class="hljs-string">"carName"</span> <span class="hljs-attr">value</span>=<span class="hljs-string">"四个圈"</span>&gt;</span><span class="hljs-tag">&lt;/<span class="hljs-name">property</span>&gt;</span><span class="hljs-tag">&lt;/<span class="hljs-name">bean</span>&gt;</span><span class="hljs-tag">&lt;<span class="hljs-name">bean</span> <span class="hljs-attr">id</span>=<span class="hljs-string">"persion"</span> <span class="hljs-attr">name</span>=<span class="hljs-string">"persion"</span> <span class="hljs-attr">class</span>=<span class="hljs-string">"me.mritd.beans.Persion"</span>&gt;</span><span class="hljs-comment">&lt;!-- 采用构造器注入(索引位置)的方式 --&gt;</span><span class="hljs-tag">&lt;<span class="hljs-name">constructor-arg</span> <span class="hljs-attr">index</span>=<span class="hljs-string">"0"</span> <span class="hljs-attr">value</span>=<span class="hljs-string">"zhangsan"</span>&gt;</span><span class="hljs-tag">&lt;/<span class="hljs-name">constructor-arg</span>&gt;</span><span class="hljs-tag">&lt;<span class="hljs-name">constructor-arg</span> <span class="hljs-attr">index</span>=<span class="hljs-string">"1"</span> <span class="hljs-attr">value</span>=<span class="hljs-string">"10"</span>&gt;</span><span class="hljs-tag">&lt;/<span class="hljs-name">constructor-arg</span>&gt;</span><span class="hljs-comment">&lt;!-- 显示指定 某个属性值为null 使用 &lt;null/&gt;标签 --&gt;</span><span class="hljs-tag">&lt;<span class="hljs-name">constructor-arg</span> <span class="hljs-attr">index</span>=<span class="hljs-string">"2"</span>&gt;</span><span class="hljs-tag">&lt;<span class="hljs-name">null</span>/&gt;</span><span class="hljs-tag">&lt;/<span class="hljs-name">constructor-arg</span>&gt;</span><span class="hljs-comment">&lt;!-- 使用 ref 属性指定 引用 Bean --&gt;</span><span class="hljs-tag">&lt;<span class="hljs-name">constructor-arg</span> <span class="hljs-attr">index</span>=<span class="hljs-string">"3"</span> <span class="hljs-attr">ref</span>=<span class="hljs-string">"car"</span>&gt;</span><span class="hljs-tag">&lt;/<span class="hljs-name">constructor-arg</span>&gt;</span><span class="hljs-tag">&lt;/<span class="hljs-name">bean</span>&gt;</span><span class="hljs-tag">&lt;<span class="hljs-name">bean</span> <span class="hljs-attr">id</span>=<span class="hljs-string">"persion1"</span> <span class="hljs-attr">name</span>=<span class="hljs-string">"persion1"</span> <span class="hljs-attr">class</span>=<span class="hljs-string">"me.mritd.beans.Persion"</span>&gt;</span><span class="hljs-comment">&lt;!-- 采用属性注入 --&gt;</span><span class="hljs-tag">&lt;<span class="hljs-name">property</span> <span class="hljs-attr">name</span>=<span class="hljs-string">"name"</span> <span class="hljs-attr">value</span>=<span class="hljs-string">"zhangsan1"</span>&gt;</span><span class="hljs-tag">&lt;/<span class="hljs-name">property</span>&gt;</span><span class="hljs-tag">&lt;<span class="hljs-name">property</span> <span class="hljs-attr">name</span>=<span class="hljs-string">"age"</span> <span class="hljs-attr">value</span>=<span class="hljs-string">"11"</span>&gt;</span><span class="hljs-tag">&lt;/<span class="hljs-name">property</span>&gt;</span><span class="hljs-tag">&lt;<span class="hljs-name">property</span> <span class="hljs-attr">name</span>=<span class="hljs-string">"address"</span> <span class="hljs-attr">value</span>=<span class="hljs-string">"伊拉克"</span>&gt;</span><span class="hljs-tag">&lt;/<span class="hljs-name">property</span>&gt;</span><span class="hljs-tag">&lt;<span class="hljs-name">property</span> <span class="hljs-attr">name</span>=<span class="hljs-string">"car"</span> <span class="hljs-attr">ref</span>=<span class="hljs-string">"car"</span>&gt;</span><span class="hljs-tag">&lt;/<span class="hljs-name">property</span>&gt;</span><span class="hljs-tag">&lt;/<span class="hljs-name">bean</span>&gt;</span><span class="hljs-tag">&lt;/<span class="hljs-name">beans</span>&gt;</span></code></pre><blockquote><p>测试获取两个 Persion Bean，打印里面的 Car</p></blockquote><pre><code class="hljs java"><span class="hljs-keyword">package</span> me.mritd.test;<span class="hljs-keyword">import</span> org.junit.Test;<span class="hljs-keyword">import</span> org.springframework.context.ApplicationContext;<span class="hljs-keyword">import</span> org.springframework.context.support.ClassPathXmlApplicationContext;<span class="hljs-keyword">import</span> me.mritd.beans.Persion;<span class="hljs-comment">/**</span><span class="hljs-comment"> *</span><span class="hljs-comment"> * Copyright © 2016 Mritd. All rights reserved.</span><span class="hljs-comment"> *</span><span class="hljs-comment"> * <span class="hljs-doctag">@ClassName</span>: Test2</span><span class="hljs-comment"> * <span class="hljs-doctag">@Description</span>: TODO</span><span class="hljs-comment"> * <span class="hljs-doctag">@author</span>: 漠然</span><span class="hljs-comment"> * <span class="hljs-doctag">@date</span>: 2016年1月2日 上午1:20:49</span><span class="hljs-comment"> */</span><span class="hljs-keyword">public</span> <span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">Test2</span> </span>&#123;<span class="hljs-meta">@Test</span><span class="hljs-function"><span class="hljs-keyword">public</span> <span class="hljs-keyword">void</span> <span class="hljs-title">test2</span><span class="hljs-params">()</span></span>&#123;<span class="hljs-comment">// 创建 ApplicationContext</span>ApplicationContext applicationContext = <span class="hljs-keyword">new</span> ClassPathXmlApplicationContext(<span class="hljs-string">"applicationContext.xml"</span>);<span class="hljs-comment">// 获取Bean 测试</span>Persion persion = (Persion) applicationContext.getBean(<span class="hljs-string">"persion"</span>);Persion persion1 = (Persion) applicationContext.getBean(<span class="hljs-string">"persion1"</span>);System.out.println(persion);System.out.println(persion1);&#125;&#125;</code></pre>]]></content>
    
    
    <categories>
      
      <category>Java</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Java</tag>
      
      <tag>Spring</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>SQL中 OR 关键字优先级问题</title>
    <link href="/2006/01/02/sql-or-keyword-priority/"/>
    <url>/2006/01/02/sql-or-keyword-priority/</url>
    
    <content type="html"><![CDATA[<blockquote><p>今天群里哥们偶尔问道一个问题，顺便查查找到了答案，记录一下。</p></blockquote><h2 id="问题"><a href="#问题" class="headerlink" title="问题"></a>问题</h2><p>问题大致如下:</p><p>SQL1</p><pre><code class="hljs sql"><span class="hljs-keyword">SELECT</span> * <span class="hljs-keyword">FROM</span> <span class="hljs-keyword">TABLE</span> <span class="hljs-keyword">WHERE</span> <span class="hljs-keyword">NAME</span>=<span class="hljs-string">'ZHANGSAN'</span> <span class="hljs-keyword">OR</span> <span class="hljs-number">1</span>=<span class="hljs-number">1</span> <span class="hljs-keyword">OR</span> <span class="hljs-string">'1'</span>=<span class="hljs-string">'1'</span> <span class="hljs-keyword">AND</span> AGE=<span class="hljs-number">10</span>;</code></pre><p>SQL2</p><pre><code class="hljs sql"><span class="hljs-keyword">SELECT</span> * <span class="hljs-keyword">FROM</span> <span class="hljs-keyword">TABLE</span> <span class="hljs-keyword">WHERE</span> <span class="hljs-keyword">NAME</span>=<span class="hljs-string">'ZAHNGSAN'</span> <span class="hljs-keyword">OR</span> <span class="hljs-number">1</span>=<span class="hljs-number">1</span> <span class="hljs-keyword">AND</span> AGE=<span class="hljs-number">10</span>;</code></pre><p><strong>两条SQL，第一条返回全表结果集，第二条返回 <code>NAME=&#39;ZHANGSAN&#39;</code> 或者 <code>AGE=10</code> 的结果集；问两条SQL为何会这样。</strong></p><h2 id="原因"><a href="#原因" class="headerlink" title="原因"></a>原因</h2><p>以前一直没留意过，搜了下资料，大致原因如下:</p><p><strong>在 SQL 中，<code>OR</code> 关键字的优先级会比 <code>AND</code> 关键字优先级要高，<code>OR</code>关键字会将条件切分成前后两部分，大致表现如下：</strong></p><p>SQL1</p><pre><code class="hljs sql"><span class="hljs-keyword">SELECT</span> * <span class="hljs-keyword">FROM</span> <span class="hljs-keyword">TABLE</span> <span class="hljs-keyword">WHERE</span> (<span class="hljs-keyword">NAME</span>=<span class="hljs-string">'ZHANGSAN'</span>) <span class="hljs-keyword">OR</span> ((<span class="hljs-number">1</span>=<span class="hljs-number">1</span>) <span class="hljs-keyword">OR</span> (<span class="hljs-string">'1'</span>=<span class="hljs-string">'1'</span> <span class="hljs-keyword">AND</span> AGE=<span class="hljs-number">10</span>));</code></pre><p><strong>所以在第一个 <code>OR</code> 分割后，后面又进行了二次分割；此时后面由于 <code>(&#39;1&#39;=&#39;1&#39;)</code> 始终为真，则后半部分一直返回为真，此时第一个 <code>OR</code> 分割整体也会返回为真，造成查出整个表的结果集；SQL2同理一样。</strong></p>]]></content>
    
    
    <categories>
      
      <category>Database</category>
      
    </categories>
    
    
    <tags>
      
      <tag>SQL</tag>
      
      <tag>Oracle</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>vim 笔记</title>
    <link href="/2006/01/02/vim-note/"/>
    <url>/2006/01/02/vim-note/</url>
    
    <content type="html"><![CDATA[<p><img src="https://cdn.oss.link/markdown/hexo_vim_logo.png" srcset="/img/loading.gif" alt="vim_logo"></p><h2 id="一、Vim-模式"><a href="#一、Vim-模式" class="headerlink" title="一、Vim 模式"></a>一、Vim 模式</h2><ul><li>编辑模式 : 键盘操作通常被解析为编辑命令</li><li>输入模式 : 大部分键盘操作被存储到文档中</li><li>末行模式 : 用于解释运行 Vim 内置命令</li></ul><h2 id="二、模式切换"><a href="#二、模式切换" class="headerlink" title="二、模式切换"></a>二、模式切换</h2><h3 id="1、-编辑模式-–-gt-输入模式"><a href="#1、-编辑模式-–-gt-输入模式" class="headerlink" title="1、 编辑模式 –&gt; 输入模式"></a>1、 编辑模式 –&gt; 输入模式</h3><ul><li><code>i</code> : 在当前光标所在处输入</li><li><code>I</code> : 在当前光标所在行的行首输入</li><li><code>a</code> : 在当前光标所在处后方输入</li><li><code>A</code> : 在当前光标所在行行尾输入</li><li><code>o</code> : 在光标所在行的下方新增空白行</li><li><code>O</code> : 在当前所在行的上方新增空白行</li></ul><h3 id="2、输入模式-–-gt-编辑模式"><a href="#2、输入模式-–-gt-编辑模式" class="headerlink" title="2、输入模式 –&gt; 编辑模式"></a>2、输入模式 –&gt; 编辑模式</h3><ul><li><code>ESC</code> : 从输入模式返回编辑模式</li></ul><h3 id="3、编辑模式-–-gt-末行模式"><a href="#3、编辑模式-–-gt-末行模式" class="headerlink" title="3、编辑模式 –&gt; 末行模式"></a>3、编辑模式 –&gt; 末行模式</h3><ul><li><code>:</code> : 从编辑模式进入末行模式，两次 ESC 返回到编辑模式</li></ul><h2 id="三、Vim-常用命令"><a href="#三、Vim-常用命令" class="headerlink" title="三、Vim 常用命令"></a>三、Vim 常用命令</h2><h3 id="1、退出"><a href="#1、退出" class="headerlink" title="1、退出"></a>1、退出</h3><ul><li><code>:q!</code> : 不保存退出</li><li><code>:wq</code> : 保存并退出</li><li><code>:x</code> : 保存并退出</li><li><code>ZZ</code> : 保存并退出(编辑模式)</li><li><code>:wq!</code> : 强制保存并退出</li><li><code>:qall</code> : 多文件打开时退出所有文件</li><li><code>:wqall</code> : 保存并退出所有文件</li></ul><h3 id="2、打开"><a href="#2、打开" class="headerlink" title="2、打开"></a>2、打开</h3><ul><li><p><code>vim FILE1 FILE2 ...</code> : 打开多个文件</p><ul><li><code>:next</code> : 跳转到下一个文件</li><li><code>:first</code> : 跳转到第一个文件</li><li><code>:last</code> : 跳转到最后一个文件</li><li><code>:prev</code> : 跳转到前一个文件</li></ul></li><li><p><code>vim -o FILE1 FILE2 ...</code> : 以上下分屏模式打开所有文件</p></li><li><p><code>vim -O FILE1 FILE2 ...</code> : 一左右分屏方式打开所有文件</p><ul><li><code>Ctrl+W</code> 在按上下/左右箭头 切换编辑区</li></ul></li><li><p><code>Ctrl+w,s</code> : 对当前文件进行上下分屏显示，类似 <code>-o</code></p></li><li><p><code>Ctrl+w,v</code> : 对当前文件进行左右分屏显示，类似 <code>-O</code></p></li><li><p><code>vim +# FILE</code> : 打开文件，并将光标定位到 # 行位置，不加数字则代表定位在文档末尾</p></li></ul><h3 id="3、光标移动"><a href="#3、光标移动" class="headerlink" title="3、光标移动"></a>3、光标移动</h3><p><strong>编辑模式下</strong></p><ul><li><code>[#] h</code> : 向左移动[#]字符，不写默认1</li><li><code>[#] l</code> : 向右移动[#]字符，不写默认1</li><li><code>[#] j</code> : 向下移动[#]行，不写默认1</li><li><code>[#] k</code> ; 向上移动[#]行，不写默认1</li><li><code>w</code> : 跳至下一个单词的词首</li><li><code>b</code> : 跳至当前/上一个单词词首</li><li><code>e</code> : 跳至当前/下一个单词词尾</li><li><code>^</code> : 跳转到当前行首</li><li><code>$</code> : 跳转到当前行尾(包含空白行)</li><li><code>0</code> : 跳转到当前行首(包含空白行)</li><li><code>)</code> : 跳转到下一句行尾</li><li><code>(</code> : 跳转到上一句行首</li><li><code>}</code> : 跳转到下一段行尾</li><li><code>{</code> : 跳转到上一段行首</li><li><code>[#] G</code> : 跳转到指定行，不写则默认文档最后一行</li><li><code>Ctrl+f</code> : 向下翻一屏</li><li><code>Ctrl+b</code> : 向上翻一屏</li><li><code>Ctrl+d</code> : 向下翻半屛</li><li><code>Ctrl+u</code> : 向上翻半屛</li></ul><h3 id="4、编辑"><a href="#4、编辑" class="headerlink" title="4、编辑"></a>4、编辑</h3><ul><li><code>[#] x</code> : 删除光标所在处其后的 # 个字符，不写删除当前字符</li><li><code>r 字符</code> : 替换光标所在处为指定字符</li><li><code>d [***]</code> : 删除命令，可配合光标跳转命令删除字符<ul><li><code>d w</code> : 删除光标后一个单词</li><li><code>d b</code> : 删除光标前一个单词</li><li><code>d $</code> : 删除光标后到行尾所有内容</li><li><code>d ^</code> : 删除光标到行首所有内容</li><li><code>[#] d d</code> : 删除光标所在处后 # 行，不写删除当前行</li></ul></li></ul><p><strong>末行模式下</strong></p><ul><li><p><code>START,END d</code> : 删除 START 到 END 行内容</p></li><li><p><code>SATRT,+# d</code> : 删除 START 行开始向后的 # 行内容</p></li><li><p><code>+#,END d</code> : 删除从 END 行开始向上的前 # 行内容</p></li><li><p><code>$ d</code> : 删除当前行开始到文件末尾</p></li><li><p><code>. d</code> : 删除当前行</p></li><li><p><code>.,$-2</code> : <strong>支持组合删除，前面的代表删除当前行开始，到文件末尾-2 行的所有内容</strong></p></li><li><p><code>% d</code> : 删除全文</p></li><li><p><code>/pat1/,/pat2/ d</code> : 从光标所在行开始，删除从第一次被 pat1 匹配到的行开始到第一次被 pat2 匹配带的行结束 中间的所有行</p></li><li><p><code>c [***]</code> : 改变命令，先删除内容再切换到输入模式，同 <code>d</code> 命令相同，可配合光标跳转命令改变字符</p><ul><li><code>[#] c c</code> : 删除光标所在处 # 行并进入输入模式，不写默认删除当前行并进入输入模式</li></ul></li><li><p><code>y [***]</code> : 复制命令，配合光标跳转命令使用，<code>[#] yy</code> 复制当前行开始的向下 # 行，不写默认复制当前行</p></li><li><p><code>p</code> : 粘贴当前光标所在位置的后面(可能是行或者单词，取决于复制的内容)</p></li><li><p><code>P</code> : 粘贴当前光标所在位置的前面(可能是行或者单词，取决于复制的内容)</p></li><li><p><code>u</code> : 撤销修改</p></li><li><p><code>Ctrl+r</code> : 撤销此前的撤销命令</p></li><li><p><code>.</code> : 重复此前命令</p></li></ul><h3 id="5、查找"><a href="#5、查找" class="headerlink" title="5、查找"></a>5、查找</h3><ul><li><p><code>/keyword</code> : 向下查找</p></li><li><p><code>?keyword</code> : 向上查找</p></li><li><p><code>n</code> : 向下循环查找</p></li><li><p><code>N</code> : 向上循环查找</p></li><li><p><code>set hlsearch</code> : 开启高亮查找</p></li><li><p><code>set nohlsearch</code> : 关闭高亮查找</p></li></ul><h3 id="6、可视化模式"><a href="#6、可视化模式" class="headerlink" title="6、可视化模式"></a>6、可视化模式</h3><ul><li><code>v</code> : 进入可视化，按照单词选择</li><li><code>V</code> : 进入可视化，按照行选择</li></ul><h2 id="四、窗口属性设置"><a href="#四、窗口属性设置" class="headerlink" title="四、窗口属性设置"></a>四、窗口属性设置</h2><ul><li><code>set nu</code> : 显示行号</li><li><code>set nonu</code> : 取消显示行号</li><li><code>set ai</code> : 自动缩进</li><li><code>set noai</code> : 取消自动缩进</li><li><code>set ic</code> : 搜索忽略大小写</li><li><code>set noic</code> : 取消搜索忽略大小写</li><li><code>set sm</code> : 显示与之匹配的括号</li><li><code>set nosm</code> : 取消与之显示的括号</li><li><code>syntax on</code> : 开启语法高亮</li><li><code>syntax off</code> : 关闭语法高亮</li><li><code>set hlsearch</code> : 开启搜索高亮</li><li><code>set nohlsearch</code> : 关闭搜索高亮</li></ul><h2 id="五、配置文件"><a href="#五、配置文件" class="headerlink" title="五、配置文件"></a>五、配置文件</h2><ul><li>全局配置文件存放于 <code>/etc/vimrc</code></li><li>用户配置文件存放于 <code>~/.vimrc</code></li></ul><h2 id="六、查找替换"><a href="#六、查找替换" class="headerlink" title="六、查找替换"></a>六、查找替换</h2><p><strong>末行模式下</strong></p><ul><li><p><code>命令格式</code> : <code>地址定界s/查找模式/替换内容/gi</code></p><ul><li><code>s</code> : 替换命令，可换成删除 <code>d</code> 等</li><li><code>g</code> : global，全局操作，<strong>默认的 vim 搜索替换只匹配每行中的第一个被模式匹配的字符，如果有多个则忽略，g 代表全部替换</strong></li><li><code>i</code> : 忽略大小写</li></ul></li><li><p><strong>对于分隔符 <code>/</code> 可以自定义，默认的 vim 认为紧跟在编辑命令 <code>s</code> 后的字符即为分隔符，所以为了避免路径替换时对 <code>/</code> 做转义可以使用自定义格式，如 <code>地址定界s@匹配模式@替换内容@gi</code></strong></p></li><li><p><code>&amp;</code> : 用于引用前面 <strong>查找模式</strong> 所匹配到的所有字符，<strong>通常用于插入式替换</strong>，如在非注释行前加入 <code>#</code> 注释符 : <code>1,$s/^[^#].*/#&amp;/gi</code></p></li></ul>]]></content>
    
    
    <categories>
      
      <category>Linux</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Linux</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>WebLogic request.getContextPath() 为 null 问题</title>
    <link href="/2006/01/02/weblogic-request-getcontextpath-null/"/>
    <url>/2006/01/02/weblogic-request-getcontextpath-null/</url>
    
    <content type="html"><![CDATA[<p>当使用 Weblogic 作为中间件，并且 Web 项目部署方式为 war 包部署时，jsp 页面<code>request.getContextPath()</code> 将返回 null，此时加入以下代码设置 <code>webRoot</code> 即可：</p><pre><code class="hljs java">String webRoot = request.getSession().getServletContext().getRealPath(<span class="hljs-string">"/"</span>);<span class="hljs-keyword">if</span>(webRoot == <span class="hljs-keyword">null</span>)&#123;    webRoot = <span class="hljs-keyword">this</span>.getClass().getClassLoader().getResource(<span class="hljs-string">"/"</span>).getPath();    webRoot = webRoot.substring(<span class="hljs-number">0</span>,webRoot.indexOf(<span class="hljs-string">"WEB-INF"</span>));&#125;</code></pre>]]></content>
    
    
    <categories>
      
      <category>Java</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Java</tag>
      
      <tag>WebLogic</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>yum 笔记</title>
    <link href="/2006/01/02/yum-note/"/>
    <url>/2006/01/02/yum-note/</url>
    
    <content type="html"><![CDATA[<h2 id="一、软件包管理概述"><a href="#一、软件包管理概述" class="headerlink" title="一、软件包管理概述"></a>一、软件包管理概述</h2><blockquote><p>在漫长的 Linux 发展史中，由于 Linux 系统的特性，所有软件都为绿色的，编译成二进制解压后即可运行，所以软件安装和管理成了一个复杂的体力劳动，而软件包管理器应运而生。</p></blockquote><h3 id="1-1、相关术语概述"><a href="#1-1、相关术语概述" class="headerlink" title="1.1、相关术语概述"></a>1.1、相关术语概述</h3><ul><li>POSIX : Portable Openratin System  跨平台调用</li><li>API 兼容 : 开发库兼容，因此源代码可跨平台</li><li>ABI 兼容 : 编译后的程序可以跨平台</li><li>库 : 可执行程序，本身不能作为程序执行入口，但可以被调用是编译好的二进制格式</li><li>程序的过程 : 预编译、编译、汇编、链接<ul><li>静态链接 : 将库包含在程序中</li><li>动态链接 : dll，so(shared object)</li></ul></li><li>编译 : 源代码翻译成cpu指令集的过程</li></ul><p><strong>软件编译主要考虑以下两个因素 :</strong></p><ul><li>os 平台 : 编译好的应用程序必须为特定平台所支持的版本</li><li>硬件平台 : 应用程序为特定的cpu所支持</li></ul><h3 id="1-2、软件包组成部分"><a href="#1-2、软件包组成部分" class="headerlink" title="1.2、软件包组成部分"></a>1.2、软件包组成部分</h3><ul><li>二进制程序 : 通常为程序源码编译后产生的二进制文件，一般存放于 <code>bin</code>、<code>/sbin</code>、<code>/usr/bin</code>、<code>/usr/sbin</code>、<code>/usr/local/bin</code>、<code>/usr/local/sbin</code></li><li>自身库文件 : 程序自身所用到的库文件，一般存放于 <code>/lib</code>、<code>/lib64</code>、<code>/usr/lib</code>、<code>/usr/lib64</code>、<code>/usr/local/lib</code>、<code>/usr/local/lib64</code>；程序开发时调用 API，运行时调用 ABI，<strong>通过调整 <code>/etc/ld.so.conf.d/*.conf</code> 可改变内核装载的共享库文件</strong></li><li>配置文件 : 配置文件一般存放在 <code>/etc</code> 目录中，同时如果程序安装选择了指定目录，如 <code>/usr/local/nginx</code>，那么此时程序的可执行文件等将可能不会在标准目录中。</li><li>帮助文件 : 帮助文件一般包括 info文件、README、INSTALL、ChangeLog</li></ul><h3 id="1-3、应用管理器功能"><a href="#1-3、应用管理器功能" class="headerlink" title="1.3、应用管理器功能"></a>1.3、应用管理器功能</h3><p>应用管理器一般应该实现以下功能 :</p><ul><li>数据库 : 应用程序管理器应该能够维护一个数据库，数据库中应该维护 <strong>程序名称和版本、安装生成的各文件路径、程序文件校验码、程序间的依赖关系(解决循环依赖、版本依赖等)、提供功能性的说明</strong></li><li>程序组成格式 : 应用程序管理器应当能够提供应用程序的组成说明，如 <strong>文件清单、安装卸载时运行的脚本等</strong></li><li>版本管理 : 应用程序管理器应当能管理应用程序的版本;应用程序源代码版本格式为 <code>name-major.minor.release.tar.gz</code></li></ul><h3 id="1-4、常用的软件包管理器"><a href="#1-4、常用的软件包管理器" class="headerlink" title="1.4、常用的软件包管理器"></a>1.4、常用的软件包管理器</h3><p>常用发行版都有其自己的软件包管理器，如 :</p><ul><li>Debian : .deb 格式，使用 <code>dpkg</code> 管理</li><li>RadHat : .rpm 格式，使用 <code>rmp</code> 管理</li><li>SUSE : .rpm 格式,使用 <code>rpm</code> 管理，<strong>注意，SUSE 的系统结构与 RadHat 不同，虽然都是用 rpm 管理，但其软件包内部文件组织不同，不能通用</strong></li></ul><p>常用的软件包管理器都具备以下功能 : 打包、安装、查询、升级、卸载、校验、数据库管理</p><p><strong>包管理器虽然解决了软件包的管理混乱问题，但是其无法解决包分发等问题，因此出现了基于软件包管理器的前端管理器，如 Debian 系列的 <code>apt-get</code>、RadHat 系列的 <code>yum</code> 等</strong></p><h2 id="二、rpm-使用"><a href="#二、rpm-使用" class="headerlink" title="二、rpm 使用"></a>二、rpm 使用</h2><h3 id="2-1、rpm-术语和概述"><a href="#2-1、rpm-术语和概述" class="headerlink" title="2.1、rpm 术语和概述"></a>2.1、rpm 术语和概述</h3><blockquote><p>rpm 全称 <code>RadHat Package Manager</code>，后经过 Linux 基金委员会吸纳为标准包管理工具，重新定义为 <code>RPM is Package Manager</code>.</p></blockquote><ul><li>rpm 包名称 : name-version-release.arch.rpm<ul><li>release : 通常包含rpm的制作发行号，还包含使用的OS</li><li>OS 平台 : OS 平台即为操作系统平台，如 el6(redhat enterprise linux6)、CentOS5、suse11 等</li><li>硬件平台 : 硬件平台用 arch 表示，常见的如 x86_64、i386、i586、i686、ppc 等，<strong>如软件包无平台要求，则表示为 noarch</strong></li></ul></li><li>分包 : 分包机制即将一个大的应用程序打包为多个 rpm 包，一般包含主包和支包，作用是满足不同用户需要，有些用户可能用不到太多的功能，可选择性安装支包</li><li>来源合法性校验 : 对于软件包来源一般要对源码进行 md5 或 sha1 校验码验证，对于 rpm 包则认为发行商提供的即为合法的，包完整性使用检验码校验，包的来源采用公钥加密机制校验</li><li><strong>rpm 包获取途径</strong> :<ul><li>首先考虑发行商的光盘或站点服务器</li><li>rpmfind : <a href="http://rpmfind.net" target="_blank" rel="noopener">http://rpmfind.net</a></li><li>rpmpbone : <a href="http://rpm.pbone.net" target="_blank" rel="noopener">http://rpm.pbone.net</a></li><li>Fedora EPEL : <strong>对于发行商尚未提供或需要更高版本的 rpm 包可考虑 Fedora 维护的 EPEL 源，但对于 RadHat 企业版某些安全机制要求较高的地方可能会不被信任</strong></li></ul></li></ul><h3 id="2-2、rpm-常用命令"><a href="#2-2、rpm-常用命令" class="headerlink" title="2.2、rpm 常用命令"></a>2.2、rpm 常用命令</h3><h4 id="2-2-1、安装和卸载"><a href="#2-2-1、安装和卸载" class="headerlink" title="2.2.1、安装和卸载"></a>2.2.1、安装和卸载</h4><ul><li><code>rpm -i PACKAGE [...]</code> : -i 即 –install，安装软件包<ul><li><code>-v</code> : 安装时显示安装信息，如进度</li><li><code>-vv</code> : 二级详细显示</li><li><code>-vvv</code> ; 三级详细显示</li><li><code>-h</code> : -h 即 hash，以 # 的形式显示安装进度，通常一个 # 代表 2%</li></ul></li><li><code>rpm -ivh --test PACKAGE</code> : 仅测试，不进行安装</li><li><code>rpm -ivh --nodeps</code> ; 忽略依赖关系，当出现包依赖时，可使用此选项强制安装(装完可能不能用)</li><li><code>rpm -ivh --replacepkgs</code> : 重新安装软件包，<strong>原来的配置文件不会被覆盖，新安装的配置文件将会重命名为以.rpmnew为后缀的文件</strong></li><li><code>rpm -e PACKAGE</code> : -e 即 –erase，卸载指定软件包，如果该软件包被其他程序依赖，<strong>可使用 <code>--nodeps</code> 忽略依赖</strong></li></ul><h4 id="2-2-2、查询"><a href="#2-2-2、查询" class="headerlink" title="2.2.2、查询"></a>2.2.2、查询</h4><ul><li><code>rpm -q PACKAGE</code> : 查询某软件包是否安装，<code>rpm -q</code> 用于查询，一般需要附带查询参数如 <code>a</code></li><li><code>rpm -qa PACKAGE</code> : 查询安装的所有软件包，包名支持通配符，如 <code>rpm -qa php*</code> 查询 php 开头的所有包</li><li><code>rpm -qi PACKAGE</code> : 查询包的描述信息，如下</li></ul><pre><code class="hljs sh">Name        : dockerVersion     : 1.10.3Release     : 44.el7.centosArchitecture: x86_64Install Date: 2016年07月25日 星期一 10时28分19秒Group       : UnspecifiedSize        : 43643976License     : ASL 2.0Signature   : RSA/SHA256, 2016年06月24日 星期五 11时43分08秒, Key ID 24c6a8a7f4a80eb5Source RPM  : docker-1.10.3-44.el7.centos.src.rpmBuild Date  : 2016年06月24日 星期五 08时10分54秒Build Host  : worker1.bsys.centos.orgRelocations : (not relocatable)Packager    : CentOS BuildSystem &lt;http://bugs.centos.org&gt;Vendor      : CentOSURL         : https://github.com/docker/dockerSummary     : Automates deployment of containerized applicationsDescription :Docker is an open-source engine that automates the deployment of anyapplication as a lightweight, portable, self-sufficient container that willrun virtually anywhere.Docker containers can encapsulate any payload, and will run consistently onand between virtually any server. The same container that a developer buildsand tests on a laptop will run at scale, <span class="hljs-keyword">in</span> production*, on VMs, bare-metalservers, OpenStack clusters, public instances, or combinations of the above.</code></pre><ul><li><code>rpm -ql PACKAGE</code> : <strong>查询软件包安装后的文件列表</strong></li><li><code>rpm -qf FILE</code> : <strong>反查询，查询某个文件由那个软件包生成/释放的</strong></li><li><code>rpm -qd PACKAGE</code> : 查询某个包安装完成后生成的文档文件</li><li><code>rpm -qc PACKAGE</code> : <strong>查询某个软件安装后生成的配置文件</strong></li><li><code>rpm -q --scripts</code> : <strong>查询软件爱你安装脚本，脚本一般包含4类，如下:</strong><ul><li>preinstall : 安装前脚本</li><li>postinstall : 安装后脚本</li><li>preuninstall : 卸载前脚本</li><li>postuninstall : 卸载后脚本</li></ul></li><li><code>rpm -qpl PACKAGE</code> : 查询某尚未安装软件包安装后生成的文件列表</li><li><code>rpm -qpi PACKAGE</code> : 查询某尚未安装软件包的描述信息</li></ul><h4 id="2-2-3、升级"><a href="#2-2-3、升级" class="headerlink" title="2.2.3、升级"></a>2.2.3、升级</h4><ul><li><code>rpm -U PACKAGE</code> : 升级或安装软件包(可以与 v、h等一起使用)</li><li><code>rpm -F PACKAGE</code> : 升级软件包(可以与 v、h等一起使用)</li><li><code>rpm -U --force PACKAGE</code> : 强行升级或安装，当当前软件包被其他软件包所依赖、或升级后新版本跟其他软件包冲突时，可使用此选项进行强制升级或安装</li></ul><h4 id="2-2-4、管理"><a href="#2-2-4、管理" class="headerlink" title="2.2.4、管理"></a>2.2.4、管理</h4><ul><li><code>rpm -V PACKAGE</code> : 检验指定软件包是否被修改，以下为修改后返回状态位含义</li></ul><pre><code class="hljs sh">S file Size differs  大小M Mode differs (includes permissions and file <span class="hljs-built_in">type</span>) 头文件5 digest (formerly MD5 sum) differs MD5D Device major/minor number mismatchL readLink(2) path mismatchU User ownership differs 属主G Group ownership differs 属组T mTime differs 修改时间P caPabilities differ 能力改变了</code></pre><ul><li><code>rpm -K PACKAGE</code> : 校验 rmp 完整性和来源(-checksig)<ul><li><code>--nosigeature</code> : 不检查来源合法性</li><li><code>--nodigest</code> : 不检查完整性</li></ul></li><li><code>rpm --import KEY</code> : 导入 rpm 包制作商密钥</li></ul><h4 id="2-2-5、rpm-数据库"><a href="#2-2-5、rpm-数据库" class="headerlink" title="2.2.5、rpm 数据库"></a>2.2.5、rpm 数据库</h4><p><strong>rpm 数据库位置: <code>/var/lib/rpm</code></strong></p><ul><li><code>rpm --initdb</code> : rpm 数据库初始化，<strong>如果事先不存在则新建</strong></li><li><code>rpm --rebuilddb</code> : rpm 数据库重建，直接重建并覆盖原有数据</li></ul><h2 id="三、yum-使用"><a href="#三、yum-使用" class="headerlink" title="三、yum 使用"></a>三、yum 使用</h2><p>yum 全称 Yellowdog Update Modifier，是一个基于 rpm 软件包管理器的前端软件包管理器，其最主要的功能便是能自动管理软件包依赖关系。</p><h3 id="3-1、yum-仓库简介"><a href="#3-1、yum-仓库简介" class="headerlink" title="3.1、yum 仓库简介"></a>3.1、yum 仓库简介</h3><p>yum 仓库顾名思义是一个 rpm 软件仓库，域名安装软件时从仓库下载至本地并安装。yum 仓库实质是一个 rpm 软件包共享服务器，其中主要包含 <strong>rpm 的各种软件包，以及各个软件包之间相互依赖关系的元数据信息</strong>，服务器使用的协议可自行选择，比如 ftp、http、nfs、file 等。</p><p>yum 客户端在使用 yun 仓库时，主要完成以下操作 :</p><ul><li>配置文件 : yum 客户端要能够自行配置使用的 yum 仓库，保证最优的下载速度和 rpm 包质量</li><li>缓存元数据 : yum 客户端要能够缓存 yum 仓库提供的软件包元数据，以便本地快速读取</li><li>分析元数据 : yum 客户端要能够分析理解 yum 仓库提供的元数据，包括软件依赖关系等</li><li>执行具体操作 : yum 客户端要能够根据仓库提供的元数据和用户指令完成各种操作，如 rpm 安装卸载等</li></ul><h3 id="3-2、yum-客户端配置"><a href="#3-2、yum-客户端配置" class="headerlink" title="3.2、yum 客户端配置"></a>3.2、yum 客户端配置</h3><p>yum 客户端配置文件主要存放于 <code>/etc/yum.conf(主配置文件)</code>、<code>/etc/yum.repo.d/*.repo(自定义仓库配置)</code><br>yum 配置文件一般分为 主配置段([main])、仓库配置段([repo])，类似于 Windows 的 ini 文件，<strong>yum 仓库配置示例如下 :</strong></p><pre><code class="hljs sh">[repo_ID]name=Stringbaseurl=仓库的访问路径enabled=&#123;1|0&#125;   1:表示启用  0:表示不启用gpgcheck=&#123;1|0&#125;  1:验证gpg   0:表示不验证gpgkey=公钥地址(可以是本地,也可以是服务器端路径)cost=定义此仓库开销,默认为1000</code></pre><h3 id="3-4、yum-常用命令"><a href="#3-4、yum-常用命令" class="headerlink" title="3.4、yum 常用命令"></a>3.4、yum 常用命令</h3><ul><li><code>yum repolist[all|enabled(默认)|disabled]</code> : 列出所有可用的 yum repo</li><li><code>yum clean [all|packeages(包)|metadata(元数据)|expire-cache(过期数据)|rpmdb(rpm数据)|plugins]</code> : 清理 yum 缓存</li><li><code>yum list [all|installed(已安装过的)|available(可用)]</code> : 列出 yum 软件包</li><li><code>yum info PACKAGE</code> : 显示软件包信息</li><li><code>yum grouplist</code> : 列出所有软件包组</li><li><code>yum groupinfo GROUP</code> : 列出软件包组信息，包括必要的安装软件包和可选的软件包，其中有三个跟开发相关的包组：<ul><li>Desktop Platform Development : 有图形程序时需安装此组</li><li>Server Platform Development</li><li>Development Tools</li><li>如果系统为Centos 5，开发包组为 <code>Development Tools</code> 和 <code>DeveLopment Libraries</code></li><li>如何系统为centos 6，常用的开发包为 <code>Development tools</code> 和 <code>Server Platform Development</code></li></ul></li><li><code>yum install PACKAGE</code> : 安装软件包，<strong>PACKAGE 可为本地 rpm 包路径，此时为安装本地 rpm 包，如包检查不通过可使用 –nogpgcheck 手动禁用包来源和完整性验证</strong></li><li><code>yum reinstall PACKAGE</code> : 重新安装软件包</li><li><code>yum check-update PACKAGE</code> : 检查软件包是否有升级</li><li><code>yum update PACKAGE</code> : 升级指定软件包</li><li><code>yum downgrade PACKAGE</code> : 降级软件包</li><li><code>yum erase|remove PACKAGE</code> : 删除/卸载软件包</li><li><code>yum whatprovides|provides FILE</code> : 查询某文件是有那个软件包提供的</li><li><code>yum groupinstll GROUP</code> : 安装软件包组</li><li><code>yum groupremove GROUP</code> : 删除软件包组</li><li><code>yum history</code> : 查看 yum 历史</li></ul><h3 id="3-5、yum-宏"><a href="#3-5、yum-宏" class="headerlink" title="3.5、yum 宏"></a>3.5、yum 宏</h3><p>yum 配置文件中有很多可用的宏变量，用于动态替换，以下列举常用的 :</p><ul><li>$releasever : 程序的版本,对Yum而言指的是redhat-relrase版本.只替换为主版本号,如Redhat6.5 则替换为6</li><li>$arch : 系统架构</li><li>$basharch : 系统基本架构,如i686,i586等的基本架构为i386</li><li>$YUM0-9 : 在系统定义的环境变量,可以在yum中使用</li></ul><h3 id="3-6、搭建-yum-仓库"><a href="#3-6、搭建-yum-仓库" class="headerlink" title="3.6、搭建 yum 仓库"></a>3.6、搭建 yum 仓库</h3><ul><li>安装仓库创建工具 <code>yum install createrepo -y</code></li><li>准备文件服务器，比如 httpd <code>yum install httpd -y</code></li><li>在 http 的访问目录下创建仓库目录</li><li>复制仓库要存放的 rpm 到创建的目录</li><li>创建仓库元数据 <code>createrepo /var/www/html/openstack(仓库目录)</code></li><li>配置客户端 yum 仓库文件指向自建的 yum 仓库</li><li>为避免仓库中 rpm 包不全最好同时配置 epel 源</li><li>测试使用</li></ul>]]></content>
    
    
    <categories>
      
      <category>Linux</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Linux</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>做自己尊重的人-2015北大最短毕业致辞</title>
    <link href="/2006/01/02/%E5%81%9A%E8%87%AA%E5%B7%B1%E5%B0%8A%E9%87%8D%E7%9A%84%E4%BA%BA-2015%E5%8C%97%E5%A4%A7%E6%9C%80%E7%9F%AD%E6%AF%95%E4%B8%9A%E8%87%B4%E8%BE%9E/"/>
    <url>/2006/01/02/%E5%81%9A%E8%87%AA%E5%B7%B1%E5%B0%8A%E9%87%8D%E7%9A%84%E4%BA%BA-2015%E5%8C%97%E5%A4%A7%E6%9C%80%E7%9F%AD%E6%AF%95%E4%B8%9A%E8%87%B4%E8%BE%9E/</url>
    
    <content type="html"><![CDATA[<p><img src="https://cdn.oss.link/markdown/zuozijizunzhongderen.jpg" srcset="/img/loading.gif" alt="zuozijizunzhongderen"></p><blockquote><p>不会有人逼迫你放弃梦想，也不会有人比你更尊重自己的梦想。</p></blockquote><hr><blockquote><p>在祝福裹着告诫呼啸而来的毕业季，请原谅我不敢祝愿每一位毕业生都成功、都幸福；因为历史不幸地记载着：</p></blockquote><blockquote><p>有人的成功代价是丧失良知；有人的幸福代价是损害他人。</p></blockquote><blockquote><p>从物理学来说，无机的原子逆热力学第二定律出现生物是奇迹；</p></blockquote><blockquote><p>从生物学来说，按进化规律产生遗传信息指导组装人类是奇迹。</p></blockquote><blockquote><p>超越化学反应结果的每一位毕业生都是值得珍惜的奇迹；</p></blockquote><blockquote><p>超越动物欲望总和的每一位毕业生都应做自己尊重的人。</p></blockquote><blockquote><p>过去、现在、将来，能够完全知道个人行为和思想的只有自己；</p></blockquote><blockquote><p>世界很多文化借助宗教信仰来指导人们生活的信念和世俗行为；</p></blockquote><blockquote><p>而对无神论者——也就是大多数中国人——来说，自我尊重是重要的正道。</p></blockquote><blockquote><p>在你们加入社会后看到各种离奇现象，知道自己更多弱点和缺陷，可能还遇到小难大灾后，</p></blockquote><blockquote><p>如何在诱惑和艰难中保持人性的尊严、赢得自己的尊重并非易事，却很值得。</p></blockquote><hr><blockquote><p>这不是：</p></blockquote><blockquote><p>自恋、自大、自负、自夸、自欺、自闭、自缚、自怜；</p></blockquote><blockquote><p>而是：</p></blockquote><blockquote><p>自信、自豪、自量、自知、自省、自赎、自勉、自强。</p></blockquote><blockquote><p>自尊支撑自由的精神、自主的工作、自在的生活。</p></blockquote><blockquote><p>我祝愿：</p></blockquote><blockquote><p>退休之日，你觉得职业中的自己值得尊重；</p></blockquote><blockquote><p>迟暮之年，你感到生活中的自己值得尊重。</p></blockquote><blockquote><p>不要问我如何做到，50年后返校时告诉母校你如何做到：</p></blockquote><blockquote><p>在你所含全部原子再度按热力学第二定律回归自然之前，</p></blockquote><blockquote><p>它们既经历过物性的神奇，</p></blockquote><blockquote><p>也产生过人性的可爱。</p></blockquote><hr><blockquote><p><a href="http://www.iwwenbo.com/?p=290" target="_blank" rel="noopener">原文地址</a></p></blockquote>]]></content>
    
    
    <categories>
      
      <category>随笔</category>
      
    </categories>
    
    
    <tags>
      
      <tag>随笔</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>其实每个人都曾饱经沧桑</title>
    <link href="/2006/01/02/%E5%85%B6%E5%AE%9E%E6%AF%8F%E4%B8%AA%E4%BA%BA%E9%83%BD%E6%9B%BE%E9%A5%B1%E7%BB%8F%E6%B2%A7%E6%A1%91/"/>
    <url>/2006/01/02/%E5%85%B6%E5%AE%9E%E6%AF%8F%E4%B8%AA%E4%BA%BA%E9%83%BD%E6%9B%BE%E9%A5%B1%E7%BB%8F%E6%B2%A7%E6%A1%91/</url>
    
    <content type="html"><![CDATA[<p><img src="https://cdn.oss.link/markdown/hexo_cangsang.png" srcset="/img/loading.gif" alt="hexo_cangsang"></p><blockquote><p>我们误以为只有自己的经历才算壮怀激烈。然后难免夸夸其谈，希望得到更多的认同，来缅怀心中那点被拼命放大的伤口与遗憾。</p></blockquote><hr><p>上学时，班上有个“富二代”男生。</p><p>同学们都很羡慕他，因为感觉他的钱总是多得花不完。穿最贵的名牌衣服，用最好的文具，生日聚会在最豪华的餐厅办，动不动中午就请全班同学吃冰激凌，还是当时我们都不舍得买的“和路雪”。就连班费用完了，班长打个招呼，他立刻掏出钱包补齐，一点儿都不含糊。</p><p>那时候还没有“土豪”或者“有钱任性”这样的说法，但他的确是全班最耀眼的那个人。女生围着他转，男生乐意跟他做朋友，老师都对他和蔼一些，算得上是“天之骄子”了。</p><p>我们坐前后桌，有一次放学，他磨蹭着不动。我问他怎么了，他说不想回家。我问为什么，他说我三妈和四爸都在，今天家里肯定特别乱。</p><p>我第一次听到“三妈”和“四爸”的说法，一头雾水。问他什么是三妈四爸，他竟也没避讳，跟我说了。</p><p>原来他很小的时候父母就离异了，他跟着亲妈生活。亲妈是个女强人，白手起家，先卖鞋子，再卖貂皮，后来居然倒起了房地产，一夜暴富。有钱了，给他找了个继父。结果亲爸闻声又回来骚扰亲妈，称自己是孩子的父亲，要给钱才不闹，亲妈一气之下把他和一笔钱全甩给了亲爸，声称给完钱以后就跟他们父子毫无关系。亲爸同意了，带着他回家。</p><p>亲爸此前已经为他找了个继母，有了钱又嫌人家不够年轻漂亮，随即离了，找了个新女人结婚。继母跑回来问他爸要钱。他那时年纪小还分不太清，索性叫“二妈”和“三妈”。</p><p>他亲妈那边也乱成一团，接连离了两次婚。她并没有真如自己所说那样完全不管他，还会偶尔带着新找的男人来看他，给他塞零花钱，他就叫那些人“二爸”、“三爸”、“四爸”… …后来上了高中，家里太乱，亲妈又出钱给他租了个房子，偶尔让新晋的四爸来给他送钱和东西，每次都留个几万块。</p><p>亲爸知道他有钱，让三妈经常来他这里，硬是要钱回去贴补自己。他烦不胜烦，就想了个办法，把四爸和三妈约在同一天见面。“都不是善茬儿，估计这会儿正打得热闹呢。”他说。</p><p>我听得脑袋轰轰响，乱成糨糊，简直无法想象这是一个十几岁男孩的人生。看上去锦衣玉食，私下的日子却是一团乱麻，甚至已经开始学着应对无解难题。再看他那张貌似满不在乎的脸，却是读出了几分与年龄不符的沉郁和哀伤。</p><p>在泰国普吉的一座小岛上，我认识了一家餐馆的老板。</p><p>他们家店面很小，却在当地非常有名，专卖烤龙虾和椰子饭，味道不错。墙上贴满了顾客们的手写贴纸，有的写“龙虾真好吃”，有的向女朋友表白，还有的画一些稀奇古怪的画和文字… …世界各地的语言都有。店名则叫“LOVE”（爱），听起来就温暖无比。</p><p>每天晚上我们出海潜水回来，披着湿漉漉的头发去他们家吃饭，大老远就能闻到扑鼻的香味。据说老板是土生土长的当地人，从不在白天营业，只是出海晒太阳或是在家休息，晚上才亲自下厨，不多的食材，做完就打烊，端的是轻松洒脱。</p><p>我们都很喜欢这里的店员，永远笑眯眯的，还经常过来问菜合不合口味。有时候来了坏脾气的客人也从不计较，送杯果汁就打发了。</p><p>我想他们应该是世界上生活得最轻松幸福的那类人，经济有保障，没有竞争压力，在如此美丽的小岛度日，心情愉悦，身体健康。这几乎可以称得上是未经任何风浪的完美人生了。</p><p>有次我跟一位店员聊天，说起我的感受。他听过后却笑起来，指着店口的招牌问：“你知道这里为什么叫‘LOVE’吗？”</p><p>“大约是因为纪念某段爱情？”我猜测。</p><p>他摇头：“你记得2004年印尼地震引发的东南亚大海啸吗？”</p><p>我当然记得。那场海啸死了几十万人，举世震惊。李连杰也险些遭难，回国后还因此成立了壹基金。</p><p>他说：“在那场海啸到来之前，我们的确像你形容的那样幸福。”</p><p>那场巨大的灾难突如其来，一夜之间吞没了他的房子和全部财产，变得一无所有。许多亲友被海浪卷走再无音讯，连自己的命也是别人搭救回来。那个时候，他孤零零地站在废墟之中，赤手空拳，身无长物，哭得像个傻瓜，绝望无比。</p><p>幸好，岛上的人互相施以援手，一起盖起房子，重新建了餐馆，他们悉心经营，辛苦工作，依然亏损了很久。好在害怕海啸的客人们在几年后陆续回到了这座岛上，人流渐渐多了起来，才开始盈利。老板把餐馆起名叫“LOVE”就是为了感恩这段经历。</p><p>我听得唏嘘，然而故事并未完结。后来的某个夏天，我重回这座岛时，正遇上一次大规模海啸预警，地震级数与上次一模一样。尽管最后海啸未啸，仍然惊魂不已。我置身其中，更觉震撼与恐惧。</p><p>没想到海啸第二天，那家饭店便开门营业，我惊奇又不解，问店员难道不害怕吗？连续经历几次这样的心惊肉跳，为什么还愿意停留在这座小岛上？真的不要命了吗？</p><p>他的表情很平和，对我说：LOVE的意义不仅仅在于爱身边的人，更是爱着命运的安排。上帝让他们重新活过一次，不是让人学会畏惧，而要学会无畏。在经历生死之后，反而可以坦然安定地生活在这座小岛上。相信一切自然而然地到来，不害怕可能重来的危险，不再顾虑未来行向何方。</p><p>彼时他坐在长尾船上，我们同看着安达曼海缓缓下沉的夕阳。他的笑容融化在身后的金红色霞光中。</p><p>那是我以为的无忧无怖，却不知包容了多少惊涛拍岸，卷起千堆雪。</p><p>她是一所小镇学校的化学老师。貌不惊人，能力平平。她的生活节奏毫无亮点，按部就班，出生、升学、上班、结婚、生子。她从不试图出人头地，没想过一夜暴富，和大多数人一样，只想安安稳稳地过好庸碌的一辈子。</p><p>当我认识她的时候，她的身影几乎缩到不存在的角落。我从未想过这样一个人身上会有任何火花。任何问题都只有简单几个字的乏味回答，然后便开始露出满足的笑，看上去相当苍白无力。一个毫无亮点的人，我不明白为什么杂志要安排我来采访她。</p><p>直到别的老师偷偷地跟我说，她家有一个傻儿子。</p><p>“傻儿子？什么样的傻儿子？”我有些吃惊。</p><p>“脑瘫，生下来就带的病。现在已经16岁了，根本治不好，智商还像两三岁一样，在床上吃床上拉，可糟心了。这些年她拼命在外面教课赚钱，就是为给儿子治病。”</p><p>当时的我不太能理解。面临这样的困境，她为什么丝毫没有表现出压力，没有痛不欲生歇斯底里，反而平静到让人忽视她的情绪。</p><p>我们聊了很久，并没有得到具体的答案，她也不能描述出内心最深处的感受。那大概是所有像她这样的人在寡淡世界里唯一的私有。</p><p>她只是反复地说着一句话，试图让我多明白一点点。</p><p>“… …再苦再难，日子还不是要这样，拼了命地过。”</p><p>那些看上去幸福的、平静的、超脱的人生，常常会让人生出错觉：他们是与烦恼绝缘的，他们已经看破世情，或者世情压根与他们无关。</p><p>在这样的错觉下，我们误以为只有自己的经历才算壮怀激烈。然后难免夸夸其谈，希望得到更多的认同，来缅怀心中那点被拼命放大的伤口与遗憾。</p><p>直到有个声音狠狠地迎头砸下，带着不屑一顾的嘲讽。</p><p>别傻了，哪有什么独一无二，是个人就饱经沧桑。</p><p>前些天，我无意中看到刚上小学一年级的小侄女QQ签名改成了“你不像他”。</p><p>正好周末她来我家玩，我就逗她：“这个‘你’是谁啊？”</p><p>她居然脸红娇羞：“当然是我男朋友。”</p><p>然后她给我看小男生的照片。虽然清秀可爱，我也不知该如何置评，只好敷衍几句。刚要转身离开，她拉住我，说还有另一个的照片。</p><p>我说另一个是什么人？她害羞地笑：“刚刚问的是‘你’，这个是‘他’啊。”</p><p>我张大了嘴看着她。</p><p>小侄女一脸苦恼：“他们两个，一个是班长，一个是学委，好难选，我夹在中间好尴尬好纠结啊。”</p><p>她又说：“小姑姑，曾经沧海难为水，其实我还是最爱幼儿园大班的浩浩。可惜我们再也不能在一起了。小姑姑，你说浩浩会不会也很在意我？”</p><p>我努力地想了又想，实在不知如何回答。最后只得点了点头说：“爱过。”</p><p>她满意地点点头，然后长长叹息了一声—</p><p>“我这一生，真是沧桑呵。”</p><hr><ul><li>Author <strong>辉姑娘</strong></li><li>转载自 <strong><a href="http://guo.lu/5473" target="_blank" rel="noopener">素锦</a></strong></li></ul>]]></content>
    
    
    <categories>
      
      <category>随笔</category>
      
    </categories>
    
    
    <tags>
      
      <tag>随笔</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>未来科技渲染视频</title>
    <link href="/2006/01/02/%E6%9C%AA%E6%9D%A5%E7%A7%91%E6%8A%80%E6%B8%B2%E6%9F%93%E8%A7%86%E9%A2%91/"/>
    <url>/2006/01/02/%E6%9C%AA%E6%9D%A5%E7%A7%91%E6%8A%80%E6%B8%B2%E6%9F%93%E8%A7%86%E9%A2%91/</url>
    
    <content type="html"><![CDATA[<p><img src="https://cdn.oss.link/markdown/The-Future2-1024x400.jpg" srcset="/img/loading.gif" alt="The-Future"></p><blockquote><p>展望未来科技…..</p></blockquote><!--more--><hr><video id="video" controls="" preload="none" poster="https://cdn.oss.link/markdown/The-Future1-1024x406.jpg">      <source id="mp4" src="http://7xjost.com1.z0.glb.clouddn.com/videos/The-future.mp4" type="video/mp4">      <p>Your user agent does not support the HTML5 Video element.</p></video>]]></content>
    
    
    <categories>
      
      <category>随笔</category>
      
    </categories>
    
    
    <tags>
      
      <tag>随笔</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>桃花依旧笑</title>
    <link href="/2006/01/02/%E6%A1%83%E8%8A%B1%E4%BE%9D%E6%97%A7%E7%AC%91/"/>
    <url>/2006/01/02/%E6%A1%83%E8%8A%B1%E4%BE%9D%E6%97%A7%E7%AC%91/</url>
    
    <content type="html"><![CDATA[<blockquote><p>昔年，有人于林中念动《桃木诗》，不知含意，如诵“咒语歌”般，便能夺了魂、摄了魄去，恍恍不可终日，那三魂七魄、五荤八素，便能离体飘飘荡荡，上了天、入了地，尘世天堂地狱各走一遭，倘能由此“管窥其理”，也不枉“桃花依旧笑，何必待人归”了。</p></blockquote><p><img src="https://cdn.oss.link/markdown/taohuayijiuxiao-1024x538.png" srcset="/img/loading.gif" alt="桃花依旧笑"></p><pre><code class="hljs sh">我想写点桃花却不知从何写起没有进过桃花的梦只在身边徘徊算好了我们之间的距离终归要回到尘世桃花依旧嬉笑良人不会再回我想写点桃花的诗意少年的思念之情干净、利落胜过天下至极的剑客 桃花剑客离了桃花便不能生存五步一人千里不留行唯愿手中栽桃花心中开桃花桃花处处不胜你模样</code></pre>]]></content>
    
    
    <categories>
      
      <category>随笔</category>
      
    </categories>
    
    
    <tags>
      
      <tag>随笔</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>我和我的倔强</title>
    <link href="/2006/01/02/%E6%88%91%E5%92%8C%E6%88%91%E7%9A%84%E5%80%94%E5%BC%BA/"/>
    <url>/2006/01/02/%E6%88%91%E5%92%8C%E6%88%91%E7%9A%84%E5%80%94%E5%BC%BA/</url>
    
    <content type="html"><![CDATA[<p><img src="https://cdn.oss.link/markdown/hexo_jujiang.png" srcset="/img/loading.gif" alt="juejiang"></p><blockquote><p><strong>是时间消磨了青春的倔强，还是岁月摧残了年轻的梦想？</strong><br><strong>我愿留守心中的那抹执着，吟唱生命中的那首不悔战歌！</strong></p></blockquote><blockquote><p><strong>一首光明 点亮梦想</strong></p></blockquote><audio  autoplay="autoplay">  <source src="https://cdn.oss.link/markdown/hexo_music_guangming.mp3" type="audio/mpeg" />Your browser does not support the audio element.</audio>]]></content>
    
    
    <categories>
      
      <category>随笔</category>
      
    </categories>
    
    
    <tags>
      
      <tag>随笔</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>浅谈 java中的 equals 和 hashcode</title>
    <link href="/2006/01/02/%E6%B5%85%E8%B0%88-java%E4%B8%AD%E7%9A%84-equals-%E5%92%8C-hashcode/"/>
    <url>/2006/01/02/%E6%B5%85%E8%B0%88-java%E4%B8%AD%E7%9A%84-equals-%E5%92%8C-hashcode/</url>
    
    <content type="html"><![CDATA[<p><img src="https://cdn.oss.link/markdown/hexo_java_equals_hashcode.jpg" srcset="/img/loading.gif" alt="hexo_java_equals_hashcode.jpg"></p><h2 id="equals-方法"><a href="#equals-方法" class="headerlink" title="equals 方法"></a>equals 方法</h2><blockquote><p>equals 方法来源于 Object 超类；该方法用于检测一个对象与另一个对象是否相等。</p></blockquote><h3 id="Object-中的-equals"><a href="#Object-中的-equals" class="headerlink" title="Object 中的 equals"></a>Object 中的 equals</h3><blockquote><p>在 java 源码中，Object 的 equals 实现如下</p></blockquote><pre><code class="hljs java"><span class="hljs-function"><span class="hljs-keyword">public</span> <span class="hljs-keyword">boolean</span> <span class="hljs-title">equals</span><span class="hljs-params">(Object obj)</span> </span>&#123;    <span class="hljs-keyword">return</span> (<span class="hljs-keyword">this</span> == obj);&#125;</code></pre><blockquote><p>由此可见，Object 中 equals 默认比较的是两个对象的 内存地址(==)，即 <strong>默认比较两个对象的引用，引用相同返回true，反之返回false。</strong>这看起来似乎合情合理，但实际开发中，这种比较方式则不适用；比如我们要比较两个 pserson 对象是否相等，<strong>从业务角度来说，只要这两个人 名字、年龄、身份证号相同，我们就可以认为两个对象相等。但由于是两个 pserson对象，所以所以引用肯定不同，这样调用默认的 equals 方法就会返回 false，显然是不合理的。</strong></p></blockquote><h3 id="重写-equals"><a href="#重写-equals" class="headerlink" title="重写 equals"></a>重写 equals</h3><blockquote><p>从上面的例子可以看出，Object 中的 equals 并不适用与实际业务场景，此时我们应该 对 equals进行重写；但是 重写 equals 必须满足以下规则(特性)：</p></blockquote><ul><li><p>自反性</p><blockquote><p>对于对象 x ，<code>x.equals(x)</code> 应当始终返回 true。</p></blockquote></li><li><p>对称性</p><blockquote><p>对于对象 x、y，如果 <code>x.equals(y)</code> 返回 true，那么 <code>y.equals(x)</code> 也必须返回 true。</p></blockquote></li><li><p>传递性</p><blockquote><p>对于对象 x、y、z，如果 <code>x.equals(y)</code> 返回 true，<code>y.equals(z)</code> 返回 true；那么 <code>x.equals(z)</code> 也必须返回 true。</p></blockquote></li><li><p>一致性</p><blockquote><p>对于对象 x、y，如果 <code>x.equals(y)</code> 返回 true，那么反复调用的结果应当一直为 true。</p></blockquote></li><li><p>空值不行等性</p><blockquote><p>对于任意非空对象 x，<code>x.equals(null)</code> 应当永远返回 false。</p></blockquote></li></ul><hr><p><strong>然而，对于以上5种特性，在某些特殊情况下需要严格考虑。</strong></p><ul><li>对象属性的冲突</li></ul><p>假设我们将对象内的属性看作是对象内容，在实际业务场景，可能一个 汽车 Car 对象 和一个人 pserson 对象具有相同的名字，比如 <code>特斯拉</code>；此时如果我们重写 equals 时仅仅比较对象内容的话，很可能误判为 <strong>一辆汽车和一个人相等</strong>；是的，这很滑稽。</p><h3 id="getClass-的使用"><a href="#getClass-的使用" class="headerlink" title="getClass 的使用"></a>getClass 的使用</h3><p>在上面列举的情况来看，我们似乎再重写 equals 时还需要考虑对象的类型；在 java 里，对象类型我们 采用 Class 描述。那么此时 我们在重写的 equals 方法里应当 增加 <code>car.getClass()==pserson.getClass()</code> 的检测，这样能有效避免上述情况的发生；伪代码如下</p><pre><code class="hljs java"><span class="hljs-function"><span class="hljs-keyword">public</span> <span class="hljs-keyword">boolean</span> <span class="hljs-title">equals</span><span class="hljs-params">(Object obj)</span></span>&#123;    <span class="hljs-comment">// 进行完全匹配检测(引用)</span>    <span class="hljs-keyword">if</span>(<span class="hljs-keyword">this</span>==obj) <span class="hljs-keyword">return</span> <span class="hljs-keyword">true</span>;    <span class="hljs-comment">// 进行空值检测</span>    <span class="hljs-keyword">if</span>(obj==<span class="hljs-keyword">null</span>) <span class="hljs-keyword">return</span> <span class="hljs-keyword">false</span>;    <span class="hljs-comment">// 进行类型匹配检测</span>    <span class="hljs-keyword">if</span>(<span class="hljs-keyword">this</span>.getClass()!=obj.getClass()) <span class="hljs-keyword">return</span> <span class="hljs-keyword">false</span>;    <span class="hljs-comment">// 进行属性相等检测，省略...</span>&#125;</code></pre><h3 id="instanceof-的使用"><a href="#instanceof-的使用" class="headerlink" title="instanceof 的使用"></a>instanceof 的使用</h3><p>然而，即使我们考虑了属性相等的情况，我们还是忽略了很多其他的业务场合。比如 一个学生 Student 对象和一个人 pserson 对象；当使用上面的检测方法时，很明显 pserson 对象和 Student 对象的 Class 不一致，直接返回了 false；而实际业务场景是 一个 Student 对象也是一个人 pserson；<strong>Student 对象可能继承于pserson对象。</strong>而此时我们应当使用 instanceof 进行检测，伪代码如下：</p><pre><code class="hljs java"><span class="hljs-function"><span class="hljs-keyword">public</span> <span class="hljs-keyword">boolean</span> <span class="hljs-title">equals</span><span class="hljs-params">(Object obj)</span></span>&#123;    <span class="hljs-comment">// 进行完全匹配检测(引用)</span>    <span class="hljs-keyword">if</span>(!(<span class="hljs-keyword">this</span> instaceof obj)) <span class="hljs-keyword">return</span> <span class="hljs-keyword">false</span>;    <span class="hljs-comment">// 进行空值检测</span>    <span class="hljs-keyword">if</span>(obj==<span class="hljs-keyword">null</span>) <span class="hljs-keyword">return</span> <span class="hljs-keyword">false</span>;    <span class="hljs-comment">// 进行类型匹配检测</span>    <span class="hljs-keyword">if</span>(<span class="hljs-keyword">this</span>.getClass()!=obj.getClass()) <span class="hljs-keyword">return</span> <span class="hljs-keyword">false</span>;    <span class="hljs-comment">// 进行属性相等检测，省略...</span>&#125;</code></pre><h3 id="getClass-与-instaceof-的取舍"><a href="#getClass-与-instaceof-的取舍" class="headerlink" title="getClass 与 instaceof 的取舍"></a>getClass 与 instaceof 的取舍</h3><p>或许从上两个例子中我们感觉使用 instaceof 更 “靠谱一些”；但其实我们注意到，<strong>采用 instaceof 检测实际上违反了 <code>对称性</code> 原则；</strong> 因为 <code>pserson instaceof Student</code> 返回 false，反之返回 true。</p><p>所以对于 <code>instanceof</code> 有时候并不那么完美；就连 JDK的开发者也遇到了这个问题；在 <code>Timestamp</code> 类中，由于继承自 <code>java.util.Date</code>；而不幸的是 Date 类的 equals 采用的是 instanceof，这就导致对称性出了问题。从上可知，我们根据实际业务进行取舍，取舍原则如下：</p><ul><li>如果子类拥有自己的相等性概念，则对称性强制要求采用 getClass 方式检测。</li><li>如果由超类决定相等性概念，那么就可以采用 instanceof 检测，保证我们可以在子类对象间进行相等性判断。</li></ul><h3 id="重写-equals-的建议"><a href="#重写-equals-的建议" class="headerlink" title="重写 equals 的建议"></a>重写 equals 的建议</h3><ul><li>首先检测 this 与 otherObject 是否引用同一对象</li></ul><pre><code class="hljs java"><span class="hljs-keyword">if</span>(<span class="hljs-keyword">this</span>==otherObject) <span class="hljs-keyword">return</span> <span class="hljs-keyword">true</span>;</code></pre><ul><li>然后检测 otherObject是否为 null，如果为 null 返回 false，这是必须的</li></ul><pre><code class="hljs java"><span class="hljs-keyword">if</span>(otherObject == <span class="hljs-keyword">null</span>) <span class="hljs-keyword">return</span> <span class="hljs-keyword">false</span>;</code></pre><ul><li>其次比较 this 与 otherObject 是否同属于一个类；如果 equals 语义在子类中有所改变，则 使用 getClass 检测</li></ul><pre><code class="hljs java"><span class="hljs-keyword">if</span>(<span class="hljs-keyword">this</span>.getClass()!=otherObject.getClass()) <span class="hljs-keyword">return</span> <span class="hljs-keyword">false</span>;</code></pre><ul><li>最后将 otherObject强制转换为 当前类型，并进行属性值检测；注意：<strong>如果在子类中重写的equals，则需要在重写时首先进行 <code>super.equals(other)</code> 判断</strong></li></ul><h2 id="hashcode-方法"><a href="#hashcode-方法" class="headerlink" title="hashcode 方法"></a>hashcode 方法</h2><blockquote><p>写这篇博客之前，也看过很多博客，大部分大家写的都是这样的一句话：<strong>重写 equals 必须重写 hashcode，两个对象 equals 返回 true 则 hashcode 必须保证相同。</strong>但是，接下来就没有然后了；搞的我刚学 java 时候也挺晕的，就像是 “知其然而不知所以然”。</p></blockquote><blockquote><p>总结一下一般会有这几个问题：</p></blockquote><ul><li>hashcode 方法是干啥的？</li><li>hashcode(哈希值) 是个什么玩意？</li><li>hashcode 有什么用？</li><li>我为啥要重写 hashcode？</li><li>我不重写它有啥后果？</li></ul><h3 id="hashcode-方法是干啥的？"><a href="#hashcode-方法是干啥的？" class="headerlink" title="hashcode 方法是干啥的？"></a>hashcode 方法是干啥的？</h3><blockquote><p>官方的解释是这样的：<strong>hashcode 方法用于返回一个对象的 哈希值。</strong>说白了就是 hashcode 方法能返回一个 哈希值，这玩意是个整数。</p></blockquote><h3 id="hashcode-哈希值-是个什么玩意？"><a href="#hashcode-哈希值-是个什么玩意？" class="headerlink" title="hashcode(哈希值) 是个什么玩意？"></a>hashcode(哈希值) 是个什么玩意？</h3><blockquote><p>由上面可知，这个 哈希值就是一个整数，可能是正数也可能是负数。</p></blockquote><h3 id="hashcode-有什么用？"><a href="#hashcode-有什么用？" class="headerlink" title="hashcode 有什么用？"></a>hashcode 有什么用？</h3><blockquote><p>hashcode(哈希值) 的作用就是用于在使用 Hash算法实现的集合中确定元素位置。</p></blockquote><p>拿我们最常见的 HashMap 来说，我们都知道 HashMap 里通过 key 取 value 时的速度 是 O(1) 级别的；</p><p>什么是 O(1)级别？</p><p>O(1)级别说白了就是 <strong>在任意数据大小的容器中，取出一个元素所使用的时间与元素个数无关；通俗的说法就是 不论你这个 HashMap 里有100个元素还是有9999999个元素，我通过 key 取出一个元素所使用的时间是一样的。</strong></p><p>为何是 O(1) 级别？为何这么吊？</p><p>这个问题就要谈一下 HashMap 等 hash 容器的存储方式了；这些容器在存储元素是是这样的：首先获取你要存储元素的 hashcode(一个整数)，然后再定义一个固定整数(标准叫桶数)，最后用 hashcode 对 另一个整数(桶数) 取余；取余的结果即为元素要存储的下标(可能存放到数组里)。当然这里是简单的取余，可能更复杂。</p><p>当我们要从一个 HashMap 中取出一个 value 时，实际上他就是通过这套算法，用 key 的 hashcode 计算出元素位置，直接取出来了；所以说 无论你这里面有多少元素，它取的时候始终是用着一个算法、一个流程，不会因为你数据多少而产生影响，这就是 O(1) 级别的存储。</p><p><strong>总结：由上面可知，这个 hashcode 的作用就是 通过算法来确立元素存放的位置，以便于放入元素或者获取元素。</strong></p><h3 id="我为啥要重写-hashcode-amp-amp-不重写有啥后果"><a href="#我为啥要重写-hashcode-amp-amp-不重写有啥后果" class="headerlink" title="我为啥要重写 hashcode &amp;&amp; 不重写有啥后果"></a>我为啥要重写 hashcode &amp;&amp; 不重写有啥后果</h3><blockquote><p>回顾一下上面：hashcode 是个整数，hashcode 方法的作用就是计算并返回这个整数；这个整数用于存放 Hash 算法实现的容器时 确定元素位置。</p></blockquote><p>接下来考虑一个业务场景：有两个对象 pserson1 和 pserson2 ，pserson1 和 pserson2 都只有两个属性，分别是名字(name)和年龄(age)。现在 pserson1 和 pserson2 的名字(name)、年龄(age) 都相同；那么我们是否可以根据业务场景来说 <strong>pserson1 和 pserson2 是同一个人</strong>？</p><p>如果说 “是” 的话，我们刚刚所认为的 “从业务角度理解 pserson1 和 pserson2 是一个人” 是不是就相当于 重写了 Pserson 的 equals 方法呢？就像下面这样：</p><pre><code class="hljs java"><span class="hljs-keyword">public</span> <span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">Pserson</span> </span>&#123;<span class="hljs-keyword">private</span> String name;<span class="hljs-keyword">private</span> <span class="hljs-keyword">int</span> age;<span class="hljs-comment">// 重写 equals</span><span class="hljs-meta">@Override</span><span class="hljs-function"><span class="hljs-keyword">public</span> <span class="hljs-keyword">boolean</span> <span class="hljs-title">equals</span><span class="hljs-params">(Object obj)</span> </span>&#123;<span class="hljs-keyword">if</span> (<span class="hljs-keyword">this</span> == obj)<span class="hljs-keyword">return</span> <span class="hljs-keyword">true</span>;<span class="hljs-keyword">if</span> (obj == <span class="hljs-keyword">null</span>)<span class="hljs-keyword">return</span> <span class="hljs-keyword">false</span>;<span class="hljs-keyword">if</span> (getClass() != obj.getClass())<span class="hljs-keyword">return</span> <span class="hljs-keyword">false</span>;<span class="hljs-comment">// 主要在这，我们根据业务逻辑，即 姓名和年龄 确立相等关系</span>pserson other = (pserson) obj;<span class="hljs-keyword">if</span> (age != other.age)<span class="hljs-keyword">return</span> <span class="hljs-keyword">false</span>;<span class="hljs-keyword">if</span> (name == <span class="hljs-keyword">null</span>) &#123;<span class="hljs-keyword">if</span> (other.name != <span class="hljs-keyword">null</span>)<span class="hljs-keyword">return</span> <span class="hljs-keyword">false</span>;&#125; <span class="hljs-keyword">else</span> <span class="hljs-keyword">if</span> (!name.equals(other.name))<span class="hljs-keyword">return</span> <span class="hljs-keyword">false</span>;<span class="hljs-keyword">return</span> <span class="hljs-keyword">true</span>;&#125;&#125;</code></pre><p>我们注意到，我们根据业务逻辑重写 equals 后，造成的结果就是，两个 属性相同的 Pserson 对象 我们就认为是相同的，即 equals 返回了 true；<br><strong>但我们没有重写 hashcode，Object 中的 hashcode 是 native(本地的)，也就是很可能不同对象返回不同的 hashcode，即使属性相同也没用。</strong></p><hr><p>到这里我们再总结一下：</p><ul><li><strong>hashcode 方法返回对象的 哈希值；</strong></li><li><strong>我们通过 哈希值 的运算(与指定数取余等)来确立元素在 hash 算法实现的容器中的位置；</strong></li><li><strong>Object 中的 hashcode 方法 对于业务逻辑上相等的两个对象(属性相同，不同引用) 返回的 hashcode 是不同的。</strong></li></ul><hr><p><strong>墨迹了那么多最终问题来了：假设我们只重写了 pserson 的 equals 方法，使之 “属性相同即为相等”，当我们把两个 “相等的(属性相同的)” Pserson 对象 放入 HashSet 中会怎样？</strong></p><p>友情提示：HashSet中默认是不许放重复元素的，放重复的是会被过滤掉的，如下代码所示：</p><pre><code class="hljs java"><span class="hljs-keyword">public</span> <span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">Test1</span> </span>&#123;<span class="hljs-function"><span class="hljs-keyword">public</span> <span class="hljs-keyword">static</span> <span class="hljs-keyword">void</span> <span class="hljs-title">main</span><span class="hljs-params">(String[] args)</span> </span>&#123;Pserson pserson1 = <span class="hljs-keyword">new</span> Pserson();pserson1.setName(<span class="hljs-string">"张三"</span>);pserson1.setAge(<span class="hljs-number">10</span>);Pserson pserson2 = <span class="hljs-keyword">new</span> Pserson();pserson2.setName(<span class="hljs-string">"张三"</span>);pserson2.setAge(<span class="hljs-number">10</span>);HashSet&lt;Pserson&gt; hashSet = <span class="hljs-keyword">new</span> HashSet&lt;Pserson&gt;();hashSet.add(pserson1);hashSet.add(pserson2);System.out.println(hashSet.size());&#125;&#125;</code></pre><blockquote><p>结论&amp;&amp;后果：当我们仅重写了 equals 保证了 “名字和年龄一样的就是一个人” 这条业务以后；把两个 pserson 对象放入 HashSet 容器里时，由于 HashSet 是通过 hashcode 来区分两个 对象存放位置，而我们又 没有根据业务逻辑重写 hashcode 方法；导致了两个 在业务上相同的对象 放到了 HashSet里，HashSet 会认为他是两个不同的对象，故最后不会去重，hashset.size()打印出来是2。</p></blockquote><h2 id="最终结论"><a href="#最终结论" class="headerlink" title="最终结论"></a>最终结论</h2><blockquote><p>对于重写 euqals ，要很据实际业务逻辑来，并满足上述的设计要求；一旦重写了 equals 那就必须重写 hashcode，除非你保证你的对象不会被放到 Hash 实现的容器里；不重写的话就会导致 Hash 容器认为两个属性相同的对象是2个，而不是业务上的1个。</p></blockquote>]]></content>
    
    
    <categories>
      
      <category>Java</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Java</tag>
      
    </tags>
    
  </entry>
  
  
  
  
</search>
