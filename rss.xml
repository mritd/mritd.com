<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0"
  xmlns:atom="http://www.w3.org/2005/Atom"
  xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>bleem</title>
    <link>https://mritd.com/</link>
    
    <image>
      <url>https://mritd.com/icon.png</url>
      <title>bleem</title>
      <link>https://mritd.com/</link>
    </image>
    
    <atom:link href="https://mritd.com/rss.xml" rel="self" type="application/rss+xml"/>
    
    <description>Upward, not Northward.</description>
    <pubDate>Thu, 08 Oct 2020 11:28:01 GMT</pubDate>
    <generator>http://hexo.io/</generator>
    
    <item>
      <title>网站切换到 Hexo</title>
      <link>https://mritd.com/2020/10/08/switch-jekyll-to-hexo/</link>
      <guid>https://mritd.com/2020/10/08/switch-jekyll-to-hexo/</guid>
      <pubDate>Thu, 08 Oct 2020 11:04:00 GMT</pubDate>
      
      <description>坚持写博客大约有 5 年多的时间了，以前的博客一直采用 jekyll 框架，由于一直缺少搜索等功能，而自己又不会前端，最近干脆直接切换到 Hexo 了；这里记录一下折腾过程。</description>
      
      
      
      <content:encoded><![CDATA[<h2 id="一、Hexo-安装"><a href="#一、Hexo-安装" class="headerlink" title="一、Hexo 安装"></a>一、Hexo 安装</h2><p>Hexo 安装根据官方文档直接操作即可，安装前提是需要先安装 Nodejs(这里不再阐述直接略过)</p><div class="hljs"><pre><code class="hljs sh">npm install -g hexo-cli</code></pre></div><p>Hexo 命令行工具安装完成后可以直接初始化一个样例项目，init 过程会 clone <code>https://github.com/hexojs/hexo-starter.git</code> 到本地，同时自动安装好相关依赖</p><div class="hljs"><pre><code class="hljs sh"><span class="hljs-comment"># mritd.com 为目录名，个人习惯直接使用网站域名作为目录名称</span>hexo init mritd.com</code></pre></div><p>进入目录启动样例站点</p><div class="hljs"><pre><code class="hljs sh"><span class="hljs-comment"># 进入目录</span><span class="hljs-built_in">cd</span> mritd.com<span class="hljs-comment"># 启动本地服务器进行预览</span>hexo serve</code></pre></div><p><img src="https://cdn.oss.link/markdown/1jb2q.png" srcset="/img/loading.gif" alt="hexo_demo"></p><h2 id="二、主题设置"><a href="#二、主题设置" class="headerlink" title="二、主题设置"></a>二、主题设置</h2><p>基本的样例博客启动完成后就需要选择一个主题，主题实质上才决定博客功能，这里目前使用了 <a href="https://github.com/fluid-dev/hexo-theme-fluid" target="_blank" rel="noopener">Fluid</a> 主题，这个主题目前兼具了个人博客所需的所有功能，而且作者提交比较活跃，文档也比较全面。</p><div class="hljs"><pre><code class="hljs sh"><span class="hljs-comment"># 下载主题</span>git <span class="hljs-built_in">clone</span> https://github.com/fluid-dev/hexo-theme-fluid.git themes/fluid<span class="hljs-comment"># 切换到最新版本</span>(<span class="hljs-built_in">cd</span> themes/fluid &amp;&amp; git checkout -b v1.8.3 v1.8.3)</code></pre></div><p>接下来修改 <code>_config.yml</code> 配置切换主题即可</p><div class="hljs"><pre><code class="hljs yaml"><span class="hljs-comment"># Extensions</span><span class="hljs-comment">## Plugins: https://hexo.io/plugins/</span><span class="hljs-comment">## Themes: https://hexo.io/themes/</span><span class="hljs-attr">theme:</span> <span class="hljs-string">fluid</span></code></pre></div><p>然后重新启动博客进行预览: <code>hexo cl &amp;&amp; hexo s</code></p><p><img src="https://cdn.oss.link/markdown/vlibl.png" srcset="/img/loading.gif" alt="fluid_demo"></p><p><strong>关于主题其他配置可自行阅读 <a href="https://hexo.fluid-dev.com/docs/guide/" target="_blank" rel="noopener">官方文档</a>，文档有时可能更新不及时，可同时参考仓库内的 <a href="https://github.com/fluid-dev/hexo-theme-fluid/blob/master/_config.yml" target="_blank" rel="noopener"><code>_config.yml</code></a> 配置。</strong></p><h2 id="三、文章导入"><a href="#三、文章导入" class="headerlink" title="三、文章导入"></a>三、文章导入</h2><p>关于 jekyll 博客的文章如何导入到 Hexo 中网上有很多脚本；但是实际上两个静态博客框架都是支持标准的 Markdown 语法书写的文章进行渲染，唯一区别就是每篇文章上的 “头”。</p><div class="hljs"><pre><code class="hljs markdown">---catalog: truecategories:<span class="hljs-bullet">  - </span>[Kubernetes]<span class="hljs-bullet">  - </span>[Golang]date: 2018-11-25 11:11:28excerpt: 最近在看 kubeadm 的源码，不过有些东西光看代码还是没法太清楚，还是需要实际运行才能看到具体代码怎么跑的，还得打断点 debug；无奈的是本机是 mac，debug 得在 Linux 下，so 研究了一下 remote debugkeywords: kubeadm,debugmultilingual: falsetags:<span class="hljs-bullet">  - </span>Golang<span class="hljs-bullet">  - </span>Kubernetestitle: 远程 Debug kubeadmindex<span class="hljs-emphasis">_img: img/remote_</span>debug.jpg---具体文章内容......</code></pre></div><p>所以直接复制 jekyll 的 md 文件到 <code>source/_posts</code> 目录，并修改文档头部即可。</p><h2 id="四、自动更新"><a href="#四、自动更新" class="headerlink" title="四、自动更新"></a>四、自动更新</h2><p>目前博客部署在自己的 VPS 上，以前都是将博客生成的静态直接使用 nginx 发布出去的；但是面临的问题就是每次博客更新都要手动去 VPS 更新，虽然可以写一些 CI 脚本但是并不算智能；得益于 Golang 官方完善的标准库支持，这次直接几行代码写一个静态服务器，同时拦截特定 URL 来更新博客:</p><div class="hljs"><pre><code class="hljs golang"><span class="hljs-keyword">package</span> main<span class="hljs-keyword">import</span> (<span class="hljs-string">"fmt"</span><span class="hljs-string">"net/http"</span><span class="hljs-string">"os"</span><span class="hljs-string">"os/exec"</span><span class="hljs-string">"path"</span>)<span class="hljs-function"><span class="hljs-keyword">func</span> <span class="hljs-title">main</span><span class="hljs-params">()</span></span> &#123;http.Handle(<span class="hljs-string">"/"</span>, fileServerWithCustom404(http.Dir(<span class="hljs-string">"/data"</span>)))http.HandleFunc(<span class="hljs-string">"/update"</span>, update)fmt.Println(<span class="hljs-string">"Updating WebSite..."</span>)_, err := gitPull()<span class="hljs-keyword">if</span> err != <span class="hljs-literal">nil</span> &#123;fmt.Printf(<span class="hljs-string">"WebSite update failed: %s"</span>, err)&#125;fmt.Println(<span class="hljs-string">"HTTP Server Listen at [:8080]..."</span>)_ = http.ListenAndServe(<span class="hljs-string">":8080"</span>, <span class="hljs-literal">nil</span>)&#125;<span class="hljs-comment">// POST 请求 /update 触发 git pull 更新博客</span><span class="hljs-function"><span class="hljs-keyword">func</span> <span class="hljs-title">update</span><span class="hljs-params">(w http.ResponseWriter, r *http.Request)</span></span> &#123;<span class="hljs-keyword">if</span> r.Method != http.MethodPost &#123;w.WriteHeader(http.StatusBadRequest)_, _ = w.Write([]<span class="hljs-keyword">byte</span>(<span class="hljs-string">"only support POST method.\n"</span>))<span class="hljs-keyword">return</span>&#125;bs, err := gitPull()<span class="hljs-keyword">if</span> err != <span class="hljs-literal">nil</span> &#123;w.WriteHeader(http.StatusInternalServerError)_, _ = w.Write([]<span class="hljs-keyword">byte</span>(err.Error()))<span class="hljs-keyword">return</span>&#125;w.WriteHeader(http.StatusOK)_, _ = w.Write(bs)&#125;<span class="hljs-comment">// 包装一下 404 状态码，返回自定义的 404 页面</span><span class="hljs-function"><span class="hljs-keyword">func</span> <span class="hljs-title">fileServerWithCustom404</span><span class="hljs-params">(fs http.FileSystem)</span> <span class="hljs-title">http</span>.<span class="hljs-title">Handler</span></span> &#123;fsh := http.FileServer(fs)<span class="hljs-keyword">return</span> http.HandlerFunc(<span class="hljs-function"><span class="hljs-keyword">func</span><span class="hljs-params">(w http.ResponseWriter, r *http.Request)</span></span> &#123;_, err := fs.Open(path.Clean(r.URL.Path))<span class="hljs-keyword">if</span> os.IsNotExist(err) &#123;r.URL.Path = <span class="hljs-string">"/404.html"</span>&#125;fsh.ServeHTTP(w, r)&#125;)&#125;<span class="hljs-function"><span class="hljs-keyword">func</span> <span class="hljs-title">gitPull</span><span class="hljs-params">()</span> <span class="hljs-params">(msg []<span class="hljs-keyword">byte</span>, err error)</span></span> &#123;cmd := exec.Command(<span class="hljs-string">"git"</span>, <span class="hljs-string">"pull"</span>)cmd.Dir = <span class="hljs-string">"/data"</span><span class="hljs-keyword">return</span> cmd.CombinedOutput()&#125;</code></pre></div><h2 id="五、Docker-化"><a href="#五、Docker-化" class="headerlink" title="五、Docker 化"></a>五、Docker 化</h2><p>有了上面的静态服务器，写个 Dockerfile 将 Hexo 生成的静态文件打包即可:</p><div class="hljs"><pre><code class="hljs Dockerfile"><span class="hljs-keyword">FROM</span> golang:<span class="hljs-number">1.15</span>-alpine3.<span class="hljs-number">12</span> AS builder<span class="hljs-keyword">ENV</span> GO111MODULE on<span class="hljs-keyword">COPY</span><span class="bash"> goserver /go/src/github.com/mritd/hexo/goserver</span><span class="hljs-keyword">WORKDIR</span><span class="bash"> /go/src/github.com/mritd/hexo/goserver</span><span class="hljs-keyword">RUN</span><span class="bash"> <span class="hljs-built_in">set</span> -e \</span><span class="bash">    &amp;&amp; go install</span><span class="hljs-keyword">FROM</span> alpine:<span class="hljs-number">3.12</span> AS dist<span class="hljs-keyword">LABEL</span><span class="bash"> maintainer=<span class="hljs-string">"mritd &lt;mritd@linux.com&gt;"</span></span><span class="hljs-keyword">ENV</span> TZ Asia/Shanghai<span class="hljs-keyword">ENV</span> REPO https://github.com/mritd/mritd.com.git<span class="hljs-keyword">COPY</span><span class="bash"> --from=builder /go/bin/goserver /usr/<span class="hljs-built_in">local</span>/bin/goserver</span><span class="hljs-keyword">RUN</span><span class="bash"> <span class="hljs-built_in">set</span> -e \</span><span class="bash">    &amp;&amp; apk upgrade \</span><span class="bash">    &amp;&amp; apk add bash tzdata git \</span><span class="bash">    &amp;&amp; git <span class="hljs-built_in">clone</span> <span class="hljs-variable">$&#123;REPO&#125;</span> /data \</span><span class="bash">    &amp;&amp; ln -sf /usr/share/zoneinfo/<span class="hljs-variable">$&#123;TZ&#125;</span> /etc/localtime \</span><span class="bash">    &amp;&amp; <span class="hljs-built_in">echo</span> <span class="hljs-variable">$&#123;TZ&#125;</span> &gt; /etc/timezone \</span><span class="bash">    &amp;&amp; rm -rf /var/cache/apk/*</span><span class="hljs-keyword">WORKDIR</span><span class="bash"> /data</span><span class="hljs-keyword">CMD</span><span class="bash"> [<span class="hljs-string">"goserver"</span>]</span></code></pre></div><p>镜像运行后将使用 <code>/data</code> 目录最为静态文件目录进行发布，Hexo 生成的静态文件(public 目录)也会完整的 clone 到当前目录，此后使用 POST 请求访问 <code>/update</code> 即可触发从 Github 更新博客内容。</p><h2 id="六、Travis-CI-集成"><a href="#六、Travis-CI-集成" class="headerlink" title="六、Travis CI 集成"></a>六、Travis CI 集成</h2><p>所有就绪以后在主仓库增加 <code>.travis.yml</code> 配置来联动 travis ci；由于每次 push 到 Github 的内容实际上已经是本地生成的 public 目录，所以 CI 只需要通知服务器更新即可；强迫症又加了一个 Telegram 通知，每次触发更新完成后 Telegram 再给自己推送一下:</p><div class="hljs"><pre><code class="hljs yml"><span class="hljs-attr">language:</span> <span class="hljs-string">go</span><span class="hljs-attr">git:</span>  <span class="hljs-attr">quiet:</span> <span class="hljs-literal">true</span><span class="hljs-attr">script:</span><span class="hljs-bullet">-</span> <span class="hljs-string">curl</span> <span class="hljs-string">-X</span> <span class="hljs-string">POST</span> <span class="hljs-string">$&#123;CALLBACK&#125;</span><span class="hljs-attr">after_script:</span><span class="hljs-bullet">-</span> <span class="hljs-string">curl</span> <span class="hljs-string">-X</span> <span class="hljs-string">POST</span> <span class="hljs-string">https://api.telegram.org/bot$&#123;TELEGRAM_TOKEN&#125;/sendMessage</span> <span class="hljs-string">-d</span> <span class="hljs-string">chat_id=$&#123;TELEGRAM_CHAT_ID&#125;</span> <span class="hljs-string">-d</span> <span class="hljs-string">"text=mritd.com deployed."</span></code></pre></div><h2 id="七、gulp-优化"><a href="#七、gulp-优化" class="headerlink" title="七、gulp 优化"></a>七、gulp 优化</h2><p>由于目前一些配图啥的还是存储在服务器本地，所以图片等比较大的静态文件仍然是访问瓶颈，这时候可以借助 gulp 来压缩并进行优化:</p><div class="hljs"><pre><code class="hljs sh"><span class="hljs-comment"># 安装 gulp</span>npm install -g gulp<span class="hljs-comment"># 安装 gulp 插件</span>npm install gulp-htmlclean gulp-htmlmin gulp-minify-css gulp-uglify-es gulp-imagemin --save<span class="hljs-comment"># 重新 link 一下</span>npm link gulp</code></pre></div><p>接下来编写 <code>gulpfile.js</code> 指定相关的优化任务</p><div class="hljs"><pre><code class="hljs js"><span class="hljs-keyword">var</span> gulp = <span class="hljs-built_in">require</span>(<span class="hljs-string">'gulp'</span>);<span class="hljs-keyword">var</span> minifycss = <span class="hljs-built_in">require</span>(<span class="hljs-string">'gulp-minify-css'</span>);<span class="hljs-keyword">var</span> uglify = <span class="hljs-built_in">require</span>(<span class="hljs-string">'gulp-uglify-es'</span>).default;<span class="hljs-keyword">var</span> htmlmin = <span class="hljs-built_in">require</span>(<span class="hljs-string">'gulp-htmlmin'</span>);<span class="hljs-keyword">var</span> htmlclean = <span class="hljs-built_in">require</span>(<span class="hljs-string">'gulp-htmlclean'</span>);<span class="hljs-keyword">var</span> imagemin = <span class="hljs-built_in">require</span>(<span class="hljs-string">'gulp-imagemin'</span>);<span class="hljs-comment">// 压缩html</span>gulp.task(<span class="hljs-string">'minify-html'</span>, <span class="hljs-function"><span class="hljs-keyword">function</span>(<span class="hljs-params"></span>) </span>&#123;    <span class="hljs-keyword">return</span> gulp.src(<span class="hljs-string">'./public/**/*.html'</span>)        .pipe(htmlclean())        .pipe(htmlmin(&#123;            removeComments: <span class="hljs-literal">true</span>,            minifyJS: <span class="hljs-literal">true</span>,            minifyCSS: <span class="hljs-literal">true</span>,            minifyURLs: <span class="hljs-literal">true</span>,        &#125;))        .pipe(gulp.dest(<span class="hljs-string">'./public'</span>))&#125;);<span class="hljs-comment">// 压缩css</span>gulp.task(<span class="hljs-string">'minify-css'</span>, <span class="hljs-function"><span class="hljs-keyword">function</span>(<span class="hljs-params"></span>) </span>&#123;    <span class="hljs-keyword">return</span> gulp.src(<span class="hljs-string">'./public/css/*.css'</span>)        .pipe(minifycss(&#123;            compatibility: <span class="hljs-string">'*'</span>        &#125;))        .pipe(gulp.dest(<span class="hljs-string">'./public/css'</span>));&#125;);<span class="hljs-comment">// 压缩js</span>gulp.task(<span class="hljs-string">'minify-js'</span>, <span class="hljs-function"><span class="hljs-keyword">function</span>(<span class="hljs-params"></span>) </span>&#123;    <span class="hljs-keyword">return</span> gulp.src(<span class="hljs-string">'./public/js/*.js'</span>, <span class="hljs-string">'!./public/js/*.min.js'</span>)        .pipe(uglify())        .pipe(gulp.dest(<span class="hljs-string">'./public/js'</span>));&#125;);<span class="hljs-comment">// 压缩图片</span>gulp.task(<span class="hljs-string">'minify-images'</span>, <span class="hljs-function"><span class="hljs-keyword">function</span>(<span class="hljs-params"></span>) </span>&#123;    <span class="hljs-keyword">return</span> gulp.src(<span class="hljs-string">'./public/img/*.*'</span>)        .pipe(imagemin(        [imagemin.gifsicle(&#123;<span class="hljs-string">'optimizationLevel'</span>: <span class="hljs-number">3</span>&#125;),        imagemin.mozjpeg(&#123;<span class="hljs-string">'progressive'</span>: <span class="hljs-literal">true</span>&#125;),        imagemin.optipng(&#123;<span class="hljs-string">'optimizationLevel'</span>: <span class="hljs-number">7</span>&#125;),        imagemin.svgo()],        &#123;<span class="hljs-string">'verbose'</span>: <span class="hljs-literal">true</span>&#125;))        .pipe(gulp.dest(<span class="hljs-string">'./public/img'</span>))&#125;);<span class="hljs-comment">// 默认任务</span><span class="hljs-comment">// 这里默认没有运行 minify-js，因为我发现 js 压缩以后 PageSpeed 评分</span><span class="hljs-comment">// 莫明其妙的降低了，目前只优先考虑桌面浏览器的性能，暂不考虑移动端</span>gulp.task(<span class="hljs-string">'default'</span>, gulp.parallel(    <span class="hljs-string">'minify-html'</span>,<span class="hljs-string">'minify-css'</span>,<span class="hljs-string">'minify-images'</span>));</code></pre></div><p>最后在每次部署时执行一下 <code>gulp</code> 命令即可完成优化: <code>hexo cl &amp;&amp; hexo g &amp;&amp; gulp</code></p>]]></content:encoded>
      
      
      <category domain="https://mritd.com/categories/linux/">Linux</category>
      
      
      <category domain="https://mritd.com/tags/hexo/">Hexo</category>
      
      
      <comments>https://mritd.com/2020/10/08/switch-jekyll-to-hexo/#disqus_thread</comments>
      
    </item>
    
    <item>
      <title>编写一个动态准入控制来实现自动化</title>
      <link>https://mritd.com/2020/08/19/write-a-dynamic-admission-control-webhook/</link>
      <guid>https://mritd.com/2020/08/19/write-a-dynamic-admission-control-webhook/</guid>
      <pubDate>Wed, 19 Aug 2020 06:35:00 GMT</pubDate>
      
      <description>前段时间弄了一个 imgsync 的工具把 gcr.io 的镜像搬运到了 Docker Hub，但是即使这样我每次还是需要编辑 yaml 配置手动改镜像名称；所以我萌生了一个想法: 能不能自动化这个过程？</description>
      
      
      
      <content:encoded><![CDATA[<h2 id="一、准入控制介绍"><a href="#一、准入控制介绍" class="headerlink" title="一、准入控制介绍"></a>一、准入控制介绍</h2><p>在 Kubernetes 整个请求链路中，请求通过认证和授权之后、对象被持久化之前需要通过一连串的 “准入控制拦截器”；这些准入控制器负载验证请求的合法性，必要情况下也可以对请求进行修改；默认准入控制器编写在 kube-apiserver 的代码中，针对于当前 kube-apiserver 默认启用的准入控制器你可以通过以下命令查看:</p><div class="hljs"><pre><code class="hljs sh">kube-apiserver -h | grep <span class="hljs-built_in">enable</span>-admission-plugins</code></pre></div><p>具体每个准入控制器的作用可以通过 <a href="https://kubernetes.io/zh/docs/reference/access-authn-authz/admission-controllers/" target="_blank" rel="noopener">Using Admission Controllers</a> 文档查看。在这些准入控制器中有两个特殊的准入控制器 <code>MutatingAdmissionWebhook</code> 和 <code>ValidatingAdmissionWebhook</code>。<strong>这两个准入控制器以 WebHook 的方式提供扩展能力，从而我们可以实现自定义的一些功能。当我们在集群中创建相关 WebHook 配置后，我们配置中描述的想要关注的资源在集群中创建、修改等都会触发 WebHook，我们再编写具体的应用来响应 WebHook 即可完成特定功能。</strong></p><h2 id="二、动态准入控制"><a href="#二、动态准入控制" class="headerlink" title="二、动态准入控制"></a>二、动态准入控制</h2><p>动态准入控制实际上指的就是上面所说的两个 WebHook，在使用动态准入控制时需要一些先决条件:</p><ul><li>确保 Kubernetes 集群版本至少为 v1.16 (以便使用 <code>admissionregistration.k8s.io/v1 API</code>)或者 v1.9 (以便使用 <code>admissionregistration.k8s.io/v1beta1</code> API)。</li><li>确保启用 MutatingAdmissionWebhook 和 ValidatingAdmissionWebhook 控制器。 </li><li>确保启用 <code>admissionregistration.k8s.io/v1</code> 或 <code>admissionregistration.k8s.io/v1beta1</code> API。</li></ul><p>如果要使用 Mutating Admission Webhook，在满足先决条件后，需要在系统中 create 一个 MutatingWebhookConfiguration:</p><div class="hljs"><pre><code class="hljs yaml"><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">admissionregistration.k8s.io/v1</span><span class="hljs-attr">kind:</span> <span class="hljs-string">MutatingWebhookConfiguration</span><span class="hljs-attr">metadata:</span>  <span class="hljs-attr">name:</span> <span class="hljs-string">"mutating-webhook.mritd.me"</span>  <span class="hljs-attr">namespace:</span> <span class="hljs-string">kube-addons</span><span class="hljs-attr">webhooks:</span>  <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">"mutating-webhook.mritd.me"</span>    <span class="hljs-attr">rules:</span>      <span class="hljs-bullet">-</span> <span class="hljs-attr">apiGroups:</span>   <span class="hljs-string">[""]</span>        <span class="hljs-attr">apiVersions:</span> <span class="hljs-string">["v1"]</span>        <span class="hljs-attr">operations:</span>  <span class="hljs-string">["CREATE","UPDATE"]</span>        <span class="hljs-attr">resources:</span>   <span class="hljs-string">["pods"]</span>        <span class="hljs-attr">scope:</span>       <span class="hljs-string">"Namespaced"</span>    <span class="hljs-attr">clientConfig:</span>      <span class="hljs-attr">service:</span>        <span class="hljs-attr">name:</span> <span class="hljs-string">"mutating-webhook"</span>        <span class="hljs-attr">namespace:</span> <span class="hljs-string">"kube-addons"</span>        <span class="hljs-attr">path:</span> <span class="hljs-string">/print</span>      <span class="hljs-attr">caBundle:</span> <span class="hljs-string">$&#123;CA_BUNDLE&#125;</span>    <span class="hljs-attr">admissionReviewVersions:</span> <span class="hljs-string">["v1",</span> <span class="hljs-string">"v1beta1"</span><span class="hljs-string">]</span>    <span class="hljs-attr">sideEffects:</span> <span class="hljs-string">None</span>    <span class="hljs-attr">timeoutSeconds:</span> <span class="hljs-number">5</span>    <span class="hljs-attr">failurePolicy:</span> <span class="hljs-string">Ignore</span>    <span class="hljs-attr">namespaceSelector:</span>      <span class="hljs-attr">matchLabels:</span>        <span class="hljs-attr">mutating-webhook.mritd.me:</span> <span class="hljs-string">"true"</span></code></pre></div><p>同样要使用 Validating Admission Webhook 也需要类似的配置:</p><div class="hljs"><pre><code class="hljs yaml"><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">admissionregistration.k8s.io/v1</span><span class="hljs-attr">kind:</span> <span class="hljs-string">ValidatingWebhookConfiguration</span><span class="hljs-attr">metadata:</span>  <span class="hljs-attr">name:</span> <span class="hljs-string">"validating-webhook.mritd.me"</span><span class="hljs-attr">webhooks:</span>  <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">"validating-webhook.mritd.me"</span>    <span class="hljs-attr">rules:</span>      <span class="hljs-bullet">-</span> <span class="hljs-attr">apiGroups:</span>   <span class="hljs-string">[""]</span>        <span class="hljs-attr">apiVersions:</span> <span class="hljs-string">["v1"]</span>        <span class="hljs-attr">operations:</span>  <span class="hljs-string">["CREATE","UPDATE"]</span>        <span class="hljs-attr">resources:</span>   <span class="hljs-string">["pods"]</span>        <span class="hljs-attr">scope:</span>       <span class="hljs-string">"Namespaced"</span>    <span class="hljs-attr">clientConfig:</span>      <span class="hljs-attr">service:</span>        <span class="hljs-attr">name:</span> <span class="hljs-string">"validating-webhook"</span>        <span class="hljs-attr">namespace:</span> <span class="hljs-string">"kube-addons"</span>        <span class="hljs-attr">path:</span> <span class="hljs-string">/print</span>      <span class="hljs-attr">caBundle:</span> <span class="hljs-string">$&#123;CA_BUNDLE&#125;</span>    <span class="hljs-attr">admissionReviewVersions:</span> <span class="hljs-string">["v1",</span> <span class="hljs-string">"v1beta1"</span><span class="hljs-string">]</span>    <span class="hljs-attr">sideEffects:</span> <span class="hljs-string">None</span>    <span class="hljs-attr">timeoutSeconds:</span> <span class="hljs-number">5</span>    <span class="hljs-attr">failurePolicy:</span> <span class="hljs-string">Ignore</span>    <span class="hljs-attr">namespaceSelector:</span>      <span class="hljs-attr">matchLabels:</span>        <span class="hljs-attr">validating-webhook.mritd.me:</span> <span class="hljs-string">"true"</span></code></pre></div><p>从配置文件中可以看到，<code>webhooks.rules</code> 段落中具体指定了我们想要关注的资源及其行为，<code>webhooks.clientConfig</code> 中指定了 webhook 触发后将其发送到那个地址以及证书配置等，这些具体字段的含义可以通过官方文档 <a href="https://kubernetes.io/zh/docs/reference/access-authn-authz/extensible-admission-controllers/" target="_blank" rel="noopener">Dynamic Admission Control</a> 来查看。</p><p><strong>值得注意的是 Mutating Admission Webhook 会在 Validating Admission Webhook 之前触发；Mutating Admission Webhook 可以修改用户的请求，比如自动调整镜像名称、增加注解等，而 Validating Admission Webhook 只能做校验(true or false)，不可以进行修改操作。</strong></p><h2 id="三、编写一个-WebHook"><a href="#三、编写一个-WebHook" class="headerlink" title="三、编写一个 WebHook"></a>三、编写一个 WebHook</h2><blockquote><p><strong>郑重提示: 本部分文章请结合 <a href="https://github.com/mritd/goadmission" target="_blank" rel="noopener">goadmission</a> 框架源码进行阅读。</strong></p></blockquote><h3 id="3-1、大体思路"><a href="#3-1、大体思路" class="headerlink" title="3.1、大体思路"></a>3.1、大体思路</h3><p>在编写之前一般我们先大体了解一下流程并制订方案再去实现，边写边思考适合在细节实现上，对于整体的把控需要提前作好预习。针对于这个准入控制的 WebHook 来说，根据其官方文档大致总结重点如下:</p><ul><li>WebHook 接收者就是一个标准的 HTTP Server，请求方式是 POST + JSON</li><li>请求响应都是一个 AdmissionReview 对象</li><li>响应时需要请求时的 UID(<code>request.uid</code>)</li><li>响应时 Mutating Admission Webhook 可以包含对请求的修改信息，格式为 JSONPatch</li></ul><p>有了以上信息以后便可以知道编写 WebHook 需要的东西，根据这些信息目前我作出的大体方案如下:</p><ul><li>最起码我们要有个 HTTP Server，考虑到后续可能会同时处理多种 WebHook，所以需要一个带有路径匹配的 HTTP 框架，Gin 什么的虽然不错但是太重，最终选择简单轻量的 <code>gorilla/mux</code>。</li><li>应该做好适当的抽象，因为对于响应需要包含的 UID 等限制在每个请求都有可以提取出来自动化完成。</li><li>针对于 Mutating Admission Webhook 响应的 JSONPatch 可以弄个结构体然后直接反序列化。</li></ul><h3 id="3-2、AdmissionReview-对象"><a href="#3-2、AdmissionReview-对象" class="headerlink" title="3.2、AdmissionReview 对象"></a>3.2、AdmissionReview 对象</h3><p>基于 3.1 部分的分析可以知道，WebHook 接收和响应都是一个 AdmissionReview 对象，在查看源码以后可以看到 AdmissionReview 结构如下:</p><p><img src="https://cdn.oss.link/markdown/jro62.png" srcset="/img/loading.gif" alt="AdmissionReview"></p><p>从代码的命名中可以很清晰的看出，在请求发送到 WebHook 时我们只需要关注内部的 AdmissionRequest(实际入参)，在我们编写的 WebHook 处理完成后只需要返回包含有 AdmissionResponse(实际返回体) 的 AdmissionReview 对象即可；总的来说 <strong>AdmissionReview 对象是个套壳，请求是里面的 AdmissionRequest，响应是里面的 AdmissionResponse</strong>。</p><h3 id="3-3、Hello-World"><a href="#3-3、Hello-World" class="headerlink" title="3.3、Hello World"></a>3.3、Hello World</h3><p>有了上面的一些基础知识，我们就可以简单的实行一个什么也不干的 WebHook 方法(本地无法直接运行，重点在于思路):</p><div class="hljs"><pre><code class="hljs go"><span class="hljs-comment">// printRequest 接收 AdmissionRequest 对象并将其打印到到控制台，接着不做任何处理直接返回一个 AdmissionResponse 对象</span><span class="hljs-function"><span class="hljs-keyword">func</span> <span class="hljs-title">printRequest</span><span class="hljs-params">(request *admissionv1.AdmissionRequest)</span> <span class="hljs-params">(*admissionv1.AdmissionResponse, error)</span></span> &#123;bs, err := jsoniter.MarshalIndent(request, <span class="hljs-string">""</span>, <span class="hljs-string">"    "</span>)<span class="hljs-keyword">if</span> err != <span class="hljs-literal">nil</span> &#123;<span class="hljs-keyword">return</span> <span class="hljs-literal">nil</span>, err&#125;logger.Infof(<span class="hljs-string">"print request: %s"</span>, <span class="hljs-keyword">string</span>(bs))<span class="hljs-keyword">return</span> &amp;admissionv1.AdmissionResponse&#123;Allowed: <span class="hljs-literal">true</span>,Result: &amp;metav1.Status&#123;Code:    http.StatusOK,Message: <span class="hljs-string">"Hello World"</span>,&#125;,&#125;, <span class="hljs-literal">nil</span>&#125;</code></pre></div><p>上面这个 <code>printRequest</code> 方法最细粒度的控制到只面向我们的实际请求和响应；而对于 WebHook Server 来说其接到的是 http 请求，<strong>所以我们还需要在外面包装一下，将 http 请求转换为 AdmissionReview 并提取 AdmissionRequest 再调用上面的 <code>printRequest</code> 来处理，最后将返回结果重新包装为 AdmissionReview 重新返回；整体的代码如下</strong></p><div class="hljs"><pre><code class="hljs go"><span class="hljs-comment">// 通用的错误返回方法</span><span class="hljs-function"><span class="hljs-keyword">func</span> <span class="hljs-title">responseErr</span><span class="hljs-params">(handlePath, msg <span class="hljs-keyword">string</span>, httpCode <span class="hljs-keyword">int</span>, w http.ResponseWriter)</span></span> &#123;logger.Errorf(<span class="hljs-string">"handle func [%s] response err: %s"</span>, handlePath, msg)review := &amp;admissionv1.AdmissionReview&#123;Response: &amp;admissionv1.AdmissionResponse&#123;Allowed: <span class="hljs-literal">false</span>,Result: &amp;metav1.Status&#123;Message: msg,&#125;,&#125;,&#125;bs, err := jsoniter.Marshal(review)<span class="hljs-keyword">if</span> err != <span class="hljs-literal">nil</span> &#123;logger.Errorf(<span class="hljs-string">"failed to marshal response: %v"</span>, err)w.WriteHeader(http.StatusInternalServerError)_, _ = w.Write([]<span class="hljs-keyword">byte</span>(fmt.Sprintf(<span class="hljs-string">"failed to marshal response: %s"</span>, err)))&#125;w.WriteHeader(httpCode)_, err = w.Write(bs)logger.Debugf(<span class="hljs-string">"write err response: %d: %v: %v"</span>, httpCode, review, err)&#125;<span class="hljs-comment">// printRequest 接收 AdmissionRequest 对象并将其打印到到控制台，接着不做任何处理直接返回一个 AdmissionResponse 对象</span><span class="hljs-function"><span class="hljs-keyword">func</span> <span class="hljs-title">printRequest</span><span class="hljs-params">(request *admissionv1.AdmissionRequest)</span> <span class="hljs-params">(*admissionv1.AdmissionResponse, error)</span></span> &#123;bs, err := jsoniter.MarshalIndent(request, <span class="hljs-string">""</span>, <span class="hljs-string">"    "</span>)<span class="hljs-keyword">if</span> err != <span class="hljs-literal">nil</span> &#123;<span class="hljs-keyword">return</span> <span class="hljs-literal">nil</span>, err&#125;logger.Infof(<span class="hljs-string">"print request: %s"</span>, <span class="hljs-keyword">string</span>(bs))<span class="hljs-keyword">return</span> &amp;admissionv1.AdmissionResponse&#123;Allowed: <span class="hljs-literal">true</span>,Result: &amp;metav1.Status&#123;Code:    http.StatusOK,Message: <span class="hljs-string">"Hello World"</span>,&#125;,&#125;, <span class="hljs-literal">nil</span>&#125;<span class="hljs-comment">// http server 的处理方法</span><span class="hljs-function"><span class="hljs-keyword">func</span> <span class="hljs-title">headler</span><span class="hljs-params">(w http.ResponseWriter, r *http.Request)</span></span> &#123;<span class="hljs-keyword">defer</span> <span class="hljs-function"><span class="hljs-keyword">func</span><span class="hljs-params">()</span></span> &#123; _ = r.Body.Close() &#125;()w.Header().Set(<span class="hljs-string">"Content-Type"</span>, <span class="hljs-string">"application/json"</span>)<span class="hljs-comment">// 读取 body，出错直接返回</span>reqBs, err := ioutil.ReadAll(r.Body)<span class="hljs-keyword">if</span> err != <span class="hljs-literal">nil</span> &#123;responseErr(handlePath, err.Error(), http.StatusInternalServerError, w)<span class="hljs-keyword">return</span>&#125;<span class="hljs-keyword">if</span> reqBs == <span class="hljs-literal">nil</span> || <span class="hljs-built_in">len</span>(reqBs) == <span class="hljs-number">0</span> &#123;responseErr(handlePath, <span class="hljs-string">"request body is empty"</span>, http.StatusBadRequest, w)<span class="hljs-keyword">return</span>&#125;logger.Debugf(<span class="hljs-string">"request body: %s"</span>, <span class="hljs-keyword">string</span>(reqBs))<span class="hljs-comment">// 将 body 反序列化为 AdmissionReview</span>reqReview := admissionv1.AdmissionReview&#123;&#125;<span class="hljs-keyword">if</span> _, _, err := deserializer.Decode(reqBs, <span class="hljs-literal">nil</span>, &amp;reqReview); err != <span class="hljs-literal">nil</span> &#123;responseErr(handlePath, fmt.Sprintf(<span class="hljs-string">"failed to decode req: %s"</span>, err), http.StatusInternalServerError, w)<span class="hljs-keyword">return</span>&#125;<span class="hljs-keyword">if</span> reqReview.Request == <span class="hljs-literal">nil</span> &#123;responseErr(handlePath, <span class="hljs-string">"admission review request is empty"</span>, http.StatusBadRequest, w)<span class="hljs-keyword">return</span>&#125;<span class="hljs-comment">// 提取 AdmissionRequest 并调用 printRequest 处理</span>resp, err := printRequest(reqReview.Request)<span class="hljs-keyword">if</span> err != <span class="hljs-literal">nil</span> &#123;responseErr(handlePath, fmt.Sprintf(<span class="hljs-string">"admission func response: %s"</span>, err), http.StatusForbidden, w)<span class="hljs-keyword">return</span>&#125;<span class="hljs-keyword">if</span> resp == <span class="hljs-literal">nil</span> &#123;responseErr(handlePath, <span class="hljs-string">"admission func response is empty"</span>, http.StatusInternalServerError, w)<span class="hljs-keyword">return</span>&#125;<span class="hljs-comment">// 复制 AdmissionRequest 中的 UID 到 AdmissionResponse 中(必须进行，否则会导致响应无效)</span>resp.UID = reqReview.Request.UID<span class="hljs-comment">// 复制 reqReview.TypeMeta 到新的响应 AdmissionReview 中</span>respReview := admissionv1.AdmissionReview&#123;TypeMeta: reqReview.TypeMeta,Response: resp,&#125;<span class="hljs-comment">// 重新序列化响应并返回</span>respBs, err := jsoniter.Marshal(respReview)<span class="hljs-keyword">if</span> err != <span class="hljs-literal">nil</span> &#123;responseErr(handlePath, fmt.Sprintf(<span class="hljs-string">"failed to marshal response: %s"</span>, err), http.StatusInternalServerError, w)logger.Errorf(<span class="hljs-string">"the expected response is: %v"</span>, respReview)<span class="hljs-keyword">return</span>&#125;w.WriteHeader(http.StatusOK)_, err = w.Write(respBs)logger.Debugf(<span class="hljs-string">"write response: %d: %s: %v"</span>, http.StatusOK, <span class="hljs-keyword">string</span>(respBs), err)&#125;</code></pre></div><h3 id="3-4、抽象出框架"><a href="#3-4、抽象出框架" class="headerlink" title="3.4、抽象出框架"></a>3.4、抽象出框架</h3><p>编写了简单的 Hello World 以后可以看出，真正在编写时我们需要实现的都是处理 AdmissionRequest 并返回 AdmissionResponse 这部份(printRequest)；外部的包装为 AdmissionReview、复制 UID、复制 TypeMeta 等都是通用的方法，所以基于这一点我们可以进行适当的抽象:</p><h4 id="3-4-1、AdmissionFunc"><a href="#3-4-1、AdmissionFunc" class="headerlink" title="3.4.1、AdmissionFunc"></a>3.4.1、AdmissionFunc</h4><p>针对每一个贴合业务的 WebHook 来说，其大致有三大属性:</p><ul><li>WebHook 的类型(Mutating/Validating)</li><li>WebHook 拦截的 URL 路径(/print_request)</li><li>WebHook 核心的处理逻辑(处理 Request 和返回 Response)</li></ul><p>我们将其抽象为 AdmissionFunc 结构体以后如下所示</p><div class="hljs"><pre><code class="hljs go"><span class="hljs-comment">// WebHook 类型</span><span class="hljs-keyword">const</span> (Mutating   AdmissionFuncType = <span class="hljs-string">"Mutating"</span>Validating AdmissionFuncType = <span class="hljs-string">"Validating"</span>)<span class="hljs-comment">// 每一个对应到我们业务的 WebHook 抽象的 struct</span><span class="hljs-keyword">type</span> AdmissionFunc <span class="hljs-keyword">struct</span> &#123;Type AdmissionFuncTypePath <span class="hljs-keyword">string</span>Func <span class="hljs-function"><span class="hljs-keyword">func</span><span class="hljs-params">(request *admissionv1.AdmissionRequest)</span> <span class="hljs-params">(*admissionv1.AdmissionResponse, error)</span></span>&#125;</code></pre></div><h4 id="3-4-2、HandleFunc"><a href="#3-4-2、HandleFunc" class="headerlink" title="3.4.2、HandleFunc"></a>3.4.2、HandleFunc</h4><p>我们知道 WebHook 是基于 HTTP 的，所以上面抽象出的 AdmissionFunc 还不能直接用在 HTTP 请求代码中；如果直接偶合到 HTTP 请求代码中，我们就没法为 HTTP 代码再增加其他拦截路径等等特殊的底层设置；<strong>所以站在 HTTP 层面来说还需要抽象一个 “更高层面的且包含 AdmissionFunc 全部能力的 HandleFunc” 来使用；HandleFunc 抽象 HTTP 层面的需求:</strong></p><ul><li>HTTP 请求方法</li><li>HTTP 请求路径</li><li>HTTP 处理方法</li></ul><p>以下为 HandleFunc 的抽象:</p><div class="hljs"><pre><code class="hljs go"><span class="hljs-keyword">type</span> HandleFunc <span class="hljs-keyword">struct</span> &#123;Path   <span class="hljs-keyword">string</span>Method <span class="hljs-keyword">string</span>Func   <span class="hljs-function"><span class="hljs-keyword">func</span><span class="hljs-params">(w http.ResponseWriter, r *http.Request)</span></span>&#125;</code></pre></div><h3 id="3-5、goadmission-框架"><a href="#3-5、goadmission-框架" class="headerlink" title="3.5、goadmission 框架"></a>3.5、goadmission 框架</h3><p>有了以上两个角度的抽象，再结合 命令行参数解析、日志处理、配置文件读取等等，我揉合出了一个 <a href="https://github.com/mritd/goadmission" target="_blank" rel="noopener">goadmission</a> 框架，以方便动态准入控制的快速开发。</p><h4 id="3-5-1、基本结构"><a href="#3-5-1、基本结构" class="headerlink" title="3.5.1、基本结构"></a>3.5.1、基本结构</h4><div class="hljs"><pre><code class="hljs sh">.├── main.go└── pkg    ├── adfunc    │   ├── adfuncs.go    │   ├── adfuncs_json.go    │   ├── func_check_deploy_time.go    │   ├── func_disable_service_links.go    │   ├── func_image_rename.go    │   └── func_print_request.go    ├── conf    │   └── conf.go    ├── route    │   ├── route_available.go    │   ├── route_health.go    │   └── router.go    └── zaplogger        ├── config.go        └── logger.go5 directories, 13 files</code></pre></div><ul><li>main.go 为程序运行入口，在此设置命令行 flag 参数等</li><li>pkg/conf 为框架配置包，所有的配置读取只读取这个包即可</li><li>pkg/zaplogger zap log 库的日志抽象和处理(copy 自 operator-sdk)</li><li>pkg/route http 级别的路由抽象(HandleFunc)</li><li>pkg/adfunc 动态准入控制 WebHook 级别的抽(AdmissionFunc)</li></ul><h4 id="3-5-2、增加动态准入控制"><a href="#3-5-2、增加动态准入控制" class="headerlink" title="3.5.2、增加动态准入控制"></a>3.5.2、增加动态准入控制</h4><p>由于框架已经作好了路由注册等相关抽象，所以只需要新建 go 文件，然后通过 init 方法注册到全局 WebHook 组中即可，新编写的 WebHook 对已有代码不会有任何侵入:</p><p><img src="https://cdn.oss.link/markdown/lg6zc.png" srcset="/img/loading.gif" alt="add_adfunc"></p><p><strong>需要注意的是所有 validating 类型的 WebHook 会在 URL 路径前自动拼接 <code>/validating</code> 路径，mutating 类型的 WebHook 会在 URL 路径前自动拼接 <code>/mutating</code> 路径；</strong>这么做是为了避免在更高层级的 HTTP Route 上添加冲突的路由。</p><p><img src="https://cdn.oss.link/markdown/nd5ez.png" srcset="/img/loading.gif" alt="auto_fix_url"></p><h4 id="3-5-3、实现-image-自动修改"><a href="#3-5-3、实现-image-自动修改" class="headerlink" title="3.5.3、实现 image 自动修改"></a>3.5.3、实现 image 自动修改</h4><p>所以一切准备就绪以后，就需要 “不忘初心”，撸一个自动修改镜像名称的 WebHook:</p><div class="hljs"><pre><code class="hljs go"><span class="hljs-keyword">package</span> adfunc<span class="hljs-keyword">import</span> (<span class="hljs-string">"fmt"</span><span class="hljs-string">"net/http"</span><span class="hljs-string">"strings"</span><span class="hljs-string">"sync"</span><span class="hljs-string">"time"</span><span class="hljs-string">"github.com/mritd/goadmission/pkg/conf"</span>jsoniter <span class="hljs-string">"github.com/json-iterator/go"</span>corev1 <span class="hljs-string">"k8s.io/api/core/v1"</span>metav1 <span class="hljs-string">"k8s.io/apimachinery/pkg/apis/meta/v1"</span><span class="hljs-string">"github.com/mritd/goadmission/pkg/route"</span>admissionv1 <span class="hljs-string">"k8s.io/api/admission/v1"</span>)<span class="hljs-comment">// 只初始化一次 renameMap</span><span class="hljs-keyword">var</span> renameOnce sync.Once<span class="hljs-comment">// renameMap 保存镜像名称的替换规则，目前粗略实现为纯文本替换</span><span class="hljs-keyword">var</span> renameMap <span class="hljs-keyword">map</span>[<span class="hljs-keyword">string</span>]<span class="hljs-keyword">string</span><span class="hljs-function"><span class="hljs-keyword">func</span> <span class="hljs-title">init</span><span class="hljs-params">()</span></span> &#123;route.Register(route.AdmissionFunc&#123;Type: route.Mutating,Path: <span class="hljs-string">"/rename"</span>,Func: <span class="hljs-function"><span class="hljs-keyword">func</span><span class="hljs-params">(request *admissionv1.AdmissionRequest)</span> <span class="hljs-params">(*admissionv1.AdmissionResponse, error)</span></span> &#123;<span class="hljs-comment">// init rename rules map</span>renameOnce.Do(<span class="hljs-function"><span class="hljs-keyword">func</span><span class="hljs-params">()</span></span> &#123;renameMap = <span class="hljs-built_in">make</span>(<span class="hljs-keyword">map</span>[<span class="hljs-keyword">string</span>]<span class="hljs-keyword">string</span>, <span class="hljs-number">10</span>)<span class="hljs-comment">// 将镜像重命名规则初始化到 renameMap 中，方便后续读取</span><span class="hljs-comment">// rename rule example: k8s.gcr.io/=gcrxio/k8s.gcr.io_</span><span class="hljs-keyword">for</span> _, s := <span class="hljs-keyword">range</span> conf.ImageRename &#123;ss := strings.Split(s, <span class="hljs-string">"="</span>)<span class="hljs-keyword">if</span> <span class="hljs-built_in">len</span>(ss) != <span class="hljs-number">2</span> &#123;logger.Fatalf(<span class="hljs-string">"failed to parse image name rename rules: %s"</span>, s)&#125;renameMap[ss[<span class="hljs-number">0</span>]] = ss[<span class="hljs-number">1</span>]&#125;&#125;)<span class="hljs-comment">// 这个准入控制的 WebHook 只针对 Pod 处理，非 Pod 类请求直接返回错误</span><span class="hljs-keyword">switch</span> request.Kind.Kind &#123;<span class="hljs-keyword">case</span> <span class="hljs-string">"Pod"</span>:<span class="hljs-comment">// 从 request 中反序列化出 Pod 实例</span><span class="hljs-keyword">var</span> pod corev1.Poderr := jsoniter.Unmarshal(request.Object.Raw, &amp;pod)<span class="hljs-keyword">if</span> err != <span class="hljs-literal">nil</span> &#123;errMsg := fmt.Sprintf(<span class="hljs-string">"[route.Mutating] /rename: failed to unmarshal object: %v"</span>, err)logger.Error(errMsg)<span class="hljs-keyword">return</span> &amp;admissionv1.AdmissionResponse&#123;Allowed: <span class="hljs-literal">false</span>,Result: &amp;metav1.Status&#123;Code:    http.StatusBadRequest,Message: errMsg,&#125;,&#125;, <span class="hljs-literal">nil</span>&#125;<span class="hljs-comment">// 后来我发现带有下面这个注解的 Pod 是没法更改成功的，这种 Pod 是由 kubelet 直接</span><span class="hljs-comment">// 启动的 static pod，在 api server 中只能看到它的 "mirror"，不能改的</span><span class="hljs-comment">// skip static pod</span><span class="hljs-keyword">for</span> k := <span class="hljs-keyword">range</span> pod.Annotations &#123;<span class="hljs-keyword">if</span> k == <span class="hljs-string">"kubernetes.io/config.mirror"</span> &#123;errMsg := fmt.Sprintf(<span class="hljs-string">"[route.Mutating] /rename: pod %s has kubernetes.io/config.mirror annotation, skip image rename"</span>, pod.Name)logger.Warn(errMsg)<span class="hljs-keyword">return</span> &amp;admissionv1.AdmissionResponse&#123;Allowed: <span class="hljs-literal">true</span>,Result: &amp;metav1.Status&#123;Code:    http.StatusOK,Message: errMsg,&#125;,&#125;, <span class="hljs-literal">nil</span>&#125;&#125;<span class="hljs-comment">// 遍历所有 Pod，然后生成 JSONPatch</span><span class="hljs-comment">// 注意: 返回结果必须是 JSONPatch，k8s api server 再将 JSONPatch 应用到 Pod 上 </span><span class="hljs-comment">// 由于有多个 Pod，所以最终会产生一个补丁数组</span><span class="hljs-keyword">var</span> patches []Patch<span class="hljs-keyword">for</span> i, c := <span class="hljs-keyword">range</span> pod.Spec.Containers &#123;<span class="hljs-keyword">for</span> s, t := <span class="hljs-keyword">range</span> renameMap &#123;<span class="hljs-keyword">if</span> strings.HasPrefix(c.Image, s) &#123;patches = <span class="hljs-built_in">append</span>(patches, Patch&#123;<span class="hljs-comment">// 指定 JSONPatch 动作为 replace </span>Option: PatchOptionReplace,<span class="hljs-comment">// 打补丁的绝对位置</span>Path:   fmt.Sprintf(<span class="hljs-string">"/spec/containers/%d/image"</span>, i),<span class="hljs-comment">// replace 为处理过的镜像名</span>Value:  strings.Replace(c.Image, s, t, <span class="hljs-number">1</span>),&#125;)<span class="hljs-comment">// 为了后期调试和留存历史，我们再为修改过的 Pod 加个注解</span>patches = <span class="hljs-built_in">append</span>(patches, Patch&#123;Option: PatchOptionAdd,Path:   <span class="hljs-string">"/metadata/annotations"</span>,Value: <span class="hljs-keyword">map</span>[<span class="hljs-keyword">string</span>]<span class="hljs-keyword">string</span>&#123;fmt.Sprintf(<span class="hljs-string">"rename-mutatingwebhook-%d.mritd.me"</span>, time.Now().Unix()): fmt.Sprintf(<span class="hljs-string">"%d-%s-%s"</span>, i, strings.ReplaceAll(s, <span class="hljs-string">"/"</span>, <span class="hljs-string">"_"</span>), strings.ReplaceAll(t, <span class="hljs-string">"/"</span>, <span class="hljs-string">"_"</span>)),&#125;,&#125;)<span class="hljs-keyword">break</span>&#125;&#125;&#125;<span class="hljs-comment">// 将所有 JSONPatch 序列化成 json，然后返回即可</span>patch, err := jsoniter.Marshal(patches)<span class="hljs-keyword">if</span> err != <span class="hljs-literal">nil</span> &#123;errMsg := fmt.Sprintf(<span class="hljs-string">"[route.Mutating] /rename: failed to marshal patch: %v"</span>, err)logger.Error(errMsg)<span class="hljs-keyword">return</span> &amp;admissionv1.AdmissionResponse&#123;Allowed: <span class="hljs-literal">false</span>,Result: &amp;metav1.Status&#123;Code:    http.StatusInternalServerError,Message: errMsg,&#125;,&#125;, <span class="hljs-literal">nil</span>&#125;logger.Infof(<span class="hljs-string">"[route.Mutating] /rename: patches: %s"</span>, <span class="hljs-keyword">string</span>(patch))<span class="hljs-keyword">return</span> &amp;admissionv1.AdmissionResponse&#123;Allowed:   <span class="hljs-literal">true</span>,Patch:     patch,PatchType: JSONPatch(),Result: &amp;metav1.Status&#123;Code:    http.StatusOK,Message: <span class="hljs-string">"success"</span>,&#125;,&#125;, <span class="hljs-literal">nil</span><span class="hljs-keyword">default</span>:errMsg := fmt.Sprintf(<span class="hljs-string">"[route.Mutating] /rename: received wrong kind request: %s, Only support Kind: Pod"</span>, request.Kind.Kind)logger.Error(errMsg)<span class="hljs-keyword">return</span> &amp;admissionv1.AdmissionResponse&#123;Allowed: <span class="hljs-literal">false</span>,Result: &amp;metav1.Status&#123;Code:    http.StatusForbidden,Message: errMsg,&#125;,&#125;, <span class="hljs-literal">nil</span>&#125;&#125;,&#125;)&#125;</code></pre></div><h2 id="四、总结"><a href="#四、总结" class="headerlink" title="四、总结"></a>四、总结</h2><ul><li>动态准入控制其实就是个 WebHook，我们弄个 HTTP Server 接收 AdmissionRequest 响应 AdmissionResponse 就行。</li><li>Request、Response 会包装到 AdmissionReview 中，我们还需要做一些边缘处理，比如复制 UID、TypeMeta 等</li><li>MutatingWebHook 想要修改东西时，要返回描述修改操作的 JSONPatch 补丁</li><li>单个 WebHook 很简单，写多个的时候要自己抽好框架，尽量优雅的作好复用和封装</li></ul>]]></content:encoded>
      
      
      <category domain="https://mritd.com/categories/kubernetes/">Kubernetes</category>
      
      
      <category domain="https://mritd.com/tags/admission/">Admission</category>
      
      
      <comments>https://mritd.com/2020/08/19/write-a-dynamic-admission-control-webhook/#disqus_thread</comments>
      
    </item>
    
    <item>
      <title>使用 etcdadm 三分钟搭建 etcd 集群</title>
      <link>https://mritd.com/2020/08/19/use-etcdadm-to-build-etcd-cluster-in-3-minutes/</link>
      <guid>https://mritd.com/2020/08/19/use-etcdadm-to-build-etcd-cluster-in-3-minutes/</guid>
      <pubDate>Wed, 19 Aug 2020 06:20:00 GMT</pubDate>
      
      <description>本文介绍一下 etcd 宿主机部署的新玩具 etcdadm，类似 kubeadm 一样可以快速的在宿主机搭建 Etcd 集群。</description>
      
      
      
      <content:encoded><![CDATA[<h2 id="一、介绍"><a href="#一、介绍" class="headerlink" title="一、介绍"></a>一、介绍</h2><p>在搭建 Kubernetes 集群的过程中首先要搞定 Etcd 集群，虽然说 kubeadm 工具已经提供了默认和 master 节点绑定的 Etcd 集群自动搭建方式，但是我个人一直是手动将 Etcd 集群搭建在宿主机；<strong>因为这个玩意太重要了，毫不夸张的说 kubernetes 所有组件崩溃我们都能在一定时间以后排查问题恢复，但是一旦 Etcd 集群没了那么 Kubernetes 集群也就真没了。</strong></p><p>在很久以前我创建了 <a href="https://github.com/Gozap/edep" target="_blank" rel="noopener">edep</a> 工具来实现 Etcd 集群的辅助部署，再后来由于我们的底层系统偶合了 Ubuntu，所以创建了 <a href="https://github.com/mritd/etcd-deb" target="_blank" rel="noopener">etcd-deb</a> 项目来自动打 deb 包来直接安装；最近逛了一下 Kubernetes 的相关项目，发现跟我的 edep 差不多的项目 <a href="https://github.com/kubernetes-sigs/etcdadm" target="_blank" rel="noopener">etcdadm</a>，试了一下 “真香”。</p><h2 id="二、安装"><a href="#二、安装" class="headerlink" title="二、安装"></a>二、安装</h2><p><a href="https://github.com/kubernetes-sigs/etcdadm" target="_blank" rel="noopener">etcdadm</a> 项目是使用 go 编写的，所以很明显只有一个二进制下载下来就能用:</p><div class="hljs"><pre><code class="hljs sh">wget https://github.com/kubernetes-sigs/etcdadm/releases/download/v0.1.3/etcdadm-linux-amd64chmod +x etcdadm-linux-amd64</code></pre></div><h2 id="三、使用"><a href="#三、使用" class="headerlink" title="三、使用"></a>三、使用</h2><h3 id="3-1、启动引导节点"><a href="#3-1、启动引导节点" class="headerlink" title="3.1、启动引导节点"></a>3.1、启动引导节点</h3><p>类似 kubeadm 一样，etcdadm 也是先启动第一个节点，然后后续节点直接 join 即可；第一个节点启动只需要执行 <code>etcdadm init</code> 命令即可:</p><div class="hljs"><pre><code class="hljs sh">k1.node ➜  ~ ./etcdadm-linux-amd64 initINFO[0000] [install] extracting etcd archive /var/cache/etcdadm/etcd/v3.3.8/etcd-v3.3.8-linux-amd64.tar.gz to /tmp/etcd664686683INFO[0001] [install] verifying etcd 3.3.8 is installed <span class="hljs-keyword">in</span> /opt/bin/INFO[0001] [certificates] creating PKI assetsINFO[0001] creating a self signed etcd CA certificate and key files[certificates] Generated ca certificate and key.INFO[0001] creating a new server certificate and key files <span class="hljs-keyword">for</span> etcd[certificates] Generated server certificate and key.[certificates] server serving cert is signed <span class="hljs-keyword">for</span> DNS names [k1.node] and IPs [127.0.0.1 172.16.10.21]INFO[0002] creating a new certificate and key files <span class="hljs-keyword">for</span> etcd peering[certificates] Generated peer certificate and key.[certificates] peer serving cert is signed <span class="hljs-keyword">for</span> DNS names [k1.node] and IPs [172.16.10.21]INFO[0002] creating a new client certificate <span class="hljs-keyword">for</span> the etcdctl[certificates] Generated etcdctl-etcd-client certificate and key.INFO[0002] creating a new client certificate <span class="hljs-keyword">for</span> the apiserver calling etcd[certificates] Generated apiserver-etcd-client certificate and key.[certificates] valid certificates and keys now exist <span class="hljs-keyword">in</span> <span class="hljs-string">"/etc/etcd/pki"</span>INFO[0006] [health] Checking <span class="hljs-built_in">local</span> etcd endpoint healthINFO[0006] [health] Local etcd endpoint is healthyINFO[0006] To add another member to the cluster, copy the CA cert/key to its certificate dir and run:INFO[0006]      etcdadm join https://172.16.10.21:2379</code></pre></div><p>从命令行输出可以看到不同阶段 etcdadm 的相关日志输出；在 <code>init</code> 命令时可以指定一些特定参数来覆盖默认行为，比如版本号、安装目录等:</p><div class="hljs"><pre><code class="hljs sh">k1.node ➜  ~ ./etcdadm-linux-amd64 init --<span class="hljs-built_in">help</span>Initialize a new etcd clusterUsage:  etcdadm init [flags]Flags:      --certs-dir string                    certificates directory (default <span class="hljs-string">"/etc/etcd/pki"</span>)      --disk-priorities stringArray         Setting etcd disk priority (default [Nice=-10,IOSchedulingClass=best-effort,IOSchedulingPriority=2])      --download-connect-timeout duration   Maximum time <span class="hljs-keyword">in</span> seconds that you allow the connection to the server to take. (default 10s)  -h, --<span class="hljs-built_in">help</span>                                <span class="hljs-built_in">help</span> <span class="hljs-keyword">for</span> init      --install-dir string                  install directory (default <span class="hljs-string">"/opt/bin/"</span>)      --name string                         etcd member name      --release-url string                  URL used to download etcd (default <span class="hljs-string">"https://github.com/coreos/etcd/releases/download"</span>)      --server-cert-extra-sans strings      optional extra Subject Alternative Names <span class="hljs-keyword">for</span> the etcd server signing cert, can be multiple comma separated DNS names or IPs      --skip-hash-check                     Ignore snapshot integrity <span class="hljs-built_in">hash</span> value (required <span class="hljs-keyword">if</span> copied from data directory)      --snapshot string                     Etcd v3 snapshot file used to initialize member      --version string                      etcd version (default <span class="hljs-string">"3.3.8"</span>)Global Flags:  -l, --<span class="hljs-built_in">log</span>-level string   <span class="hljs-built_in">set</span> <span class="hljs-built_in">log</span> level <span class="hljs-keyword">for</span> output, permitted values debug, info, warn, error, fatal and panic (default <span class="hljs-string">"info"</span>)</code></pre></div><h3 id="3-2、其他节点加入"><a href="#3-2、其他节点加入" class="headerlink" title="3.2、其他节点加入"></a>3.2、其他节点加入</h3><p>在首个节点启动完成后，将集群 ca 证书复制到其他节点然后执行 <code>etcdadm join ENDPOINT_ADDRESS</code> 即可:</p><div class="hljs"><pre><code class="hljs sh"><span class="hljs-comment"># 复制 ca 证书</span>k1.node ➜  ~ rsync -avR /etc/etcd/pki/ca.* 172.16.10.22:/root@172.16.10.22<span class="hljs-string">'s password:</span><span class="hljs-string">sending incremental file list</span><span class="hljs-string">/etc/etcd/</span><span class="hljs-string">/etc/etcd/pki/</span><span class="hljs-string">/etc/etcd/pki/ca.crt</span><span class="hljs-string">/etc/etcd/pki/ca.key</span><span class="hljs-string"></span><span class="hljs-string">sent 2,932 bytes  received 67 bytes  856.86 bytes/sec</span><span class="hljs-string">total size is 2,684  speedup is 0.89</span><span class="hljs-string"></span><span class="hljs-string"># 执行 join</span><span class="hljs-string">k2.node ➜  ~ ./etcdadm-linux-amd64 join https://172.16.10.21:2379</span><span class="hljs-string">INFO[0000] [certificates] creating PKI assets</span><span class="hljs-string">INFO[0000] creating a self signed etcd CA certificate and key files</span><span class="hljs-string">[certificates] Using the existing ca certificate and key.</span><span class="hljs-string">INFO[0000] creating a new server certificate and key files for etcd</span><span class="hljs-string">[certificates] Generated server certificate and key.</span><span class="hljs-string">[certificates] server serving cert is signed for DNS names [k2.node] and IPs [172.16.10.22 127.0.0.1]</span><span class="hljs-string">INFO[0000] creating a new certificate and key files for etcd peering</span><span class="hljs-string">[certificates] Generated peer certificate and key.</span><span class="hljs-string">[certificates] peer serving cert is signed for DNS names [k2.node] and IPs [172.16.10.22]</span><span class="hljs-string">INFO[0000] creating a new client certificate for the etcdctl</span><span class="hljs-string">[certificates] Generated etcdctl-etcd-client certificate and key.</span><span class="hljs-string">INFO[0001] creating a new client certificate for the apiserver calling etcd</span><span class="hljs-string">[certificates] Generated apiserver-etcd-client certificate and key.</span><span class="hljs-string">[certificates] valid certificates and keys now exist in "/etc/etcd/pki"</span><span class="hljs-string">INFO[0001] [membership] Checking if this member was added</span><span class="hljs-string">INFO[0001] [membership] Member was not added</span><span class="hljs-string">INFO[0001] Removing existing data dir "/var/lib/etcd"</span><span class="hljs-string">INFO[0001] [membership] Adding member</span><span class="hljs-string">INFO[0001] [membership] Checking if member was started</span><span class="hljs-string">INFO[0001] [membership] Member was not started</span><span class="hljs-string">INFO[0001] [membership] Removing existing data dir "/var/lib/etcd"</span><span class="hljs-string">INFO[0001] [install] extracting etcd archive /var/cache/etcdadm/etcd/v3.3.8/etcd-v3.3.8-linux-amd64.tar.gz to /tmp/etcd315786364</span><span class="hljs-string">INFO[0003] [install] verifying etcd 3.3.8 is installed in /opt/bin/</span><span class="hljs-string">INFO[0006] [health] Checking local etcd endpoint health</span><span class="hljs-string">INFO[0006] [health] Local etcd endpoint is healthy</span></code></pre></div><h2 id="四、细节分析"><a href="#四、细节分析" class="headerlink" title="四、细节分析"></a>四、细节分析</h2><h3 id="4-1、默认配置"><a href="#4-1、默认配置" class="headerlink" title="4.1、默认配置"></a>4.1、默认配置</h3><p>在目前 etcdadm 尚未支持配置文件，目前所有默认配置存放在 <a href="https://github.com/kubernetes-sigs/etcdadm/blob/master/constants/constants.go#L22" target="_blank" rel="noopener">constants.go</a> 中，这里面包含了默认安装位置、systemd 配置、环境变量配置等，限于篇幅请自行查看代码；下面简单介绍一些一些刚须的配置:</p><h4 id="4-1-1、etcdctl"><a href="#4-1-1、etcdctl" class="headerlink" title="4.1.1、etcdctl"></a>4.1.1、etcdctl</h4><p>etcdctl 默认安装在 <code>/opt/bin</code> 目录下，同时你会发现该目录下还存在一个 <code>etcdctl.sh</code> 脚本，<strong>这个脚本将会自动读取 etcdctl 配置文件(<code>/etc/etcd/etcdctl.env</code>)，所以推荐使用这个脚本来替代 etcdctl 命令。</strong></p><h4 id="4-1-2、数据目录"><a href="#4-1-2、数据目录" class="headerlink" title="4.1.2、数据目录"></a>4.1.2、数据目录</h4><p>默认的数据目录存储在 <code>/var/lib/etcd</code> 目录，目前 etcdadm 尚未提供任何可配置方式，当然你可以自己改源码。</p><h4 id="4-2-3、配置文件"><a href="#4-2-3、配置文件" class="headerlink" title="4.2.3、配置文件"></a>4.2.3、配置文件</h4><p>配置文件总共有两个，一个是 <code>/etc/etcd/etcdctl.env</code> 用于 <code>/opt/bin/etcdctl.sh</code> 读取；另一个是 <code>/etc/etcd/etcd.env</code> 用于 systemd 读取并启动 etcd server。</p><h3 id="4-2、Join-流程"><a href="#4-2、Join-流程" class="headerlink" title="4.2、Join 流程"></a>4.2、Join 流程</h3><blockquote><p>其实很久以前由于我自己部署方式导致了我一直以来理解的一个错误，我一直以为 etcd server 证书要包含所有 server 地址，当然这个想法是怎么来的我也不知道，但是当我看了以下 Join 操作源码以后突然意识到 “为什么要包含所有？包含当前 server 不就行了么。”；当然对于 HTTPS 证书的理解一直是明白的，但是很奇怪就是不知道怎么就产生了这个想法(哈哈，我自己都觉的不可思议)…</p></blockquote><ul><li>由于预先拷贝了 ca 证书，所以 join 开始前 etcdadm 使用这个 ca 证书会签发自己需要的所有证书。</li><li>接下来 etcdadmin 通过 etcdctl-etcd-client 证书创建 client，然后调用 <code>MemberAdd</code> 添加新集群</li><li>最后老套路下载安装+启动就完成了</li></ul><h3 id="4-3、目前不足"><a href="#4-3、目前不足" class="headerlink" title="4.3、目前不足"></a>4.3、目前不足</h3><p>目前 etcdadm 虽然已经基本生产可用，但是仍有些不足的地方:</p><ul><li>不支持配置文件，很多东西无法定制</li><li>join 加入集群是在内部 api 完成，并未持久化到物理配置文件，后续重建可能忘记节点 ip</li><li>集群证书目前不支持自动续期，默认证书为 1 年很容易过期</li><li>下载动作调用了系统命令(curl)依赖性有点强</li><li>日志格式有点不友好，比如 level 和日期</li></ul>]]></content:encoded>
      
      
      <category domain="https://mritd.com/categories/kubernetes/">Kubernetes</category>
      
      
      <category domain="https://mritd.com/tags/etcd/">etcd</category>
      
      
      <comments>https://mritd.com/2020/08/19/use-etcdadm-to-build-etcd-cluster-in-3-minutes/#disqus_thread</comments>
      
    </item>
    
    <item>
      <title>如何编写 CSI 插件</title>
      <link>https://mritd.com/2020/08/19/how-to-write-a-csi-driver-for-kubernetes/</link>
      <guid>https://mritd.com/2020/08/19/how-to-write-a-csi-driver-for-kubernetes/</guid>
      <pubDate>Wed, 19 Aug 2020 06:10:00 GMT</pubDate>
      
      <description>本篇文章详细介绍 CSI 插件，同时涉及到的源码比较多，主要倾向于使用 go 来开发 CSI 驱动。</description>
      
      
      
      <content:encoded><![CDATA[<h2 id="一、为什么需要-CSI"><a href="#一、为什么需要-CSI" class="headerlink" title="一、为什么需要 CSI"></a>一、为什么需要 CSI</h2><p>在 Kubernetes 以前的版本中，其所有受官方支持的存储驱动全部在 Kubernetes 的主干代码中，其他第三方开发的自定义插件通过 <a href="https://github.com/kubernetes/community/blob/master/contributors/devel/sig-storage/flexvolume.md" target="_blank" rel="noopener">FlexVolume</a> 插件的形势提供服务；<strong>相对于 kubernetes 的源码树来说，内置的存储我们称之为 “树内存储”，外部第三方实现我们称之为 “树外存储”；</strong>在很长一段时间里树内存储和树外存储并行开发和使用，但是随着时间推移渐渐的就出现了很严重的问题:</p><ul><li>想要添加官方支持的存储必须在树内修改，这意味着需要 Kubernetes 发版</li><li>如果树内存储出现问题则也必须等待 Kubernetes 发版才能修复</li></ul><p>为了解决这种尴尬的问题，Kubernetes 必须抽象出一个合适的存储接口，并将所有存储驱动全部适配到这个接口上，存储驱动最好与 Kubernetes 之间进行 RPC 调用完成解耦，这样就造就了 CSI(Container Storage Interface)。</p><h2 id="二、CSI-基础知识"><a href="#二、CSI-基础知识" class="headerlink" title="二、CSI 基础知识"></a>二、CSI 基础知识</h2><h3 id="2-1、CSI-Sidecar-Containers"><a href="#2-1、CSI-Sidecar-Containers" class="headerlink" title="2.1、CSI Sidecar Containers"></a>2.1、CSI Sidecar Containers</h3><p>在开发 CSI 之前我们最好熟悉一下 CSI 开发中的一些常识；了解过 Kubernetes API 开发的朋友应该清楚，所有的资源定义(Deployment、Service…)在 Kubernetes 中其实就是一个 Object，此时可以将 Kubernetes 看作是一个 Database，无论是 Operator 还是 CSI 其核心本质都是不停的 Watch 特定的 Object，一但 kubectl 或者其他客户端 “动了” 这个 Object，我们的对应实现程序就 Watch 到变更然后作出相应的响应；<strong>对于 CSI 编写者来说，这些 Watch 动作已经不必自己实现 <a href="https://kubernetes.io/docs/concepts/extend-kubernetes/api-extension/custom-resources/#custom-controllers" target="_blank" rel="noopener">Custom Controller</a>，官方为我们提供了 <a href="https://kubernetes-csi.github.io/docs/sidecar-containers.html" target="_blank" rel="noopener">CSI Sidecar Containers</a>；</strong>并且在新版本中这些 Sidecar Containers 实现极其完善，比如自动的多节点 HA(Etcd 选举)等。</p><p><strong>所以到迄今为止，所谓的 CSI 插件开发事实上并非面向 Kubernetes API 开发，而是面向 Sidecar Containers 的 gRPC 开发，Sidecar Containers 一般会和我们自己开发的 CSI 驱动程序在同一个 Pod 中启动，然后 Sidecar Containers Watch API 中 CSI 相关 Object 的变动，接着通过本地 unix 套接字调用我们编写的 CSI 驱动：</strong></p><p><img src="https://cdn.oss.link/markdown/10w5g.png" srcset="/img/loading.gif" alt="CSI_Sidecar_Containers"></p><p>目前官方提供的 Sidecar Containers 如下:</p><ul><li><a href="https://kubernetes-csi.github.io/docs/external-provisioner.html" target="_blank" rel="noopener">external-provisioner</a></li><li><a href="https://kubernetes-csi.github.io/docs/external-attacher.html" target="_blank" rel="noopener">external-attacher</a></li><li><a href="https://kubernetes-csi.github.io/docs/external-snapshotter.html" target="_blank" rel="noopener">external-snapshotter</a></li><li><a href="https://kubernetes-csi.github.io/docs/external-resizer.html" target="_blank" rel="noopener">external-resizer</a></li><li><a href="https://kubernetes-csi.github.io/docs/node-driver-registrar.html" target="_blank" rel="noopener">node-driver-registrar</a></li><li><a href="https://kubernetes-csi.github.io/docs/cluster-driver-registrar.html" target="_blank" rel="noopener">cluster-driver-registrar (deprecated)</a></li><li><a href="https://kubernetes-csi.github.io/docs/livenessprobe.html" target="_blank" rel="noopener">livenessprobe</a></li></ul><p>每个 Sidecar Container 的作用可以通过对应链接查看，需要注意的是 cluster-driver-registrar 已经停止维护，请改用 node-driver-registrar。</p><h3 id="2-2、CSI-处理阶段"><a href="#2-2、CSI-处理阶段" class="headerlink" title="2.2、CSI 处理阶段"></a>2.2、CSI 处理阶段</h3><blockquote><p>在理解了 CSI Sidecar Containers 以后，我们仍需要大致的了解 CSI 挂载过程中的大致流程，以此来针对性的实现每个阶段所需要的功能；CSI 整个流程实际上大致分为以下三大阶段:</p></blockquote><h4 id="2-2-1、Provisioning-and-Deleting"><a href="#2-2-1、Provisioning-and-Deleting" class="headerlink" title="2.2.1、Provisioning and Deleting"></a>2.2.1、Provisioning and Deleting</h4><p><a href="https://github.com/kubernetes/community/blob/master/contributors/design-proposals/storage/container-storage-interface.md#provisioning-and-deleting" target="_blank" rel="noopener">Provisioning and Deleting</a> 阶段实现与外部存储供应商协调卷的创建/删除处理，简单地说就是需要实现 CreateVolume 和 DeleteVolume；假设外部存储供应商为阿里云存储那么此阶段应该完成在阿里云存储商创建一个指定大小的块设备，或者在用户删除 volume 时完成在阿里云存储上删除这个块设备；除此之外此阶段还应当响应存储拓扑分布从而保证 volume 分布在正确的集群拓扑上(此处描述不算清晰，推荐查看设计文档)。</p><h4 id="2-2-2、Attaching-and-Detaching"><a href="#2-2-2、Attaching-and-Detaching" class="headerlink" title="2.2.2、Attaching and Detaching"></a>2.2.2、Attaching and Detaching</h4><p><a href="https://github.com/kubernetes/community/blob/master/contributors/design-proposals/storage/container-storage-interface.md#attaching-and-detaching" target="_blank" rel="noopener">Attaching and Detaching</a> 阶段实现将外部存储供应商提供好的卷设备挂载到本地或者从本地卸载，简单地说就是实现 ControllerPublishVolume 和 ControllerUnpublishVolume；同样以外部存储供应商为阿里云存储为例，在 Provisioning 阶段创建好的卷的块设备，在此阶段应该实现将其挂载到服务器本地或从本地卸载，在必要的情况下还需要进行格式化等操作。</p><h4 id="2-2-3、Mount-and-Umount"><a href="#2-2-3、Mount-and-Umount" class="headerlink" title="2.2.3、Mount and Umount"></a>2.2.3、Mount and Umount</h4><p>这个阶段在 CSI 设计文档中没有做详细描述，在前两个阶段完成后，当一个目标 Pod 在某个 Node 节点上调度时，kubelet 会根据前两个阶段返回的结果来创建这个 Pod；同样以外部存储供应商为阿里云存储为例，此阶段将会把已经 Attaching 的本地块设备以目录形式挂载到 Pod 中或者从 Pod 中卸载这个块设备。</p><h3 id="2-3、CSI-gRPC-Server"><a href="#2-3、CSI-gRPC-Server" class="headerlink" title="2.3、CSI gRPC Server"></a>2.3、CSI gRPC Server</h3><p>CSI 的三大阶段实际上更细粒度的划分到 CSI Sidecar Containers 中，上面已经说过我们开发 CSI 实际上是面向 CSI Sidecar Containers 编程，针对于 CSI Sidecar Containers 我们主要需要实现以下三个 gRPC Server:</p><h4 id="2-3-1、Identity-Server"><a href="#2-3-1、Identity-Server" class="headerlink" title="2.3.1、Identity Server"></a>2.3.1、Identity Server</h4><p>在当前 CSI Spec v1.3.0 中 IdentityServer 定义如下:</p><div class="hljs"><pre><code class="hljs go"><span class="hljs-comment">// IdentityServer is the server API for Identity service.</span><span class="hljs-keyword">type</span> IdentityServer <span class="hljs-keyword">interface</span> &#123;GetPluginInfo(context.Context, *GetPluginInfoRequest) (*GetPluginInfoResponse, error)GetPluginCapabilities(context.Context, *GetPluginCapabilitiesRequest) (*GetPluginCapabilitiesResponse, error)Probe(context.Context, *ProbeRequest) (*ProbeResponse, error)&#125;</code></pre></div><p>从代码上可以看出 IdentityServer 主要负责像 Kubernetes 提供 CSI 插件名称可选功能等，所以此 Server 是必须实现的。</p><h4 id="2-3-2、Node-Server"><a href="#2-3-2、Node-Server" class="headerlink" title="2.3.2、Node Server"></a>2.3.2、Node Server</h4><p>同样当前 CSI v1.3.0 Spec 中 NodeServer 定义如下:</p><div class="hljs"><pre><code class="hljs go"><span class="hljs-comment">// NodeServer is the server API for Node service.</span><span class="hljs-keyword">type</span> NodeServer <span class="hljs-keyword">interface</span> &#123;NodeStageVolume(context.Context, *NodeStageVolumeRequest) (*NodeStageVolumeResponse, error)NodeUnstageVolume(context.Context, *NodeUnstageVolumeRequest) (*NodeUnstageVolumeResponse, error)NodePublishVolume(context.Context, *NodePublishVolumeRequest) (*NodePublishVolumeResponse, error)NodeUnpublishVolume(context.Context, *NodeUnpublishVolumeRequest) (*NodeUnpublishVolumeResponse, error)NodeGetVolumeStats(context.Context, *NodeGetVolumeStatsRequest) (*NodeGetVolumeStatsResponse, error)NodeExpandVolume(context.Context, *NodeExpandVolumeRequest) (*NodeExpandVolumeResponse, error)NodeGetCapabilities(context.Context, *NodeGetCapabilitiesRequest) (*NodeGetCapabilitiesResponse, error)NodeGetInfo(context.Context, *NodeGetInfoRequest) (*NodeGetInfoResponse, error)&#125;</code></pre></div><p>在最小化的实现中，NodeServer 中仅仅需要实现 <code>NodePublishVolume</code>、<code>NodeUnpublishVolume</code>、<code>NodeGetCapabilities</code> 三个方法，在 Mount 阶段 kubelet 会通过 <a href="https://kubernetes-csi.github.io/docs/node-driver-registrar.html" target="_blank" rel="noopener">node-driver-registrar</a> 容器调用这三个方法。</p><h4 id="2-3-3、Controller-Server"><a href="#2-3-3、Controller-Server" class="headerlink" title="2.3.3、Controller Server"></a>2.3.3、Controller Server</h4><p>在当前 CSI Spec v1.3.0 ControllerServer 定义如下:</p><div class="hljs"><pre><code class="hljs go"><span class="hljs-comment">// ControllerServer is the server API for Controller service.</span><span class="hljs-keyword">type</span> ControllerServer <span class="hljs-keyword">interface</span> &#123;CreateVolume(context.Context, *CreateVolumeRequest) (*CreateVolumeResponse, error)DeleteVolume(context.Context, *DeleteVolumeRequest) (*DeleteVolumeResponse, error)ControllerPublishVolume(context.Context, *ControllerPublishVolumeRequest) (*ControllerPublishVolumeResponse, error)ControllerUnpublishVolume(context.Context, *ControllerUnpublishVolumeRequest) (*ControllerUnpublishVolumeResponse, error)ValidateVolumeCapabilities(context.Context, *ValidateVolumeCapabilitiesRequest) (*ValidateVolumeCapabilitiesResponse, error)ListVolumes(context.Context, *ListVolumesRequest) (*ListVolumesResponse, error)GetCapacity(context.Context, *GetCapacityRequest) (*GetCapacityResponse, error)ControllerGetCapabilities(context.Context, *ControllerGetCapabilitiesRequest) (*ControllerGetCapabilitiesResponse, error)CreateSnapshot(context.Context, *CreateSnapshotRequest) (*CreateSnapshotResponse, error)DeleteSnapshot(context.Context, *DeleteSnapshotRequest) (*DeleteSnapshotResponse, error)ListSnapshots(context.Context, *ListSnapshotsRequest) (*ListSnapshotsResponse, error)ControllerExpandVolume(context.Context, *ControllerExpandVolumeRequest) (*ControllerExpandVolumeResponse, error)ControllerGetVolume(context.Context, *ControllerGetVolumeRequest) (*ControllerGetVolumeResponse, error)&#125;</code></pre></div><p>从这些方法上可以看出，大部分的核心逻辑应该在 ControllerServer 中实现，比如创建/销毁 Volume，创建/销毁 Snapshot 等；在一般情况下我们自己编写的 CSI 都会实现 <code>CreateVolume</code> 和 <code>DeleteVolume</code>，至于其他方法根据业务需求以及外部存储供应商实际情况来决定是否进行实现。</p><h4 id="2-3-4、整体部署加构图"><a href="#2-3-4、整体部署加构图" class="headerlink" title="2.3.4、整体部署加构图"></a>2.3.4、整体部署加构图</h4><p><img src="https://cdn.oss.link/markdown/vopox.jpg" srcset="/img/loading.gif" alt="CSI Deploy Mechanism"></p><p><strong>从这个部署架构图上可以看出在实际上 CSI 部署时，Mount and Umount 阶段(对应 Node Server 实现)以 Daemonset 方式保证其部署到每个节点，当 Volume 创建完成后由其挂载到 Pod 中；其他阶段(Provisioning and Deleting 和 Attaching and Detaching) 只要部署多个实例保证 HA 即可(最新版本的 Sidecar Containers 已经实现了多节点自动选举)；每次 PV 创建时首先由其他两个阶段的 Sidecar Containers 做处理，处理完成后信息返回给 Kubernetes 再传递到 Node Driver(Node Server) 上，然后 Node Driver 将其 Mount 到 Pod 中。</strong></p><h2 id="三、编写一个-NFS-CSI-插件"><a href="#三、编写一个-NFS-CSI-插件" class="headerlink" title="三、编写一个 NFS CSI 插件"></a>三、编写一个 NFS CSI 插件</h2><h3 id="3-1、前置准备及分析"><a href="#3-1、前置准备及分析" class="headerlink" title="3.1、前置准备及分析"></a>3.1、前置准备及分析</h3><p>根据以上文档的描述，针对于需要编写一个 NFS CSI 插件这个需求，大致我们可以作出如下分析:</p><ul><li>三大阶段中我们只需要实现 Provisioning and Deleting 和 Mount and Umount；因为以 NFS 作为外部存储供应商来说我们并非是块设备，所以也不需要挂载到宿主机(Attaching and Detaching)。</li><li>Provisioning and Deleting 阶段我们需要实现 <code>CreateVolume</code> 和 <code>DeleteVolume</code> 逻辑，其核心逻辑应该是针对每个 PV 在 NFS Server 目录下执行 <code>mkdir</code>，并将生成的目录名称等信息返回给 Kubernetes。</li><li>Mount and Umount 阶段需要实现 Node Server 的 <code>NodePublishVolume</code> 和 <code>NodeUnpublishVolume</code> 方法，然后将上一阶段提供的目录名称等信息组合成挂载命令 Mount 到 Pod 即可。</li></ul><p>在明确了这个需求以后我们需要开始编写 gRPC Server，当然不能盲目的自己乱造轮子，<strong>因为这些 gRPC Server 需要是 <code>NonBlocking</code> 的，</strong>所以最佳实践就是参考官方给出的样例项目 <a href="https://github.com/kubernetes-csi/csi-driver-host-path" target="_blank" rel="noopener">csi-driver-host-path</a>，这是一名合格的 CCE 必备的技能(CCE = Ctrl C + Ctrl V + Engineer)。</p><h3 id="3-2、Hostpath-CSI-源码分析"><a href="#3-2、Hostpath-CSI-源码分析" class="headerlink" title="3.2、Hostpath CSI 源码分析"></a>3.2、Hostpath CSI 源码分析</h3><p>针对官方给出的 CSI 样例，首先把源码弄到本地，然后通过 IDE 打开；这里默认为读者熟悉 Go 语言相关语法以及 go mod 等依赖配置，开发 IDE 默认为 GoLand</p><p><img src="https://cdn.oss.link/markdown/jlsdg.png" srcset="/img/loading.gif" alt="source tree"></p><p>从源码树上可以看到，hostpath 的 CSI 实现非常简单；首先是 <code>cmd</code> 包下的命令行部分，main 方法在这里定义，然后就是 <code>pkg/hostpath</code> 包的具体实现部分，CSI 需要实现的三大 gRPC Server 全部在此。</p><h4 id="3-2-1、命令行解析"><a href="#3-2-1、命令行解析" class="headerlink" title="3.2.1、命令行解析"></a>3.2.1、命令行解析</h4><p><code>cmd</code> 包下主要代码就是一些命令行解析，方便从外部传入一些参数供 CSI 使用；针对于 NFS CSI 我们需要从外部传入 NFS Server 地址、挂载目录等参数，如果外部存储供应商为其他云存储可能就需要从命令行传入 AccessKey、AccessToken 等参数。</p><p><img src="https://cdn.oss.link/markdown/t4mje.png" srcset="/img/loading.gif" alt="flag_parse"></p><p>目前 go 原生的命令行解析非常弱鸡，所以更推荐使用 <a href="https://github.com/spf13/cobra" target="_blank" rel="noopener">cobra</a> 命令行库完成解析。</p><h4 id="3-2-2、Hostpath-结构体"><a href="#3-2-2、Hostpath-结构体" class="headerlink" title="3.2.2、Hostpath 结构体"></a>3.2.2、Hostpath 结构体</h4><p>从上面命令行解析的图中可以看到，在完成命令行解析后交由 <code>handle</code> 方法处理；<code>handle</code> 方法很简单，通过命令行拿到的参数创建一个 <code>hostpath</code> 结构体指针，然后 <code>Run</code> 起来就行了，所以接下来要着重看一下这个结构体</p><p><img src="https://cdn.oss.link/markdown/0dc0j.png" srcset="/img/loading.gif" alt="hostpath_struct"></p><p>从代码上可以看到，<code>hostpath</code> 结构体内有一系列的字段用来存储命令行传入的特定参数，然后还有三个 gRPC Server 的引用；命令行参数解析完成后通过 <code>NewHostPathDriver</code> 方法设置到 <code>hostpath</code> 结构体内，然后通过调用结构体的 <code>Run</code> 方法创建三个 gRPC Server 并运行</p><p><img src="https://cdn.oss.link/markdown/wt4ha.png" srcset="/img/loading.gif" alt="hostpath_run"></p><h4 id="3-2-3、代码分布"><a href="#3-2-3、代码分布" class="headerlink" title="3.2.3、代码分布"></a>3.2.3、代码分布</h4><p>经过这么简单的一看，基本上一个最小化的 CSI 代码分布已经可以出来了:</p><ul><li>首先需要做命令行解析，一般放在 <code>cmd</code> 包</li><li>然后需要一个一般与 CSI 插件名称相同的结构体用来承载参数</li><li>结构体内持有三个 gRPC Server 引用，并通过适当的方法使用内部参数还初始化这个三个 gRPC Server</li><li>有了这些 gRPC Server 以后通过 <code>server.go</code> 中的 <code>NewNonBlockingGRPCServer</code> 方法将其启动(这里也可以看出 server.go 里面的方法我们后面可以 copy 直接用)</li></ul><h3 id="3-3、创建-CSI-插件骨架"><a href="#3-3、创建-CSI-插件骨架" class="headerlink" title="3.3、创建 CSI 插件骨架"></a>3.3、创建 CSI 插件骨架</h3><blockquote><p>项目骨架已经提交到 Github <a href="https://github.com/mritd/csi-archetype" target="_blank" rel="noopener">mritd/csi-archetype</a> 项目，可直接 clone 并使用。</p></blockquote><p>大致的研究完 Hostpath 的 CSI 源码，我们就可以根据其实现细节抽象出一个项目 CSI 骨架:</p><p><img src="https://cdn.oss.link/markdown/7y8qu.png" srcset="/img/loading.gif" alt="csi_archetype"></p><p>在这个骨架中我们采用 <a href="https://github.com/spf13/cobra" target="_blank" rel="noopener">corba</a> 完成命令行参数解析，同时使用 <a href="github.com/sirupsen/logrus">logrus</a> 作为日志输出库，这两个库都是 Kubernetes 以及 docker 比较常用的库；我们创建了一个叫 <code>archetype</code> 的结构体作为 CSI 的主承载类，这个结构体需要定义一些参数(parameter1…)方便后面初始化相关 gRPC Server 实现相关调用。</p><div class="hljs"><pre><code class="hljs go"><span class="hljs-keyword">type</span> archetype <span class="hljs-keyword">struct</span> &#123;name     <span class="hljs-keyword">string</span>nodeID   <span class="hljs-keyword">string</span>version  <span class="hljs-keyword">string</span>endpoint <span class="hljs-keyword">string</span><span class="hljs-comment">// Add CSI plugin parameters here</span>parameter1 <span class="hljs-keyword">string</span>parameter2 <span class="hljs-keyword">int</span>parameter3 time.Duration<span class="hljs-built_in">cap</span>   []*csi.VolumeCapability_AccessModecscap []*csi.ControllerServiceCapability&#125;</code></pre></div><p>与 Hostpath CSI 实现相同，我们创建一个 <code>NewCSIDriver</code> 方法来返回 <code>archetype</code> 结构体实例，在 <code>NewCSIDriver</code> 方法中将命令行解析得到的相关参数设置进结构体中并添加一些 <code>AccessModes</code> 和 <code>ServiceCapabilities</code> 方便后面 <code>Identity Server</code> 调用。</p><div class="hljs"><pre><code class="hljs go"><span class="hljs-function"><span class="hljs-keyword">func</span> <span class="hljs-title">NewCSIDriver</span><span class="hljs-params">(version, nodeID, endpoint, parameter1 <span class="hljs-keyword">string</span>, parameter2 <span class="hljs-keyword">int</span>, parameter3 time.Duration)</span> *<span class="hljs-title">archetype</span></span> &#123;logrus.Infof(<span class="hljs-string">"Driver: %s version: %s"</span>, driverName, version)<span class="hljs-comment">// Add some check here</span><span class="hljs-keyword">if</span> parameter1 == <span class="hljs-string">""</span> &#123;logrus.Fatal(<span class="hljs-string">"parameter1 is empty"</span>)&#125;n := &amp;archetype&#123;name:     driverName,nodeID:   nodeID,version:  version,endpoint: endpoint,parameter1: parameter1,parameter2: parameter2,parameter3: parameter3,&#125;<span class="hljs-comment">// Add access modes for CSI here</span>n.AddVolumeCapabilityAccessModes([]csi.VolumeCapability_AccessMode_Mode&#123;csi.VolumeCapability_AccessMode_MULTI_NODE_MULTI_WRITER,&#125;)<span class="hljs-comment">// Add service capabilities for CSI here</span>n.AddControllerServiceCapabilities([]csi.ControllerServiceCapability_RPC_Type&#123;csi.ControllerServiceCapability_RPC_CREATE_DELETE_VOLUME,csi.ControllerServiceCapability_RPC_CREATE_DELETE_SNAPSHOT,&#125;)<span class="hljs-keyword">return</span> n&#125;</code></pre></div><p><strong>整个骨架源码树中，命令行解析自己重构使用一些更加方便的命令行解析、日志输出库；结构体部分参考 Hostpath 结构体自己调整，<code>server.go</code> 用来创建 <code>NonBlocking</code> 的 gRPC Server(直接从 Hotspath 样例项目 copy 即可)；然后就是三大 gRPC Server 的实现，由于是 “项目骨架” 所以相关方法我们都返回未实现，后续我们主要来实现这些方法就能让自己写的这个 CSI 插件 work。</strong></p><p><img src="https://cdn.oss.link/markdown/876sk.png" srcset="/img/loading.gif" alt="Unimplemented_gRPC_Server"></p><h3 id="3-4、创建-NFS-CSI-插件骨架"><a href="#3-4、创建-NFS-CSI-插件骨架" class="headerlink" title="3.4、创建 NFS CSI 插件骨架"></a>3.4、创建 NFS CSI 插件骨架</h3><p>有了 CSI 的项目骨架以后，我们只需要简单地修改名字将其重命名为 NFS CSI 插件即可；由于这篇文章是先实现好了 NFS CSI(已经 work) 再来写的，所以 NFS CSI 的源码可以直接参考 <a href="https://github.com/Gozap/csi-nfs" target="_blank" rel="noopener">Gozap/csi-nfs</a> 即可，下面的部分主要介绍三大 gRPC Server 的实现</p><p><img src="https://cdn.oss.link/markdown/kk42j.png" srcset="/img/loading.gif" alt="csi-nfs"></p><h3 id="3-5、实现-Identity-Server"><a href="#3-5、实现-Identity-Server" class="headerlink" title="3.5、实现 Identity Server"></a>3.5、实现 Identity Server</h3><p><img src="https://cdn.oss.link/markdown/r8etm.png" srcset="/img/loading.gif" alt="Identity Server"></p><p>Identity Server 实现相对简单，总共就三个接口；<code>GetPluginInfo</code> 接口返回插件名称版本即可(注意版本号好像只能是 <code>1.1.1</code> 这种，<code>v1.1.1</code> 好像会报错)；<code>Probe</code> 接口用来做健康检测可以直接返回空 response 即可，当然最理想的情况应该是做一些业务逻辑判活；<code>GetPluginCapabilities</code> 接口看起来简单但是要清楚返回的 <code>Capabilities</code> 含义，由于我们的 NFS 插件必然需要响应 <code>CreateVolume</code> 等请求(实现 Controller Server)，所以 cap 必须给予 <code>PluginCapability_Service_CONTROLLER_SERVICE</code>，除此之外如果节点不支持均匀的创建外部存储供应商的 Volume，那么应当同时返回 <code>PluginCapability_Service_VOLUME_ACCESSIBILITY_CONSTRAINTS</code> 以表示 CSI 处理时需要根据集群拓扑作调整；具体的可以查看 gRPC 注释:</p><div class="hljs"><pre><code class="hljs go"><span class="hljs-keyword">const</span> (PluginCapability_Service_UNKNOWN PluginCapability_Service_Type = <span class="hljs-number">0</span><span class="hljs-comment">// CONTROLLER_SERVICE indicates that the Plugin provides RPCs for</span><span class="hljs-comment">// the ControllerService. Plugins SHOULD provide this capability.</span><span class="hljs-comment">// In rare cases certain plugins MAY wish to omit the</span><span class="hljs-comment">// ControllerService entirely from their implementation, but such</span><span class="hljs-comment">// SHOULD NOT be the common case.</span><span class="hljs-comment">// The presence of this capability determines whether the CO will</span><span class="hljs-comment">// attempt to invoke the REQUIRED ControllerService RPCs, as well</span><span class="hljs-comment">// as specific RPCs as indicated by ControllerGetCapabilities.</span>PluginCapability_Service_CONTROLLER_SERVICE PluginCapability_Service_Type = <span class="hljs-number">1</span><span class="hljs-comment">// VOLUME_ACCESSIBILITY_CONSTRAINTS indicates that the volumes for</span><span class="hljs-comment">// this plugin MAY NOT be equally accessible by all nodes in the</span><span class="hljs-comment">// cluster. The CO MUST use the topology information returned by</span><span class="hljs-comment">// CreateVolumeRequest along with the topology information</span><span class="hljs-comment">// returned by NodeGetInfo to ensure that a given volume is</span><span class="hljs-comment">// accessible from a given node when scheduling workloads.</span>PluginCapability_Service_VOLUME_ACCESSIBILITY_CONSTRAINTS PluginCapability_Service_Type = <span class="hljs-number">2</span>)</code></pre></div><h3 id="3-6、实现-Controller-Server"><a href="#3-6、实现-Controller-Server" class="headerlink" title="3.6、实现 Controller Server"></a>3.6、实现 Controller Server</h3><p>Controller Server 实际上对应着 Provisioning and Deleting 阶段；换句话说核心的创建/删除卷、快照等都应在此做实现，针对于本次编写的 NFS 插件仅做最小实现(创建/删除卷)；需要注意的是除了核心的创建删除卷要实现以外还需要实现 <code>ControllerGetCapabilities</code> 方法，该方法返回 Controller Server 的 cap:</p><p><img src="https://cdn.oss.link/markdown/pl0n3.png" srcset="/img/loading.gif" alt="ControllerGetCapabilities"></p><p><code>ControllerGetCapabilities</code> 返回的实际上是在创建驱动时设置的 cscap:</p><div class="hljs"><pre><code class="hljs go">n.AddControllerServiceCapabilities([]csi.ControllerServiceCapability_RPC_Type&#123;csi.ControllerServiceCapability_RPC_CREATE_DELETE_VOLUME,csi.ControllerServiceCapability_RPC_CREATE_DELETE_SNAPSHOT,&#125;)</code></pre></div><p><code>ControllerServiceCapability_RPC_CREATE_DELETE_VOLUME</code> 表示这个 Controller Server 支持创建/删除卷，<code>ControllerServiceCapability_RPC_CREATE_DELETE_SNAPSHOT</code> 表示支持创建/删除快照(快照功能是后来闲的没事加的)；<strong>应该明确的是我们返回了特定的 cap 那就要针对特定方法做实现，因为你一旦声明了这些 cap Kubernetes 就认为有相应请求可以让你处理(你不能吹完牛逼然后关键时刻掉链子)。</strong>针对于可以返回哪些 cscap 可以通过这些 gRPC 常量来查看:</p><div class="hljs"><pre><code class="hljs go"><span class="hljs-keyword">const</span> (ControllerServiceCapability_RPC_UNKNOWN                  ControllerServiceCapability_RPC_Type = <span class="hljs-number">0</span>ControllerServiceCapability_RPC_CREATE_DELETE_VOLUME     ControllerServiceCapability_RPC_Type = <span class="hljs-number">1</span>ControllerServiceCapability_RPC_PUBLISH_UNPUBLISH_VOLUME ControllerServiceCapability_RPC_Type = <span class="hljs-number">2</span>ControllerServiceCapability_RPC_LIST_VOLUMES             ControllerServiceCapability_RPC_Type = <span class="hljs-number">3</span>ControllerServiceCapability_RPC_GET_CAPACITY             ControllerServiceCapability_RPC_Type = <span class="hljs-number">4</span><span class="hljs-comment">// Currently the only way to consume a snapshot is to create</span><span class="hljs-comment">// a volume from it. Therefore plugins supporting</span><span class="hljs-comment">// CREATE_DELETE_SNAPSHOT MUST support creating volume from</span><span class="hljs-comment">// snapshot.</span>ControllerServiceCapability_RPC_CREATE_DELETE_SNAPSHOT ControllerServiceCapability_RPC_Type = <span class="hljs-number">5</span>ControllerServiceCapability_RPC_LIST_SNAPSHOTS         ControllerServiceCapability_RPC_Type = <span class="hljs-number">6</span><span class="hljs-comment">// Plugins supporting volume cloning at the storage level MAY</span><span class="hljs-comment">// report this capability. The source volume MUST be managed by</span><span class="hljs-comment">// the same plugin. Not all volume sources and parameters</span><span class="hljs-comment">// combinations MAY work.</span>ControllerServiceCapability_RPC_CLONE_VOLUME ControllerServiceCapability_RPC_Type = <span class="hljs-number">7</span><span class="hljs-comment">// Indicates the SP supports ControllerPublishVolume.readonly</span><span class="hljs-comment">// field.</span>ControllerServiceCapability_RPC_PUBLISH_READONLY ControllerServiceCapability_RPC_Type = <span class="hljs-number">8</span><span class="hljs-comment">// See VolumeExpansion for details.</span>ControllerServiceCapability_RPC_EXPAND_VOLUME ControllerServiceCapability_RPC_Type = <span class="hljs-number">9</span><span class="hljs-comment">// Indicates the SP supports the</span><span class="hljs-comment">// ListVolumesResponse.entry.published_nodes field</span>ControllerServiceCapability_RPC_LIST_VOLUMES_PUBLISHED_NODES ControllerServiceCapability_RPC_Type = <span class="hljs-number">10</span><span class="hljs-comment">// Indicates that the Controller service can report volume</span><span class="hljs-comment">// conditions.</span><span class="hljs-comment">// An SP MAY implement `VolumeCondition` in only the Controller</span><span class="hljs-comment">// Plugin, only the Node Plugin, or both.</span><span class="hljs-comment">// If `VolumeCondition` is implemented in both the Controller and</span><span class="hljs-comment">// Node Plugins, it SHALL report from different perspectives.</span><span class="hljs-comment">// If for some reason Controller and Node Plugins report</span><span class="hljs-comment">// misaligned volume conditions, CO SHALL assume the worst case</span><span class="hljs-comment">// is the truth.</span><span class="hljs-comment">// Note that, for alpha, `VolumeCondition` is intended be</span><span class="hljs-comment">// informative for humans only, not for automation.</span>ControllerServiceCapability_RPC_VOLUME_CONDITION ControllerServiceCapability_RPC_Type = <span class="hljs-number">11</span><span class="hljs-comment">// Indicates the SP supports the ControllerGetVolume RPC.</span><span class="hljs-comment">// This enables COs to, for example, fetch per volume</span><span class="hljs-comment">// condition after a volume is provisioned.</span>ControllerServiceCapability_RPC_GET_VOLUME ControllerServiceCapability_RPC_Type = <span class="hljs-number">12</span>)</code></pre></div><p>当声明了 <code>ControllerServiceCapability_RPC_CREATE_DELETE_VOLUME</code> 以后针对创建删除卷方法 <code>CreateVolume</code>、<code>DeleteVolume</code> 做实现即可；这两个方法实现就是常规的业务逻辑层面没什么技术含量，对于外部存储供应商是 NFS 来说无非就是接到一个 <code>CreateVolumeRequest</code> ，然后根据 request 给的 volume name 啥的信息自己执行一下在 NFS Server 上 <code>mkdir</code> ，删除卷处理就是反向的 <code>rm -rf dir</code>；在两个方法的处理中可能额外掺杂一些校验等其他的辅助实现。</p><p><img src="https://cdn.oss.link/markdown/jkhb6.png" srcset="/img/loading.gif" alt="CreateVolume"></p><p><img src="https://cdn.oss.link/markdown/96ij8.png" srcset="/img/loading.gif" alt="DeleteVolume"></p><p><strong>最后有几点需要注意的地方:</strong></p><ul><li><strong>幂等性: Kubernetes 可能由于一些其他原因会重复发出请求(比如超时重试)，此时一定要保证创建/删除卷实现的幂等性，简单地说 Kubernetes 连续两次调用同一个卷创建 CSI 插件应当实现自动去重过滤，不能调用两次返回两个新卷。</strong></li><li><strong>数据回写: 要明白的是 Controller Server 是 Provisioning and Deleting 阶段，此时还没有真正挂载到 Pod，所以就本地使用 NFS 作为存储后端来说 <code>mkdir</code> 以后要把目录、NFS Server 地址等必要信息通过 VolumeContext 返回给 Kubernetes，Kubernetes 接下来会传递给 Node Driver(Mount/Umount)用。</strong></li><li><strong>预挂载: 当然这个问题目前只存在在 NFS 作为存储后端中，问题核心在于在创建卷进行 <code>mkdir</code> 之前，NFS 应该已经确保 mount 到了 Controller Server 容器本地，所以目前的做法就是启动 Controller Server 时就执行 NFS 挂载；如果用其他的后端存储比如阿里云存储时也要考虑在创建卷之前相关的 API Client 是否可用。</strong></li></ul><h3 id="3-7、实现-Node-Server"><a href="#3-7、实现-Node-Server" class="headerlink" title="3.7、实现 Node Server"></a>3.7、实现 Node Server</h3><p>Node Server 实际上就是 Node Driver，简单地说当 Controller Server 完成一个卷的创建，并且已经 Attach 到 Node 以后(当然这里的 NFS 不需要 Attach)，Node Server 就需要实现根据给定的信息将卷 Mount 到 Pod 或者从 Pod Umount 掉卷；同样的 Node Server 也许要返回一些信息来告诉 Kubernetes 自己的详细情况，这部份由两个方法完成 <code>NodeGetInfo</code> 和 <code>NodeGetCapabilities</code></p><p><img src="https://cdn.oss.link/markdown/ts3l9.png" srcset="/img/loading.gif" alt="NodeGetInfo_NodeGetCapabilities"></p><p><code>NodeGetInfo</code> 中返回节点的常规信息，比如 Node ID、最大允许的 Volume 数量、集群拓扑信息等；<code>NodeGetCapabilities</code> 返回这个 Node 的 cap，由于我们的 NFS 是真的啥也不支持，所以只好返回 <code>NodeServiceCapability_RPC_UNKNOWN</code>，至于其他的 cap 如下(含义自己看注释):</p><div class="hljs"><pre><code class="hljs go"><span class="hljs-keyword">const</span> (NodeServiceCapability_RPC_UNKNOWN              NodeServiceCapability_RPC_Type = <span class="hljs-number">0</span>NodeServiceCapability_RPC_STAGE_UNSTAGE_VOLUME NodeServiceCapability_RPC_Type = <span class="hljs-number">1</span><span class="hljs-comment">// If Plugin implements GET_VOLUME_STATS capability</span><span class="hljs-comment">// then it MUST implement NodeGetVolumeStats RPC</span><span class="hljs-comment">// call for fetching volume statistics.</span>NodeServiceCapability_RPC_GET_VOLUME_STATS NodeServiceCapability_RPC_Type = <span class="hljs-number">2</span><span class="hljs-comment">// See VolumeExpansion for details.</span>NodeServiceCapability_RPC_EXPAND_VOLUME NodeServiceCapability_RPC_Type = <span class="hljs-number">3</span><span class="hljs-comment">// Indicates that the Node service can report volume conditions.</span><span class="hljs-comment">// An SP MAY implement `VolumeCondition` in only the Node</span><span class="hljs-comment">// Plugin, only the Controller Plugin, or both.</span><span class="hljs-comment">// If `VolumeCondition` is implemented in both the Node and</span><span class="hljs-comment">// Controller Plugins, it SHALL report from different</span><span class="hljs-comment">// perspectives.</span><span class="hljs-comment">// If for some reason Node and Controller Plugins report</span><span class="hljs-comment">// misaligned volume conditions, CO SHALL assume the worst case</span><span class="hljs-comment">// is the truth.</span><span class="hljs-comment">// Note that, for alpha, `VolumeCondition` is intended to be</span><span class="hljs-comment">// informative for humans only, not for automation.</span>NodeServiceCapability_RPC_VOLUME_CONDITION NodeServiceCapability_RPC_Type = <span class="hljs-number">4</span>)</code></pre></div><p>剩下的核心方法 <code>NodePublishVolume</code> 和 <code>NodeUnpublishVolume</code> 挂载/卸载卷同 Controller Server 创建删除卷一样都是业务处理，没啥可说的，按步就班的调用一下 Mount 上就行；<strong>唯一需要注意的点就是这里也要保证幂等性，同时由于要操作 Pod 目录，所以要把宿主机的 <code>/var/lib/kubelet/pods</code> 目录挂载到 Node Server 容器里。</strong></p><h3 id="3-8、部署测试-NFS-插件"><a href="#3-8、部署测试-NFS-插件" class="headerlink" title="3.8、部署测试 NFS 插件"></a>3.8、部署测试 NFS 插件</h3><p>NFS 插件写完以后就可以实体环境做测试了，测试方法不同插件可能并不相同，本 NFS 插件可以直接使用源码项目的 <code>deploy</code> 目录创建相关容器做测试(需要根据自己的 NFS Server 修改一些参数)。针对于如何部署下面做一下简单说明:</p><p>三大阶段笼统的其实对应着三个 Sidecar Container:</p><ul><li>Provisioning and Deleting: external-provisioner</li><li>Attaching and Detaching: external-attacher</li><li>Mount and Umount: node-driver-registrar</li></ul><p><strong>我们的 NFS CSI 插件不需要 Attach，所以 external-attacher 也不需要部署；external-provisioner 只响应创建删除卷请求，所以通过 Deployment 部署足够多的复本保证 HA 就行；由于 Pod 不一定会落到那个节点上，理论上任意 Node 都可能有 Mount/Umount 行为，所以 node-driver-registrar 要以 Daemonset 方式部署保证每个节点都有一个。</strong></p><h2 id="四、其他说明"><a href="#四、其他说明" class="headerlink" title="四、其他说明"></a>四、其他说明</h2><h3 id="4-1、前期调试"><a href="#4-1、前期调试" class="headerlink" title="4.1、前期调试"></a>4.1、前期调试</h3><p>在前期代码编写时一般都是 “盲狙”，就是按照自己的理解无脑实现，这时候可能离实际部署还很远，但是只是单纯的想知道某个 Request 里面到底是什么个东西，这时候你可以利用 <code>mritd/socket2tcp</code> 容器模拟监听 socket 文件，然后将请求转发到你的 IDE 监听端口上，然后再进行 Debug。</p><p>可能有人会问: “我直接在 Sidecar Containers 里写个 tcp 地址不就行了，还转发毛线，这不是脱裤子放屁多此一举么？”，但是这里我友情提醒一下，Sidecar Containers 指定 CSI 地址时填写非 socket 类型的地址是不好使的，会直接启动失败。</p><h3 id="4-2、后期调试"><a href="#4-2、后期调试" class="headerlink" title="4.2、后期调试"></a>4.2、后期调试</h3><p>等到代码编写到后期其实就开始 “真机” 调试了，这时候其实不必使用原始的打日志调试方法，NFS CSI 的项目源码中的 <code>Dockerfile.debug</code> 提供了使用 dlv 做远程调试的样例；具体怎么配合 IDE 做远程调试请自行 Google。</p><h3 id="4-3、其他功能实现"><a href="#4-3、其他功能实现" class="headerlink" title="4.3、其他功能实现"></a>4.3、其他功能实现</h3><p>其他功能根据需要可以自己酌情实现，比如创建/删除快照功能；对于 NFS 插件来说 NFS Server 又没有 API，所以最简单最 low 的办法当然是 <code>tar -zcvf</code> 了(哈哈哈(超大声))，当然性能么就不要提了。</p><h2 id="五、总结"><a href="#五、总结" class="headerlink" title="五、总结"></a>五、总结</h2><p><strong>CSI 开发其实是针对 Kubernetes CSI Sidecar Containers 的 gRPC 开发，根据自己需求实现三大阶段中对应三大 gRPC Server 相应方法即可；相关功能要保证幂等性，cap 要看文档根据实际情况返回。</strong></p><h2 id="六、参考文档"><a href="#六、参考文档" class="headerlink" title="六、参考文档"></a>六、参考文档</h2><ul><li><a href="https://kubernetes-csi.github.io/docs/introduction.html" target="_blank" rel="noopener">https://kubernetes-csi.github.io/docs/introduction.html</a></li><li><a href="https://github.com/container-storage-interface/spec" target="_blank" rel="noopener">https://github.com/container-storage-interface/spec</a></li></ul>]]></content:encoded>
      
      
      <category domain="https://mritd.com/categories/kubernetes/">Kubernetes</category>
      
      
      <category domain="https://mritd.com/tags/csi/">CSI</category>
      
      
      <comments>https://mritd.com/2020/08/19/how-to-write-a-csi-driver-for-kubernetes/#disqus_thread</comments>
      
    </item>
    
    <item>
      <title>树莓派4 Manjaro 系统定制</title>
      <link>https://mritd.com/2020/08/19/make-a-custom-manjaro-image-for-rpi4/</link>
      <guid>https://mritd.com/2020/08/19/make-a-custom-manjaro-image-for-rpi4/</guid>
      <pubDate>Wed, 19 Aug 2020 06:05:00 GMT</pubDate>
      
      <description>最近入手了新玩具 &quot;吃灰派4&quot;，这一代性能提升真的很大，所以买回来是真的没办法 &quot;吃灰&quot; 了；但是由于目前 64bit 系统比较难产，所以只能自己定义一下 Manjaro 了。</description>
      
      
      
      <content:encoded><![CDATA[<h2 id="一、目前的系统现状"><a href="#一、目前的系统现状" class="headerlink" title="一、目前的系统现状"></a>一、目前的系统现状</h2><p>截止本文编写时间，树莓派4 官方系统仍然不支持 64bit；但是当我在 3b+ 上使用 arch 64bit 以后我发现 32bit 系统和 64bit 系统装在同一个树莓派上在使用时那就是两个完全不一样的树莓派…所以对于这个新的 rpi4 那么必需要用 64bit 的系统；而当前我大致查看到支持 64bit 的系统只有 Ubuntu20、Manjaro 两个，Ubuntu 对我来说太重了(虽然服务器上我一直是 Ubuntu，但是 rpi 上我选择说 “不”)，Manjaro 基于 Arch 这种非常轻量的系统非常适合树莓派这种开发板，所以最终我选择了 Manjaro。但是万万没想到的是 Manjaro 都是带 KDE 什么的图形化的，而我的树莓派只想仍在角落里跑东西，所以说图形化这东西对我来说也没啥用，最后迫于无奈只能自己通过 Manjaro 的工具自己定制了。</p><h2 id="二、manjaro-arm-tools"><a href="#二、manjaro-arm-tools" class="headerlink" title="二、manjaro-arm-tools"></a>二、manjaro-arm-tools</h2><p>经过几经查找各种 Google，发现了 Manjaro 官方提供了自定义创建 arm 镜像的工具 <a href="https://gitlab.manjaro.org/manjaro-arm/applications/manjaro-arm-tools" target="_blank" rel="noopener">manjaro-arm-tools</a>，这个工具简单使用如下:</p><ul><li>首先准备一个 Manjaro 系统(虚拟机 x86 即可)</li><li>然后安装 manjaro-arm-tool 所需<a href="https://gitlab.manjaro.org/manjaro-arm/applications/manjaro-arm-tools#dependencies" target="_blank" rel="noopener">依赖工具</a></li><li>添加 Manjaro 的<a href="https://gitlab.manjaro.org/manjaro-arm/applications/manjaro-arm-tools#git-version-from-manjaro-strit-repo" target="_blank" rel="noopener">软件源</a></li><li>安装 manjaro-arm-tool <code>sudo pacman -Syyu manjaro-strit-keyring &amp;&amp; sudo pacman -S manjaro-arm-tools-git</code></li></ul><p>当工具都准备完成后，只需要执行 <code>sudo buildarmimg -d rpi4 -e minimal</code> 即可创建 manjaro 的 rpi4 最小镜像。</p><h2 id="三、系统定制"><a href="#三、系统定制" class="headerlink" title="三、系统定制"></a>三、系统定制</h2><p>在使用 manjaro-arm-tool 创建系统以后发现一些细微的东西需要自己调整，比如网络设置常用软件包等，而 manjaro-arm-tool 工具又没有提供太好的自定义处理的一些 hook，所以最后萌生了自己根据 manjaro-arm-tool 来创建自己的 rpi4 系统定制工具的想法。</p><h3 id="3-1、常用软件包安装"><a href="#3-1、常用软件包安装" class="headerlink" title="3.1、常用软件包安装"></a>3.1、常用软件包安装</h3><p>在查看了 manjaro-arm-tool 的源码后可以看到实际上软件安装就是利用 systemd-nspawn 进入到 arm 系统执行 pacman 安装，自己依葫芦画瓢增加一些常用的软件包安装:</p><div class="hljs"><pre><code class="hljs sh">systemd-nspawn -q --resolv-conf=copy-host --timezone=off -D <span class="hljs-variable">$&#123;ROOTFS_DIR&#125;</span> pacman -Syyu zsh htop vim wget <span class="hljs-built_in">which</span> git make net-tools dnsutils inetutils iproute2 sysstat nload lsof --noconfirm</code></pre></div><h3 id="3-2、pacman-镜像"><a href="#3-2、pacman-镜像" class="headerlink" title="3.2、pacman 镜像"></a>3.2、pacman 镜像</h3><p>在安装软件包时发现安装速读奇慢，研究以后发现是没有使用国内的镜像源，故增加了国内镜像源的处理:</p><div class="hljs"><pre><code class="hljs sh">systemd-nspawn -q --resolv-conf=copy-host --timezone=off -D <span class="hljs-variable">$&#123;ROOTFS_DIR&#125;</span> pacman-mirrors -c China</code></pre></div><h3 id="3-3、网络处理"><a href="#3-3、网络处理" class="headerlink" title="3.3、网络处理"></a>3.3、网络处理</h3><h4 id="3-3-1、有线连接"><a href="#3-3-1、有线连接" class="headerlink" title="3.3.1、有线连接"></a>3.3.1、有线连接</h4><p>默认的 manjaro-arm-tool 创建的系统网络部分采用 dhspcd 做 dhcp 处理，但是我个人感觉一切尽量精简统一还是比较好的；所以准备网络部分完全由 systemd 接管处理，即直接使用 systemd-networkd 和 systemd-resolved；systemd-networkd 处理相对简单，编写一个配置文件然后 enable systemd-networkd 服务即可:</p><p><strong>/etc/systemd/network/10-eth-dhcp.network</strong></p><div class="hljs"><pre><code class="hljs sh">[Match]Name=eth*[Network]DHCP=yes</code></pre></div><p><strong>让 systemd-networkd 开机自启动</strong></p><div class="hljs"><pre><code class="hljs sh">systemd-nspawn -q --resolv-conf=copy-host --timezone=off -D <span class="hljs-variable">$&#123;ROOTFS_DIR&#125;</span> systemctl <span class="hljs-built_in">enable</span> systemd-networkd.service</code></pre></div><p>一开始以为 systemd-resolved 同样 enable 一下就行，后来发现每次开机初始化以后 systemd-resolved 都会被莫明其妙的 disable 掉；经过几经寻找和开 issue 问作者，发现这个操作是被 manjaro-arm-oem-install 包下的脚本执行的，作者的回复意思是大部分带有图形化的版本网络管理工具都会与 systemd-resolved 冲突，所以默认关闭了，这时候我们就要针对 manjaro-arm-oem-install 单独处理一下:</p><div class="hljs"><pre><code class="hljs sh">systemd-nspawn -q --resolv-conf=copy-host --timezone=off -D <span class="hljs-variable">$&#123;ROOTFS_DIR&#125;</span> systemctl <span class="hljs-built_in">enable</span> systemd-resolved.servicesed -i <span class="hljs-string">'s@systemctl disable systemd-resolved.service 1&gt; /dev/null 2&gt;&amp;1@@g'</span> <span class="hljs-variable">$&#123;ROOTFS_DIR&#125;</span>/usr/share/manjaro-arm-oem-install/manjaro-arm-oem-install</code></pre></div><h4 id="3-3-2、无限连接"><a href="#3-3-2、无限连接" class="headerlink" title="3.3.2、无限连接"></a>3.3.2、无限连接</h4><p>有线连接只要 systemd-networkd 处理好就能很好的工作，而无线连接目前有很多方案，我一开始想用 <a href="https://wiki.archlinux.org/index.php/Netctl_(%E7%AE%80%E4%BD%93%E4%B8%AD%E6%96%87)" target="_blank" rel="noopener">netctl</a>，后来发现这东西虽然是 Arch 亲儿子，但是在系统定制时采用 systemd-nspawn 调用不兼容(因为里面调用了 systemd 的一些命令，这些命令一般只有在开机时才可用)，而且只用 netctl 来管理 wifi 还感觉怪怪的，后来我的想法是要么用就全都用，要么就纯手动不要用这些东西，所以最后的方案是 wpa_supplicant + systemd-networkd 一把梭:</p><p><strong>/etc/systemd/network/10-wlan-dhcp.network.example</strong></p><div class="hljs"><pre><code class="hljs sh"><span class="hljs-comment"># 1. Generate wifi configuration (don't modify the name of wpa_supplicant-wlan0.conf file)</span><span class="hljs-comment"># $ wpa_passphrase MyNetwork SuperSecretPassphrase &gt; /etc/wpa_supplicant/wpa_supplicant-wlan0.conf</span><span class="hljs-comment">#</span><span class="hljs-comment"># 2. Connect to wifi automatically after booting</span><span class="hljs-comment"># $ systemctl enable wpa_supplicant@wlan0</span><span class="hljs-comment">#</span><span class="hljs-comment"># 3.Systemd automatically makes dhcp request</span><span class="hljs-comment"># $ cp /etc/systemd/network/10-wlan-dhcp.network.example /etc/systemd/network/10-wlan-dhcp.network</span>[Match]Name=wlan*[Network]DHCP=yes</code></pre></div><h3 id="3-4、内核调整"><a href="#3-4、内核调整" class="headerlink" title="3.4、内核调整"></a>3.4、内核调整</h3><p>在上面的一些调整完成后我就启动系统实体机测试了，测试过程中发现安装 docker 以后会有两个警告，大致意思就是不支持 swap limit 和 cpu limit；查询资料以后发现是内核有两个参数没开启(<code>CONFIG_MEMCG_SWAP</code>、<code>CONFIG_CFS_BANDWIDTH</code>)…当然我这种强迫症是不能忍的，没办法就自己在 rpi4 上重新编译了内核(后来我想想还不如用 arch 32bit 然后自己编译 64bit 内核了):</p><div class="hljs"><pre><code class="hljs sh">git <span class="hljs-built_in">clone</span> https://github.com/mritd/linux-rpi4.git<span class="hljs-built_in">cd</span> linux-rpi4MAKEFLAGS=<span class="hljs-string">'-j4'</span> makepkg</code></pre></div><h3 id="3-5、外壳驱动"><a href="#3-5、外壳驱动" class="headerlink" title="3.5、外壳驱动"></a>3.5、外壳驱动</h3><p>由于我的 rpi4 配的是 ARGON ONE 的外壳，所以电源按钮还有风扇需要驱动才能完美工作，没办法我又编译了 ARGON ONE 外壳的驱动:</p><div class="hljs"><pre><code class="hljs sh">git <span class="hljs-built_in">clone</span> https://github.com/mritd/argonone.git<span class="hljs-built_in">cd</span> argononemakepkg</code></pre></div><h2 id="四、定制脚本"><a href="#四、定制脚本" class="headerlink" title="四、定制脚本"></a>四、定制脚本</h2><p>综合以上的各种修改以后，我从 manjaro-arm-tool 提取出了定制化的 rpi4 的编译脚本，该脚本目前存放在 <a href="https://github.com/mritd/manjaro-rpi4" target="_blank" rel="noopener">mritd/manjaro-rpi4</a> 仓库中；目前使用此脚本编译的系统镜像默认进行了以下处理:</p><ul><li>调整 pacman mirror 为中国</li><li>安装常用软件包(zsh htop vim wget which…)</li><li>有线网络完全的 systemd-networkd 接管，resolv.conf 由 systemd-resolved 接管</li><li>无线网络由 wpa_supplicant 和 systemd-networkd 接管</li><li>安装自行编译的内核以消除 docker 警告(<strong>自编译内核不影响升级，升级/覆盖安装后自动恢复</strong>)</li></ul><p>至于 ARGON ONE 的外壳驱动只在 resources 目录下提供了安装包，并未默认安装到系统。</p>]]></content:encoded>
      
      
      <category domain="https://mritd.com/categories/linux/">Linux</category>
      
      
      <category domain="https://mritd.com/tags/linux/">Linux</category>
      
      <category domain="https://mritd.com/tags/manjaro/">Manjaro</category>
      
      
      <comments>https://mritd.com/2020/08/19/make-a-custom-manjaro-image-for-rpi4/#disqus_thread</comments>
      
    </item>
    
    <item>
      <title>如何在 Filebeat 端进行日志处理</title>
      <link>https://mritd.com/2020/08/19/how-to-modify-filebeat-source-code-to-processing-logs/</link>
      <guid>https://mritd.com/2020/08/19/how-to-modify-filebeat-source-code-to-processing-logs/</guid>
      <pubDate>Wed, 19 Aug 2020 06:01:00 GMT</pubDate>
      
      <description>本文主要介绍在 ELK 日志系统中，日志切割处理直接在 filebeat 端实现的一些方式；其中包括 filebeat processor 的扩展以及 module 扩展等。</description>
      
      
      
      <content:encoded><![CDATA[<h2 id="一、起因"><a href="#一、起因" class="headerlink" title="一、起因"></a>一、起因</h2><p>目前某项目组日志需要做切割处理，针对日志信息进行分割并提取 k/v 放入 es 中方便查询。这种需求在传统 ELK 中应当由 logstash 组件完成，通过 <code>gork</code> 等操作对日志进行过滤、切割等处理。不过很尴尬的是我并不会 ruby，logstash pipeline 的一些配置我也是极其头疼，而且还不想学…更不凑巧的是我会写点 go，<strong>那么理所应当的此时的我对 filebeat 源码产生了一些想法，比如我直接在 filebeat 端完成日志处理，然后直接发 es/logstash，这样似乎更方便，而且还能分摊 logstash 的压力，我感觉这个操作并不过分😂…</strong></p><h2 id="二、需求"><a href="#二、需求" class="headerlink" title="二、需求"></a>二、需求</h2><p>目前某项目组 java 日志格式如下:</p><div class="hljs"><pre><code class="hljs sh">2020-04-30 21:56:30.117$<span class="hljs-variable">$api</span>-<span class="hljs-built_in">test</span>-65c8c7cf7f-lng7h$<span class="hljs-variable">$http</span>-nio-8080-exec-3$<span class="hljs-variable">$INFO</span>$<span class="hljs-variable">$com</span>.example.api.common.filter.GlobalDataFilter$<span class="hljs-variable">$GlobalDataFilter</span>.java$<span class="hljs-variable">$95</span>$<span class="hljs-variable">$test</span>build commonData from header :&#123;<span class="hljs-string">"romVersion"</span>:<span class="hljs-string">"W_V2.1.4"</span>,<span class="hljs-string">"softwareVersion"</span>:<span class="hljs-string">"15"</span>,<span class="hljs-string">"token"</span>:<span class="hljs-string">"aFxANNM3pnRYpohvLMSmENydgFSfsmFMgCbFWAosIE="</span>&#125;$$$$</code></pre></div><p>目前开发约定格式为日志通过 <code>$$</code> 进行分割，日志格式比较简单，但是 logstash 共用(nginx 等各种日志都会往这个 logstash 输出)，不想去折腾 logstash 配置的情况下，只需要让 filebeat 能够直接切割并设置好 k/v 对应既可。</p><h2 id="三、filebeat-module"><a href="#三、filebeat-module" class="headerlink" title="三、filebeat module"></a>三、filebeat module</h2><blockquote><p>module 部份只做简介，以为实际上依托 es 完成，意义不大。</p></blockquote><p>当然在考虑修改 filebeat 源码后，我第一想到的是 filebeat 的 module，这个 module 在官方文档中是个很神奇的东西；通过开启一个 module 就可以对某种日志直接做处理，这种东西似乎就是我想要的；比如我写一个 “项目名” module，然后 filebeat 直接开启这个 module，这个项目的日志就直接自动处理好(听起来就很 “上流”)…</p><p>针对于自定义 module，官方给出了文档: <a href="https://www.elastic.co/guide/en/beats/devguide/current/filebeat-modules-devguide.html" target="_blank" rel="noopener">Creating a New Filebeat Module</a></p><p>按照文档操作如下(假设我们的项目名为 cdm):</p><div class="hljs"><pre><code class="hljs sh"><span class="hljs-comment"># 克隆源码</span>git <span class="hljs-built_in">clone</span> git@github.com:elastic/beats.git<span class="hljs-comment"># 切换到稳定分支</span><span class="hljs-built_in">cd</span> bests &amp;&amp; git checkout -b v7.6.2 v7.6.2-module<span class="hljs-comment"># 创建 module，GO111MODULE 需要设置为 off</span><span class="hljs-comment"># 在 7.6.2 版本官方尚未开始支持 go mod</span><span class="hljs-built_in">cd</span> filebeatGO111MODULE=off make create-module MODULE=cdm</code></pre></div><p>创建完成后目录结构如下</p><div class="hljs"><pre><code class="hljs sh">➜  filebeat git:(v7.6.2-module) ✗ tree module/cdmmodule/cdm├── _meta│   ├── config.yml│   ├── docs.asciidoc│   └── fields.yml└── module.yml1 directory, 4 files</code></pre></div><p>这几个文件具体作用<a href="https://www.elastic.co/guide/en/beats/devguide/current/filebeat-modules-devguide.html" target="_blank" rel="noopener">官方文档</a>都有详细的描述；但是根据文档描述光有这几个文件是不够的，<strong>module 只是一个处理集合的定义，尚未包含任何处理，针对真正的处理需要继续创建 fileset，fileset 简单的理解就是针对具体的一组文件集合的处理；</strong>例如官方 nginx module 中包含两个 fileset: <code>access</code> 和 <code>error</code>，这两个一个针对 access 日志处理一个针对 error 日志进行处理；在 fileset 中可以设置默认文件位置、处理方式。</p><p><strong>But… 我翻了 nginx module 的样例配置才发现，module 这个东西实质上只做定义和存储处理表达式，具体的切割处理实际上交由 es 的 <a href="https://www.elastic.co/guide/en/elasticsearch/reference/current/ingest.html" target="_blank" rel="noopener">Ingest Node</a> 处理；表达式里仍需要定义 <code>grok</code> 等操作，而且这东西最终会编译到 go 静态文件里；</strong>此时的我想说一句 “MMP”，本来我是不像写 grok 啥的才来折腾 filebeat，结果这个 module 折腾一圈还是要写 grok 啥的，而且这东西直接借助 es 完成导致压力回到了 es 同时每次修改还得重新编译 filebeat… 所以折腾到这我就放弃了，这已经违背了当初的目的，有兴趣的可以参考以下文档继续折腾:</p><ul><li><a href="https://www.elastic.co/guide/en/beats/devguide/current/filebeat-modules-devguide.html" target="_blank" rel="noopener">Creating a New Filebeat Module</a></li><li><a href="https://www.elastic.co/guide/en/elasticsearch/reference/current/ingest.html" target="_blank" rel="noopener">Ingest nodeedit</a></li><li><a href="https://www.elastic.co/guide/en/elasticsearch/reference/current/ingest-apis.html" target="_blank" rel="noopener">Ingest APIs</a></li><li><a href="https://www.elastic.co/guide/en/elasticsearch/reference/current/ingest-processors.html" target="_blank" rel="noopener">Processors</a></li></ul><h2 id="四、filebeat-processors"><a href="#四、filebeat-processors" class="headerlink" title="四、filebeat processors"></a>四、filebeat processors</h2><p>经历了 module 的失望以后，我把目光对准了 processors；processors 是 filebeat 一个强大的功能，顾名思义它可以对 filbeat 收集到的日志进行一些处理；从官方 <a href="https://www.elastic.co/guide/en/beats/filebeat/current/filtering-and-enhancing-data.html" target="_blank" rel="noopener">Processors</a> 页面可以看到其内置了大量的 processor；这些 processor 大部份都是直接对日志进行 “写” 操作，所以理论上我们自己写一个 processor 就可以 “为所欲为+为所欲为=为所欲为”。</p><p>不过不幸的是关于 processor 的开发官方并未给出文档，官方认为这是一个 <code>high level</code> 的东西，不过也找到了一个 issue 对其做了相关回答: <a href="https://github.com/elastic/beats/issues/6760" target="_blank" rel="noopener">How do I write a processor plugin by myself</a>；所以最好的办法就是直接看已有 processor 的源码抄一个。</p><p>理所应当的找了一个软柿子捏: <code>add_host_metadata</code>，add_host_metadata processor 顾名思义在每个日志事件(以下简称为 event)中加入宿主机的信息，比如 hostname 啥的；以下为 add_host_metadata processor 的文件结构(processors 代码存储在 <code>libbeat/processors</code> 目录下)。</p><p><img src="https://cdn.oss.link/markdown/axucc.jpg" srcset="/img/loading.gif" alt="dir_tree"></p><p>通过阅读源码和 issue 的回答可以看出，我们自定义的 processor 只需要实现 <a href="https://godoc.org/github.com/elastic/beats/libbeat/processors#Processor" target="_blank" rel="noopener">Processor interface</a> 既可，这个接口定义如下:</p><p><img src="https://cdn.oss.link/markdown/xuja6.png" srcset="/img/loading.gif" alt="Processor interface"></p><p>通过查看 add_host_metadata 的源码，<code>String() string</code> 方法只需要返回这个 processor 名称既可(可以包含必要的配置信息)；<strong>而 <code>Run(event *beat.Event) (*beat.Event, error)</code> 方法表示在每一条日志被读取后都会转换为一个 event 对象，我们在方法内进行处理然后把 event 返回既可(其他 processor 可能也要处理)。</strong></p><p><img src="https://cdn.oss.link/markdown/jhtnx.png" srcset="/img/loading.gif" alt="add_host_metadata source"></p><p>有了这些信息就简单得多了，毕竟作为<strong>一名合格的 CCE(Ctrl C + Ctrl V + Engineer)</strong> 抄这种操作还是很简单的，直接照猫画虎写一个就行了</p><p>config.go</p><div class="hljs"><pre><code class="hljs go"><span class="hljs-keyword">package</span> cmd<span class="hljs-comment">// Config for cdm processor.</span><span class="hljs-keyword">type</span> Config <span class="hljs-keyword">struct</span> &#123;Name           <span class="hljs-keyword">string</span>          <span class="hljs-string">`config:"name"`</span>&#125;<span class="hljs-function"><span class="hljs-keyword">func</span> <span class="hljs-title">defaultConfig</span><span class="hljs-params">()</span> <span class="hljs-title">Config</span></span> &#123;<span class="hljs-keyword">return</span> Config&#123;&#125;&#125;</code></pre></div><p>cdm.go</p><div class="hljs"><pre><code class="hljs go"><span class="hljs-keyword">package</span> cmd<span class="hljs-keyword">import</span> (<span class="hljs-string">"strings"</span><span class="hljs-string">"github.com/elastic/beats/libbeat/logp"</span><span class="hljs-string">"github.com/pkg/errors"</span><span class="hljs-string">"github.com/elastic/beats/libbeat/beat"</span><span class="hljs-string">"github.com/elastic/beats/libbeat/common"</span><span class="hljs-string">"github.com/elastic/beats/libbeat/processors"</span>jsprocessor <span class="hljs-string">"github.com/elastic/beats/libbeat/processors/script/javascript/module/processor"</span>)<span class="hljs-function"><span class="hljs-keyword">func</span> <span class="hljs-title">init</span><span class="hljs-params">()</span></span> &#123;processors.RegisterPlugin(<span class="hljs-string">"cdm"</span>, New)jsprocessor.RegisterPlugin(<span class="hljs-string">"CDM"</span>, New)&#125;<span class="hljs-keyword">type</span> cdm <span class="hljs-keyword">struct</span> &#123;config Configfields []<span class="hljs-keyword">string</span>log    *logp.Logger&#125;<span class="hljs-keyword">const</span> (processorName = <span class="hljs-string">"cdm"</span>logName       = <span class="hljs-string">"processor.cdm"</span>)<span class="hljs-comment">// New constructs a new cdm processor.</span><span class="hljs-function"><span class="hljs-keyword">func</span> <span class="hljs-title">New</span><span class="hljs-params">(cfg *common.Config)</span> <span class="hljs-params">(processors.Processor, error)</span></span> &#123;<span class="hljs-comment">// 配置文件里就一个 Name 字段，结构体留着以后方便扩展</span>config := defaultConfig()<span class="hljs-keyword">if</span> err := cfg.Unpack(&amp;config); err != <span class="hljs-literal">nil</span> &#123;<span class="hljs-keyword">return</span> <span class="hljs-literal">nil</span>, errors.Wrapf(err, <span class="hljs-string">"fail to unpack the %v configuration"</span>, processorName)&#125;p := &amp;cdm&#123;config: config,<span class="hljs-comment">// 待分割的每段日志对应的 key</span>fields: []<span class="hljs-keyword">string</span>&#123;<span class="hljs-string">"timestamp"</span>, <span class="hljs-string">"hostname"</span>, <span class="hljs-string">"thread"</span>, <span class="hljs-string">"level"</span>, <span class="hljs-string">"logger"</span>, <span class="hljs-string">"file"</span>, <span class="hljs-string">"line"</span>, <span class="hljs-string">"serviceName"</span>, <span class="hljs-string">"traceId"</span>, <span class="hljs-string">"feTraceId"</span>, <span class="hljs-string">"msg"</span>, <span class="hljs-string">"exception"</span>&#125;,log:    logp.NewLogger(logName),&#125;<span class="hljs-keyword">return</span> p, <span class="hljs-literal">nil</span>&#125;<span class="hljs-comment">// 真正的日志处理逻辑</span><span class="hljs-comment">// 为了保证后面的 processor 正常处理，这里面没有 return 任何 error，只是简单的打印</span><span class="hljs-function"><span class="hljs-keyword">func</span> <span class="hljs-params">(p *cdm)</span> <span class="hljs-title">Run</span><span class="hljs-params">(event *beat.Event)</span> <span class="hljs-params">(*beat.Event, error)</span></span> &#123;<span class="hljs-comment">// 尝试获取 message，理论上这一步不应该出现问题</span>msg, err := event.GetValue(<span class="hljs-string">"message"</span>)<span class="hljs-keyword">if</span> err != <span class="hljs-literal">nil</span> &#123;p.log.Error(err)<span class="hljs-keyword">return</span> event, <span class="hljs-literal">nil</span>&#125;message, ok := msg.(<span class="hljs-keyword">string</span>)<span class="hljs-keyword">if</span> !ok &#123;p.log.Error(<span class="hljs-string">"failed to parse message"</span>)<span class="hljs-keyword">return</span> event, <span class="hljs-literal">nil</span>&#125;<span class="hljs-comment">// 分割日志</span>fieldsValue := strings.Split(message, <span class="hljs-string">"$$"</span>)p.log.Debugf(<span class="hljs-string">"message fields: %v"</span>, fieldsVaule)<span class="hljs-comment">// 为了保证不会出现数组越界需要判断一下(万一弄出个格式不正常的日志过来保证不崩)</span><span class="hljs-keyword">if</span> <span class="hljs-built_in">len</span>(fieldsValue) &lt; <span class="hljs-built_in">len</span>(p.fields) &#123;p.log.Errorf(<span class="hljs-string">"incorrect field length: %d, expected length: %d"</span>, <span class="hljs-built_in">len</span>(fieldsValue), <span class="hljs-built_in">len</span>(p.fields))<span class="hljs-keyword">return</span> event, <span class="hljs-literal">nil</span>&#125;<span class="hljs-comment">// 这里遍历然后赛会到 event 既可</span>data := common.MapStr&#123;&#125;<span class="hljs-keyword">for</span> i, k := <span class="hljs-keyword">range</span> p.fields &#123;_, _ = event.PutValue(k, strings.TrimSpace(fieldsValue[i]))&#125;event.Fields.DeepUpdate(data)<span class="hljs-keyword">return</span> event, <span class="hljs-literal">nil</span>&#125;<span class="hljs-function"><span class="hljs-keyword">func</span> <span class="hljs-params">(p *cdm)</span> <span class="hljs-title">String</span><span class="hljs-params">()</span> <span class="hljs-title">string</span></span> &#123;<span class="hljs-keyword">return</span> processorName&#125;</code></pre></div><p>写好代码以后就可以编译一个自己的 filebeat 了(开心ing)</p><div class="hljs"><pre><code class="hljs sh"><span class="hljs-built_in">cd</span> filebeat<span class="hljs-comment"># 如果想交叉编译 linux 需要增加 GOOS=linux 变量 </span>GO111MODULE=off make</code></pre></div><p>然后编写配置文件进行测试，日志相关字段已经成功塞到了 event 中，这样我直接发到 es 或者 logstash 就行了。</p><div class="hljs"><pre><code class="hljs yaml"><span class="hljs-attr">filebeat.inputs:</span><span class="hljs-bullet">-</span> <span class="hljs-attr">type:</span> <span class="hljs-string">log</span>  <span class="hljs-attr">enabled:</span> <span class="hljs-literal">true</span>  <span class="hljs-attr">paths:</span>    <span class="hljs-bullet">-</span> <span class="hljs-string">/Users/natural/tmp/cdm.log</span>  <span class="hljs-attr">processors:</span>    <span class="hljs-bullet">-</span> <span class="hljs-attr">cdm:</span> <span class="hljs-string">~</span>  <span class="hljs-attr">multiline.pattern:</span> <span class="hljs-string">^\d&#123;4&#125;-\d&#123;1,2&#125;-\d&#123;1,2&#125;</span>  <span class="hljs-attr">multiline.match:</span> <span class="hljs-string">after</span>  <span class="hljs-attr">multiline.negate:</span> <span class="hljs-literal">true</span>  <span class="hljs-attr">multiline.timeout:</span> <span class="hljs-string">5s</span></code></pre></div><h2 id="五、script-processor"><a href="#五、script-processor" class="headerlink" title="五、script processor"></a>五、script processor</h2><p>在我折腾完源码以后，反思一下其实这种方式需要自己编译 filebeat，而且每次规则修改也很不方便，唯一的好处真的就是用代码可以 “为所欲为”；反过来一想 “filebeat 有没有 processor 的扩展呢？脚本热加载那种？” 答案是使用 script processor，<strong>script processor 虽然名字上是个 processor，实际上其包含了完整的 ECMA 5.1 js 规范实现；结论就是我们可以写一些 js 脚本来处理日志，然后 filebeat 每次启动后加载这些脚本既可。</strong></p><p>script processor 的使用方式很简单，js 文件中只需要包含一个 <code>function process(event)</code> 方法既可，与自己用 go 实现的 processor 类似，每行日志也会形成一个 event 对象然后调用这个方法进行处理；目前 event 对象可用的 api 需要参考<a href="https://www.elastic.co/guide/en/beats/filebeat/current/processor-script.html#_event_api" target="_blank" rel="noopener">官方文档</a>；<strong>需要注意的是 script processor 目前只支持 ECMA 5.1 语法规范，超过这个范围的语法是不被支持；</strong>实际上其根本是借助了 <a href="https://github.com/dop251/goja" target="_blank" rel="noopener">https://github.com/dop251/goja</a> 这个库来实现的。同时为了方便开发调试，script processor 也增加了一些 nodejs 的兼容 module，比如 <code>console.log</code> 等方法是可用的；以下为 js 处理上面日志的逻辑:</p><div class="hljs"><pre><code class="hljs js"><span class="hljs-keyword">var</span> <span class="hljs-built_in">console</span> = <span class="hljs-built_in">require</span>(<span class="hljs-string">'console'</span>);<span class="hljs-keyword">var</span> fileds = <span class="hljs-keyword">new</span> <span class="hljs-built_in">Array</span>(<span class="hljs-string">"timestamp"</span>, <span class="hljs-string">"hostname"</span>, <span class="hljs-string">"thread"</span>, <span class="hljs-string">"level"</span>, <span class="hljs-string">"logger"</span>, <span class="hljs-string">"file"</span>, <span class="hljs-string">"line"</span>, <span class="hljs-string">"serviceName"</span>, <span class="hljs-string">"traceId"</span>, <span class="hljs-string">"feTraceId"</span>, <span class="hljs-string">"msg"</span>, <span class="hljs-string">"exception"</span>)<span class="hljs-function"><span class="hljs-keyword">function</span> <span class="hljs-title">process</span>(<span class="hljs-params">event</span>) </span>&#123;    <span class="hljs-keyword">var</span> message = event.Get(<span class="hljs-string">"message"</span>);    <span class="hljs-keyword">if</span> (message == <span class="hljs-literal">null</span> || message == <span class="hljs-literal">undefined</span> || message == <span class="hljs-string">''</span>) &#123;        <span class="hljs-built_in">console</span>.log(<span class="hljs-string">"failed to get message"</span>);        <span class="hljs-keyword">return</span>    &#125;    <span class="hljs-keyword">var</span> fieldValues = message.split(<span class="hljs-string">"$$"</span>);    <span class="hljs-keyword">if</span> (fieldValues.length&lt;fileds.length) &#123;        <span class="hljs-built_in">console</span>.log(<span class="hljs-string">"incorrect field length"</span>);        <span class="hljs-keyword">return</span>    &#125;    <span class="hljs-keyword">for</span> (<span class="hljs-keyword">var</span> i = <span class="hljs-number">0</span>; i &lt; fileds.length; ++i) &#123;        event.Put(fileds[i],fieldValues[i].trim())    &#125;&#125;</code></pre></div><p>写好脚本后调整配置测试既可，如果 js 编写有问题，可以通过 <code>console.log</code> 来打印日志进行不断的调试</p><div class="hljs"><pre><code class="hljs yaml"><span class="hljs-attr">filebeat.inputs:</span><span class="hljs-bullet">-</span> <span class="hljs-attr">type:</span> <span class="hljs-string">log</span>  <span class="hljs-attr">enabled:</span> <span class="hljs-literal">true</span>  <span class="hljs-attr">paths:</span>    <span class="hljs-bullet">-</span> <span class="hljs-string">/Users/natural/tmp/cdm.log</span>  <span class="hljs-attr">processors:</span>    <span class="hljs-bullet">-</span> <span class="hljs-attr">script:</span>        <span class="hljs-attr">lang:</span> <span class="hljs-string">js</span>        <span class="hljs-attr">id:</span> <span class="hljs-string">cdm</span>        <span class="hljs-attr">file:</span> <span class="hljs-string">cdm.js</span>  <span class="hljs-attr">multiline.pattern:</span> <span class="hljs-string">^\d&#123;4&#125;-\d&#123;1,2&#125;-\d&#123;1,2&#125;</span>  <span class="hljs-attr">multiline.match:</span> <span class="hljs-string">after</span>  <span class="hljs-attr">multiline.negate:</span> <span class="hljs-literal">true</span>  <span class="hljs-attr">multiline.timeout:</span> <span class="hljs-string">5s</span></code></pre></div><p><strong>需要注意的是目前 <code>lang</code> 的值只能为 <code>javascript</code> 和 <code>js</code>(官方文档写的只能是 <code>javascript</code>)；根据代码来看后续 script processor 有可能支持其他脚本语言，个人认为主要取决于其他脚本语言有没有纯 go 实现的 runtime，如果有的话未来很有可能被整合到 script processor 中。</strong></p><p><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1gegg80j1gmj31nc0u0wpa.jpg" srcset="/img/loading.gif" alt="script processor"></p><h2 id="六、其他-processor"><a href="#六、其他-processor" class="headerlink" title="六、其他 processor"></a>六、其他 processor</h2><p>研究完 script processor 后我顿时对其他 processor 也产生了兴趣，随着更多的查看processor 文档，我发现其实大部份过滤分割能力已经有很多 processor 进行了实现，<strong>其完善程度外加可扩展的 script processor 实际能力已经足矣替换掉 logstash 的日志分割过滤处理了。</strong>比如上面的日志切割其实使用 dissect processor 实现更加简单(这个配置并不完善，只是样例):</p><div class="hljs"><pre><code class="hljs yaml"><span class="hljs-attr">processors:</span>  <span class="hljs-bullet">-</span> <span class="hljs-attr">dissect:</span>      <span class="hljs-attr">field:</span> <span class="hljs-string">"message"</span>      <span class="hljs-attr">tokenizer:</span> <span class="hljs-string">"<span class="hljs-template-variable">%&#123;timestamp&#125;</span>$$<span class="hljs-template-variable">%&#123;hostname&#125;</span>$$<span class="hljs-template-variable">%&#123;thread&#125;</span>$$<span class="hljs-template-variable">%&#123;level&#125;</span>$$<span class="hljs-template-variable">%&#123;logger&#125;</span>$$<span class="hljs-template-variable">%&#123;file&#125;</span>$$<span class="hljs-template-variable">%&#123;line&#125;</span>$$<span class="hljs-template-variable">%&#123;serviceName&#125;</span>$$<span class="hljs-template-variable">%&#123;traceId&#125;</span>$$<span class="hljs-template-variable">%&#123;feTraceId&#125;</span>$$<span class="hljs-template-variable">%&#123;msg&#125;</span>$$<span class="hljs-template-variable">%&#123;exception&#125;</span>$$"</span></code></pre></div><p>除此之外还有很多 processor，例如 <code>drop_event</code>、<code>drop_fields</code>、<code>timestamp</code> 等等，感兴趣的可以自行研究。</p><h2 id="七、总结"><a href="#七、总结" class="headerlink" title="七、总结"></a>七、总结</h2><p>基本上折腾完以后做了一个总结:</p><ul><li><strong>filebeat module</strong>: 这就是个华而不实的东西，每次修改需要重新编译且扩展能力几近于零，最蛋疼的是实际逻辑通过 es 来完成；我能想到的是唯一应用场景就是官方给我们弄一些 demo 来炫耀用的，比如 nginx module；实际生产中 nginx 日志格式保持原封不动的人我相信少之又少。</li><li><strong>filebeat custom processor</strong>: 每次修改也需要重新编译且需要会 go 语言还有相关工具链，但是好处就是完全通过代码实现真正的为所欲为；扩展性取决于外部是否对特定位置做了可配置化，比如预留可以配置切割用正则表达式的变量等，最终取决于代码编写者(怎么为所欲为的问题)。</li><li><strong>filebeat script processor</strong>: 完整 ECMA 5.1 js 规范支持，代码化对日志进行为所欲为，修改不需要重新编译；普通用户我个人觉得是首选，当然同时会写 go 和 js 的就看你想用哪个了。</li><li><strong>filebeat other processor</strong>: 基本上实现了很多 logstash 的功能，简单用用很舒服，复杂场景还是得撸代码；但是一些特定的 processor 很实用，比如加入宿主机信息的 add_host_metadata processor 等。</li></ul>]]></content:encoded>
      
      
      <category domain="https://mritd.com/categories/golang/">Golang</category>
      
      
      <category domain="https://mritd.com/tags/kubernetes/">Kubernetes</category>
      
      <category domain="https://mritd.com/tags/golang/">Golang</category>
      
      
      <comments>https://mritd.com/2020/08/19/how-to-modify-filebeat-source-code-to-processing-logs/#disqus_thread</comments>
      
    </item>
    
    <item>
      <title>如何不通过 docker 下载 docker image</title>
      <link>https://mritd.com/2020/03/31/how-to-download-docker-image-without-docker/</link>
      <guid>https://mritd.com/2020/03/31/how-to-download-docker-image-without-docker/</guid>
      <pubDate>Tue, 31 Mar 2020 15:52:38 GMT</pubDate>
      
      <description>这是一个比较骚的动作，但是事实上确实有这个需求，折腾半天找工具看源码，这里记录一下(不想看源码分析啥的请直接跳转到第五部份)。</description>
      
      
      
      <content:encoded><![CDATA[<blockquote><p>这是一个比较骚的动作，但是事实上确实有这个需求，折腾半天找工具看源码，这里记录一下(不想看源码分析啥的请直接跳转到第五部份)。</p></blockquote><h2 id="一、起因"><a href="#一、起因" class="headerlink" title="一、起因"></a>一、起因</h2><p>由于最近某个爬虫业务需要抓取微信公众号的一些文章，某开发小伙伴想到了通过启动安卓虚拟机然后抓包的方式实现；经过几番寻找最终我们选择采用 docker 的方式启动安卓虚拟机，docker 里安卓虚拟机比较成熟的项目我们找到了 <a href="https://github.com/budtmo/docker-android" target="_blank" rel="noopener">https://github.com/budtmo/docker-android</a> 这个项目；但是由于众所周知的原因这个 2G+ 的镜像国内拉取是非常慢的，于是我想到了通过国外 VPS 拉取然后 scp 回来… 由于贫穷的原因，当我实际操作的时候遇到了比较尴尬的问题: <strong>VPS 磁盘空间 25G，镜像拉取后解压接近 10G，我需要 <code>docker save</code> 成 tar 包再进行打包成 <code>tar.gz</code> 格式 scp 回来，这个时候空间不够用了…</strong>所以我当时就在想有没有办法让 docker daemon 拉取镜像时不解压？或者说自己通过 HTTP 下载镜像直接存储为 tar？</p><h2 id="二、尝试造轮子"><a href="#二、尝试造轮子" class="headerlink" title="二、尝试造轮子"></a>二、尝试造轮子</h2><p>当出现了上面的问题后，我第一反应就是:</p><ul><li>1、docker 拆分为 moby</li><li>2、moby 模块化，大部份开源到 <a href="https://github.com/containers" target="_blank" rel="noopener">containers</a></li><li>3、<a href="https://github.com/containers/image" target="_blank" rel="noopener">containers/image</a> 项目是镜像部份源码</li><li>4、看 <a href="https://github.com/containers/image" target="_blank" rel="noopener">containers/image</a> 源码造轮子</li><li>5、不确定是否需要 <a href="https://github.com/containers/storage" target="_blank" rel="noopener">containers/storage</a> 做存储</li></ul><h2 id="三、猜测源码"><a href="#三、猜测源码" class="headerlink" title="三、猜测源码"></a>三、猜测源码</h2><p>当我查看 <a href="https://github.com/containers/image" target="_blank" rel="noopener">containers/image</a> README 文档时发现其提到了 <a href="https://github.com/containers/skopeo" target="_blank" rel="noopener">skopeo</a> 项目，并且很明确的说了</p><blockquote><p>The containers/image project is only a library with no user interface; you can either incorporate it into your Go programs, or use the skopeo tool:<br>The skopeo tool uses the containers/image library and takes advantage of many of its features, e.g. skopeo copy exposes the containers/image/copy.Image functionality.</p></blockquote><p>那么也就是说镜像下载这块很大可能应该调用 <code>containers/image/copy.Image</code> 完成，随即就看了下源码文档</p><p><img src="https://cdn.oss.link/markdown/tv7iy.png" srcset="/img/loading.gif" alt=""></p><p>很明显，<code>types.ImageReference</code>、<code>Options</code> 里面的属性啥的我完全看不懂… 😂😂😂</p><h2 id="四、看-skopeo-源码"><a href="#四、看-skopeo-源码" class="headerlink" title="四、看 skopeo 源码"></a>四、看 skopeo 源码</h2><p>当 <a href="https://github.com/containers/image" target="_blank" rel="noopener">containers/image</a> 源码看不懂时，突然想到 <a href="https://github.com/containers/skopeo" target="_blank" rel="noopener">skopeo</a> 调用的是这个玩意，那么依葫芦画瓢看 <a href="https://github.com/containers/skopeo" target="_blank" rel="noopener">skopeo</a> 源码应该能行；接下来常规操作 clone skopeo 源码然后编译运行测试；编译后 skopeo 支持命令如下</p><div class="hljs"><pre><code class="hljs sh">NAME:   skopeo - Various operations with container images and container image registriesUSAGE:   skopeo [global options] <span class="hljs-built_in">command</span> [<span class="hljs-built_in">command</span> options] [arguments...]VERSION:   0.1.42-dev commit: 018a0108b103341526b41289c434b59d65783f6fCOMMANDS:   copy               Copy an IMAGE-NAME from one location to another   inspect            Inspect image IMAGE-NAME   delete             Delete image IMAGE-NAME   manifest-digest    Compute a manifest digest of a file   sync               Synchronize one or more images from one location to another   standalone-sign    Create a signature using <span class="hljs-built_in">local</span> files   standalone-verify  Verify a signature using <span class="hljs-built_in">local</span> files   list-tags          List tags <span class="hljs-keyword">in</span> the transport/repository specified by the REPOSITORY-NAME   <span class="hljs-built_in">help</span>, h            Shows a list of commands or <span class="hljs-built_in">help</span> <span class="hljs-keyword">for</span> one <span class="hljs-built_in">command</span>GLOBAL OPTIONS:   --debug                     <span class="hljs-built_in">enable</span> debug output   --policy value              Path to a trust policy file   --insecure-policy           run the tool without any policy check   --registries.d DIR          use registry configuration files <span class="hljs-keyword">in</span> DIR (e.g. <span class="hljs-keyword">for</span> container signature storage)   --override-arch ARCH        use ARCH instead of the architecture of the machine <span class="hljs-keyword">for</span> choosing images   --override-os OS            use OS instead of the running OS <span class="hljs-keyword">for</span> choosing images   --override-variant VARIANT  use VARIANT instead of the running architecture variant <span class="hljs-keyword">for</span> choosing images   --<span class="hljs-built_in">command</span>-timeout value     timeout <span class="hljs-keyword">for</span> the <span class="hljs-built_in">command</span> execution (default: 0s)   --<span class="hljs-built_in">help</span>, -h                  show <span class="hljs-built_in">help</span>   --version, -v               <span class="hljs-built_in">print</span> the version</code></pre></div><p><strong>我掐指一算调用 copy 命令应该是我要找的那个它</strong>，所以常规操作打开源码直接看</p><p><img src="https://cdn.oss.link/markdown/urn3l.png" srcset="/img/loading.gif" alt="copy_cmd"></p><p>通过继续追踪 <code>alltransports.ParseImageName</code> 方法最终可以得知 copy 命令的 <code>SOURCE-IMAGE</code> 和 <code>DESTINATION-IMAGE</code> 都支持哪些写法</p><p><img src="https://cdn.oss.link/markdown/ush4t.png" srcset="/img/loading.gif" alt="tp_register"></p><p><strong>每一个 Transport 的实现都提供了 Name 方法，其名称即为 src 或 dest 镜像名称的前缀，例如 <code>docker://nginx:1.17.6</code></strong></p><p><img src="https://cdn.oss.link/markdown/7fpap.png" srcset="/img/loading.gif" alt="tp_docker"></p><p><strong>经过测试不同的 Transport 格式并不完全一致(具体看源码)，比如 <code>docker://nginx:1.17.6</code> 和 <code>dir:/tmp/nginx</code>；同时这些 Transport 并非完全都适用与 src 与 dest，比如 <code>tarball:/tmp/nginx.tar</code> 支持 src 而不支持 dest；</strong>其判断核心依据为 <code>ImageReference.NewImageSource</code> 和 <code>ImageReference.NewImageDestination</code> 方法实现是否返回 error</p><p><img src="https://cdn.oss.link/markdown/jb087.png" srcset="/img/loading.gif" alt="NewImageDestination"></p><p>当我看了一会各种 Transport 源码后我发现一件事: <strong>这特么不就是我要造的轮子么！😱😱😱</strong></p><h2 id="五、skopeo-copy-使用"><a href="#五、skopeo-copy-使用" class="headerlink" title="五、skopeo copy 使用"></a>五、skopeo copy 使用</h2><h3 id="5-1、不借助-docker-下载镜像"><a href="#5-1、不借助-docker-下载镜像" class="headerlink" title="5.1、不借助 docker 下载镜像"></a>5.1、不借助 docker 下载镜像</h3><div class="hljs"><pre><code class="hljs sh">skopeo --insecure-policy copy docker://nginx:1.17.6 docker-archive:/tmp/nginx.tar</code></pre></div><p><code>--insecure-policy</code> 选项用于忽略安全策略配置文件，该命令将会直接通过 http 下载目标镜像并存储为 <code>/tmp/nginx.tar</code>，此文件可以直接通过 <code>docker load</code> 命令导入</p><h3 id="5-2、从-docker-daemon-导出镜像"><a href="#5-2、从-docker-daemon-导出镜像" class="headerlink" title="5.2、从 docker daemon 导出镜像"></a>5.2、从 docker daemon 导出镜像</h3><div class="hljs"><pre><code class="hljs sh">skopeo --insecure-policy copy docker-daemon:nginx:1.17.6 docker-archive:/tmp/nginx.tar</code></pre></div><p>该命令将会从 docker daemon 导出镜像到 <code>/tmp/nginx.tar</code>；为什么不用 <code>docker save</code>？因为我是偷懒 dest 也是 docker-archive，实际上 skopeo 可以导出为其他格式比如 <code>oci</code>、<code>oci-archive</code>、<code>ostree</code> 等</p><h3 id="5-3、其他命令"><a href="#5-3、其他命令" class="headerlink" title="5.3、其他命令"></a>5.3、其他命令</h3><p>skopeo 还有一些其他的实用命令，比如 <code>sync</code> 可以在两个位置之间同步镜像(😂早知道我还写个鸡儿 gcrsync)，<code>inspect</code> 可以查看镜像信息等，迫于本人太懒，剩下的请自行查阅文档、<code>--help</code> 以及源码(没错，整篇文章都没写 skopeo 怎么安装)。</p>]]></content:encoded>
      
      
      <category domain="https://mritd.com/categories/docker/">Docker</category>
      
      
      <category domain="https://mritd.com/tags/linux/">Linux</category>
      
      <category domain="https://mritd.com/tags/docker/">Docker</category>
      
      
      <comments>https://mritd.com/2020/03/31/how-to-download-docker-image-without-docker/#disqus_thread</comments>
      
    </item>
    
    <item>
      <title>kubeadm 证书期限调整</title>
      <link>https://mritd.com/2020/01/21/how-to-extend-the-validity-of-your-kubeadm-certificate/</link>
      <guid>https://mritd.com/2020/01/21/how-to-extend-the-validity-of-your-kubeadm-certificate/</guid>
      <pubDate>Tue, 21 Jan 2020 04:43:36 GMT</pubDate>
      
      <description>最近 kubeadm HA 的集群折腾完了，发现集群证书始终是 1 年有效期，然后自己还有点子担心；无奈只能研究一下源码一探究竟了...</description>
      
      
      
      <content:encoded><![CDATA[<h2 id="一、证书管理"><a href="#一、证书管理" class="headerlink" title="一、证书管理"></a>一、证书管理</h2><p>kubeadm 集群安装完成后，证书管理上实际上大致是两大类型:</p><ul><li>自动滚动续期</li><li>手动定期续期</li></ul><p>自动滚动续期类型的证书目前从我所阅读文档和实际测试中目前只有 kubelet 的 client 证书；kubelet client 证书自动滚动涉及到了 <a href="https://kubernetes.io/docs/reference/command-line-tools-reference/kubelet-tls-bootstrapping/" target="_blank" rel="noopener">TLS bootstrapping</a> 部份，<strong>其核心由两个 ClusterRole 完成(<code>system:certificates.k8s.io:certificatesigningrequests:nodeclient</code> 和 <code>system:certificates.k8s.io:certificatesigningrequests:selfnodeclient</code>)，针对这两个 ClusterRole kubeadm 在引导期间创建了 <a href="https://kubernetes.io/docs/reference/setup-tools/kubeadm/implementation-details/#create-a-bootstrap-token" target="_blank" rel="noopener">bootstrap token</a> 来完成引导期间证书签发(该 Token 24h 失效)，后续通过预先创建的 ClusterRoleBinding(<code>kubeadm:node-autoapprove-bootstrap</code> 和 <code>kubeadm:node-autoapprove-certificate-rotation</code>) 完成自动的 node 证书续期；</strong>kubelet client 证书续期部份涉及到 TLS bootstrapping 太多了，有兴趣的可以仔细查看(最后还是友情提醒: <strong>用 kubeadm 一定要看看 <a href="https://kubernetes.io/docs/reference/setup-tools/kubeadm/implementation-details" target="_blank" rel="noopener">Implementation details</a></strong>)。</p><p>手动续期的证书目前需要在到期前使用 kubeadm 命令自行续期，这些证书目前可以通过以下命令列出</p><div class="hljs"><pre><code class="hljs sh"><span class="hljs-comment"># 不要在意我的证书过期时间是 10 年，下面会说</span>k1.node ➜ kubeadm alpha certs check-expiration[check-expiration] Reading configuration from the cluster...[check-expiration] FYI: You can look at this config file with <span class="hljs-string">'kubectl -n kube-system get cm kubeadm-config -oyaml'</span>CERTIFICATE                EXPIRES                  RESIDUAL TIME   CERTIFICATE AUTHORITY   EXTERNALLY MANAGEDadmin.conf                 Dec 06, 2029 20:58 UTC   9y                                      noapiserver                  Dec 06, 2029 20:59 UTC   9y              ca                      noapiserver-kubelet-client   Dec 06, 2029 20:59 UTC   9y              ca                      nocontroller-manager.conf    Dec 06, 2029 20:59 UTC   9y                                      nofront-proxy-client         Dec 06, 2029 20:59 UTC   9y              front-proxy-ca          noscheduler.conf             Dec 06, 2029 20:59 UTC   9y                                      noCERTIFICATE AUTHORITY   EXPIRES                  RESIDUAL TIME   EXTERNALLY MANAGEDca                      Jan 13, 2030 08:45 UTC   9y              nofront-proxy-ca          Jan 13, 2030 08:45 UTC   9y              no</code></pre></div><h2 id="二、证书期限调整"><a href="#二、证书期限调整" class="headerlink" title="二、证书期限调整"></a>二、证书期限调整</h2><p>上面已经提到了，手动管理部份的证书需要自己用命令续签(<code>kubeadm alpha certs renew all</code>)，而且你会发现续签以后有效期还是 1 年；kubeadm 的初衷是 <strong>“为快速创建 kubernetes 集群的最佳实践”</strong>，当然最佳实践包含确保证书安全性，毕竟 Let’s Encrypt 的证书有效期只有 3 个月的情况下 kubeadm 有效期有 1 年已经很不错了；但是对于最佳实践来说，我们公司的集群安全性并不需要那么高，一年续期一次无疑在增加运维人员心智负担(它并不最佳)，所以我们迫切需要一种 “一劳永逸” 的解决方案；当然我目前能想到的就是找到证书签发时在哪设置的有效期，然后想办法改掉它。</p><h3 id="2-1、源码分析"><a href="#2-1、源码分析" class="headerlink" title="2.1、源码分析"></a>2.1、源码分析</h3><p>目前通过宏观角度看整个 kubeadm 集群搭建过程，其中涉及到证书签署大致有两大部份: init  阶段和后期 renew，下面开始分析两个阶段的源码</p><h4 id="2-1-1、init-阶段"><a href="#2-1-1、init-阶段" class="headerlink" title="2.1.1、init 阶段"></a>2.1.1、init 阶段</h4><p>由于 kubernetes 整个命令行都是通过 cobra 库构建的，那么根据这个库的习惯首先直接从 <code>cmd</code> 包开始翻，而 kubernetes 源码组织的又比较清晰进而直接定位到 kubeadm 命令包下面；接着打开 <code>app</code> 目录一眼就看到了 <code>phases</code>… <code>phases</code> 顾名思义啊，整个 init 都是通过不同的 <code>phases</code> 完成的，那么直接去 <code>phases</code> 包下面找证书阶段的源码既可</p><p><img src="https://cdn.oss.link/markdown/ssdo7.jpg" srcset="/img/loading.gif" alt="init_source"></p><p>进入到这个 <code>certs.go</code> 里面，直接列出所有方法，go 的规范里只有首字母大写才会被暴露出去，那么我们直接查看这些方法名既可；从名字上很轻松的看到了这个方法…基本上就是它了</p><p><img src="https://cdn.oss.link/markdown/uoqx4.jpg" srcset="/img/loading.gif" alt="certs.go"></p><p>通过这个方法的代码会发现最终还是调用了 <code>certSpec.CreateFromCA(cfg, caCert, caKey)</code>，那么接着看看这个方法</p><p><img src="https://cdn.oss.link/markdown/psrho.jpg" srcset="/img/loading.gif" alt="pkiutil.NewCertAndKey"></p><p>通过这个方法继续往下翻发现调用了 <code>pkiutil.NewCertAndKey(caCert, caKey, cfg)</code>，这个方法里最终调用了 <code>NewSignedCert(config, key, caCert, caKey)</code></p><p><img src="https://cdn.oss.link/markdown/nel5u.jpg" srcset="/img/loading.gif" alt="NewSignedCert"></p><p>从 <code>NewSignedCert</code> 方法里看到证书有效期实际上是个常量，<strong>那也就意味着我改了这个常量 init 阶段的证书有效期八九不离十的就变了，再通过包名看这个是个 <code>pkiutil</code>… <code>xxxxxutil</code> 明显是公共的，所以推测改了它 renew 阶段应该也会变</strong></p><p><img src="https://cdn.oss.link/markdown/t3amy.jpg" srcset="/img/loading.gif" alt="CertificateValidity"></p><h4 id="2-1-2、renew-阶段"><a href="#2-1-2、renew-阶段" class="headerlink" title="2.1.2、renew 阶段"></a>2.1.2、renew 阶段</h4><p>renew 阶段也是老套路，不过稳妥点先从 cmd 找起来，所以先看 <code>alpha</code> 包下的 <code>certs.go</code>；这时候方法名语义清晰就很有好处，一下就能找到 <code>newCmdCertsRenewal</code> 方法</p><p><img src="https://cdn.oss.link/markdown/amupo.jpg" srcset="/img/loading.gif" alt="alpha_certs.go"></p><p>而这个 <code>newCmdCertsRenewal</code> 方法实际上没啥实现，所以目测实现是从 <code>getRenewSubCommands</code> 实现的</p><p><img src="https://cdn.oss.link/markdown/8c38y.jpg" srcset="/img/loading.gif" alt="getRenewSubCommands"></p><p>看了 <code>getRenewSubCommands</code> 以后发现上面全是命令行库、配置文件参数啥的处理，核心在 <code>renewCert</code> 上，从这个方法里发现还有意外收获: <strong>renew 时实际上分两种情况处理，一种是使用了 <code>--use-api</code> 选项，另一种是未使用</strong>；当然根据上面的命令来说我们没使用，那么看 else 部份就行了(没看源码之前我特么居然没看 <code>--help</code> 不知道有这个选项)</p><p><img src="https://cdn.oss.link/markdown/9zsgp.jpg" srcset="/img/loading.gif" alt="renewCert"></p><p>else 部份源码最终还是调用了 <code>RenewUsingLocalCA</code> 方法，这个方法一直往下跟会有一个 <code>Renew</code> 方法</p><p><img src="https://cdn.oss.link/markdown/s3a5c.jpg" srcset="/img/loading.gif" alt="Renew"></p><p>这个方法一点进去… <strong>我上面的想法是对的</strong></p><p><img src="https://cdn.oss.link/markdown/08cnb.jpg" srcset="/img/loading.gif" alt="FileRenewer_Renew"></p><h4 id="2-1-3、其他推测"><a href="#2-1-3、其他推测" class="headerlink" title="2.1.3、其他推测"></a>2.1.3、其他推测</h4><p>根据刚刚查看代码可以看到在 renew 阶段判断了 <code>--use-api</code> 选项是否使用，通过跟踪源码发现最终会调用到 <code>RenewUsingCSRAPI</code> 方法上，<code>RenewUsingCSRAPI</code> 会调用集群 CSR Api 执行证书签署</p><p><img src="https://cdn.oss.link/markdown/xivs9.png" srcset="/img/loading.gif" alt="RenewUsingCSRAPI"></p><p>有了这个发现后基本上可以推测出这一步通过集群完成，那么按理说是应该受到 <code>kube-controller-manager</code> 组件的 <code>--experimental-cluster-signing-duration</code> 影响。</p><h3 id="2-2、测试验证"><a href="#2-2、测试验证" class="headerlink" title="2.2、测试验证"></a>2.2、测试验证</h3><h4 id="2-2-1、验证修改源码"><a href="#2-2-1、验证修改源码" class="headerlink" title="2.2.1、验证修改源码"></a>2.2.1、验证修改源码</h4><p>想验证修改源码是否有效只需要修改源码重新 build 出 kubeadm 命令，然后使用这个特定版本的 kubeadm renew 证书测试既可，源码调整的位置如下</p><p><img src="https://cdn.oss.link/markdown/qaavr.png" srcset="/img/loading.gif" alt="update_source"></p><p>然后命令行下执行 <code>make cross</code> 进行跨平台交叉编译(如果过你在 linux amd64 平台下则直接 <code>make</code> 既可)</p><div class="hljs"><pre><code class="hljs sh">➜  kubernetes git:(v1.17.4) ✗ make crossgrep: /proc/meminfo: No such file or directorygrep: /proc/meminfo: No such file or directory+++ [0116 23:43:19] Multiple platforms requested and available 64G &gt;= threshold 40G, building platforms <span class="hljs-keyword">in</span> parallel+++ [0116 23:43:19] Building go targets <span class="hljs-keyword">for</span> &#123;linux/amd64 linux/arm linux/arm64 linux/s390x linux/ppc64le&#125; <span class="hljs-keyword">in</span> parallel (output will appear <span class="hljs-keyword">in</span> a burst when complete):    cmd/kube-proxy    cmd/kube-apiserver    cmd/kube-controller-manager    cmd/kubelet    cmd/kubeadm    cmd/kube-scheduler    vendor/k8s.io/apiextensions-apiserver    cluster/gce/gci/mounter+++ [0116 23:43:19] linux/amd64: build started+++ [0116 23:47:24] linux/amd64: build finished+++ [0116 23:43:19] linux/arm: build started+++ [0116 23:47:23] linux/arm: build finished+++ [0116 23:43:19] linux/arm64: build started+++ [0116 23:47:23] linux/arm64: build finished+++ [0116 23:43:19] linux/s390x: build started+++ [0116 23:47:24] linux/s390x: build finished+++ [0116 23:43:19] linux/ppc64le: build started+++ [0116 23:47:24] linux/ppc64le: build finishedgrep: /proc/meminfo: No such file or directorygrep: /proc/meminfo: No such file or directory+++ [0116 23:47:52] Multiple platforms requested and available 64G &gt;= threshold 40G, building platforms <span class="hljs-keyword">in</span> parallel+++ [0116 23:47:52] Building go targets <span class="hljs-keyword">for</span> &#123;linux/amd64 linux/arm<span class="hljs-comment"># ... 省略编译日志</span></code></pre></div><p>编译完成后能够在 <code>_output/local/bin/linux/amd64</code> 下找到刚刚编译成功的 <code>kubeadm</code> 文件，将编译好的 kubeadm scp 到已经存在集群上执行 renew，然后查看证书时间</p><p><img src="https://cdn.oss.link/markdown/i3laa.png" srcset="/img/loading.gif" alt="kubeadm_renew"></p><p><strong>经过测试后确认源码修改方式有效</strong></p><h4 id="2-2-2、验证调整-CSR-API"><a href="#2-2-2、验证调整-CSR-API" class="headerlink" title="2.2.2、验证调整 CSR API"></a>2.2.2、验证调整 CSR API</h4><p>根据推测当使用 <code>--use-api</code> 会受到 <code>kube-controller-manager</code> 组件的 <code>--experimental-cluster-signing-duration</code> 影响，从而从集群中下发证书；所以首先在启动集群时需要将 <code>--experimental-cluster-signing-duration</code> 调整为 10 年，然后再进行测试</p><div class="hljs"><pre><code class="hljs yaml"><span class="hljs-attr">controllerManager:</span>  <span class="hljs-attr">extraArgs:</span>    <span class="hljs-attr">v:</span> <span class="hljs-string">"4"</span>    <span class="hljs-attr">node-cidr-mask-size:</span> <span class="hljs-string">"19"</span>    <span class="hljs-attr">deployment-controller-sync-period:</span> <span class="hljs-string">"10s"</span>    <span class="hljs-comment"># 在 kubeadm 配置文件中设置证书有效期为 10 年</span>    <span class="hljs-attr">experimental-cluster-signing-duration:</span> <span class="hljs-string">"86700h"</span>    <span class="hljs-attr">node-monitor-grace-period:</span> <span class="hljs-string">"20s"</span>    <span class="hljs-attr">pod-eviction-timeout:</span> <span class="hljs-string">"2m"</span>    <span class="hljs-attr">terminated-pod-gc-threshold:</span> <span class="hljs-string">"30"</span></code></pre></div><p>然后使用 <code>--use-api</code> 选项进行 renew</p><div class="hljs"><pre><code class="hljs sh">kubeadm alpha certs renew all --use-api</code></pre></div><p>此时会发现日志中打印出 <code>[certs] Certificate request &quot;kubeadm-cert-kubernetes-admin-648w4&quot; created</code> 字样，接下来从 <code>kube-system</code> 的 namespace 中能够看到相关 csr</p><p><img src="https://cdn.oss.link/markdown/54awl.png" srcset="/img/loading.gif" alt="list_csr"></p><p>这时我们开始手动批准证书，每次批准完成一个 csr，紧接着 kubeadm 会创建另一个 csr</p><p><img src="https://cdn.oss.link/markdown/tdde7.png" srcset="/img/loading.gif" alt="approve_csr"></p><p>当所有 csr 被批准后，再次查看集群证书发现证书期限确实被调整了</p><p><img src="https://cdn.oss.link/markdown/081qe.png" srcset="/img/loading.gif" alt="success"></p><h2 id="三、总结"><a href="#三、总结" class="headerlink" title="三、总结"></a>三、总结</h2><p>总结一下，调整 kubeadm 证书期限有两种方案；第一种直接修改源码，耗时耗力还得会 go，最后还要跑跨平台编译(很耗时)；第二种在启动集群时调整 <code>kube-controller-manager</code> 组件的 <code>--experimental-cluster-signing-duration</code> 参数，集群创建好后手动 renew 一下并批准相关 csr。</p><p>两种方案各有利弊，修改源码方式意味着在 client 端签发处理，不会对集群产生永久性影响，也就是说哪天你想 “反悔了” 你不需要修改集群什么配置，直接用官方 kubeadm renew 一下就会变回一年期限的证书；改集群参数实现的方式意味着你不需要懂 go 代码，只需要常规的集群配置既可实现，同时你也不需要跑几十分钟的交叉编译，不需要为编译过程中的网络问题而烦恼；所以最后使用哪种方案因人因情况而定吧。</p>]]></content:encoded>
      
      
      <category domain="https://mritd.com/categories/kubernetes/">Kubernetes</category>
      
      
      <category domain="https://mritd.com/tags/kubernetes/">Kubernetes</category>
      
      
      <comments>https://mritd.com/2020/01/21/how-to-extend-the-validity-of-your-kubeadm-certificate/#disqus_thread</comments>
      
    </item>
    
    <item>
      <title>kubeadm 集群升级</title>
      <link>https://mritd.com/2020/01/21/how-to-upgrade-kubeadm-cluster/</link>
      <guid>https://mritd.com/2020/01/21/how-to-upgrade-kubeadm-cluster/</guid>
      <pubDate>Tue, 21 Jan 2020 04:41:46 GMT</pubDate>
      
      <description>真是不巧，刚折腾完 kubeadm 搭建集群(v1.17.0)，第二天早上醒来特么的 v1.17.1 发布了；这我能忍么，肯定不能忍，然后就开始了集群升级之路...</description>
      
      
      
      <content:encoded><![CDATA[<h2 id="一、升级前准备"><a href="#一、升级前准备" class="headerlink" title="一、升级前准备"></a>一、升级前准备</h2><ul><li>确保你的集群是 kubeadm 搭建的(等同于废话)</li><li>确保当前集群已经完成 HA(多个 master 节点)</li><li>确保在夜深人静的时候(无大量业务流量)</li><li>确保集群版本大于 v1.16.0</li><li>确保已经仔细阅读了目标版本 CHANGELOG</li><li>确保做好了完整地集群备份</li></ul><h2 id="二、升级注意事项"><a href="#二、升级注意事项" class="headerlink" title="二、升级注意事项"></a>二、升级注意事项</h2><ul><li>升级后所有集群组件 Pod 会重启(hash 变更)</li><li><strong>升级时 <code>kubeadm</code> 版本必须大于或等于目标版本</strong></li><li><strong>升级期间所有 <code>kube-proxy</code> 组件会有一次全节点滚动更新</strong></li><li><strong>升级只支持顺次进行，不支持跨版本升级(You only can upgrade from one MINOR version to the next MINOR version, or between PATCH versions of the same MINOR. That is, you cannot skip MINOR versions when you upgrade. For example, you can upgrade from 1.y to 1.y+1, but not from 1.y to 1.y+2.)</strong></li></ul><p>关于升级版本问题…虽然是这么说的，但是官方文档样例代码里是从 <code>v1.16.0</code> 升级到 <code>v1.17.0</code>；可能是我理解有误，跨大版本升级好像官方没提，具体啥后果不清楚…</p><h2 id="三、升级-Master"><a href="#三、升级-Master" class="headerlink" title="三、升级 Master"></a>三、升级 Master</h2><blockquote><p>事实上所有升级工作主要是针对 master 节点做的，所以整个升级流程中最重要的是如何把 master 升级好。</p></blockquote><h3 id="3-1、升级-kubeadm、kubectl"><a href="#3-1、升级-kubeadm、kubectl" class="headerlink" title="3.1、升级 kubeadm、kubectl"></a>3.1、升级 kubeadm、kubectl</h3><p>首先由于升级限制，必须先将 <code>kubeadm</code> 和 <code>kubectl</code> 升级到大于等于目标版本</p><div class="hljs"><pre><code class="hljs sh"><span class="hljs-comment"># replace x in 1.17.x-00 with the latest patch version</span>apt-mark unhold kubeadm kubectlapt-get updateapt-get install -y kubeadm=1.17.x-00 kubectl=1.17.x-00apt-mark hold kubeadm kubectl</code></pre></div><p>当然如果你之前没有 <code>hold</code> 住这几个软件包的版本，那么就不需要 <code>unhold</code>；我的做法可能比较极端…一般为了防止后面的误升级安装完成后我会直接 <code>rename</code> 掉相关软件包的 <code>apt source</code> 配置(从根本上防止手贱)。</p><h3 id="3-2、升级前准备"><a href="#3-2、升级前准备" class="headerlink" title="3.2、升级前准备"></a>3.2、升级前准备</h3><h4 id="3-2-1、配置修改"><a href="#3-2-1、配置修改" class="headerlink" title="3.2.1、配置修改"></a>3.2.1、配置修改</h4><p>对于高级玩家一般安装集群时都会自定义很多组件参数，此时不可避免的会采用配置文件；所以安装完新版本的 <code>kubeadm</code> 后就要着手修改配置文件中的 <code>kubernetesVersion</code> 字段为目标集群版本，当然有其他变更也可以一起修改。</p><h4 id="3-2-2、节点驱逐"><a href="#3-2-2、节点驱逐" class="headerlink" title="3.2.2、节点驱逐"></a>3.2.2、节点驱逐</h4><p>如果你的 master 节点也当作 node 在跑一些工作负载，则需要执行以下命令驱逐这些 pod 并使节点进入维护模式(禁止调度)。</p><div class="hljs"><pre><code class="hljs sh"><span class="hljs-comment"># 将 NODE_NAME 换成 Master 节点名称</span>kubectl drain NODE_NAME --ignore-daemonsets</code></pre></div><h4 id="3-2-3、查看升级计划"><a href="#3-2-3、查看升级计划" class="headerlink" title="3.2.3、查看升级计划"></a>3.2.3、查看升级计划</h4><p>完成节点驱逐以后，可以通过以下命令查看升级计划；<strong>升级计划中列出了升级期间要执行的所有步骤以及相关警告，一定要仔细查看。</strong></p><div class="hljs"><pre><code class="hljs sh">k8s16.node ➜  ~ kubeadm upgrade plan --config /etc/kubernetes/kubeadm.yamlW0115 10:59:52.586204     983 validation.go:28] Cannot validate kube-proxy config - no validator is availableW0115 10:59:52.586241     983 validation.go:28] Cannot validate kubelet config - no validator is available[upgrade/config] Making sure the configuration is correct:W0115 10:59:52.605458     983 common.go:94] WARNING: Usage of the --config flag <span class="hljs-keyword">for</span> reconfiguring the cluster during upgrade is not recommended!W0115 10:59:52.607258     983 validation.go:28] Cannot validate kube-proxy config - no validator is availableW0115 10:59:52.607274     983 validation.go:28] Cannot validate kubelet config - no validator is available[preflight] Running pre-flight checks.[upgrade] Making sure the cluster is healthy:[upgrade] Fetching available versions to upgrade to[upgrade/versions] Cluster version: v1.17.0[upgrade/versions] kubeadm version: v1.17.1External components that should be upgraded manually before you upgrade the control plane with <span class="hljs-string">'kubeadm upgrade apply'</span>:COMPONENT   CURRENT   AVAILABLEEtcd        3.3.18    3.4.3-0Components that must be upgraded manually after you have upgraded the control plane with <span class="hljs-string">'kubeadm upgrade apply'</span>:COMPONENT   CURRENT       AVAILABLEKubelet     5 x v1.17.0   v1.17.1Upgrade to the latest version <span class="hljs-keyword">in</span> the v1.17 series:COMPONENT            CURRENT   AVAILABLEAPI Server           v1.17.0   v1.17.1Controller Manager   v1.17.0   v1.17.1Scheduler            v1.17.0   v1.17.1Kube Proxy           v1.17.0   v1.17.1CoreDNS              1.6.5     1.6.5You can now apply the upgrade by executing the following <span class="hljs-built_in">command</span>:        kubeadm upgrade apply v1.17.1_____________________________________________________________________</code></pre></div><h3 id="3-3、执行升级"><a href="#3-3、执行升级" class="headerlink" title="3.3、执行升级"></a>3.3、执行升级</h3><p>确认好升级计划以后，只需要一条命令既可将当前 master 节点升级到目标版本</p><div class="hljs"><pre><code class="hljs sh">kubeadm upgrade apply v1.17.1 --config /etc/kubernetes/kubeadm.yaml</code></pre></div><p>升级期间会打印很详细的日志，在日志中可以实时观察到升级流程，建议仔细关注升级流程；<strong>在最后一步会有一条日志 <code>[addons] Applied essential addon: kube-proxy</code>，这意味着集群开始更新 <code>kube-proxy</code> 组件，该组件目前是通过 <code>daemonset</code> 方式启动的；这会意味着此时会造成全节点的 <code>kube-proxy</code> 更新；</strong>理论上不会有很大影响，但是升级是还是需要注意一下这一步操作，在我的观察中似乎 <code>kube-proxy</code> 也是通过滚动更新完成的，所以问题应该不大。</p><h3 id="3-4、升级-kubelet"><a href="#3-4、升级-kubelet" class="headerlink" title="3.4、升级 kubelet"></a>3.4、升级 kubelet</h3><p>在单个 master 上升级完成后，<strong>只会升级本节点的 master 相关组件和全节点的 <code>kube-proxy</code> 组件；</strong>由于 kubelet 是在宿主机安装的，所以需要通过包管理器手动升级 kubelet</p><div class="hljs"><pre><code class="hljs sh"><span class="hljs-comment"># replace x in 1.17.x-00 with the latest patch version</span>apt-mark unhold kubeletapt-get install -y kubelet=1.17.x-00apt-mark hold kubelet</code></pre></div><p>更新完成后执行 <code>systemctl restart kubelet</code> 重启，并等待启动成功既可；最后不要忘记解除当前节点的维护模式(<code>uncordon</code>)。</p><h3 id="3-5、升级其他-Master"><a href="#3-5、升级其他-Master" class="headerlink" title="3.5、升级其他 Master"></a>3.5、升级其他 Master</h3><p>当其中一个 master 节点升级完成后，其他的 master 升级就会相对简单的多；<strong>首先国际惯例升级一下 <code>kubeadm</code> 和 <code>kubectl</code> 软件包，然后直接在其他 master 节点执行 <code>kubeadm upgrade node</code> 既可。</strong>由于 apiserver 等组件配置已经在升级第一个 master 时上传到了集群的 configMap 中，所以事实上其他 master 节点只是正常拉取然后重启相关组件既可；这一步同样会输出详细日志，可以仔细观察进度，<strong>最后不要忘记升级之前先进入维护模式，升级完成后重新安装 <code>kubelet</code> 并关闭节点维护模式。</strong></p><h2 id="四、升级-Node"><a href="#四、升级-Node" class="headerlink" title="四、升级 Node"></a>四、升级 Node</h2><p>node 节点的升级实际上在升级完 master 节点以后不需要什么特殊操作，node 节点唯一需要升级的就是 <code>kubelet</code> 组件；<strong>首先在 node 节点执行 <code>kubeadm upgrade node</code> 命令，该命令会拉取集群内的 <code>kubelet</code> 配置文件，然后重新安装 <code>kubelet</code> 重启既可；</strong>同样升级 node 节点时不要忘记开启维护模式。针对于 CNI 组件请按需手动升级，并且确认好 CNI 组件的兼容版本。</p><h2 id="五、验证集群"><a href="#五、验证集群" class="headerlink" title="五、验证集群"></a>五、验证集群</h2><p>所有组件升级完成后，可以通过 <code>kubectl describe POD_NAME</code> 的方式验证 master 组件是否都升级到了最新版本；通过 <code>kuebctl version</code> 命令验证 api 相关信息(HA rr 轮训模式下可以多执行几遍)；还有就是通过 <code>kubectl get node -o wide</code> 查看相关 node 的信息，确保 <code>kubelet</code> 都升级成功，同时全部节点维护模式都已经关闭，其他细节可以参考<a href="https://kubernetes.io/docs/tasks/administer-cluster/kubeadm/kubeadm-upgrade" target="_blank" rel="noopener">官方文档</a>。</p>]]></content:encoded>
      
      
      <category domain="https://mritd.com/categories/kubernetes/">Kubernetes</category>
      
      
      <category domain="https://mritd.com/tags/kubernetes/">Kubernetes</category>
      
      
      <comments>https://mritd.com/2020/01/21/how-to-upgrade-kubeadm-cluster/#disqus_thread</comments>
      
    </item>
    
    <item>
      <title>kubeadm 搭建 HA kubernetes 集群</title>
      <link>https://mritd.com/2020/01/21/set-up-kubernetes-ha-cluster-by-kubeadm/</link>
      <guid>https://mritd.com/2020/01/21/set-up-kubernetes-ha-cluster-by-kubeadm/</guid>
      <pubDate>Tue, 21 Jan 2020 04:40:36 GMT</pubDate>
      
      <description>距离上一次折腾 kubeadm 大约已经一两年了(记不太清了)，在很久一段时间内一直采用二进制部署的方式来部署 kubernetes 集群，随着 kubeadm 的不断稳定，目前终于可以重新试试这个不错的工具了</description>
      
      
      
      <content:encoded><![CDATA[<h2 id="一、环境准备"><a href="#一、环境准备" class="headerlink" title="一、环境准备"></a>一、环境准备</h2><p>搭建环境为 5 台虚拟机，每台虚拟机配置为 4 核心 8G 内存，虚拟机 IP 范围为 <code>172.16.10.21~25</code>，其他软件配置如下</p><ul><li>os version: ubuntu 18.04</li><li>kubeadm version: 1.17.0</li><li>kubernetes version: 1.17.0</li><li>etcd version: 3.3.18</li><li>docker version: 19.03.5</li></ul><h2 id="二、HA-方案"><a href="#二、HA-方案" class="headerlink" title="二、HA 方案"></a>二、HA 方案</h2><p>目前的 HA 方案与官方的不同，官方 HA 方案推荐使用类似 haproxy 等工具进行 4 层代理 apiserver，但是同样会有一个问题就是我们还需要对这个 haproxy 做 HA；由于目前我们实际生产环境都是多个独立的小集群，所以单独弄 2 台 haproxy + keeplived 去维持这个 apiserver LB 的 HA 有点不划算；所以还是准备延续老的 HA 方案，将外部 apiserver 的 4 层 LB 前置到每个 node 节点上；<strong>目前是采用在每个 node 节点上部署 nginx 4 层代理所有 apiserver，nginx 本身资源消耗低而且请求量不大，综合来说对宿主机影响很小；</strong>以下为 HA 的大致方案图</p><p><img src="https://cdn.oss.link/markdown/mktld.png" srcset="/img/loading.gif" alt="ha"></p><h2 id="三、环境初始化"><a href="#三、环境初始化" class="headerlink" title="三、环境初始化"></a>三、环境初始化</h2><h3 id="3-1、系统环境"><a href="#3-1、系统环境" class="headerlink" title="3.1、系统环境"></a>3.1、系统环境</h3><p>由于个人操作习惯原因，目前已经将常用的初始化环境整理到一个小脚本里了，脚本具体参见 <a href="https://github.com/mritd/shell_scripts/blob/master/init_ubuntu.sh" target="_blank" rel="noopener">mritd/shell_scripts</a> 仓库，基本上常用的初始化内容为: </p><ul><li>设置 locale(en_US.UTF-8)</li><li>设置时区(Asia/Shanghai)</li><li>更新所有系统软件包(system update)</li><li>配置 vim(vim8 + 常用插件、配色)</li><li>ohmyzsh(别跟我说不兼容 bash 脚本，我就是喜欢)</li><li>docker</li><li>ctop(一个 docker 的辅助工具)</li><li>docker-compose</li></ul><p><strong>在以上初始化中，实际对 kubernetes 安装产生影响的主要有三个地方:</strong></p><ul><li><strong>docker 的 cgroup driver 调整为 systemd，具体参考 <a href="https://github.com/mritd/config/blob/master/docker/docker.service" target="_blank" rel="noopener">docker.service</a></strong></li><li><strong>docker 一定要限制 conatiner 日志大小，防止 apiserver 等日志大量输出导致磁盘占用过大</strong></li><li><strong>安装 <code>conntrack</code> 和 <code>ipvsadm</code>，后面可能需要借助其排查问题</strong></li></ul><h3 id="3-2、配置-ipvs"><a href="#3-2、配置-ipvs" class="headerlink" title="3.2、配置 ipvs"></a>3.2、配置 ipvs</h3><p>由于后面 kube-proxy 需要使用 ipvs 模式，所以需要对内核参数、模块做一些调整，调整命令如下:</p><div class="hljs"><pre><code class="hljs sh">cat &gt;&gt; /etc/sysctl.conf &lt;&lt;EOFnet.ipv4.ip_forward=1net.bridge.bridge-nf-call-iptables=1net.bridge.bridge-nf-call-ip6tables=1EOFsysctl -pcat &gt;&gt; /etc/modules &lt;&lt;EOFip_vsip_vs_lcip_vs_wlcip_vs_rrip_vs_wrrip_vs_lblcip_vs_lblcrip_vs_dhip_vs_ship_vs_foip_vs_nqip_vs_sedip_vs_ftpEOF</code></pre></div><p><strong>配置完成后切记需要重启，重启完成后使用 <code>lsmod | grep ip_vs</code> 验证相关 ipvs 模块加载是否正常，本文将主要使用 <code>ip_vs_wrr</code>，所以目前只关注这个模块既可。</strong></p><p><img src="https://cdn.oss.link/markdown/4irz1.png" srcset="/img/loading.gif" alt="ipvs_mode"></p><h2 id="四、安装-Etcd"><a href="#四、安装-Etcd" class="headerlink" title="四、安装 Etcd"></a>四、安装 Etcd</h2><h3 id="4-1、方案选择"><a href="#4-1、方案选择" class="headerlink" title="4.1、方案选择"></a>4.1、方案选择</h3><p>官方对于集群 HA 给出了两种有关于 Etcd 的部署方案: </p><ul><li>一种是深度耦合到 <code>control plane</code> 上，即每个 <code>control plane</code> 一个 etcd</li><li>另一种是使用外部的 Etcd 集群，通过在配置中指定外部集群让 apiserver 等组件连接</li></ul><p>在测试深度耦合 <code>control plane</code> 方案后，发现一些比较恶心的问题；比如说开始创建第二个 <code>control plane</code> 时配置写错了需要重建，此时你一旦删除第二个 <code>control plane</code> 会导致第一个 <code>control plane</code> 也会失败，原因是<strong>创建第二个 <code>control plane</code> 时 kubeadm 已经自动完成了 etcd 的集群模式，当删除第二个 <code>control plane</code> 的时候由于集群可用原因会导致第一个 <code>control plane</code> 下的 etcd 发现节点失联从而也不提供服务；</strong>所以综合考虑到后续迁移、灾备等因素，这里选择了将 etcd 放置在外部集群中；同样也方便我以后各种折腾应对一些极端情况啥的。</p><h3 id="4-2、部署-Etcd"><a href="#4-2、部署-Etcd" class="headerlink" title="4.2、部署 Etcd"></a>4.2、部署 Etcd</h3><p>确定了需要在外部部署 etcd 集群后，只需要开干就完事了；查了一下 ubuntu 官方源已经有了 etcd 安装包，但是版本比较老，测试了一下 golang 的 build 版本是 1.10；所以我还是选择了从官方 release 下载最新的版本安装；当然最后还是因为懒，我自己打了一个 deb 包… deb 包可以从这个项目 <a href="https://github.com/mritd/etcd-deb/releases" target="_blank" rel="noopener">mritd/etcd-deb</a> 下载，担心安全性的可以利用项目脚本自己打包，以下是安装过程:</p><div class="hljs"><pre><code class="hljs sh"><span class="hljs-comment"># 下载软件包</span>wget https://github.com/mritd/etcd-deb/releases/download/v3.3.18/etcd_3.3.18_amd64.debwget https://github.com/mritd/etcd-deb/releases/download/v3.3.18/cfssl_1.4.1_amd64.deb<span class="hljs-comment"># 安装 etcd(至少在 3 台节点上执行)</span>dpkg -i etcd_3.3.18_amd64.deb cfssl_1.4.1_amd64.deb</code></pre></div><p><strong>既然自己部署 etcd，那么证书签署啥的还得自己来了，证书签署这里借助 cfssl 工具，cfssl 目前提供了 deb 的 make target，但是没找到 deb 包，所以也自己 build 了(担心安全性的可自行去官方下载)；</strong>接着编辑一下 <code>/etc/etcd/cfssl/etcd-csr.json</code> 文件，用 <code>/etc/etcd/cfssl/create.sh</code> 脚本创建证书，并将证书复制到指定目录</p><div class="hljs"><pre><code class="hljs sh"><span class="hljs-comment"># 创建证书</span><span class="hljs-built_in">cd</span> /etc/etcd/cfssl &amp;&amp; ./create.sh<span class="hljs-comment"># 复制证书</span>mv /etc/etcd/cfssl/*.pem /etc/etcd/ssl</code></pre></div><p>最后在 3 台节点上修改配置，并将刚刚创建的证书同步到其他两台节点启动既可；下面是单台节点的配置样例</p><div class="hljs"><pre><code class="hljs sh"><span class="hljs-comment"># /etc/etcd/etcd.conf</span><span class="hljs-comment"># [member]</span>ETCD_NAME=etcd1ETCD_DATA_DIR=<span class="hljs-string">"/var/lib/etcd/data"</span>ETCD_WAL_DIR=<span class="hljs-string">"/var/lib/etcd/wal"</span>ETCD_SNAPSHOT_COUNT=<span class="hljs-string">"100"</span>ETCD_HEARTBEAT_INTERVAL=<span class="hljs-string">"100"</span>ETCD_ELECTION_TIMEOUT=<span class="hljs-string">"1000"</span>ETCD_LISTEN_PEER_URLS=<span class="hljs-string">"https://172.16.10.21:2380"</span>ETCD_LISTEN_CLIENT_URLS=<span class="hljs-string">"https://172.16.10.21:2379,http://127.0.0.1:2379"</span>ETCD_MAX_SNAPSHOTS=<span class="hljs-string">"5"</span>ETCD_MAX_WALS=<span class="hljs-string">"5"</span><span class="hljs-comment">#ETCD_CORS=""</span><span class="hljs-comment"># [cluster]</span>ETCD_INITIAL_ADVERTISE_PEER_URLS=<span class="hljs-string">"https://172.16.10.21:2380"</span><span class="hljs-comment"># if you use different ETCD_NAME (e.g. test), set ETCD_INITIAL_CLUSTER value for this name, i.e. "test=http://..."</span>ETCD_INITIAL_CLUSTER=<span class="hljs-string">"etcd1=https://172.16.10.21:2380,etcd2=https://172.16.10.22:2380,etcd3=https://172.16.10.23:2380"</span>ETCD_INITIAL_CLUSTER_STATE=<span class="hljs-string">"new"</span>ETCD_INITIAL_CLUSTER_TOKEN=<span class="hljs-string">"etcd-cluster"</span>ETCD_ADVERTISE_CLIENT_URLS=<span class="hljs-string">"https://172.16.10.21:2379"</span><span class="hljs-comment">#ETCD_DISCOVERY=""</span><span class="hljs-comment">#ETCD_DISCOVERY_SRV=""</span><span class="hljs-comment">#ETCD_DISCOVERY_FALLBACK="proxy"</span><span class="hljs-comment">#ETCD_DISCOVERY_PROXY=""</span><span class="hljs-comment">#ETCD_STRICT_RECONFIG_CHECK="false"</span>ETCD_AUTO_COMPACTION_RETENTION=<span class="hljs-string">"24"</span><span class="hljs-comment"># [proxy]</span><span class="hljs-comment">#ETCD_PROXY="off"</span><span class="hljs-comment">#ETCD_PROXY_FAILURE_WAIT="5000"</span><span class="hljs-comment">#ETCD_PROXY_REFRESH_INTERVAL="30000"</span><span class="hljs-comment">#ETCD_PROXY_DIAL_TIMEOUT="1000"</span><span class="hljs-comment">#ETCD_PROXY_WRITE_TIMEOUT="5000"</span><span class="hljs-comment">#ETCD_PROXY_READ_TIMEOUT="0"</span><span class="hljs-comment"># [security]</span>ETCD_CERT_FILE=<span class="hljs-string">"/etc/etcd/ssl/etcd.pem"</span>ETCD_KEY_FILE=<span class="hljs-string">"/etc/etcd/ssl/etcd-key.pem"</span>ETCD_CLIENT_CERT_AUTH=<span class="hljs-string">"true"</span>ETCD_TRUSTED_CA_FILE=<span class="hljs-string">"/etc/etcd/ssl/etcd-root-ca.pem"</span>ETCD_AUTO_TLS=<span class="hljs-string">"true"</span>ETCD_PEER_CERT_FILE=<span class="hljs-string">"/etc/etcd/ssl/etcd.pem"</span>ETCD_PEER_KEY_FILE=<span class="hljs-string">"/etc/etcd/ssl/etcd-key.pem"</span>ETCD_PEER_CLIENT_CERT_AUTH=<span class="hljs-string">"true"</span>ETCD_PEER_TRUSTED_CA_FILE=<span class="hljs-string">"/etc/etcd/ssl/etcd-root-ca.pem"</span>ETCD_PEER_AUTO_TLS=<span class="hljs-string">"true"</span><span class="hljs-comment"># [logging]</span><span class="hljs-comment">#ETCD_DEBUG="false"</span><span class="hljs-comment"># examples for -log-package-levels etcdserver=WARNING,security=DEBUG</span><span class="hljs-comment">#ETCD_LOG_PACKAGE_LEVELS=""</span><span class="hljs-comment"># [performance]</span>ETCD_QUOTA_BACKEND_BYTES=<span class="hljs-string">"5368709120"</span>ETCD_AUTO_COMPACTION_RETENTION=<span class="hljs-string">"3"</span></code></pre></div><p><strong>注意: 其他两台节点请调整 <code>ETCD_NAME</code> 为不重复的其他名称，调整 <code>ETCD_LISTEN_PEER_URLS</code>、<code>ETCD_LISTEN_CLIENT_URLS</code>、<code>ETCD_INITIAL_ADVERTISE_PEER_URLS</code>、<code>ETCD_ADVERTISE_CLIENT_URLS</code> 为其他节点对应的 IP；同时生产环境请将 <code>ETCD_INITIAL_CLUSTER_TOKEN</code> 替换为复杂的 token</strong></p><div class="hljs"><pre><code class="hljs sh"><span class="hljs-comment"># 同步证书</span>scp -r /etc/etcd/ssl 172.16.10.22:/etc/etcd/sslscp -r /etc/etcd/ssl 172.16.10.23:/etc/etcd/ssl<span class="hljs-comment"># 修复权限(3台节点都要执行)</span>chown -R etcd:etcd /etc/etcd<span class="hljs-comment"># 最后每个节点依次启动既可</span>systemctl start etcd</code></pre></div><p>启动完成后可以通过以下命令测试是否正常</p><div class="hljs"><pre><code class="hljs sh"><span class="hljs-comment"># 查看集群成员</span>k1.node ➜ etcdctl member list3cbbaf77904c6153, started, etcd2, https://172.16.10.22:2380, https://172.16.10.22:23798eb7652b6bd99c30, started, etcd1, https://172.16.10.21:2380, https://172.16.10.21:237991f4e10726460d8c, started, etcd3, https://172.16.10.23:2380, https://172.16.10.23:2379<span class="hljs-comment"># 检测集群健康状态</span>k1.node ➜ etcdctl endpoint health --cacert /etc/etcd/ssl/etcd-root-ca.pem --cert /etc/etcd/ssl/etcd.pem --key /etc/etcd/ssl/etcd-key.pem --endpoints https://172.16.10.21:2379,https://172.16.10.22:2379,https://172.16.10.23:2379https://172.16.10.21:2379 is healthy: successfully committed proposal: took = 16.632246mshttps://172.16.10.23:2379 is healthy: successfully committed proposal: took = 21.122603mshttps://172.16.10.22:2379 is healthy: successfully committed proposal: took = 22.592005ms</code></pre></div><h2 id="五、部署-Kubernetes"><a href="#五、部署-Kubernetes" class="headerlink" title="五、部署 Kubernetes"></a>五、部署 Kubernetes</h2><h3 id="5-1、安装-kueadm"><a href="#5-1、安装-kueadm" class="headerlink" title="5.1、安装 kueadm"></a>5.1、安装 kueadm</h3><p>安装 kubeadm 没什么好说的，国内被墙用阿里的源既可</p><div class="hljs"><pre><code class="hljs sh">apt-get install -y apt-transport-httpscurl https://mirrors.aliyun.com/kubernetes/apt/doc/apt-key.gpg | apt-key add -cat &lt;&lt;EOF &gt;/etc/apt/sources.list.d/kubernetes.listdeb https://mirrors.aliyun.com/kubernetes/apt/ kubernetes-xenial mainEOFapt update<span class="hljs-comment"># ebtables、ethtool kubelet 可能会用，具体忘了，反正从官方文档上看到的</span>apt install kubelet kubeadm kubectl ebtables ethtool -y</code></pre></div><h3 id="5-2、部署-Nginx"><a href="#5-2、部署-Nginx" class="headerlink" title="5.2、部署 Nginx"></a>5.2、部署 Nginx</h3><p>从上面的 HA 架构图上可以看到，为了维持 apiserver 的 HA，需要在每个机器上部署一个 nginx 做 4 层的 LB；为保证后续的 node 节点正常加入，需要首先行部署 nginx；nginx 安装同样喜欢偷懒，直接 docker 跑了…毕竟都开始 kubeadm 了，那么也没必要去纠结 docker 是否稳定的问题了；以下为 nginx 相关配置</p><p><strong>apiserver-proxy.conf</strong></p><div class="hljs"><pre><code class="hljs sh">error_log stderr notice;worker_processes auto;events &#123;multi_accept on;use epoll;worker_connections 1024;&#125;stream &#123;    upstream kube_apiserver &#123;        least_conn;        <span class="hljs-comment"># 后端为三台 master 节点的 apiserver 地址</span>        server 172.16.10.21:5443;        server 172.16.10.22:5443;        server 172.16.10.23:5443;    &#125;        server &#123;        listen        0.0.0.0:6443;        proxy_pass    kube_apiserver;        proxy_timeout 10m;        proxy_connect_timeout 1s;    &#125;&#125;</code></pre></div><p><strong>kube-apiserver-proxy.service</strong></p><div class="hljs"><pre><code class="hljs sh">[Unit]Description=kubernetes apiserver docker wrapperWants=docker.socketAfter=docker.service[Service]User=rootPermissionsStartOnly=<span class="hljs-literal">true</span>ExecStart=/usr/bin/docker run -p 6443:6443 \                          -v /etc/kubernetes/apiserver-proxy.conf:/etc/nginx/nginx.conf \                          --name kube-apiserver-proxy \                          --net=host \                          --restart=on-failure:5 \                          --memory=512M \                          nginx:1.17.6-alpineExecStartPre=-/usr/bin/docker rm -f kube-apiserver-proxyExecStop=/usr/bin/docker rm -rf kube-apiserver-proxyRestart=alwaysRestartSec=15sTimeoutStartSec=30s[Install]WantedBy=multi-user.target</code></pre></div><p>启动 nginx 代理(每台机器都要启动，包括 master 节点)</p><div class="hljs"><pre><code class="hljs sh">cp apiserver-proxy.conf /etc/kubernetescp kube-apiserver-proxy.service /lib/systemd/systemsystemctl daemon-reloadsystemctl <span class="hljs-built_in">enable</span> kube-apiserver-proxy.service &amp;&amp; systemctl start kube-apiserver-proxy.service</code></pre></div><h3 id="5-3、启动-control-plane"><a href="#5-3、启动-control-plane" class="headerlink" title="5.3、启动 control plane"></a>5.3、启动 control plane</h3><h4 id="5-3-1、关于-Swap"><a href="#5-3-1、关于-Swap" class="headerlink" title="5.3.1、关于 Swap"></a>5.3.1、关于 Swap</h4><p>目前 kubelet 为了保证内存 limit，需要在每个节点上关闭 swap；但是说实话我看了这篇文章 <a href="https://chrisdown.name/2018/01/02/in-defence-of-swap.html" target="_blank" rel="noopener">In defence of swap: common misconceptions</a> 以后还是不想关闭 swap；更确切的说其实我们生产环境比较 “富”，pod 都不 limit 内存，所以下面的部署我忽略了 swap 错误检测</p><h4 id="5-3-2、kubeadm-配置"><a href="#5-3-2、kubeadm-配置" class="headerlink" title="5.3.2、kubeadm 配置"></a>5.3.2、kubeadm 配置</h4><p>当前版本的 kubeadm 已经支持了完善的配置管理(当然细节部分还有待支持)，以下为我目前使用的配置，相关位置已经做了注释，更具体的配置自行查阅官方文档</p><p><strong>kubeadm.yaml</strong></p><div class="hljs"><pre><code class="hljs yaml"><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">kubeadm.k8s.io/v1beta2</span><span class="hljs-attr">kind:</span> <span class="hljs-string">InitConfiguration</span><span class="hljs-attr">localAPIEndpoint:</span>  <span class="hljs-comment"># 第一个 master 节点 IP</span>  <span class="hljs-attr">advertiseAddress:</span> <span class="hljs-string">"172.16.10.21"</span>  <span class="hljs-comment"># 6443 留给了 nginx，apiserver 换到 5443</span>  <span class="hljs-attr">bindPort:</span> <span class="hljs-number">5443</span><span class="hljs-comment"># 这个 token 使用以下命令生成</span><span class="hljs-comment"># kubeadm alpha certs certificate-key</span><span class="hljs-attr">certificateKey:</span> <span class="hljs-string">7373f829c733b46fb78f0069f90185e0f00254381641d8d5a7c5984b2cf17cd3</span> <span class="hljs-meta">---</span><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">kubeadm.k8s.io/v1beta2</span><span class="hljs-attr">kind:</span> <span class="hljs-string">ClusterConfiguration</span><span class="hljs-comment"># 使用外部 etcd 配置</span><span class="hljs-attr">etcd:</span>  <span class="hljs-attr">external:</span>    <span class="hljs-attr">endpoints:</span>    <span class="hljs-bullet">-</span> <span class="hljs-string">"https://172.16.10.21:2379"</span>    <span class="hljs-bullet">-</span> <span class="hljs-string">"https://172.16.10.22:2379"</span>    <span class="hljs-bullet">-</span> <span class="hljs-string">"https://172.16.10.23:2379"</span>    <span class="hljs-attr">caFile:</span> <span class="hljs-string">"/etc/etcd/ssl/etcd-root-ca.pem"</span>    <span class="hljs-attr">certFile:</span> <span class="hljs-string">"/etc/etcd/ssl/etcd.pem"</span>    <span class="hljs-attr">keyFile:</span> <span class="hljs-string">"/etc/etcd/ssl/etcd-key.pem"</span><span class="hljs-comment"># 网络配置</span><span class="hljs-attr">networking:</span>  <span class="hljs-attr">serviceSubnet:</span> <span class="hljs-string">"10.25.0.0/16"</span>  <span class="hljs-attr">podSubnet:</span> <span class="hljs-string">"10.30.0.1/16"</span>  <span class="hljs-attr">dnsDomain:</span> <span class="hljs-string">"cluster.local"</span><span class="hljs-attr">kubernetesVersion:</span> <span class="hljs-string">"v1.17.0"</span><span class="hljs-comment"># 全局 apiserver LB 地址，由于采用了 nginx 负载，所以直接指向本地既可</span><span class="hljs-attr">controlPlaneEndpoint:</span> <span class="hljs-string">"127.0.0.1:6443"</span><span class="hljs-attr">apiServer:</span>  <span class="hljs-comment"># apiserver 的自定义扩展参数</span>  <span class="hljs-attr">extraArgs:</span>    <span class="hljs-attr">v:</span> <span class="hljs-string">"4"</span>    <span class="hljs-attr">alsologtostderr:</span> <span class="hljs-string">"true"</span>    <span class="hljs-comment"># 审计日志相关配置</span>    <span class="hljs-attr">audit-log-maxage:</span> <span class="hljs-string">"20"</span>    <span class="hljs-attr">audit-log-maxbackup:</span> <span class="hljs-string">"10"</span>    <span class="hljs-attr">audit-log-maxsize:</span> <span class="hljs-string">"100"</span>    <span class="hljs-attr">audit-log-path:</span> <span class="hljs-string">"/var/log/kube-audit/audit.log"</span>    <span class="hljs-attr">audit-policy-file:</span> <span class="hljs-string">"/etc/kubernetes/audit-policy.yaml"</span>    <span class="hljs-attr">authorization-mode:</span> <span class="hljs-string">"Node,RBAC"</span>    <span class="hljs-attr">event-ttl:</span> <span class="hljs-string">"720h"</span>    <span class="hljs-attr">runtime-config:</span> <span class="hljs-string">"api/all=true"</span>    <span class="hljs-attr">service-node-port-range:</span> <span class="hljs-string">"30000-50000"</span>    <span class="hljs-attr">service-cluster-ip-range:</span> <span class="hljs-string">"10.25.0.0/16"</span>  <span class="hljs-comment"># 由于自行定义了审计日志配置，所以需要将宿主机上的审计配置</span>  <span class="hljs-comment"># 挂载到 kube-apiserver 的 pod 容器中</span>  <span class="hljs-attr">extraVolumes:</span>  <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">"audit-config"</span>    <span class="hljs-attr">hostPath:</span> <span class="hljs-string">"/etc/kubernetes/audit-policy.yaml"</span>    <span class="hljs-attr">mountPath:</span> <span class="hljs-string">"/etc/kubernetes/audit-policy.yaml"</span>    <span class="hljs-attr">readOnly:</span> <span class="hljs-literal">true</span>    <span class="hljs-attr">pathType:</span> <span class="hljs-string">"File"</span>  <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">"audit-log"</span>    <span class="hljs-attr">hostPath:</span> <span class="hljs-string">"/var/log/kube-audit"</span>    <span class="hljs-attr">mountPath:</span> <span class="hljs-string">"/var/log/kube-audit"</span>    <span class="hljs-attr">pathType:</span> <span class="hljs-string">"DirectoryOrCreate"</span>  <span class="hljs-comment"># 这里是 apiserver 的证书地址配置</span>  <span class="hljs-comment"># 为了防止以后出特殊情况，我增加了一个泛域名</span>  <span class="hljs-attr">certSANs:</span>  <span class="hljs-bullet">-</span> <span class="hljs-string">"*.kubernetes.node"</span>  <span class="hljs-bullet">-</span> <span class="hljs-string">"172.16.10.21"</span>  <span class="hljs-bullet">-</span> <span class="hljs-string">"172.16.10.22"</span>  <span class="hljs-bullet">-</span> <span class="hljs-string">"172.16.10.23"</span>  <span class="hljs-attr">timeoutForControlPlane:</span> <span class="hljs-string">5m</span><span class="hljs-attr">controllerManager:</span>  <span class="hljs-attr">extraArgs:</span>    <span class="hljs-attr">v:</span> <span class="hljs-string">"4"</span>    <span class="hljs-comment"># 宿主机 ip 掩码</span>    <span class="hljs-attr">node-cidr-mask-size:</span> <span class="hljs-string">"19"</span>    <span class="hljs-attr">deployment-controller-sync-period:</span> <span class="hljs-string">"10s"</span>    <span class="hljs-attr">experimental-cluster-signing-duration:</span> <span class="hljs-string">"87600h"</span>    <span class="hljs-attr">node-monitor-grace-period:</span> <span class="hljs-string">"20s"</span>    <span class="hljs-attr">pod-eviction-timeout:</span> <span class="hljs-string">"2m"</span>    <span class="hljs-attr">terminated-pod-gc-threshold:</span> <span class="hljs-string">"30"</span><span class="hljs-attr">scheduler:</span>  <span class="hljs-attr">extraArgs:</span>    <span class="hljs-attr">v:</span> <span class="hljs-string">"4"</span><span class="hljs-attr">certificatesDir:</span> <span class="hljs-string">"/etc/kubernetes/pki"</span><span class="hljs-comment"># gcr.io 被墙，换成微软的镜像地址</span><span class="hljs-attr">imageRepository:</span> <span class="hljs-string">"gcr.azk8s.cn/google_containers"</span><span class="hljs-attr">clusterName:</span> <span class="hljs-string">"kuberentes"</span><span class="hljs-meta">---</span><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">kubelet.config.k8s.io/v1beta1</span><span class="hljs-attr">kind:</span> <span class="hljs-string">KubeletConfiguration</span><span class="hljs-comment"># kubelet specific options here</span><span class="hljs-comment"># 此配置保证了 kubelet 能在 swap 开启的情况下启动</span><span class="hljs-attr">failSwapOn:</span> <span class="hljs-literal">false</span><span class="hljs-attr">nodeStatusUpdateFrequency:</span> <span class="hljs-string">5s</span><span class="hljs-comment"># 一些驱逐阀值，具体自行查文档修改</span><span class="hljs-attr">evictionSoft:</span>  <span class="hljs-attr">"imagefs.available":</span> <span class="hljs-string">"15%"</span>  <span class="hljs-attr">"memory.available":</span> <span class="hljs-string">"512Mi"</span>  <span class="hljs-attr">"nodefs.available":</span> <span class="hljs-string">"15%"</span>  <span class="hljs-attr">"nodefs.inodesFree":</span> <span class="hljs-string">"10%"</span><span class="hljs-attr">evictionSoftGracePeriod:</span>  <span class="hljs-attr">"imagefs.available":</span> <span class="hljs-string">"3m"</span>  <span class="hljs-attr">"memory.available":</span> <span class="hljs-string">"1m"</span>  <span class="hljs-attr">"nodefs.available":</span> <span class="hljs-string">"3m"</span>  <span class="hljs-attr">"nodefs.inodesFree":</span> <span class="hljs-string">"1m"</span><span class="hljs-attr">evictionHard:</span>  <span class="hljs-attr">"imagefs.available":</span> <span class="hljs-string">"10%"</span>  <span class="hljs-attr">"memory.available":</span> <span class="hljs-string">"256Mi"</span>  <span class="hljs-attr">"nodefs.available":</span> <span class="hljs-string">"10%"</span>  <span class="hljs-attr">"nodefs.inodesFree":</span> <span class="hljs-string">"5%"</span><span class="hljs-attr">evictionMaxPodGracePeriod:</span> <span class="hljs-number">30</span><span class="hljs-attr">imageGCLowThresholdPercent:</span> <span class="hljs-number">70</span><span class="hljs-attr">imageGCHighThresholdPercent:</span> <span class="hljs-number">80</span><span class="hljs-attr">kubeReserved:</span>  <span class="hljs-attr">"cpu":</span> <span class="hljs-string">"500m"</span>  <span class="hljs-attr">"memory":</span> <span class="hljs-string">"512Mi"</span>  <span class="hljs-attr">"ephemeral-storage":</span> <span class="hljs-string">"1Gi"</span><span class="hljs-attr">rotateCertificates:</span> <span class="hljs-literal">true</span><span class="hljs-meta">---</span><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">kubeproxy.config.k8s.io/v1alpha1</span><span class="hljs-attr">kind:</span> <span class="hljs-string">KubeProxyConfiguration</span><span class="hljs-comment"># kube-proxy specific options here</span><span class="hljs-attr">clusterCIDR:</span> <span class="hljs-string">"10.30.0.1/16"</span><span class="hljs-comment"># 启用 ipvs 模式</span><span class="hljs-attr">mode:</span> <span class="hljs-string">"ipvs"</span><span class="hljs-attr">ipvs:</span>  <span class="hljs-attr">minSyncPeriod:</span> <span class="hljs-string">5s</span>  <span class="hljs-attr">syncPeriod:</span> <span class="hljs-string">5s</span>  <span class="hljs-comment"># ipvs 负载策略</span>  <span class="hljs-attr">scheduler:</span> <span class="hljs-string">"wrr"</span></code></pre></div><p><strong>关于这个配置配置文件的文档还是很不完善，对于不懂 golang 的人来说很难知道具体怎么配置，以下做一下简要说明(请确保你已经拉取了 kubernetes 源码和安装了 Goland)</strong></p><p><strong>kubeadm 配置中每个配置段都会有个 <code>kind</code> 字段，<code>kind</code> 实际上对应了 go 代码中的 <code>struct</code> 结构体；同时从 <code>apiVersion</code> 字段中能够看到具体的版本，比如 <code>v1alpha1</code> 等；有了这两个信息事实上你就可以直接在源码中去找到对应的结构体</strong></p><p><img src="https://cdn.oss.link/markdown/dwo5h.png" srcset="/img/loading.gif" alt="struct_search"></p><p>在结构体中所有的配置便可以一目了然</p><p><img src="https://cdn.oss.link/markdown/0jc9b.png" srcset="/img/loading.gif" alt="struct_detail"></p><p>关于数据类型，如果是 <code>string</code> 的类型，那么意味着你要在 yaml 里写 <code>&quot;xxxx&quot;</code> 带引号这种，当然有些时候不写能兼容，有些时候不行比如 <code>extraArgs</code> 字段是一个 <code>map[string]string</code> 如果 value 不带引号就报错；<strong>如果数据类型为 <code>metav1.Duration</code>(实际上就是 <code>time.Duration</code>)，那么你看着它是个 <code>int64</code> 但实际上你要写 <code>1h2m3s</code> 这种人类可读的格式，这是 go 的特色…</strong></p><p><strong>audit-policy.yaml</strong></p><div class="hljs"><pre><code class="hljs yaml"><span class="hljs-comment"># Log all requests at the Metadata level.</span><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">audit.k8s.io/v1</span><span class="hljs-attr">kind:</span> <span class="hljs-string">Policy</span><span class="hljs-attr">rules:</span><span class="hljs-bullet">-</span> <span class="hljs-attr">level:</span> <span class="hljs-string">Metadata</span></code></pre></div><p>可能 <code>Metadata</code> 级别的审计日志比较多，想自行调整审计日志级别的可以参考<a href="https://kubernetes.io/docs/tasks/debug-application-cluster/audit/#audit-policy" target="_blank" rel="noopener">官方文档</a></p><h4 id="5-3-3、拉起-control-plane"><a href="#5-3-3、拉起-control-plane" class="headerlink" title="5.3.3、拉起 control plane"></a>5.3.3、拉起 control plane</h4><p>有了完整的 <code>kubeadm.yaml</code> 和 <code>audit-policy.yaml</code> 配置后，直接一条命令拉起 control plane 既可</p><div class="hljs"><pre><code class="hljs sh"><span class="hljs-comment"># 先将审计配置放到目标位置(3 台 master 都要执行)</span>cp audit-policy.yaml /etc/kubernetes<span class="hljs-comment"># 拉起 control plane</span>kubeadm init --config kubeadm.yaml --upload-certs --ignore-preflight-errors=Swap</code></pre></div><p><strong>control plane 拉起以后注意要保存屏幕输出，方便后续添加其他集群节点</strong></p><div class="hljs"><pre><code class="hljs sh">Your Kubernetes control-plane has initialized successfully!To start using your cluster, you need to run the following as a regular user:  mkdir -p <span class="hljs-variable">$HOME</span>/.kube  sudo cp -i /etc/kubernetes/admin.conf <span class="hljs-variable">$HOME</span>/.kube/config  sudo chown $(id -u):$(id -g) <span class="hljs-variable">$HOME</span>/.kube/configYou should now deploy a pod network to the cluster.Run <span class="hljs-string">"kubectl apply -f [podnetwork].yaml"</span> with one of the options listed at:  https://kubernetes.io/docs/concepts/cluster-administration/addons/You can now join any number of the control-plane node running the following <span class="hljs-built_in">command</span> on each as root:  kubeadm join 127.0.0.1:6443 --token r4t3l3.14mmuivm7xbtaeoj \    --discovery-token-ca-cert-hash sha256:06f49f1f29d08b797fbf04d87b9b0fd6095a4693e9b1d59c429745cfa082b31d \    --control-plane --certificate-key 7373f829c733b46fb78f0069f90185e0f00254381641d8d5a7c5984b2cf17cd3Please note that the certificate-key gives access to cluster sensitive data, keep it secret!As a safeguard, uploaded-certs will be deleted <span class="hljs-keyword">in</span> two hours; If necessary, you can use<span class="hljs-string">"kubeadm init phase upload-certs --upload-certs"</span> to reload certs afterward.Then you can join any number of worker nodes by running the following on each as root:kubeadm join 127.0.0.1:6443 --token r4t3l3.14mmuivm7xbtaeoj \    --discovery-token-ca-cert-hash sha256:06f49f1f29d08b797fbf04d87b9b0fd6095a4693e9b1d59c429745cfa082b31d</code></pre></div><p><strong>根据屏幕提示配置 kubectl</strong></p><div class="hljs"><pre><code class="hljs sh">mkdir -p <span class="hljs-variable">$HOME</span>/.kubesudo cp -i /etc/kubernetes/admin.conf <span class="hljs-variable">$HOME</span>/.kube/configsudo chown $(id -u):$(id -g) <span class="hljs-variable">$HOME</span>/.kube/config</code></pre></div><h3 id="5-4、部署-CNI"><a href="#5-4、部署-CNI" class="headerlink" title="5.4、部署 CNI"></a>5.4、部署 CNI</h3><p>关于网络插件的选择，以前一直喜欢 Calico，因为其性能确实好；到后来 flannel 出了 <code>host-gw</code> 以后现在两者性能也差不多了；但是 <strong>flannel 好处是一个工具通吃所有环境(云环境+裸机2层直通)，坏处是 flannel 缺乏比较好的策略管理(当然可以使用两者结合的 Canal)；</strong>后来思来想去其实我们生产倒是很少需要策略管理，所以这回怂回到 flannel 了(逃…)</p><p>Flannel 部署非常简单，根据官方文档下载配置，根据情况调整 <code>backend</code> 和 pod 的 CIDR，然后 apply 一下既可</p><div class="hljs"><pre><code class="hljs sh"><span class="hljs-comment"># 下载配置文件</span>wget https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml<span class="hljs-comment"># 调整 backend 为 host-gw(测试环境 2 层直连)</span>k1.node ➜  grep -A 35 ConfigMap kube-flannel.ymlkind: ConfigMapapiVersion: v1metadata:  name: kube-flannel-cfg  namespace: kube-system  labels:    tier: node    app: flanneldata:  cni-conf.json: |    &#123;      <span class="hljs-string">"name"</span>: <span class="hljs-string">"cbr0"</span>,      <span class="hljs-string">"cniVersion"</span>: <span class="hljs-string">"0.3.1"</span>,      <span class="hljs-string">"plugins"</span>: [        &#123;          <span class="hljs-string">"type"</span>: <span class="hljs-string">"flannel"</span>,          <span class="hljs-string">"delegate"</span>: &#123;            <span class="hljs-string">"hairpinMode"</span>: <span class="hljs-literal">true</span>,            <span class="hljs-string">"isDefaultGateway"</span>: <span class="hljs-literal">true</span>          &#125;        &#125;,        &#123;          <span class="hljs-string">"type"</span>: <span class="hljs-string">"portmap"</span>,          <span class="hljs-string">"capabilities"</span>: &#123;            <span class="hljs-string">"portMappings"</span>: <span class="hljs-literal">true</span>          &#125;        &#125;      ]    &#125;  net-conf.json: |    &#123;      <span class="hljs-string">"Network"</span>: <span class="hljs-string">"10.30.0.0/16"</span>,      <span class="hljs-string">"Backend"</span>: &#123;        <span class="hljs-string">"Type"</span>: <span class="hljs-string">"host-gw"</span>      &#125;    &#125;<span class="hljs-comment"># 调整完成后 apply 一下</span>kubectl apply -f kube-flannel.yml</code></pre></div><h3 id="5-5、启动其他-control-plane"><a href="#5-5、启动其他-control-plane" class="headerlink" title="5.5、启动其他 control plane"></a>5.5、启动其他 control plane</h3><p>为了保证 HA 架构，还需要在另外两台 master 上启动 control plane；<strong>在启动之前请确保另外两台 master 节点节点上 <code>/etc/kubernetes/audit-policy.yaml</code> 审计配置已经分发完成，确保 <code>127.0.0.1:6443</code> 上监听的 4 层 LB 工作正常(可尝试使用 <code>curl -k https://127.0.0.1:6443</code> 测试)；</strong>根据第一个 control plane 终端输出，其他 control plane 加入命令如下</p><div class="hljs"><pre><code class="hljs sh">kubeadm join 127.0.0.1:6443 --token r4t3l3.14mmuivm7xbtaeoj \    --discovery-token-ca-cert-hash sha256:06f49f1f29d08b797fbf04d87b9b0fd6095a4693e9b1d59c429745cfa082b31d \    --control-plane --certificate-key 7373f829c733b46fb78f0069f90185e0f00254381641d8d5a7c5984b2cf17cd3</code></pre></div><p><strong>由于在使用 <code>kubeadm join</code> 时相关选项(<code>--discovery-token-ca-cert-hash</code>、<code>--control-plane</code>)无法与 <code>--config</code> 一起使用，这也就意味着我们必须增加一些附加指令来提供 <code>kubeadm.yaml</code> 配置文件中的一些属性</strong>；最终完整的 control plane 加入命令如下，在其他 master 直接执行既可(<strong><code>--apiserver-advertise-address</code> 的 IP 地址是目标 master 的 IP</strong>)</p><div class="hljs"><pre><code class="hljs sh">kubeadm join 127.0.0.1:6443 --token r4t3l3.14mmuivm7xbtaeoj \    --discovery-token-ca-cert-hash sha256:06f49f1f29d08b797fbf04d87b9b0fd6095a4693e9b1d59c429745cfa082b31d \    --control-plane --certificate-key 7373f829c733b46fb78f0069f90185e0f00254381641d8d5a7c5984b2cf17cd3 \    --apiserver-advertise-address 172.16.10.22 \    --apiserver-bind-port 5443 \    --ignore-preflight-errors=Swap</code></pre></div><p><strong>所有 control plane 启动完成后应当通过在每个节点上运行 <code>kubectl get cs</code> 验证各个组件运行状态</strong></p><div class="hljs"><pre><code class="hljs sh">k2.node ➜ kubectl get csNAME                 STATUS    MESSAGE             ERRORscheduler            Healthy   okcontroller-manager   Healthy   oketcd-1               Healthy   &#123;<span class="hljs-string">"health"</span>:<span class="hljs-string">"true"</span>&#125;etcd-0               Healthy   &#123;<span class="hljs-string">"health"</span>:<span class="hljs-string">"true"</span>&#125;etcd-2               Healthy   &#123;<span class="hljs-string">"health"</span>:<span class="hljs-string">"true"</span>&#125;k2.node ➜ kubectl get node -o wideNAME      STATUS   ROLES    AGE   VERSION   INTERNAL-IP    EXTERNAL-IP   OS-IMAGE             KERNEL-VERSION      CONTAINER-RUNTIMEk1.node   Ready    master   28m   v1.17.0   172.16.10.21   &lt;none&gt;        Ubuntu 18.04.3 LTS   4.15.0-74-generic   docker://19.3.5k2.node   Ready    master   10m   v1.17.0   172.16.10.22   &lt;none&gt;        Ubuntu 18.04.3 LTS   4.15.0-74-generic   docker://19.3.5k3.node   Ready    master   3m    v1.17.0   172.16.10.23   &lt;none&gt;        Ubuntu 18.04.3 LTS   4.15.0-74-generic   docker://19.3.5</code></pre></div><h3 id="5-6、启动-Node"><a href="#5-6、启动-Node" class="headerlink" title="5.6、启动 Node"></a>5.6、启动 Node</h3><p>node 节点的启动相较于 master 来说要简单得多，只需要增加一个防止 <code>swap</code> 开启拒绝启动的参数既可</p><div class="hljs"><pre><code class="hljs sh">kubeadm join 127.0.0.1:6443 --token r4t3l3.14mmuivm7xbtaeoj \    --discovery-token-ca-cert-hash sha256:06f49f1f29d08b797fbf04d87b9b0fd6095a4693e9b1d59c429745cfa082b31d \    --ignore-preflight-errors=Swap</code></pre></div><p>启动成功后在 master 上可以看到所有 node 信息</p><div class="hljs"><pre><code class="hljs sh">k1.node ➜ kubectl get node -o wideNAME      STATUS   ROLES    AGE     VERSION   INTERNAL-IP    EXTERNAL-IP   OS-IMAGE             KERNEL-VERSION      CONTAINER-RUNTIMEk1.node   Ready    master   32m     v1.17.0   172.16.10.21   &lt;none&gt;        Ubuntu 18.04.3 LTS   4.15.0-74-generic   docker://19.3.5k2.node   Ready    master   14m     v1.17.0   172.16.10.22   &lt;none&gt;        Ubuntu 18.04.3 LTS   4.15.0-74-generic   docker://19.3.5k3.node   Ready    master   6m35s   v1.17.0   172.16.10.23   &lt;none&gt;        Ubuntu 18.04.3 LTS   4.15.0-74-generic   docker://19.3.5k4.node   Ready    &lt;none&gt;   72s     v1.17.0   172.16.10.24   &lt;none&gt;        Ubuntu 18.04.3 LTS   4.15.0-74-generic   docker://19.3.5k5.node   Ready    &lt;none&gt;   66s     v1.17.0   172.16.10.25   &lt;none&gt;        Ubuntu 18.04.3 LTS   4.15.0-74-generic   docker://19.3.5</code></pre></div><h3 id="5-7、调整及测试"><a href="#5-7、调整及测试" class="headerlink" title="5.7、调整及测试"></a>5.7、调整及测试</h3><p>集群搭建好以后，如果想让 master 节点也参与调度任务，需要在任意一台 master 节点执行以下命令</p><div class="hljs"><pre><code class="hljs sh"><span class="hljs-comment"># node 节点报错属于正常情况</span>k1.node ➜ kubectl taint nodes --all node-role.kubernetes.io/master-node/k1.node untaintednode/k2.node untaintednode/k3.node untaintedtaint <span class="hljs-string">"node-role.kubernetes.io/master"</span> not foundtaint <span class="hljs-string">"node-role.kubernetes.io/master"</span> not found</code></pre></div><p>最后创建一个 deployment 和一个 service，并在不同主机上 ping pod IP 测试网络联通性，在 pod 内直接 curl service 名称测试 dns 解析既可</p><p><strong>test-nginx.deploy.yaml</strong></p><div class="hljs"><pre><code class="hljs yaml"><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">apps/v1</span><span class="hljs-attr">kind:</span> <span class="hljs-string">Deployment</span><span class="hljs-attr">metadata:</span>  <span class="hljs-attr">name:</span> <span class="hljs-string">test-nginx</span>  <span class="hljs-attr">labels:</span>    <span class="hljs-attr">app:</span> <span class="hljs-string">test-nginx</span><span class="hljs-attr">spec:</span>  <span class="hljs-attr">replicas:</span> <span class="hljs-number">3</span>  <span class="hljs-attr">selector:</span>    <span class="hljs-attr">matchLabels:</span>      <span class="hljs-attr">app:</span> <span class="hljs-string">test-nginx</span>  <span class="hljs-attr">template:</span>    <span class="hljs-attr">metadata:</span>      <span class="hljs-attr">labels:</span>        <span class="hljs-attr">app:</span> <span class="hljs-string">test-nginx</span>    <span class="hljs-attr">spec:</span>      <span class="hljs-attr">containers:</span>      <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">test-nginx</span>        <span class="hljs-attr">image:</span> <span class="hljs-string">nginx:1.17.6-alpine</span>        <span class="hljs-attr">ports:</span>        <span class="hljs-bullet">-</span> <span class="hljs-attr">containerPort:</span> <span class="hljs-number">80</span></code></pre></div><p><strong>test-nginx.svc.yaml</strong></p><div class="hljs"><pre><code class="hljs yaml"><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">v1</span><span class="hljs-attr">kind:</span> <span class="hljs-string">Service</span><span class="hljs-attr">metadata:</span>  <span class="hljs-attr">name:</span> <span class="hljs-string">test-nginx</span><span class="hljs-attr">spec:</span>  <span class="hljs-attr">selector:</span>    <span class="hljs-attr">app:</span> <span class="hljs-string">test-nginx</span>  <span class="hljs-attr">ports:</span>    <span class="hljs-bullet">-</span> <span class="hljs-attr">protocol:</span> <span class="hljs-string">TCP</span>      <span class="hljs-attr">port:</span> <span class="hljs-number">80</span>      <span class="hljs-attr">targetPort:</span> <span class="hljs-number">80</span></code></pre></div><h2 id="六、后续处理"><a href="#六、后续处理" class="headerlink" title="六、后续处理"></a>六、后续处理</h2><blockquote><p>说实话使用 kubeadm 后，我更关注的是集群后续的扩展性调整是否能达到目标；搭建其实很简单，大部份时间都在测试后续调整上</p></blockquote><h3 id="6-1、Etcd-迁移"><a href="#6-1、Etcd-迁移" class="headerlink" title="6.1、Etcd 迁移"></a>6.1、Etcd 迁移</h3><p>由于我们采用的是外部的 Etcd，所以迁移起来比较简单怎么折腾都行；需要注意的是换 IP 的时候注意保证老的 3 个节点至少有一个可用，否则可能导致集群崩溃；调整完成后记得分发相关 Etcd 节点的证书，重启时顺序一个一个重启，不要并行操作</p><h3 id="6-2、Master-配置修改"><a href="#6-2、Master-配置修改" class="headerlink" title="6.2、Master 配置修改"></a>6.2、Master 配置修改</h3><p>如果需要修改 conrol plane 上 apiserver、scheduler 等配置，直接修改 <code>kubeadm.yaml</code> 配置文件(<strong>所以集群搭建好后务必保存好</strong>)，然后执行 <code>kubeadm upgrade apply --config kubeadm.yaml</code> 升级集群既可，升级前一定作好相关备份工作；我只在测试环境测试这个命令工作还可以，生产环境还是需要谨慎</p><h3 id="6-3、证书续期"><a href="#6-3、证书续期" class="headerlink" title="6.3、证书续期"></a>6.3、证书续期</h3><p>目前根据我测试的结果，controller manager 的 <strong>experimental-cluster-signing-duration</strong> 参数在 init 的签发证书阶段似乎并未生效；<strong>目前根据文档描述 <code>kubelet</code> client 的证书会自动滚动，其他证书默认 1 年有效期，需要自己使用命令续签；</strong>续签命令如下</p><div class="hljs"><pre><code class="hljs sh"><span class="hljs-comment"># 查看证书过期时间</span>k1.node ➜ kubeadm alpha certs check-expiration[check-expiration] Reading configuration from the cluster...[check-expiration] FYI: You can look at this config file with <span class="hljs-string">'kubectl -n kube-system get cm kubeadm-config -oyaml'</span>CERTIFICATE                EXPIRES                  RESIDUAL TIME   CERTIFICATE AUTHORITY   EXTERNALLY MANAGEDadmin.conf                 Jan 11, 2021 10:06 UTC   364d                                    noapiserver                  Jan 11, 2021 10:06 UTC   364d            ca                      noapiserver-kubelet-client   Jan 11, 2021 10:06 UTC   364d            ca                      nocontroller-manager.conf    Jan 11, 2021 10:06 UTC   364d                                    nofront-proxy-client         Jan 11, 2021 10:06 UTC   364d            front-proxy-ca          noscheduler.conf             Jan 11, 2021 10:06 UTC   364d                                    noCERTIFICATE AUTHORITY   EXPIRES                  RESIDUAL TIME   EXTERNALLY MANAGEDca                      Jan 09, 2030 10:06 UTC   9y              nofront-proxy-ca          Jan 09, 2030 10:06 UTC   9y              no<span class="hljs-comment"># 续签证书</span>k1.node ➜ kubeadm alpha certs renew all[renew] Reading configuration from the cluster...[renew] FYI: You can look at this config file with <span class="hljs-string">'kubectl -n kube-system get cm kubeadm-config -oyaml'</span>certificate embedded <span class="hljs-keyword">in</span> the kubeconfig file <span class="hljs-keyword">for</span> the admin to use and <span class="hljs-keyword">for</span> kubeadm itself renewedcertificate <span class="hljs-keyword">for</span> serving the Kubernetes API renewedcertificate <span class="hljs-keyword">for</span> the API server to connect to kubelet renewedcertificate embedded <span class="hljs-keyword">in</span> the kubeconfig file <span class="hljs-keyword">for</span> the controller manager to use renewedcertificate <span class="hljs-keyword">for</span> the front proxy client renewedcertificate embedded <span class="hljs-keyword">in</span> the kubeconfig file <span class="hljs-keyword">for</span> the scheduler manager to use renewed</code></pre></div><h3 id="6-4、Node-重加入"><a href="#6-4、Node-重加入" class="headerlink" title="6.4、Node 重加入"></a>6.4、Node 重加入</h3><p>默认的 bootstrap token 会在 24h 后失效，所以后续增加新节点需要重新创建 token，重新创建 token 可以通过以下命令完成</p><div class="hljs"><pre><code class="hljs sh"><span class="hljs-comment"># 列出 token</span>k1.node ➜ kubeadm token listTOKEN                     TTL         EXPIRES                     USAGES                   DESCRIPTION                                                EXTRA GROUPSr4t3l3.14mmuivm7xbtaeoj   22h         2020-01-13T18:06:54+08:00   authentication,signing   &lt;none&gt;                                                     system:bootstrappers:kubeadm:default-node-tokenzady4i.57f9i2o6zl9vf9hy   45m         2020-01-12T20:06:53+08:00   &lt;none&gt;                   Proxy <span class="hljs-keyword">for</span> managing TTL <span class="hljs-keyword">for</span> the kubeadm-certs secret        &lt;none&gt;<span class="hljs-comment"># 创建新 token</span>k1.node ➜ kubeadm token create --<span class="hljs-built_in">print</span>-join-commandW0112 19:21:15.174765   26626 validation.go:28] Cannot validate kube-proxy config - no validator is availableW0112 19:21:15.174836   26626 validation.go:28] Cannot validate kubelet config - no validator is availablekubeadm join 127.0.0.1:6443 --token 2dz4dc.mobzgjbvu0bkxz7j     --discovery-token-ca-cert-hash sha256:06f49f1f29d08b797fbf04d87b9b0fd6095a4693e9b1d59c429745cfa082b31d</code></pre></div><p>如果忘记了 certificate-key 可以通过一下命令重新 upload 并查看</p><div class="hljs"><pre><code class="hljs sh">k1.node ➜ kubeadm init --config kubeadm.yaml phase upload-certs --upload-certsW0112 19:23:06.466711   28637 validation.go:28] Cannot validate kubelet config - no validator is availableW0112 19:23:06.466778   28637 validation.go:28] Cannot validate kube-proxy config - no validator is available[upload-certs] Storing the certificates <span class="hljs-keyword">in</span> Secret <span class="hljs-string">"kubeadm-certs"</span> <span class="hljs-keyword">in</span> the <span class="hljs-string">"kube-system"</span> Namespace[upload-certs] Using certificate key:7373f829c733b46fb78f0069f90185e0f00254381641d8d5a7c5984b2cf17cd3</code></pre></div><h3 id="6-5、调整-kubelet"><a href="#6-5、调整-kubelet" class="headerlink" title="6.5、调整 kubelet"></a>6.5、调整 kubelet</h3><p>node 节点一旦启动完成后，kubelet 配置便不可再修改；如果想要修改 kubelet 配置，可以通过调整 <code>/etc/systemd/system/kubelet.service.d/10-kubeadm.conf</code> 配置文件完成</p><h2 id="七、其他"><a href="#七、其他" class="headerlink" title="七、其他"></a>七、其他</h2><p>本文参考了许多官方文档，以下是一些个人认为比较有价值并且在使用 kubeadm 后应该阅读的文档</p><ul><li><a href="https://kubernetes.io/docs/reference/setup-tools/kubeadm/implementation-details" target="_blank" rel="noopener">Implementation details</a></li><li><a href="https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/kubelet-integration/" target="_blank" rel="noopener">Configuring each kubelet in your cluster using kubeadm</a></li><li><a href="https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/control-plane-flags/" target="_blank" rel="noopener">Customizing control plane configuration with kubeadm</a></li><li><a href="https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/high-availability/" target="_blank" rel="noopener">Creating Highly Available clusters with kubeadm</a></li><li><a href="https://kubernetes.io/docs/tasks/administer-cluster/kubeadm/kubeadm-certs/" target="_blank" rel="noopener">Certificate Management with kubeadm</a></li><li><a href="https://kubernetes.io/docs/tasks/administer-cluster/kubeadm/kubeadm-upgrade/" target="_blank" rel="noopener">Upgrading kubeadm clusters</a></li><li><a href="https://kubernetes.io/docs/tasks/administer-cluster/reconfigure-kubelet/" target="_blank" rel="noopener">Reconfigure a Node’s Kubelet in a Live Cluster</a></li></ul>]]></content:encoded>
      
      
      <category domain="https://mritd.com/categories/kubernetes/">Kubernetes</category>
      
      
      <category domain="https://mritd.com/tags/kubernetes/">Kubernetes</category>
      
      
      <comments>https://mritd.com/2020/01/21/set-up-kubernetes-ha-cluster-by-kubeadm/#disqus_thread</comments>
      
    </item>
    
    <item>
      <title>云服务器下 Ubuntu 18 正确的 DNS 修改</title>
      <link>https://mritd.com/2020/01/21/how-to-modify-dns-on-ubuntu18-server/</link>
      <guid>https://mritd.com/2020/01/21/how-to-modify-dns-on-ubuntu18-server/</guid>
      <pubDate>Tue, 21 Jan 2020 04:35:46 GMT</pubDate>
      
      <description>博客服务器换成了阿里云香港，个人还偶尔看美剧，所以做了一下 Netflix 分流；分流过程主要是做 DNS 解析 SNI 代理，调了半天记录一下</description>
      
      
      
      <content:encoded><![CDATA[<h2 id="一、起因"><a href="#一、起因" class="headerlink" title="一、起因"></a>一、起因</h2><p>Netflix DNS 分流实际上我目前的方案是通过 CoreDNS 作为主 DNS Server，然后在 CoreDNS 上针对 Netflix 全部域名解析 forward 到一台国外可以解锁 Netflix 机器上；如果直接将 CoreDNS 暴露在公网，那么无疑是在作死，为 DNS 反射 DDos 提供肉鸡；所以想到的方案是自己编写一个不可描述的工具，本地 Client 到 Server 端以后，Server 端再去设置到 CoreDNS 做分流；其中不可避免的需要调整 Server 端默认 DNS。</p><h2 id="二、已废弃修改方式"><a href="#二、已废弃修改方式" class="headerlink" title="二、已废弃修改方式"></a>二、已废弃修改方式</h2><p>目前大部份人还是习惯修改 <code>/etc/resolv.conf</code> 配置文件，这个配置文件上面已经明确标注了不要去修改它；<strong>因为自 Systemd 一统江山以后，系统 DNS 已经被 <code>systemd-resolved</code> 服务接管；一但修改了 <code>/etc/resolv.conf</code>，机器重启后就会被恢复；</strong>所以根源解决方案还是需要修改 <code>systemd-resolved</code> 的配置。</p><h2 id="三、netplan-的调整"><a href="#三、netplan-的调整" class="headerlink" title="三、netplan 的调整"></a>三、netplan 的调整</h2><p>在调整完 <code>systemd-resolved</code> 配置后其实有些地方仍然是不生效的；<strong>原因是 Ubuntu 18 开始网络已经被 netplan 接管，所以问题又回到了如何修改 netplan；</strong>由于云服务器初始化全部是由 cloud-init 完成的，netplan 配置里 IP 全部是由 DHCP 完成；那么直接修改 netplan 为 static IP 理论上可行，但是事实上还是不够优雅；后来研究了一下其实更优雅的方式是覆盖掉 DHCP 的某些配置，比如 DNS 配置；在阿里云上配置如下(<code>/etc/netplan/99-netcfg.yaml</code>)</p><div class="hljs"><pre><code class="hljs yaml"><span class="hljs-attr">network:</span>  <span class="hljs-attr">version:</span> <span class="hljs-number">2</span>  <span class="hljs-attr">renderer:</span> <span class="hljs-string">networkd</span>  <span class="hljs-attr">ethernets:</span>    <span class="hljs-attr">eth0:</span>      <span class="hljs-attr">dhcp4:</span> <span class="hljs-literal">yes</span>      <span class="hljs-attr">dhcp4-overrides:</span>        <span class="hljs-attr">use-dns:</span> <span class="hljs-literal">no</span>      <span class="hljs-attr">dhcp6:</span> <span class="hljs-literal">no</span>      <span class="hljs-attr">nameservers:</span>        <span class="hljs-attr">search:</span> <span class="hljs-string">[local,node]</span>        <span class="hljs-comment"># 我自己的 CoreDNS 服务器</span>        <span class="hljs-attr">addresses:</span> <span class="hljs-string">[172.17.3.17]</span></code></pre></div><p>修改完成后执行 <code>netplan try</code> 等待几秒钟，如果屏幕的读秒倒计时一直在动，说明修改没问题，接着回车既可(尽量不要 <code>netplan apply</code>，一旦修改错误你就再也连不上了…)</p><h2 id="四、DNS-分流"><a href="#四、DNS-分流" class="headerlink" title="四、DNS 分流"></a>四、DNS 分流</h2><p>顺便贴一下 CoreDNS 配置吧，可能有些人也需要；第一部分的域名是目前我整理的 Netflix 全部访问域名，针对这些域名的流量转发到自己其他可解锁 Netflix 的机器既可</p><div class="hljs"><pre><code class="hljs sh">netflix.com nflxext.com nflximg.net nflxso.net nflxvideo.net &#123;    <span class="hljs-built_in">bind</span> 172.17.3.17    cache 30 . &#123;        success 4096    &#125;    forward . 158.1.1.1 &#123;        max_fails 2        prefer_udp        expire 20s        policy random        health_check 0.2s    &#125;    errors    <span class="hljs-built_in">log</span> . <span class="hljs-string">"&#123;remote&#125;:&#123;port&#125; - &#123;&gt;id&#125; \"&#123;type&#125; &#123;class&#125; &#123;name&#125; &#123;proto&#125; &#123;size&#125; &#123;&gt;do&#125; &#123;&gt;bufsize&#125;\" &#123;rcode&#125; &#123;&gt;rflags&#125; &#123;rsize&#125; &#123;duration&#125;"</span>&#125;.:53 &#123;    <span class="hljs-built_in">bind</span> 172.17.3.17    cache 30 . &#123;        success 4096    &#125;    forward . 8.8.8.8 1.1.1.1 &#123;        except netflix.com nflxext.com nflximg.net nflxso.net nflxvideo.net        max_fails 2        expire 20s        policy random        health_check 0.2s    &#125;    errors    <span class="hljs-built_in">log</span> . <span class="hljs-string">"&#123;remote&#125;:&#123;port&#125; - &#123;&gt;id&#125; \"&#123;type&#125; &#123;class&#125; &#123;name&#125; &#123;proto&#125; &#123;size&#125; &#123;&gt;do&#125; &#123;&gt;bufsize&#125;\" &#123;rcode&#125; &#123;&gt;rflags&#125; &#123;rsize&#125; &#123;duration&#125;"</span></code></pre></div><h2 id="五、关于-docker"><a href="#五、关于-docker" class="headerlink" title="五、关于 docker"></a>五、关于 docker</h2><p>当 netplan 修改完成后，只需要重启 docker 既可保证 docker 内所有容器 DNS 请求全部发送到自己定义的 DNS 服务器上；<strong>请不要尝试将自己的 CoreDNS 监听到 <code>127.*</code> 或者 <code>::1</code> 上，这两个地址会导致 docker 中的 DNS 无效</strong>，因为在 <a href="https://github.com/docker/libnetwork/blob/fec6476dfa21380bf8ee4d74048515d968c1ee63/resolvconf/resolvconf.go#L148" target="_blank" rel="noopener">libnetwork</a> 中针对这两个地址做了过滤，并且 <code>FilterResolvDNS</code> 方法在剔除这两种地址时不会给予任何警告日志</p>]]></content:encoded>
      
      
      <category domain="https://mritd.com/categories/linux/">Linux</category>
      
      
      <category domain="https://mritd.com/tags/linux/">Linux</category>
      
      
      <comments>https://mritd.com/2020/01/21/how-to-modify-dns-on-ubuntu18-server/#disqus_thread</comments>
      
    </item>
    
    <item>
      <title>Percona MySQL 搭建</title>
      <link>https://mritd.com/2020/01/20/set-up-percona-server/</link>
      <guid>https://mritd.com/2020/01/20/set-up-percona-server/</guid>
      <pubDate>Mon, 20 Jan 2020 11:45:46 GMT</pubDate>
      
      <description>最近被拉去折腾 MySQL 了，Kuberntes 相关的文章停更了好久... MySQL 折腾完了顺便记录一下折腾过程，值得注意的是本篇文章从实际生产环境文档中摘录，部分日志和数据库敏感信息已被胡乱替换，所以不要盲目复制粘贴。</description>
      
      
      
      <content:encoded><![CDATA[<h2 id="一、版本信息"><a href="#一、版本信息" class="headerlink" title="一、版本信息"></a>一、版本信息</h2><p>目前采用 MySQL fork 版本 Percona Server 5.7.28，监控方面选择 Percona Monitoring and Management 2.1.0，对应监控 Client 版本为 2.1.0</p><h2 id="二、Percona-Server-安装"><a href="#二、Percona-Server-安装" class="headerlink" title="二、Percona Server 安装"></a>二、Percona Server 安装</h2><p>为保证兼容以及稳定性，MySQL 宿主机系统选择 CentOS 7，Percona Server 安装方式为 rpm 包，安装后由 Systemd 守护</p><h3 id="2-1、下载安装包"><a href="#2-1、下载安装包" class="headerlink" title="2.1、下载安装包"></a>2.1、下载安装包</h3><p>安装包下载地址为 <a href="https://www.percona.com/downloads/Percona-Server-5.7/LATEST/" target="_blank" rel="noopener">https://www.percona.com/downloads/Percona-Server-5.7/LATEST/</a>，下载时选择 <code>Download All Packages Together</code>，下载后是所有组件全量的压缩 tar 包。</p><h3 id="2-2、安装前准备"><a href="#2-2、安装前准备" class="headerlink" title="2.2、安装前准备"></a>2.2、安装前准备</h3><p>针对 CentOS 7 系统，安装前升级所有系统组件库，执行 <code>yum update</code> 既可；大部份 <strong>CentOS 7 安装后可能会附带 <code>mariadb-libs</code> 包，这个包会默认创建一些配置文件，导致后面的 Percona Server 无法覆盖它(例如 <code>/etc/my.cnf</code>)，所以安装 Percona Server 之前需要卸载它 <code>yum remove mariadb-libs</code></strong></p><p>针对于数据存储硬盘，目前统一为 SSD 硬盘，挂载点为 <code>/data</code>，挂载方式可以采用 <code>fstab</code>、<code>systemd-mount</code>，分区格式目前采用 <code>xfs</code> 格式。</p><p><strong>SSD 优化有待补充…</strong></p><h3 id="2-3、安装-Percona-Server"><a href="#2-3、安装-Percona-Server" class="headerlink" title="2.3、安装 Percona Server"></a>2.3、安装 Percona Server</h3><p>Percona Server tar 包解压后会有 9 个 rpm 包，实际安装时只需要安装其中 4 个既可</p><div class="hljs"><pre><code class="hljs sh">yum install Percona-Server-client-57-5.7.28-31.1.el7.x86_64.rpm Percona-Server-server-57-5.7.28-31.1.el7.x86_64.rpm Percona-Server-shared-57-5.7.28-31.1.el7.x86_64.rpm Percona-Server-shared-compat-57-5.7.28-31.1.el7.x86_64.rpm</code></pre></div><h3 id="2-4、安装后调整"><a href="#2-4、安装后调整" class="headerlink" title="2.4、安装后调整"></a>2.4、安装后调整</h3><h4 id="2-4-1、硬盘调整"><a href="#2-4-1、硬盘调整" class="headerlink" title="2.4.1、硬盘调整"></a>2.4.1、硬盘调整</h4><p>目前 MySQL 数据会统一存放到 <code>/data</code> 目录下，所以需要将单独的数据盘挂载到 <code>/data</code> 目录；<strong>如果是 SSD 硬盘还需要调整系统 I/O 调度器等其他优化。</strong></p><h4 id="2-4-2、目录预创建"><a href="#2-4-2、目录预创建" class="headerlink" title="2.4.2、目录预创建"></a>2.4.2、目录预创建</h4><p>Percona Server 安装完成后，由于配置调整原因，还会用到一些其他的数据目录，这些目录可以预先创建并授权</p><div class="hljs"><pre><code class="hljs sh">mkdir -p /var/<span class="hljs-built_in">log</span>/mysql /data/mysql_tmpchown -R mysql:mysql /var/<span class="hljs-built_in">log</span>/mysql /data/mysql_tmp</code></pre></div><p><code>/var/log/mysql</code> 目录用来存放 MySQL 相关的日志(不包括 binlog)，<code>/data/mysql_tmp</code> 用来存放 MySQL 运行时产生的缓存文件。</p><h4 id="2-4-3、文件描述符调整"><a href="#2-4-3、文件描述符调整" class="headerlink" title="2.4.3、文件描述符调整"></a>2.4.3、文件描述符调整</h4><p>由于 rpm 安装的 Percona Server 会采用 Systemd 守护，所以如果想修改文件描述符配置应当调整 Systemd 配置文件</p><div class="hljs"><pre><code class="hljs sh">vim /usr/lib/systemd/system/mysqld.service<span class="hljs-comment"># Sets open_files_limit</span><span class="hljs-comment"># 注意 infinity = 65536</span>LimitCORE = infinityLimitNOFILE = infinityLimitNPROC = infinity</code></pre></div><p>然后执行 <code>systemctl daemon-reload</code> 重载既可。</p><h4 id="2-4-4、配置文件调整"><a href="#2-4-4、配置文件调整" class="headerlink" title="2.4.4、配置文件调整"></a>2.4.4、配置文件调整</h4><p>Percona Server 安装完成后也会生成 <code>/etc/my.cnf</code> 配置文件，不过不建议直接修改该文件；修改配置文件需要进入到 <code>/etc/percona-server.conf.d/</code> 目录调整相应配置；以下为配置样例(<strong>生产环境 mysqld 配置需要优化调整</strong>)</p><p><strong>mysql.cnf</strong></p><div class="hljs"><pre><code class="hljs ini"><span class="hljs-section">[mysql]</span>auto-rehash<span class="hljs-attr">default_character_set</span>=utf8mb4</code></pre></div><p><strong>mysqld.cnf</strong></p><div class="hljs"><pre><code class="hljs ini"><span class="hljs-comment"># Percona Server template configuration</span><span class="hljs-section">[mysqld]</span><span class="hljs-comment">#</span><span class="hljs-comment"># Remove leading # and set to the amount of RAM for the most important data</span><span class="hljs-comment"># cache in MySQL. Start at 70% of total RAM for dedicated server, else 10%.</span><span class="hljs-comment"># innodb_buffer_pool_size = 128M</span><span class="hljs-comment">#</span><span class="hljs-comment"># Remove leading # to turn on a very important data integrity option: logging</span><span class="hljs-comment"># changes to the binary log between backups.</span><span class="hljs-comment"># log_bin</span><span class="hljs-comment">#</span><span class="hljs-comment"># Remove leading # to set options mainly useful for reporting servers.</span><span class="hljs-comment"># The server defaults are faster for transactions and fast SELECTs.</span><span class="hljs-comment"># Adjust sizes as needed, experiment to find the optimal values.</span><span class="hljs-comment"># join_buffer_size = 128M</span><span class="hljs-comment"># sort_buffer_size = 2M</span><span class="hljs-comment"># read_rnd_buffer_size = 2M</span><span class="hljs-attr">port</span>=<span class="hljs-number">3306</span><span class="hljs-attr">datadir</span>=/data/mysql<span class="hljs-attr">socket</span>=/data/mysql/mysql.sock<span class="hljs-attr">pid_file</span>=/data/mysql/mysqld.pid<span class="hljs-comment"># 服务端编码</span><span class="hljs-attr">character_set_server</span>=utf8mb4<span class="hljs-comment"># 服务端排序</span><span class="hljs-attr">collation_server</span>=utf8mb4_general_ci<span class="hljs-comment"># 强制使用 utf8mb4 编码集，忽略客户端设置</span><span class="hljs-attr">skip_character_set_client_handshake</span>=<span class="hljs-number">1</span><span class="hljs-comment"># 日志输出到文件</span><span class="hljs-attr">log_output</span>=FILE<span class="hljs-comment"># 开启常规日志输出</span><span class="hljs-attr">general_log</span>=<span class="hljs-number">1</span><span class="hljs-comment"># 常规日志输出文件位置</span><span class="hljs-attr">general_log_file</span>=/var/log/mysql/mysqld.log<span class="hljs-comment"># 错误日志位置</span><span class="hljs-attr">log_error</span>=/var/log/mysql/mysqld-error.log<span class="hljs-comment"># 记录慢查询</span><span class="hljs-attr">slow_query_log</span>=<span class="hljs-number">1</span><span class="hljs-comment"># 慢查询时间(大于 1s 被视为慢查询)</span><span class="hljs-attr">long_query_time</span>=<span class="hljs-number">1</span><span class="hljs-comment"># 慢查询日志文件位置</span><span class="hljs-attr">slow_query_log_file</span>=/var/log/mysql/mysqld-slow.log<span class="hljs-comment"># 临时文件位置</span><span class="hljs-attr">tmpdir</span>=/data/mysql_tmp<span class="hljs-comment"># 线程池缓存(refs https://my.oschina.net/realfighter/blog/363853)</span><span class="hljs-attr">thread_cache_size</span>=<span class="hljs-number">30</span><span class="hljs-comment"># The number of open tables for all threads.(refs https://dev.mysql.com/doc/refman/5.7/en/server-system-variables.html#sysvar_table_open_cache)</span><span class="hljs-attr">table_open_cache</span>=<span class="hljs-number">16384</span><span class="hljs-comment"># 文件描述符(此处修改不生效，请修改 systemd service 配置) </span><span class="hljs-comment"># refs https://www.percona.com/blog/2017/10/12/open_files_limit-mystery/</span><span class="hljs-comment"># refs https://www.cnblogs.com/wxxjianchi/p/10370419.html</span><span class="hljs-comment">#open_files_limit=65535</span><span class="hljs-comment"># 表定义缓存(5.7 以后自动调整)</span><span class="hljs-comment"># refs https://dev.mysql.com/doc/refman/5.6/en/server-system-variables.html#sysvar_table_definition_cache</span><span class="hljs-comment"># refs http://mysql.taobao.org/monthly/2015/08/10/</span><span class="hljs-comment">#table_definition_cache=16384</span><span class="hljs-attr">sort_buffer_size</span>=<span class="hljs-number">1</span>M<span class="hljs-attr">join_buffer_size</span>=<span class="hljs-number">1</span>M<span class="hljs-comment"># MyiSAM 引擎专用(内部临时磁盘表可能会用)</span><span class="hljs-attr">read_buffer_size</span>=<span class="hljs-number">1</span>M<span class="hljs-attr">read_rnd_buffer_size</span>=<span class="hljs-number">1</span>M<span class="hljs-comment"># MyiSAM 引擎专用(内部临时磁盘表可能会用)</span><span class="hljs-attr">key_buffer_size</span>=<span class="hljs-number">32</span>M<span class="hljs-comment"># MyiSAM 引擎专用(内部临时磁盘表可能会用)</span><span class="hljs-attr">bulk_insert_buffer_size</span>=<span class="hljs-number">16</span>M<span class="hljs-comment"># myisam_sort_buffer_size 与 sort_buffer_size 区别请参考(https://stackoverflow.com/questions/7871027/myisam-sort-buffer-size-vs-sort-buffer-size)</span><span class="hljs-attr">myisam_sort_buffer_size</span>=<span class="hljs-number">64</span>M<span class="hljs-comment"># 内部内存临时表大小</span><span class="hljs-attr">tmp_table_size</span>=<span class="hljs-number">32</span>M<span class="hljs-comment"># 用户创建的 MEMORY 表最大大小(tmp_table_size 受此值影响)</span><span class="hljs-attr">max_heap_table_size</span>=<span class="hljs-number">32</span>M<span class="hljs-comment"># 开启查询缓存</span><span class="hljs-attr">query_cache_type</span>=<span class="hljs-number">1</span><span class="hljs-comment"># 查询缓存大小</span><span class="hljs-attr">query_cache_size</span>=<span class="hljs-number">32</span>M<span class="hljs-comment"># sql mode</span><span class="hljs-attr">sql_mode</span>=<span class="hljs-string">'STRICT_TRANS_TABLES,NO_ZERO_IN_DATE,NO_ZERO_DATE,ERROR_FOR_DIVISION_BY_ZERO,NO_AUTO_CREATE_USER,NO_ENGINE_SUBSTITUTION'</span><span class="hljs-comment">########### Network ###########</span><span class="hljs-comment"># 最大连接数(该参数受到最大文件描述符影响，如果不生效请检查最大文件描述符设置)</span><span class="hljs-comment"># refs https://stackoverflow.com/questions/39976756/the-max-connections-in-mysql-5-7</span><span class="hljs-attr">max_connections</span>=<span class="hljs-number">1500</span><span class="hljs-comment"># mysql 堆栈内暂存的链接数量</span><span class="hljs-comment"># 当短时间内链接数量超过 max_connections 时，部分链接会存储在堆栈内，存储数量受此参数控制</span><span class="hljs-attr">back_log</span>=<span class="hljs-number">256</span><span class="hljs-comment"># 最大链接错误，针对于 client 主机，超过此数量的链接错误将会导致 mysql server 针对此主机执行锁定(禁止链接 ERROR 1129 )</span><span class="hljs-comment"># 此错误计数仅在 mysql 链接握手失败才会计算，一般出现问题时都是网络故障</span><span class="hljs-comment"># refs https://www.cnblogs.com/kerrycode/p/8405862.html</span><span class="hljs-attr">max_connect_errors</span>=<span class="hljs-number">100000</span><span class="hljs-comment"># mysql server 允许的最大数据包大小</span><span class="hljs-attr">max_allowed_packet</span>=<span class="hljs-number">64</span>M<span class="hljs-comment"># 交互式客户端链接超时(30分钟自动断开)</span><span class="hljs-attr">interactive_timeout</span>=<span class="hljs-number">1800</span><span class="hljs-comment"># 非交互式链接超时时间(10分钟)</span><span class="hljs-comment"># 如果客户端有连接池，则需要协商此参数(refs https://database.51cto.com/art/201909/603519.htm)</span><span class="hljs-attr">wait_timeout</span>=<span class="hljs-number">600</span><span class="hljs-comment"># 跳过外部文件系统锁定</span><span class="hljs-comment"># If you run multiple servers that use the same database directory (not recommended), </span><span class="hljs-comment"># each server must have external locking enabled.</span><span class="hljs-comment"># refs https://dev.mysql.com/doc/refman/5.7/en/external-locking.html</span><span class="hljs-attr">skip_external_locking</span>=<span class="hljs-number">1</span><span class="hljs-comment"># 跳过链接的域名解析(开启此选项后 mysql 用户授权的 host 方式失效)</span><span class="hljs-attr">skip_name_resolve</span>=<span class="hljs-number">0</span><span class="hljs-comment"># 禁用主机名缓存，每次都会走 DNS</span><span class="hljs-attr">host_cache_size</span>=<span class="hljs-number">0</span><span class="hljs-comment">########### REPL ###########</span><span class="hljs-comment"># 开启 binlog</span><span class="hljs-attr">log_bin</span>=mysql-bin<span class="hljs-comment"># 作为从库时，同步信息依然写入 binlog，方便此从库再作为其他从库的主库</span><span class="hljs-attr">log_slave_updates</span>=<span class="hljs-number">1</span><span class="hljs-comment"># server id，默认为 ipv4 地址去除第一段</span><span class="hljs-comment"># eg: 172.16.10.11 =&gt; 161011</span><span class="hljs-attr">server_id</span>=<span class="hljs-number">161011</span><span class="hljs-comment"># 每次次事务 binlog 刷新到磁盘</span><span class="hljs-comment"># refs http://liyangliang.me/posts/2014/03/innodb_flush_log_at_trx_commit-and-sync_binlog/</span><span class="hljs-attr">sync_binlog</span>=<span class="hljs-number">100</span><span class="hljs-comment"># binlog 格式(refs https://zhuanlan.zhihu.com/p/33504555)</span><span class="hljs-attr">binlog_format</span>=row<span class="hljs-comment"># binlog 自动清理时间</span><span class="hljs-attr">expire_logs_days</span>=<span class="hljs-number">10</span><span class="hljs-comment"># 开启 relay-log，一般作为 slave 时开启</span><span class="hljs-attr">relay_log</span>=mysql-replay<span class="hljs-comment"># 主从复制时跳过 test 库</span><span class="hljs-attr">replicate_ignore_db</span>=test<span class="hljs-comment"># 每个 session binlog 缓存</span><span class="hljs-attr">binlog_cache_size</span>=<span class="hljs-number">4</span>M<span class="hljs-comment"># binlog 滚动大小</span><span class="hljs-attr">max_binlog_size</span>=<span class="hljs-number">1024</span>M<span class="hljs-comment"># GTID 相关(refs https://keithlan.github.io/2016/06/23/gtid/)</span><span class="hljs-comment">#gtid_mode=1</span><span class="hljs-comment">#enforce_gtid_consistency=1</span><span class="hljs-comment">########### InnoDB ###########</span><span class="hljs-comment"># 永久表默认存储引擎</span><span class="hljs-attr">default_storage_engine</span>=InnoDB<span class="hljs-comment"># 系统表空间数据文件大小(初始化为 1G，并且自动增长)</span><span class="hljs-attr">innodb_data_file_path</span>=ibdata1:<span class="hljs-number">1</span>G:autoextend<span class="hljs-comment"># InnoDB 缓存池大小</span><span class="hljs-comment"># innodb_buffer_pool_size 必须等于 innodb_buffer_pool_chunk_size*innodb_buffer_pool_instances，或者是其整数倍</span><span class="hljs-comment"># refs https://dev.mysql.com/doc/refman/5.7/en/innodb-buffer-pool-resize.html</span><span class="hljs-comment"># refs https://zhuanlan.zhihu.com/p/60089484</span><span class="hljs-attr">innodb_buffer_pool_size</span>=<span class="hljs-number">7680</span>M<span class="hljs-attr">innodb_buffer_pool_instances</span>=<span class="hljs-number">10</span><span class="hljs-attr">innodb_buffer_pool_chunk_size</span>=<span class="hljs-number">128</span>M<span class="hljs-comment"># InnoDB 强制恢复(refs https://www.askmaclean.com/archives/mysql-innodb-innodb_force_recovery.html)</span><span class="hljs-attr">innodb_force_recovery</span>=<span class="hljs-number">0</span><span class="hljs-comment"># InnoDB buffer 预热(refs http://www.dbhelp.net/2017/01/12/mysql-innodb-buffer-pool-warmup.html)</span><span class="hljs-attr">innodb_buffer_pool_dump_at_shutdown</span>=<span class="hljs-number">1</span><span class="hljs-attr">innodb_buffer_pool_load_at_startup</span>=<span class="hljs-number">1</span><span class="hljs-comment"># InnoDB 日志组中的日志文件数</span><span class="hljs-attr">innodb_log_files_in_group</span>=<span class="hljs-number">2</span><span class="hljs-comment"># InnoDB redo 日志大小</span><span class="hljs-comment"># refs https://www.percona.com/blog/2017/10/18/chose-mysql-innodb_log_file_size/</span><span class="hljs-attr">innodb_log_file_size</span>=<span class="hljs-number">256</span>MB<span class="hljs-comment"># 缓存还未提交的事务的缓冲区大小</span><span class="hljs-attr">innodb_log_buffer_size</span>=<span class="hljs-number">16</span>M<span class="hljs-comment"># InnoDB 在事务提交后的日志写入频率</span><span class="hljs-comment"># refs http://liyangliang.me/posts/2014/03/innodb_flush_log_at_trx_commit-and-sync_binlog/</span><span class="hljs-attr">innodb_flush_log_at_trx_commit</span>=<span class="hljs-number">2</span><span class="hljs-comment"># InnoDB DML 操作行级锁等待时间</span><span class="hljs-comment"># 超时返回 ERROR 1205 (HY000): Lock wait timeout exceeded; try restarting transaction</span><span class="hljs-comment"># refs https://ningyu1.github.io/site/post/75-mysql-lock-wait-timeout-exceeded/</span><span class="hljs-attr">innodb_lock_wait_timeout</span>=<span class="hljs-number">30</span><span class="hljs-comment"># InnoDB 行级锁超时是否回滚整个事务，默认为 OFF 仅回滚上一条语句</span><span class="hljs-comment"># 此时应用程序可以接受到错误后选择是否继续提交事务(并没有违反 ACID 原子性)</span><span class="hljs-comment"># refs https://www.cnblogs.com/hustcat/archive/2012/11/18/2775487.html</span><span class="hljs-comment">#innodb_rollback_on_timeout=ON</span><span class="hljs-comment"># InnoDB 数据写入磁盘的方式，具体见博客文章</span><span class="hljs-comment"># refs https://www.cnblogs.com/gomysql/p/3595806.html</span><span class="hljs-attr">innodb_flush_method</span>=O_DIRECT<span class="hljs-comment"># InnoDB 缓冲池脏页刷新百分比</span><span class="hljs-comment"># refs https://dbarobin.com/2015/08/29/mysql-optimization-under-ssd</span><span class="hljs-attr">innodb_max_dirty_pages_pct</span>=<span class="hljs-number">50</span><span class="hljs-comment"># InnoDB 每秒执行的写IO量</span><span class="hljs-comment"># refs https://www.centos.bz/2016/11/mysql-performance-tuning-15-config-item/#10.INNODB_IO_CAPACITY,%20INNODB_IO_CAPACITY_MAX</span><span class="hljs-attr">innodb_io_capacity</span>=<span class="hljs-number">500</span><span class="hljs-attr">innodb_io_capacity_max</span>=<span class="hljs-number">1000</span><span class="hljs-comment"># 请求并发 InnoDB 线程数</span><span class="hljs-comment"># refs https://www.cnblogs.com/xinysu/p/6439715.html#_lab2_1_0</span><span class="hljs-attr">innodb_thread_concurrency</span>=<span class="hljs-number">60</span><span class="hljs-comment"># 再使用多个 InnoDB 表空间时，允许打开的最大 ".ibd" 文件个数，不设置默认 300，</span><span class="hljs-comment"># 并且取与 table_open_cache 相比较大的一个，此选项独立于 open_files_limit</span><span class="hljs-comment"># refs https://dev.mysql.com/doc/refman/5.7/en/innodb-parameters.html#sysvar_innodb_open_files</span><span class="hljs-attr">innodb_open_files</span>=<span class="hljs-number">65535</span><span class="hljs-comment"># 每个 InnoDB 表都存储在独立的表空间(.ibd)中</span><span class="hljs-attr">innodb_file_per_table</span>=<span class="hljs-number">1</span><span class="hljs-comment"># 事务级别(可重复读，会出幻读)</span><span class="hljs-attr">transaction_isolation</span>=REPEATABLE-READ<span class="hljs-comment"># 是否在搜索和索引扫描中使用间隙锁(gap locking)，不建议使用未来将删除</span><span class="hljs-attr">innodb_locks_unsafe_for_binlog</span>=<span class="hljs-number">0</span><span class="hljs-comment"># InnoDB 后台清理线程数，更大的值有助于 DML 执行性能，&gt;= 5.7.8 默认为 4</span><span class="hljs-attr">innodb_purge_threads</span>=<span class="hljs-number">4</span></code></pre></div><p><strong>mysqld_safe.cnf</strong></p><div class="hljs"><pre><code class="hljs ini"><span class="hljs-comment">#</span><span class="hljs-comment"># The Percona Server 5.7 configuration file.</span><span class="hljs-comment">#</span><span class="hljs-comment"># One can use all long options that the program supports.</span><span class="hljs-comment"># Run program with --help to get a list of available options and with</span><span class="hljs-comment"># --print-defaults to see which it would actually understand and use.</span><span class="hljs-comment">#</span><span class="hljs-comment"># For explanations see</span><span class="hljs-comment"># http://dev.mysql.com/doc/mysql/en/server-system-variables.html</span><span class="hljs-section">[mysqld_safe]</span><span class="hljs-attr">pid-file</span> = /var/run/mysqld/mysqld.pid<span class="hljs-attr">socket</span>   = /var/run/mysqld/mysqld.sock<span class="hljs-attr">nice</span>     = <span class="hljs-number">0</span></code></pre></div><p><strong>mysqldump.cnf</strong></p><div class="hljs"><pre><code class="hljs ini"><span class="hljs-section">[mysqldump]</span>quick<span class="hljs-attr">default-character-set</span>=utf8mb4<span class="hljs-attr">max_allowed_packet</span>=<span class="hljs-number">256</span>M</code></pre></div><h3 id="2-5、启动"><a href="#2-5、启动" class="headerlink" title="2.5、启动"></a>2.5、启动</h3><p>配置文件调整完成后启动既可</p><div class="hljs"><pre><code class="hljs sh">systemctl start mysqld</code></pre></div><p>启动完成后默认 root 密码会自动生成，通过 <code>grep &#39;temporary password&#39; /var/log/mysql/*</code> 查看默认密码；获得默认密码后可以通过 <code>mysqladmin -S /data/mysql/mysql.sock -u root -p password</code> 修改 root 密码。</p><h2 id="三、Percona-Monitoring-and-Management"><a href="#三、Percona-Monitoring-and-Management" class="headerlink" title="三、Percona Monitoring and Management"></a>三、Percona Monitoring and Management</h2><p>数据库创建成功后需要增加 pmm 监控，后续将会通过监控信息来调优数据库，所以数据库监控必不可少。</p><h3 id="3-1、安装前准备"><a href="#3-1、安装前准备" class="headerlink" title="3.1、安装前准备"></a>3.1、安装前准备</h3><p>pmm 监控需要使用特定用户来监控数据信息，所以需要预先为 pmm 创建用户</p><div class="hljs"><pre><code class="hljs sql"><span class="hljs-keyword">USE</span> mysql;<span class="hljs-keyword">GRANT</span> <span class="hljs-keyword">ALL</span> <span class="hljs-keyword">PRIVILEGES</span> <span class="hljs-keyword">ON</span> *.* <span class="hljs-keyword">TO</span> <span class="hljs-string">'pmm'</span>@<span class="hljs-string">'%'</span> <span class="hljs-keyword">IDENTIFIED</span> <span class="hljs-keyword">BY</span> <span class="hljs-string">'pmm12345'</span> <span class="hljs-keyword">WITH</span> <span class="hljs-keyword">GRANT</span> <span class="hljs-keyword">OPTION</span>;<span class="hljs-keyword">FLUSH</span> <span class="hljs-keyword">PRIVILEGES</span>;</code></pre></div><h3 id="3-2、安装-PMM-Server"><a href="#3-2、安装-PMM-Server" class="headerlink" title="3.2、安装 PMM Server"></a>3.2、安装 PMM Server</h3><p>pmm server 端推荐直接使用 docker 启动，以下为样例 docker compose</p><div class="hljs"><pre><code class="hljs yaml"><span class="hljs-attr">version:</span> <span class="hljs-string">'3.7'</span><span class="hljs-attr">services:</span>  <span class="hljs-attr">pmm:</span>    <span class="hljs-attr">image:</span> <span class="hljs-string">percona/pmm-server:2.1.0</span>    <span class="hljs-attr">container_name:</span> <span class="hljs-string">pmm</span>    <span class="hljs-attr">restart:</span> <span class="hljs-string">always</span>    <span class="hljs-attr">volumes:</span>      <span class="hljs-bullet">-</span> <span class="hljs-string">data:/srv</span>    <span class="hljs-attr">ports:</span>      <span class="hljs-bullet">-</span> <span class="hljs-string">"80:80"</span>      <span class="hljs-bullet">-</span> <span class="hljs-string">"443:443"</span><span class="hljs-attr">volumes:</span>  <span class="hljs-attr">data:</span></code></pre></div><p><strong>如果想要自定义证书，请将证书复制到 volume 内的 nginx 目录下，自定义证书需要以下证书文件</strong></p><div class="hljs"><pre><code class="hljs sh">pmmserver.node ➜ tree.├── ca-certs.pem├── certificate.conf  <span class="hljs-comment"># 此文件是 pmm 默认生成自签证书的配置文件，不需要关注</span>├── certificate.crt├── certificate.key└── dhparam.pem</code></pre></div><p><strong>pmm server 启动后访问 <code>http(s)://IP_ADDRESS</code> 既可进入 granafa 面板，默认账户名和密码都是 <code>admin</code></strong></p><h3 id="3-3、安装-PMM-Client"><a href="#3-3、安装-PMM-Client" class="headerlink" title="3.3、安装 PMM Client"></a>3.3、安装 PMM Client</h3><p>PMM Client 同样采用 rpm 安装，下载地址 <a href="https://www.percona.com/downloads/pmm2/" target="_blank" rel="noopener">https://www.percona.com/downloads/pmm2/</a>，当前采用最新的 2.1.0 版本；rpm 下载完成后直接 <code>yum install</code> 既可。</p><p>rpm 安装完成后使用 <code>pmm-admin</code> 命令配置服务端地址，并添加当前 mysql 实例监控</p><div class="hljs"><pre><code class="hljs sh"><span class="hljs-comment"># 配置服务端地址</span>pmm-admin config --server-url https://admin:admin@pmm.mysql.node 172.16.0.11 generic mysql<span class="hljs-comment"># 配置当前 mysql 实例</span>pmm-admin add mysql --username=pmm --password=pmm12345 mysql 172.16.0.11:3306</code></pre></div><p>完成后稍等片刻既可在 pmm server 端的 granafa 中看到相关数据。</p><h2 id="四、数据导入"><a href="#四、数据导入" class="headerlink" title="四、数据导入"></a>四、数据导入</h2><p>从原始数据库 dump 相关库，并导入到新数据库既可</p><div class="hljs"><pre><code class="hljs sh"><span class="hljs-comment"># dump</span>mysqldump -h 172.16.1.10 -u root -p --master-data=2 --routines --triggers --single_transaction --databases DATABASE_NAME &gt; dump.sql<span class="hljs-comment"># load</span>mysql -S /data/mysql/mysql.sock -u root -p &lt; dump.sql</code></pre></div><p>数据导入后重建业务用户既可</p><div class="hljs"><pre><code class="hljs sql"><span class="hljs-keyword">USE</span> mysql;<span class="hljs-keyword">GRANT</span> <span class="hljs-keyword">ALL</span> <span class="hljs-keyword">PRIVILEGES</span> <span class="hljs-keyword">ON</span> *.* <span class="hljs-keyword">TO</span> <span class="hljs-string">'test_user'</span>@<span class="hljs-string">'%'</span> <span class="hljs-keyword">IDENTIFIED</span> <span class="hljs-keyword">BY</span> <span class="hljs-string">'test_user'</span> <span class="hljs-keyword">WITH</span> <span class="hljs-keyword">GRANT</span> <span class="hljs-keyword">OPTION</span>;<span class="hljs-keyword">FLUSH</span> <span class="hljs-keyword">PRIVILEGES</span>;</code></pre></div><h2 id="五、数据备份"><a href="#五、数据备份" class="headerlink" title="五、数据备份"></a>五、数据备份</h2><h3 id="5-1、安装-xtrabackup"><a href="#5-1、安装-xtrabackup" class="headerlink" title="5.1、安装 xtrabackup"></a>5.1、安装 xtrabackup</h3><p>目前数据备份采用 Perconra xtrabackup 工具，xtrabackup 可以实现高速、压缩带增量的备份；xtrabackup 安装同样采用 rpm 方式，下载地址为 <a href="https://www.percona.com/downloads/Percona-XtraBackup-2.4/LATEST/" target="_blank" rel="noopener">https://www.percona.com/downloads/Percona-XtraBackup-2.4/LATEST/</a>，下载完成后执行 <code>yum install</code> 既可</p><h3 id="5-2、备份工具"><a href="#5-2、备份工具" class="headerlink" title="5.2、备份工具"></a>5.2、备份工具</h3><p>目前备份工具开源在 <a href="https://github.com/gozap/mybak" target="_blank" rel="noopener">GitHub</a> 上，每次全量备份会写入 <code>.full-backup</code> 文件，增量备份会写入 <code>.inc-backup</code> 文件</p><h3 id="5-3、配置-systemd"><a href="#5-3、配置-systemd" class="headerlink" title="5.3、配置 systemd"></a>5.3、配置 systemd</h3><p>为了使备份自动运行，目前将定时任务配置到 systemd 中，由 systemd 调度并执行；以下为相关 systemd 配置文件</p><p><strong>mysql-backup-full.service</strong></p><div class="hljs"><pre><code class="hljs sh">[Unit]Description=mysql full backupAfter=network.target[Service]Type=simpleRestart=on-failureExecStart=/usr/<span class="hljs-built_in">local</span>/bin/mybak --backup-dir /data/mysql_backup --prefix mysql full[Install]WantedBy=multi-user.target</code></pre></div><p><strong>mysql-backup-inc.service</strong></p><div class="hljs"><pre><code class="hljs sh">[Unit]Description=mysql incremental backupAfter=network.target[Service]Type=simpleRestart=on-failureExecStart=/usr/<span class="hljs-built_in">local</span>/bin/mybak --backup-dir /data/mysql_backup --prefix mysql inc[Install]WantedBy=multi-user.target</code></pre></div><p><strong>mysql-backup-compress.service</strong></p><div class="hljs"><pre><code class="hljs sh">[Unit]Description=mysql backup compressAfter=network.target[Service]Type=simpleRestart=on-failureExecStart=/usr/<span class="hljs-built_in">local</span>/bin/mybak --backup-dir /data/mysql_backup --prefix mysql compress --clean[Install]WantedBy=multi-user.target</code></pre></div><p><strong>mysql-backup-full.timer</strong></p><div class="hljs"><pre><code class="hljs sh">[Unit]Description=mysql weekly full backup<span class="hljs-comment"># 备份之前依赖相关目录的挂载</span>After=data.mountAfter=data-mysql_backup.mount[Timer]<span class="hljs-comment"># 目前每周日一个全量备份</span>OnCalendar=Sun *-*-* 3:00Persistent=<span class="hljs-literal">true</span>[Install]WantedBy=timers.target</code></pre></div><p><strong>mysql-backup-inc.timer</strong></p><div class="hljs"><pre><code class="hljs sh">[Unit]Description=mysql weekly full backupAfter=data.mountAfter=data-mysql_backup.mount[Timer]<span class="hljs-comment"># 每天三个增量备份</span>OnCalendar=*-*-* 9:00OnCalendar=*-*-* 13:00OnCalendar=*-*-* 18:00Persistent=<span class="hljs-literal">true</span>[Install]WantedBy=timers.target</code></pre></div><p><strong>mysql-backup-compress.timer</strong></p><div class="hljs"><pre><code class="hljs sh">[Unit]Description=mysql weekly backup compress<span class="hljs-comment"># 备份之前依赖相关目录的挂载</span>After=data.mountAfter=data-mysql_backup.mount[Timer]<span class="hljs-comment"># 目前每周日一个全量备份，自动压缩后同时完成清理</span>OnCalendar=Sun *-*-* 5:00Persistent=<span class="hljs-literal">true</span>[Install]WantedBy=timers.target</code></pre></div><p>创建好相关文件后启动相关定时器既可</p><div class="hljs"><pre><code class="hljs sh">cp *.timer *.service /lib/systemd/systemsystemctl daemon-reloadsystemctl start mysql-backup-full.timer mysql-backup-inc.timer mysql-backup-compress.timersystemctl <span class="hljs-built_in">enable</span> mysql-backup-full.timer mysql-backup-inc.timer mysql-backup-compress.timer</code></pre></div><h2 id="六、数据恢复"><a href="#六、数据恢复" class="headerlink" title="六、数据恢复"></a>六、数据恢复</h2><h3 id="6-1、全量备份恢复"><a href="#6-1、全量备份恢复" class="headerlink" title="6.1、全量备份恢复"></a>6.1、全量备份恢复</h3><p>针对于全量备份，只需要按照官方文档的还原顺序进行还原既可</p><div class="hljs"><pre><code class="hljs sh"><span class="hljs-comment"># 由于备份时进行了压缩，所以先解压备份文件</span>xtrabackup --decompress --parallel 4 --target-dir /data/mysql_backup/mysql-20191205230502<span class="hljs-comment"># 执行预处理</span>xtrabackup --prepare --target-dir /data/mysql_backup/mysql-20191205230502<span class="hljs-comment"># 执行恢复(恢复时自动根据 my.cnf 将数据覆盖到 data 数据目录)</span>xtrabackup --copy-back --target-dir /data/mysql_backup/mysql-20191205230502<span class="hljs-comment"># 修复数据目录权限</span>chown -R mysql:mysql /data/mysql<span class="hljs-comment"># 启动 mysql</span>systemctl start mysqld</code></pre></div><h3 id="6-2、增量备份恢复"><a href="#6-2、增量备份恢复" class="headerlink" title="6.2、增量备份恢复"></a>6.2、增量备份恢复</h3><p>对于增量备份恢复，其与全量备份恢复的根本区别在于: <strong>对于非最后一个增量文件的预处理必须使用 <code>--apply-log-only</code> 选项防止运行回滚阶段的处理</strong></p><div class="hljs"><pre><code class="hljs sh"><span class="hljs-comment"># 对所有备份文件进行解压处理</span><span class="hljs-keyword">for</span> dir <span class="hljs-keyword">in</span> `ls`; <span class="hljs-keyword">do</span> xtrabackup --decompress --parallel 4 --target-dir <span class="hljs-variable">$dir</span>; <span class="hljs-keyword">done</span><span class="hljs-comment"># 对全量备份文件执行预处理(注意增加 --apply-log-only 选项)</span>xtrabackup --prepare --apply-log-only --target-dir /data/mysql_backup/mysql-20191205230502<span class="hljs-comment"># 对非最后一个增量备份执行预处理</span>xtrabackup --prepare --apply-log-only --target-dir /data/mysql_backup/mysql-20191205230502 --incremental-dir /data/mysql_backup/mysql-inc-20191206230802<span class="hljs-comment"># 对最后一个增量备份执行预处理(不需要 --apply-log-only)</span>xtrabackup --prepare --target-dir /data/mysql_backup/mysql-20191205230502 --incremental-dir /data/mysql_backup/mysql-inc-20191207031005<span class="hljs-comment"># 执行恢复(恢复时自动根据 my.cnf 将数据覆盖到 data 数据目录)</span>xtrabackup --copy-back --target-dir /data/mysql_backup/mysql-20191205230502<span class="hljs-comment"># 修复数据目录权限</span>chown -R mysql:mysql /data/mysql<span class="hljs-comment"># 启动 mysql</span>systemctl start mysqld</code></pre></div><h3 id="6-3、创建-slave"><a href="#6-3、创建-slave" class="headerlink" title="6.3、创建 slave"></a>6.3、创建 slave</h3><p>针对 xtrabackup 备份的数据可以直接恢复成 slave 节点，具体步骤如下:</p><p>首先将备份文件复制到目标机器，然后执行解压(默认备份工具采用 lz4 压缩)</p><div class="hljs"><pre><code class="hljs sh">xtrabackup --decompress --target-dir=xxxxxx</code></pre></div><p>解压完成后执行预处理操作(<strong>在执行预处理之前请确保 slave 机器上相关配置文件与 master 相同，并且处理好数据目录存放等</strong>)</p><div class="hljs"><pre><code class="hljs sh">xtrabackup --user=root --password=xxxxxxx --prepare --target-dir=xxxx</code></pre></div><p>预处理成功后便可执行恢复，以下命令将自动读取 <code>my.cnf</code> 配置，自动识别数据目录位置并将数据文件移动到该位置</p><div class="hljs"><pre><code class="hljs sh">xtrabackup --move-back --target-dir=xxxxx</code></pre></div><p>所由准备就绪后需要进行权限修复</p><div class="hljs"><pre><code class="hljs sh">chown -R mysql:mysql MYSQL_DATA_DIR</code></pre></div><p>最后在 mysql 内启动 slave 既可，slave 信息可通过从数据备份目录的 <code>xtrabackup_binlog_info</code> 中获取</p><div class="hljs"><pre><code class="hljs sh"><span class="hljs-comment"># 获取备份 POS 信息</span>cat xxxxxx/xtrabackup_binlog_info<span class="hljs-comment"># 创建 slave 节点</span>CHANGE MASTER TO    MASTER_HOST=<span class="hljs-string">'192.168.2.48'</span>,    MASTER_USER=<span class="hljs-string">'repl'</span>,    MASTER_PASSWORD=<span class="hljs-string">'xxxxxxx'</span>,    MASTER_LOG_FILE=<span class="hljs-string">'mysql-bin.000005'</span>,    MASTER_LOG_POS=52500595;<span class="hljs-comment"># 启动 slave</span>start slave;show slave status \G;</code></pre></div><h2 id="七、生产处理"><a href="#七、生产处理" class="headerlink" title="七、生产处理"></a>七、生产处理</h2><h3 id="7-1、数据目录"><a href="#7-1、数据目录" class="headerlink" title="7.1、数据目录"></a>7.1、数据目录</h3><p>目前生产环境数据目录位置调整到 <code>/home/mysql</code>，所以目录权限处理也要做对应调整</p><div class="hljs"><pre><code class="hljs sh">mkdir -p /var/<span class="hljs-built_in">log</span>/mysql /home/mysql_tmpchown -R mysql:mysql /var/<span class="hljs-built_in">log</span>/mysql /home/mysql_tmp</code></pre></div><h3 id="7-2、配置文件"><a href="#7-2、配置文件" class="headerlink" title="7.2、配置文件"></a>7.2、配置文件</h3><p>生产环境目前节点配置如下</p><ul><li>CPU: <code>Intel(R) Xeon(R) CPU E5-2620 v4 @ 2.10GHz</code></li><li>RAM: <code>128G</code></li></ul><p>所以配置文件也需要做相应的优化调整</p><p><strong>mysql.cnf</strong></p><div class="hljs"><pre><code class="hljs ini"><span class="hljs-section">[mysql]</span>auto-rehash<span class="hljs-attr">default_character_set</span>=utf8mb4</code></pre></div><p><strong>mysqld.cnf</strong></p><div class="hljs"><pre><code class="hljs ini"><span class="hljs-comment"># Percona Server template configuration</span><span class="hljs-section">[mysqld]</span><span class="hljs-comment">#</span><span class="hljs-comment"># Remove leading # and set to the amount of RAM for the most important data</span><span class="hljs-comment"># cache in MySQL. Start at 70% of total RAM for dedicated server, else 10%.</span><span class="hljs-comment"># innodb_buffer_pool_size = 128M</span><span class="hljs-comment">#</span><span class="hljs-comment"># Remove leading # to turn on a very important data integrity option: logging</span><span class="hljs-comment"># changes to the binary log between backups.</span><span class="hljs-comment"># log_bin</span><span class="hljs-comment">#</span><span class="hljs-comment"># Remove leading # to set options mainly useful for reporting servers.</span><span class="hljs-comment"># The server defaults are faster for transactions and fast SELECTs.</span><span class="hljs-comment"># Adjust sizes as needed, experiment to find the optimal values.</span><span class="hljs-comment"># join_buffer_size = 128M</span><span class="hljs-comment"># sort_buffer_size = 2M</span><span class="hljs-comment"># read_rnd_buffer_size = 2M</span><span class="hljs-attr">port</span>=<span class="hljs-number">3306</span><span class="hljs-attr">datadir</span>=/home/mysql/mysql<span class="hljs-attr">socket</span>=/home/mysql/mysql/mysql.sock<span class="hljs-attr">pid_file</span>=/home/mysql/mysql/mysqld.pid<span class="hljs-comment"># 服务端编码</span><span class="hljs-attr">character_set_server</span>=utf8mb4<span class="hljs-comment"># 服务端排序</span><span class="hljs-attr">collation_server</span>=utf8mb4_general_ci<span class="hljs-comment"># 强制使用 utf8mb4 编码集，忽略客户端设置</span><span class="hljs-attr">skip_character_set_client_handshake</span>=<span class="hljs-number">1</span><span class="hljs-comment"># 日志输出到文件</span><span class="hljs-attr">log_output</span>=FILE<span class="hljs-comment"># 开启常规日志输出</span><span class="hljs-attr">general_log</span>=<span class="hljs-number">1</span><span class="hljs-comment"># 常规日志输出文件位置</span><span class="hljs-attr">general_log_file</span>=/var/log/mysql/mysqld.log<span class="hljs-comment"># 错误日志位置</span><span class="hljs-attr">log_error</span>=/var/log/mysql/mysqld-error.log<span class="hljs-comment"># 记录慢查询</span><span class="hljs-attr">slow_query_log</span>=<span class="hljs-number">1</span><span class="hljs-comment"># 慢查询时间(大于 1s 被视为慢查询)</span><span class="hljs-attr">long_query_time</span>=<span class="hljs-number">1</span><span class="hljs-comment"># 慢查询日志文件位置</span><span class="hljs-attr">slow_query_log_file</span>=/var/log/mysql/mysqld-slow.log<span class="hljs-comment"># 临时文件位置</span><span class="hljs-attr">tmpdir</span>=/home/mysql/mysql_tmp<span class="hljs-comment"># The number of open tables for all threads.(refs https://dev.mysql.com/doc/refman/5.7/en/server-system-variables.html#sysvar_table_open_cache)</span><span class="hljs-attr">table_open_cache</span>=<span class="hljs-number">16384</span><span class="hljs-comment"># 文件描述符(此处修改不生效，请修改 systemd service 配置) </span><span class="hljs-comment"># refs https://www.percona.com/blog/2017/10/12/open_files_limit-mystery/</span><span class="hljs-comment"># refs https://www.cnblogs.com/wxxjianchi/p/10370419.html</span><span class="hljs-comment">#open_files_limit=65535</span><span class="hljs-comment"># 表定义缓存(5.7 以后自动调整)</span><span class="hljs-comment"># refs https://dev.mysql.com/doc/refman/5.6/en/server-system-variables.html#sysvar_table_definition_cache</span><span class="hljs-comment"># refs http://mysql.taobao.org/monthly/2015/08/10/</span><span class="hljs-comment">#table_definition_cache=16384</span><span class="hljs-attr">sort_buffer_size</span>=<span class="hljs-number">1</span>M<span class="hljs-attr">join_buffer_size</span>=<span class="hljs-number">1</span>M<span class="hljs-comment"># MyiSAM 引擎专用(内部临时磁盘表可能会用)</span><span class="hljs-attr">read_buffer_size</span>=<span class="hljs-number">1</span>M<span class="hljs-attr">read_rnd_buffer_size</span>=<span class="hljs-number">1</span>M<span class="hljs-comment"># MyiSAM 引擎专用(内部临时磁盘表可能会用)</span><span class="hljs-attr">key_buffer_size</span>=<span class="hljs-number">32</span>M<span class="hljs-comment"># MyiSAM 引擎专用(内部临时磁盘表可能会用)</span><span class="hljs-attr">bulk_insert_buffer_size</span>=<span class="hljs-number">16</span>M<span class="hljs-comment"># myisam_sort_buffer_size 与 sort_buffer_size 区别请参考(https://stackoverflow.com/questions/7871027/myisam-sort-buffer-size-vs-sort-buffer-size)</span><span class="hljs-attr">myisam_sort_buffer_size</span>=<span class="hljs-number">64</span>M<span class="hljs-comment"># 内部内存临时表大小</span><span class="hljs-attr">tmp_table_size</span>=<span class="hljs-number">32</span>M<span class="hljs-comment"># 用户创建的 MEMORY 表最大大小(tmp_table_size 受此值影响)</span><span class="hljs-attr">max_heap_table_size</span>=<span class="hljs-number">32</span>M<span class="hljs-comment"># 开启查询缓存</span><span class="hljs-attr">query_cache_type</span>=<span class="hljs-number">1</span><span class="hljs-comment"># 查询缓存大小</span><span class="hljs-attr">query_cache_size</span>=<span class="hljs-number">32</span>M<span class="hljs-comment"># sql mode</span><span class="hljs-attr">sql_mode</span>=<span class="hljs-string">'STRICT_TRANS_TABLES,NO_ZERO_IN_DATE,NO_ZERO_DATE,ERROR_FOR_DIVISION_BY_ZERO,NO_AUTO_CREATE_USER,NO_ENGINE_SUBSTITUTION'</span><span class="hljs-comment">########### Network ###########</span><span class="hljs-comment"># 最大连接数(该参数受到最大文件描述符影响，如果不生效请检查最大文件描述符设置)</span><span class="hljs-comment"># refs https://stackoverflow.com/questions/39976756/the-max-connections-in-mysql-5-7</span><span class="hljs-attr">max_connections</span>=<span class="hljs-number">1500</span><span class="hljs-comment"># mysql 堆栈内暂存的链接数量</span><span class="hljs-comment"># 当短时间内链接数量超过 max_connections 时，部分链接会存储在堆栈内，存储数量受此参数控制</span><span class="hljs-attr">back_log</span>=<span class="hljs-number">256</span><span class="hljs-comment"># 最大链接错误，针对于 client 主机，超过此数量的链接错误将会导致 mysql server 针对此主机执行锁定(禁止链接 ERROR 1129 )</span><span class="hljs-comment"># 此错误计数仅在 mysql 链接握手失败才会计算，一般出现问题时都是网络故障</span><span class="hljs-comment"># refs https://www.cnblogs.com/kerrycode/p/8405862.html</span><span class="hljs-attr">max_connect_errors</span>=<span class="hljs-number">100000</span><span class="hljs-comment"># mysql server 允许的最大数据包大小</span><span class="hljs-attr">max_allowed_packet</span>=<span class="hljs-number">64</span>M<span class="hljs-comment"># 交互式客户端链接超时(30分钟自动断开)</span><span class="hljs-attr">interactive_timeout</span>=<span class="hljs-number">1800</span><span class="hljs-comment"># 非交互式链接超时时间(10分钟)</span><span class="hljs-comment"># 如果客户端有连接池，则需要协商此参数(refs https://database.51cto.com/art/201909/603519.htm)</span><span class="hljs-attr">wait_timeout</span>=<span class="hljs-number">28800</span><span class="hljs-comment"># 跳过外部文件系统锁定</span><span class="hljs-comment"># If you run multiple servers that use the same database directory (not recommended), </span><span class="hljs-comment"># each server must have external locking enabled.</span><span class="hljs-comment"># refs https://dev.mysql.com/doc/refman/5.7/en/external-locking.html</span><span class="hljs-attr">skip_external_locking</span>=<span class="hljs-number">1</span><span class="hljs-comment"># 跳过链接的域名解析(开启此选项后 mysql 用户授权的 host 方式失效)</span><span class="hljs-attr">skip_name_resolve</span>=<span class="hljs-number">0</span><span class="hljs-comment"># 禁用主机名缓存，每次都会走 DNS</span><span class="hljs-attr">host_cache_size</span>=<span class="hljs-number">0</span><span class="hljs-comment">########### REPL ###########</span><span class="hljs-comment"># 开启 binlog</span><span class="hljs-attr">log_bin</span>=mysql-bin<span class="hljs-comment"># 作为从库时，同步信息依然写入 binlog，方便此从库再作为其他从库的主库</span><span class="hljs-attr">log_slave_updates</span>=<span class="hljs-number">1</span><span class="hljs-comment"># server id，默认为 ipv4 地址去除第一段</span><span class="hljs-comment"># eg: 192.168.2.48 =&gt; 168248</span><span class="hljs-attr">server_id</span>=<span class="hljs-number">168248</span><span class="hljs-comment"># 每 n 次事务 binlog 刷新到磁盘</span><span class="hljs-comment"># refs http://liyangliang.me/posts/2014/03/innodb_flush_log_at_trx_commit-and-sync_binlog/</span><span class="hljs-attr">sync_binlog</span>=<span class="hljs-number">100</span><span class="hljs-comment"># binlog 格式(refs https://zhuanlan.zhihu.com/p/33504555)</span><span class="hljs-attr">binlog_format</span>=row<span class="hljs-comment"># binlog 自动清理时间</span><span class="hljs-attr">expire_logs_days</span>=<span class="hljs-number">20</span><span class="hljs-comment"># 开启 relay-log，一般作为 slave 时开启</span><span class="hljs-attr">relay_log</span>=mysql-replay<span class="hljs-comment"># 主从复制时跳过 test 库</span><span class="hljs-attr">replicate_ignore_db</span>=test<span class="hljs-comment"># 每个 session binlog 缓存</span><span class="hljs-attr">binlog_cache_size</span>=<span class="hljs-number">4</span>M<span class="hljs-comment"># binlog 滚动大小</span><span class="hljs-attr">max_binlog_size</span>=<span class="hljs-number">1024</span>M<span class="hljs-comment"># GTID 相关(refs https://keithlan.github.io/2016/06/23/gtid/)</span><span class="hljs-comment">#gtid_mode=1</span><span class="hljs-comment">#enforce_gtid_consistency=1</span><span class="hljs-comment">########### InnoDB ###########</span><span class="hljs-comment"># 永久表默认存储引擎</span><span class="hljs-attr">default_storage_engine</span>=InnoDB<span class="hljs-comment"># 系统表空间数据文件大小(初始化为 1G，并且自动增长)</span><span class="hljs-attr">innodb_data_file_path</span>=ibdata1:<span class="hljs-number">1</span>G:autoextend<span class="hljs-comment"># InnoDB 缓存池大小(资源充足，为所欲为)</span><span class="hljs-comment"># innodb_buffer_pool_size 必须等于 innodb_buffer_pool_chunk_size*innodb_buffer_pool_instances，或者是其整数倍</span><span class="hljs-comment"># refs https://dev.mysql.com/doc/refman/5.7/en/innodb-buffer-pool-resize.html</span><span class="hljs-comment"># refs https://zhuanlan.zhihu.com/p/60089484</span><span class="hljs-attr">innodb_buffer_pool_size</span>=<span class="hljs-number">61440</span>M<span class="hljs-attr">innodb_buffer_pool_instances</span>=<span class="hljs-number">16</span><span class="hljs-comment"># 默认 128M</span><span class="hljs-attr">innodb_buffer_pool_chunk_size</span>=<span class="hljs-number">128</span>M<span class="hljs-comment"># InnoDB 强制恢复(refs https://www.askmaclean.com/archives/mysql-innodb-innodb_force_recovery.html)</span><span class="hljs-attr">innodb_force_recovery</span>=<span class="hljs-number">0</span><span class="hljs-comment"># InnoDB buffer 预热(refs http://www.dbhelp.net/2017/01/12/mysql-innodb-buffer-pool-warmup.html)</span><span class="hljs-attr">innodb_buffer_pool_dump_at_shutdown</span>=<span class="hljs-number">1</span><span class="hljs-attr">innodb_buffer_pool_load_at_startup</span>=<span class="hljs-number">1</span><span class="hljs-comment"># InnoDB 日志组中的日志文件数</span><span class="hljs-attr">innodb_log_files_in_group</span>=<span class="hljs-number">2</span><span class="hljs-comment"># InnoDB redo 日志大小</span><span class="hljs-comment"># refs https://www.percona.com/blog/2017/10/18/chose-mysql-innodb_log_file_size/</span><span class="hljs-attr">innodb_log_file_size</span>=<span class="hljs-number">256</span>MB<span class="hljs-comment"># 缓存还未提交的事务的缓冲区大小</span><span class="hljs-attr">innodb_log_buffer_size</span>=<span class="hljs-number">16</span>M<span class="hljs-comment"># InnoDB 在事务提交后的日志写入频率</span><span class="hljs-comment"># refs http://liyangliang.me/posts/2014/03/innodb_flush_log_at_trx_commit-and-sync_binlog/</span><span class="hljs-attr">innodb_flush_log_at_trx_commit</span>=<span class="hljs-number">2</span><span class="hljs-comment"># InnoDB DML 操作行级锁等待时间</span><span class="hljs-comment"># 超时返回 ERROR 1205 (HY000): Lock wait timeout exceeded; try restarting transaction</span><span class="hljs-comment"># refs https://ningyu1.github.io/site/post/75-mysql-lock-wait-timeout-exceeded/</span><span class="hljs-attr">innodb_lock_wait_timeout</span>=<span class="hljs-number">30</span><span class="hljs-comment"># InnoDB 行级锁超时是否回滚整个事务，默认为 OFF 仅回滚上一条语句</span><span class="hljs-comment"># 此时应用程序可以接受到错误后选择是否继续提交事务(并没有违反 ACID 原子性)</span><span class="hljs-comment"># refs https://www.cnblogs.com/hustcat/archive/2012/11/18/2775487.html</span><span class="hljs-comment">#innodb_rollback_on_timeout=ON</span><span class="hljs-comment"># InnoDB 数据写入磁盘的方式，具体见博客文章</span><span class="hljs-comment"># refs https://www.cnblogs.com/gomysql/p/3595806.html</span><span class="hljs-attr">innodb_flush_method</span>=O_DIRECT<span class="hljs-comment"># InnoDB 缓冲池脏页刷新百分比</span><span class="hljs-comment"># refs https://dbarobin.com/2015/08/29/mysql-optimization-under-ssd</span><span class="hljs-attr">innodb_max_dirty_pages_pct</span>=<span class="hljs-number">50</span><span class="hljs-comment"># InnoDB 每秒执行的写IO量</span><span class="hljs-comment"># refs https://www.centos.bz/2016/11/mysql-performance-tuning-15-config-item/#10.INNODB_IO_CAPACITY,%20INNODB_IO_CAPACITY_MAX</span><span class="hljs-comment"># refs https://www.alibabacloud.com/blog/testing-io-performance-with-sysbench_594709</span><span class="hljs-attr">innodb_io_capacity</span>=<span class="hljs-number">8000</span><span class="hljs-attr">innodb_io_capacity_max</span>=<span class="hljs-number">16000</span><span class="hljs-comment"># 请求并发 InnoDB 线程数</span><span class="hljs-comment"># refs https://www.cnblogs.com/xinysu/p/6439715.html#_lab2_1_0</span><span class="hljs-attr">innodb_thread_concurrency</span>=<span class="hljs-number">0</span><span class="hljs-comment"># 再使用多个 InnoDB 表空间时，允许打开的最大 ".ibd" 文件个数，不设置默认 300，</span><span class="hljs-comment"># 并且取与 table_open_cache 相比较大的一个，此选项独立于 open_files_limit</span><span class="hljs-comment"># refs https://dev.mysql.com/doc/refman/5.7/en/innodb-parameters.html#sysvar_innodb_open_files</span><span class="hljs-attr">innodb_open_files</span>=<span class="hljs-number">65535</span><span class="hljs-comment"># 每个 InnoDB 表都存储在独立的表空间(.ibd)中</span><span class="hljs-attr">innodb_file_per_table</span>=<span class="hljs-number">1</span><span class="hljs-comment"># 事务级别(可重复读，会出幻读)</span><span class="hljs-attr">transaction_isolation</span>=REPEATABLE-READ<span class="hljs-comment"># 是否在搜索和索引扫描中使用间隙锁(gap locking)，不建议使用未来将删除</span><span class="hljs-attr">innodb_locks_unsafe_for_binlog</span>=<span class="hljs-number">0</span><span class="hljs-comment"># InnoDB 后台清理线程数，更大的值有助于 DML 执行性能，&gt;= 5.7.8 默认为 4</span><span class="hljs-attr">innodb_purge_threads</span>=<span class="hljs-number">4</span></code></pre></div><p><strong>mysqld_safe.cnf</strong></p><div class="hljs"><pre><code class="hljs ini"><span class="hljs-comment">#</span><span class="hljs-comment"># The Percona Server 5.7 configuration file.</span><span class="hljs-comment">#</span><span class="hljs-comment"># One can use all long options that the program supports.</span><span class="hljs-comment"># Run program with --help to get a list of available options and with</span><span class="hljs-comment"># --print-defaults to see which it would actually understand and use.</span><span class="hljs-comment">#</span><span class="hljs-comment"># For explanations see</span><span class="hljs-comment"># http://dev.mysql.com/doc/mysql/en/server-system-variables.html</span><span class="hljs-section">[mysqld_safe]</span><span class="hljs-attr">pid-file</span> = /var/run/mysqld/mysqld.pid<span class="hljs-attr">socket</span>   = /var/run/mysqld/mysqld.sock<span class="hljs-attr">nice</span>     = <span class="hljs-number">0</span></code></pre></div><p><strong>mysqldump.cnf</strong></p><div class="hljs"><pre><code class="hljs ini"><span class="hljs-section">[mysqldump]</span>quick<span class="hljs-attr">default-character-set</span>=utf8mb4<span class="hljs-attr">max_allowed_packet</span>=<span class="hljs-number">256</span>M</code></pre></div><h2 id="八、常用诊断"><a href="#八、常用诊断" class="headerlink" title="八、常用诊断"></a>八、常用诊断</h2><h3 id="8-1、动态配置-diff"><a href="#8-1、动态配置-diff" class="headerlink" title="8.1、动态配置 diff"></a>8.1、动态配置 diff</h3><p>mysql 默认允许在实例运行后使用 <code>set global VARIABLES=VALUE</code> 的方式动态调整一些配置，这可能导致在运行一段时间后(运维动态修改)实例运行配置和配置文件中配置不一致；所以<strong>建议定期 diff 运行时配置与配置文件配置差异，防制特殊情况下 mysql 重启后运行期配置丢失</strong></p><div class="hljs"><pre><code class="hljs sh">pt-config-diff /etc/percona-server.conf.d/mysqld.cnf h=127.0.0.1 --user root --ask-pass --report-width 100Enter MySQL password:2 config differencesVariable                  /etc/percona-server.conf.d/mysqld.cnf mysql47.test.com========================= ===================================== ==================innodb_max_dirty_pages... 50                                    50.000000skip_name_resolve         0                                     ON</code></pre></div><h3 id="8-2、配置优化建议"><a href="#8-2、配置优化建议" class="headerlink" title="8.2、配置优化建议"></a>8.2、配置优化建议</h3><p>Percona Toolkit 提供了一个诊断工具，用于对 mysql 内的配置进行扫描并给出优化建议，在初始化时可以使用此工具评估 mysql 当前配置的具体情况</p><div class="hljs"><pre><code class="hljs sh">pt-variable-advisor 127.0.0.1 --user root --ask-pass | grep -v <span class="hljs-string">'^$'</span>Enter password: <span class="hljs-comment"># WARN delay_key_write: MyISAM index blocks are never flushed until necessary.</span><span class="hljs-comment"># WARN innodb_flush_log_at_trx_commit-1: InnoDB is not configured in strictly ACID mode.</span><span class="hljs-comment"># NOTE innodb_max_dirty_pages_pct: The innodb_max_dirty_pages_pct is lower than the default.</span><span class="hljs-comment"># WARN max_connections: If the server ever really has more than a thousand threads running, then the system is likely to spend more time scheduling threads than really doing useful work.</span><span class="hljs-comment"># NOTE read_buffer_size-1: The read_buffer_size variable should generally be left at its default unless an expert determines it is necessary to change it.</span><span class="hljs-comment"># NOTE read_rnd_buffer_size-1: The read_rnd_buffer_size variable should generally be left at its default unless an expert determines it is necessary to change it.</span><span class="hljs-comment"># NOTE sort_buffer_size-1: The sort_buffer_size variable should generally be left at its default unless an expert determines it is necessary to change it.</span><span class="hljs-comment"># NOTE innodb_data_file_path: Auto-extending InnoDB files can consume a lot of disk space that is very difficult to reclaim later.</span><span class="hljs-comment"># WARN myisam_recover_options: myisam_recover_options should be set to some value such as BACKUP,FORCE to ensure that table corruption is noticed.</span><span class="hljs-comment"># WARN sync_binlog: Binary logging is enabled, but sync_binlog isn't configured so that every transaction is flushed to the binary log for durability.</span></code></pre></div><h3 id="8-3、死锁诊断"><a href="#8-3、死锁诊断" class="headerlink" title="8.3、死锁诊断"></a>8.3、死锁诊断</h3><p>使用 pt-deadlock-logger 工具可以诊断当前的死锁状态，以下为对死锁检测的测试</p><p>首先创建测试数据库和表</p><div class="hljs"><pre><code class="hljs sql"><span class="hljs-comment"># 创建测试库</span><span class="hljs-keyword">CREATE</span> <span class="hljs-keyword">DATABASE</span> dbatest <span class="hljs-built_in">CHARACTER</span> <span class="hljs-keyword">SET</span> utf8mb4 <span class="hljs-keyword">COLLATE</span> utf8mb4_unicode_ci;<span class="hljs-comment"># 切换到测试库并建立测试表</span><span class="hljs-keyword">USE</span> dbatest;<span class="hljs-keyword">CREATE</span> <span class="hljs-keyword">TABLE</span> <span class="hljs-keyword">IF</span> <span class="hljs-keyword">NOT</span> <span class="hljs-keyword">EXISTS</span> <span class="hljs-keyword">test</span> (<span class="hljs-keyword">id</span> <span class="hljs-built_in">INT</span> AUTO_INCREMENT PRIMARY <span class="hljs-keyword">KEY</span>, <span class="hljs-keyword">value</span> <span class="hljs-built_in">VARCHAR</span>(<span class="hljs-number">255</span>), createtime <span class="hljs-built_in">TIMESTAMP</span> <span class="hljs-keyword">DEFAULT</span> <span class="hljs-keyword">CURRENT_TIMESTAMP</span>) <span class="hljs-keyword">ENGINE</span>=<span class="hljs-keyword">INNODB</span>;</code></pre></div><p>在一个其他终端上开启 pt-deadlock-logger 检测</p><div class="hljs"><pre><code class="hljs sh">pt-deadlock-logger 127.0.0.1 --user root --ask-pass --tab</code></pre></div><p>检测开启后进行死锁测试</p><div class="hljs"><pre><code class="hljs sql"><span class="hljs-comment"># 插入两条测试数据</span><span class="hljs-keyword">INSERT</span> <span class="hljs-keyword">INTO</span> <span class="hljs-keyword">test</span>(<span class="hljs-keyword">value</span>) <span class="hljs-keyword">VALUES</span>(<span class="hljs-string">'test1'</span>);<span class="hljs-keyword">INSERT</span> <span class="hljs-keyword">INTO</span> <span class="hljs-keyword">test</span>(<span class="hljs-keyword">value</span>) <span class="hljs-keyword">VALUES</span>(<span class="hljs-string">'test2'</span>);<span class="hljs-comment"># 在两个终端下进行交叉事务</span><span class="hljs-comment"># 统一关闭自动提交</span>terminal_1 <span class="hljs-comment"># SET AUTOCOMMIT = 0;</span>terminal_2 <span class="hljs-comment"># SET AUTOCOMMIT = 0;</span><span class="hljs-comment"># 交叉事务，终端 1 先更新第一条数据，终端 2 先更新第二条数据</span>terminal_1 <span class="hljs-comment"># BEGIN;</span>terminal_1 <span class="hljs-comment"># UPDATE test set value='x1' where id=1;</span>terminal_2 <span class="hljs-comment"># BEGIN;</span>terminal_2 <span class="hljs-comment"># UPDATE test set value='x2' where id=2;</span><span class="hljs-comment"># 此后终端 1 再尝试更新第二条数据，终端 2 再尝试更新第一条数据；造成等待互向释放锁的死锁</span>terminal_1 <span class="hljs-comment"># UPDATE test set value='lock2' where id=2;</span>terminal_2 <span class="hljs-comment"># UPDATE test set value='lock1' where id=1;</span><span class="hljs-comment"># 此时由于开启了 mysql innodb 的死锁自动检测机制，会导致终端 2 弹出错误</span>ERROR 1213 (40001): Deadlock found when trying to get <span class="hljs-keyword">lock</span>; try restarting transaction<span class="hljs-comment"># 同时 pt-deadlock-logger 有日志输出</span>server  ts      thread  txn_id  txn_time        user    hostname    ip      db      tbl     idx     lock_type       lock_mode       wait_hold       victim  query127.0.0.1       2019-12-24T14:57:10     87      0       52      root            127.0.0.1       dbatest test    PRIMARY RECORD  X       w       0       <span class="hljs-keyword">UPDATE</span> <span class="hljs-keyword">test</span> <span class="hljs-keyword">set</span> <span class="hljs-keyword">value</span>=<span class="hljs-string">'lock2'</span> <span class="hljs-keyword">where</span> <span class="hljs-keyword">id</span>=<span class="hljs-number">2</span><span class="hljs-number">127.0</span><span class="hljs-number">.0</span><span class="hljs-number">.1</span>       <span class="hljs-number">2019</span><span class="hljs-number">-12</span><span class="hljs-number">-24</span>T14:<span class="hljs-number">57</span>:<span class="hljs-number">10</span>     <span class="hljs-number">89</span>      <span class="hljs-number">0</span>       <span class="hljs-number">41</span>      root            <span class="hljs-number">127.0</span><span class="hljs-number">.0</span><span class="hljs-number">.1</span>       dbatest <span class="hljs-keyword">test</span>    PRIMARY <span class="hljs-built_in">RECORD</span>  X       w       <span class="hljs-number">1</span>       <span class="hljs-keyword">UPDATE</span> <span class="hljs-keyword">test</span> <span class="hljs-keyword">set</span> <span class="hljs-keyword">value</span>=<span class="hljs-string">'lock1'</span> <span class="hljs-keyword">where</span> <span class="hljs-keyword">id</span>=<span class="hljs-number">1</span></code></pre></div><h3 id="8-4、查看-IO-详情"><a href="#8-4、查看-IO-详情" class="headerlink" title="8.4、查看 IO 详情"></a>8.4、查看 IO 详情</h3><p>不同于 <code>iostat</code>，<code>pt-diskstats</code> 提供了更加详细的 IO 详情统计，并且据有交互式处理，执行一下命令将会实时检测 IO 状态</p><div class="hljs"><pre><code class="hljs sh">pt-diskstats --show-timestamps</code></pre></div><p>其中几个关键值含义如下(更详细的请参考官方文档 <a href="https://www.percona.com/doc/percona-toolkit/LATEST/pt-diskstats.html#output" target="_blank" rel="noopener">https://www.percona.com/doc/percona-toolkit/LATEST/pt-diskstats.html#output</a>)</p><ul><li>rd_s: 每秒平均读取次数。这是发送到基础设备的 IO 请求数。通常，此数量少于应用程序发出的逻辑IO请求的数量。更多请求可能已排队到块设备，但是其中一些请求通常在发送到磁盘之前先进行合并。</li><li>rd_avkb: 读取的平均大小，以千字节为单位。</li><li>rd_mb_s: 每秒读取的平均兆字节数。</li><li>rd_mrg: 在发送到物理设备之前在队列调度程序中合并在一起的读取请求的百分比。</li><li>rd_rt: 读取操作的平均响应时间(以毫秒为单位)；这是端到端响应时间，包括在队列中花费的时间。这是发出 IO 请求的应用程序看到的响应时间，而不是块设备下的物理磁盘的响应时间。</li><li>busy: 设备至少有一个请求 wall-clock 时间的比例；等同于 <code>iostat</code> 的 <code>％util</code>。</li><li>in_prg: 正在进行的请求数。与读写并发是从可靠数字中生成的平均值不同，该数字是一个时样本，您可以看到它可能表示请求峰值，而不是真正的长期平均值。如果此数字很大，则从本质上讲意味着设备高负载运行。</li><li>ios_s: 物理设备的平均吞吐量，以每秒 IO 操作(IOPS)为单位。此列显示基础设备正在处理的总 IOPS；它是 rd_s 和 wr_s 的总和。</li><li>qtime: 平均排队时间；也就是说，请求在发送到物理设备之前在设备调度程序队列中花费的时间。</li><li>stime: 平均服务时间；也就是说，请求完成在队列中的等待之后，物理设备处理请求的时间。</li></ul><h3 id="8-5、重复索引优化"><a href="#8-5、重复索引优化" class="headerlink" title="8.5、重复索引优化"></a>8.5、重复索引优化</h3><p>pt-duplicate-key-checker 工具提供了对数据库重复索引和外键的自动查找功能，工具使用如下</p><div class="hljs"><pre><code class="hljs sh">pt-duplicate-key-checker 127.0.0.1 --user root --ask-passEnter password:<span class="hljs-comment"># A software update is available:</span><span class="hljs-comment"># ########################################################################</span><span class="hljs-comment"># aaaaaa.aaaaaa_audit</span><span class="hljs-comment"># ########################################################################</span><span class="hljs-comment"># index_linkId is a duplicate of unique_linkId</span><span class="hljs-comment"># Key definitions:</span><span class="hljs-comment">#   KEY `index_linkId` (`link_id`)</span><span class="hljs-comment">#   UNIQUE KEY `unique_linkId` (`link_id`),</span><span class="hljs-comment"># Column types:</span><span class="hljs-comment">#         `link_id` bigint(20) not null comment 'bdid'</span><span class="hljs-comment"># To remove this duplicate index, execute:</span>ALTER TABLE `aaaaaa.aaaaaa_audit` DROP INDEX `index_linkId`;<span class="hljs-comment"># ########################################################################</span><span class="hljs-comment"># Summary of indexes</span><span class="hljs-comment"># ########################################################################</span><span class="hljs-comment"># Size Duplicate Indexes   927420</span><span class="hljs-comment"># Total Duplicate Indexes  3</span><span class="hljs-comment"># Total Indexes            847</span></code></pre></div><h3 id="8-6、表统计"><a href="#8-6、表统计" class="headerlink" title="8.6、表统计"></a>8.6、表统计</h3><p>pt-find 是一个很方便的表查找统计工具，默认的一些选项可以实现批量查找符合条件的表，甚至执行一些 SQL 处理命令</p><div class="hljs"><pre><code class="hljs sh"><span class="hljs-comment"># 批量查找大于 5G 的表，并排序</span>pt-find --host 127.0.0.1 --user root --ask-pass --tablesize +5G | sort -rnEnter password: `rss_service`.`test_feed_news``db_log_history`.`test_mobile_click_201912``db_log_history`.`test_mobile_click_201911``db_log_history`.`test_mobile_click_201910``test_dix`.`test_user_messages``test_dix`.`test_user_link_history``test_dix`.`test_mobile_click``test_dix`.`test_message``test_dix`.`test_link_votes``test_dix`.`test_links_mobile_content``test_dix`.`test_links``test_dix`.`test_comment_votes``test_dix`.`test_comments`</code></pre></div><p>如果想要定制输出可以采用 <code>--printf</code> 选项</p><div class="hljs"><pre><code class="hljs sh">pt-find --host 127.0.0.1 --user root --ask-pass --tablesize +5G --<span class="hljs-built_in">printf</span> <span class="hljs-string">"%T\t%D.%N\n"</span> | sort -rnEnter password: 13918404608     `test_dix`.`test_links_mobile_content`13735231488     `test_dix`.`test_comment_votes`12633227264     `test_dix`.`test_user_messages`12610174976     `test_dix`.`test_user_link_history`10506305536     `test_dix`.`test_links`9686745088      `test_dix`.`test_message`9603907584      `rss_service`.`test_feed_news`9004122112      `db_log_history`.`test_mobile_click_201910`8919007232      `test_dix`.`test_comments`8045707264      `db_log_history`.`test_mobile_click_201912`7855915008      `db_log_history`.`test_mobile_click_201911`6099566592      `test_dix`.`test_mobile_click`5892898816      `test_dix`.`test_link_votes`</code></pre></div><p><strong>遗憾的是目前 <code>printf</code> 格式来源与 Perl 的 <code>sprintf</code> 函数，所以支持格式有限，不过简单的格式定制已经基本实现，复杂的建议通过 awk 处理</strong>；其他的可选参数具体参考官方文档 <a href="https://www.percona.com/doc/percona-toolkit/LATEST/pt-find.html" target="_blank" rel="noopener">https://www.percona.com/doc/percona-toolkit/LATEST/pt-find.html</a></p><h3 id="8-7、其他命令"><a href="#8-7、其他命令" class="headerlink" title="8.7、其他命令"></a>8.7、其他命令</h3><p>迫于篇幅，其他更多的高级命令请自行查阅官方文档 <a href="https://www.percona.com/doc/percona-toolkit/LATEST/index.html" target="_blank" rel="noopener">https://www.percona.com/doc/percona-toolkit/LATEST/index.html</a></p>]]></content:encoded>
      
      
      <category domain="https://mritd.com/categories/linux/">Linux</category>
      
      <category domain="https://mritd.com/categories/database/">Database</category>
      
      
      <category domain="https://mritd.com/tags/linux/">Linux</category>
      
      <category domain="https://mritd.com/tags/mysql/">MySQL</category>
      
      <category domain="https://mritd.com/tags/percona/">Percona</category>
      
      
      <comments>https://mritd.com/2020/01/20/set-up-percona-server/#disqus_thread</comments>
      
    </item>
    
    <item>
      <title>Writing Plugin for Coredns</title>
      <link>https://mritd.com/2019/11/05/writing-plugin-for-coredns/</link>
      <guid>https://mritd.com/2019/11/05/writing-plugin-for-coredns/</guid>
      <pubDate>Tue, 05 Nov 2019 12:57:41 GMT</pubDate>
      
      <description>目前测试环境中有很多个 DNS 服务器，不同项目组使用的 DNS 服务器不同，但是不可避免的他们会访问一些公共域名；老的 DNS 服务器都是 dnsmasq，改起来很麻烦，最近研究了一下 CoreDNS，通过编写插件的方式可以实现让多个 CoreDNS 实例实现分布式的统一控制，以下记录了插件编写过程</description>
      
      
      
      <content:encoded><![CDATA[<blockquote><p>目前测试环境中有很多个 DNS 服务器，不同项目组使用的 DNS 服务器不同，但是不可避免的他们会访问一些公共域名；老的 DNS 服务器都是 dnsmasq，改起来很麻烦，最近研究了一下 CoreDNS，通过编写插件的方式可以实现让多个 CoreDNS 实例实现分布式的统一控制，以下记录了插件编写过程</p></blockquote><h2 id="一、CoreDNS-简介"><a href="#一、CoreDNS-简介" class="headerlink" title="一、CoreDNS 简介"></a>一、CoreDNS 简介</h2><p>CoreDNS 目前是 CNCF 旗下的项目(已毕业)，为 Kubernetes 等云原生环境提供可靠的 DNS 服务发现等功能；官网的描述只有一句话: <strong>CoreDNS: DNS and Service Discovery</strong>，而实际上分析源码以后发现 CoreDNS 实际上是基于 Caddy (一个现代化的负载均衡器)而开发的，通过插件式注入，并监听 TCP/UDP 端口提供 DNS 服务；<strong>得益于 Caddy 的插件机制，CoreDNS 支持自行编写插件，拦截 DNS 请求然后处理，</strong>通过这个插件机制你可以在 CoreDNS 上实现各种功能，比如构建分布式一致性的 DNS 集群、动态的 DNS 负载均衡等等</p><h2 id="二、CoreDNS-插件规范"><a href="#二、CoreDNS-插件规范" class="headerlink" title="二、CoreDNS 插件规范"></a>二、CoreDNS 插件规范</h2><h3 id="2-1、插件模式"><a href="#2-1、插件模式" class="headerlink" title="2.1、插件模式"></a>2.1、插件模式</h3><p>CoreDNS 插件编写目前有两种方式:</p><ul><li>深度耦合 CoreDNS，使用 Go 编写插件，直接编译进 CoreDNS 二进制文件</li><li>通过 GRPC 解耦，任意语言编写 GRPC 接口实现，CoreDNS 通过 GRPC 与插件交互</li></ul><p>由于 GRPC 链接实际上借助于 CoreDNS 的 GRPC 插件，同时 GRPC 会有网络开销，TCP 链接不稳定可能造成 DNS 响应过慢等问题，所以本文只介绍如何使用 Go 编写 CoreDNS 的插件，这种插件将直接编译进 CoreDNS 二进制文件中</p><h3 id="2-2、插件注册"><a href="#2-2、插件注册" class="headerlink" title="2.2、插件注册"></a>2.2、插件注册</h3><p>在通常情况下，插件中应当包含一个 <code>setup.go</code> 文件，这个文件的 <code>init</code> 方法调用插件注册，类似这样</p><div class="hljs"><pre><code class="hljs go"><span class="hljs-function"><span class="hljs-keyword">func</span> <span class="hljs-title">init</span><span class="hljs-params">()</span></span> &#123;     plugin.Register(<span class="hljs-string">"gdns"</span>, setup) &#125;</code></pre></div><p>注册方法的第一个参数是插件名称，第二个是一个 func，func 签名如下</p><div class="hljs"><pre><code class="hljs go"><span class="hljs-comment">// SetupFunc is used to set up a plugin, or in other words,</span><span class="hljs-comment">// execute a directive. It will be called once per key for</span><span class="hljs-comment">// each server block it appears in.</span><span class="hljs-keyword">type</span> SetupFunc <span class="hljs-function"><span class="hljs-keyword">func</span><span class="hljs-params">(c *Controller)</span> <span class="hljs-title">error</span></span></code></pre></div><p><strong>在这个 SetupFunc 中，插件编写者应当通过 <code>*Controller</code> 拿到 CoreDNS 的配置并解析它，从而完成自己插件的初始化配置；</strong>比如你的插件需要连接 Etcd，那么在这个方法里你要通过 <code>*Controller</code> 遍历配置，拿到 Etcd 的地址、证书、用户名密码配置等信息；</p><p>如果配置信息没有问题，该插件应当初始化完成；如果有问题就报错退出，然后整个 CoreDNS 启动失败；如果插件初始化完成，最后不要忘记将自己的插件加入到整个插件链路中(CoreDNS 根据情况逐个调用)</p><div class="hljs"><pre><code class="hljs go"><span class="hljs-function"><span class="hljs-keyword">func</span> <span class="hljs-title">setup</span><span class="hljs-params">(c *caddy.Controller)</span> <span class="hljs-title">error</span></span> &#123;e, err := etcdParse(c)<span class="hljs-keyword">if</span> err != <span class="hljs-literal">nil</span> &#123;<span class="hljs-keyword">return</span> plugin.Error(<span class="hljs-string">"gdns"</span>, err)&#125;dnsserver.GetConfig(c).AddPlugin(<span class="hljs-function"><span class="hljs-keyword">func</span><span class="hljs-params">(next plugin.Handler)</span> <span class="hljs-title">plugin</span>.<span class="hljs-title">Handler</span></span> &#123;e.Next = next<span class="hljs-keyword">return</span> e&#125;)<span class="hljs-keyword">return</span> <span class="hljs-literal">nil</span>&#125;</code></pre></div><h3 id="2-3、插件结构体"><a href="#2-3、插件结构体" class="headerlink" title="2.3、插件结构体"></a>2.3、插件结构体</h3><p>一般来说，每一个插件都会定义一个结构体，<strong>结构体中包含必要的 CoreDNS 内置属性，以及当前插件特性的相关配置；</strong>一个样例的插件结构体如下所示</p><div class="hljs"><pre><code class="hljs go"><span class="hljs-keyword">type</span> GDNS <span class="hljs-keyword">struct</span> &#123;  <span class="hljs-comment">// Next 属性在 Setup 之后会被设置到下一个插件的引用，以便在本插件解析失败后可以交由下面的插件继续解析</span>Next       plugin.Handler<span class="hljs-comment">// Fall 列表用来控制哪些域名的请求解析失败后可以继续穿透到下一个插件重新处理</span>Fall       fall.F<span class="hljs-comment">// Zones 表示当前插件应该 case 哪些域名的 DNS 请求</span>Zones      []<span class="hljs-keyword">string</span><span class="hljs-comment">// PathPrefix 和 Client 就是插件本身的业务属性了，由于插件要连 Etcd</span><span class="hljs-comment">// PathPrefix 就是 Etcd 目录前缀，Client 是一个 Etcd 的 client</span><span class="hljs-comment">// endpoints 是 Etcd api 端点的地址</span>PathPrefix <span class="hljs-keyword">string</span>Client     *etcdcv3.Clientendpoints []<span class="hljs-keyword">string</span> <span class="hljs-comment">// Stored here as well, to aid in testing.</span>&#125;</code></pre></div><h3 id="2-4、插件接口"><a href="#2-4、插件接口" class="headerlink" title="2.4、插件接口"></a>2.4、插件接口</h3><p>一个 Go 编写的 CoreDNS 插件实际上只需要实现一个 <code>Handler</code> 接口既可，接口定义如下</p><div class="hljs"><pre><code class="hljs go"><span class="hljs-comment">// Handler is like dns.Handler except ServeDNS may return an rcode</span><span class="hljs-comment">// and/or error.</span><span class="hljs-comment">//</span><span class="hljs-comment">// If ServeDNS writes to the response body, it should return a status</span><span class="hljs-comment">// code. CoreDNS assumes *no* reply has yet been written if the status</span><span class="hljs-comment">// code is one of the following:</span><span class="hljs-comment">//</span><span class="hljs-comment">// * SERVFAIL (dns.RcodeServerFailure)</span><span class="hljs-comment">//</span><span class="hljs-comment">// * REFUSED (dns.RecodeRefused)</span><span class="hljs-comment">//</span><span class="hljs-comment">// * FORMERR (dns.RcodeFormatError)</span><span class="hljs-comment">//</span><span class="hljs-comment">// * NOTIMP (dns.RcodeNotImplemented)</span><span class="hljs-comment">//</span><span class="hljs-comment">// All other response codes signal other handlers above it that the</span><span class="hljs-comment">// response message is already written, and that they should not write</span><span class="hljs-comment">// to it also.</span><span class="hljs-comment">//</span><span class="hljs-comment">// If ServeDNS encounters an error, it should return the error value</span><span class="hljs-comment">// so it can be logged by designated error-handling plugin.</span><span class="hljs-comment">//</span><span class="hljs-comment">// If writing a response after calling another ServeDNS method, the</span><span class="hljs-comment">// returned rcode SHOULD be used when writing the response.</span><span class="hljs-comment">//</span><span class="hljs-comment">// If handling errors after calling another ServeDNS method, the</span><span class="hljs-comment">// returned error value SHOULD be logged or handled accordingly.</span><span class="hljs-comment">//</span><span class="hljs-comment">// Otherwise, return values should be propagated down the plugin</span><span class="hljs-comment">// chain by returning them unchanged.</span>Handler <span class="hljs-keyword">interface</span> &#123;ServeDNS(context.Context, dns.ResponseWriter, *dns.Msg) (<span class="hljs-keyword">int</span>, error)Name() <span class="hljs-keyword">string</span>&#125;</code></pre></div><ul><li><code>ServeDNS</code> 方法是插件需要实现的主要逻辑方法，DNS 请求接受后会从这个方法传入，插件编写者需要实现查询并返回结果</li><li><code>Name</code> 方法只返回一个插件名称标识，具体作用记不太清楚，好像是为了判断插件命名唯一性然后做链式顺序调用的，原则只要你不跟系统插件重名就行</li></ul><p><strong>基本逻辑就是在 setup 阶段通过配置文件创建你的插件结构体对象；然后插件结构体实现这个 <code>Handler</code> 接口，运行期 CoreDNS 会调用接口的 <code>ServeDNS</code> 方法来向插件查询 DNS 请求</strong></p><h3 id="2-5、ServeDNS-方法"><a href="#2-5、ServeDNS-方法" class="headerlink" title="2.5、ServeDNS 方法"></a>2.5、ServeDNS 方法</h3><p>ServeDNS 方法入参有 3 个:</p><ul><li><code>context.Context</code> 用来控制超时等情况的 context</li><li><code>dns.ResponseWriter</code> 插件通过这个对象写入对 Client DNS 请求的响应结果</li><li><code>*dns.Msg</code> 这个是 Client 发起的 DNS 请求，插件负责处理它，比如当你发现请求类型是 <code>AAAA</code> 而你的插件又不想去支持时要如何返回结果</li></ul><p>对于返回结果，插件编写者应当通过 <code>dns.ResponseWriter.WriteMsg</code> 方法写入返回结果，基本代码如下</p><div class="hljs"><pre><code class="hljs go"><span class="hljs-comment">// ServeDNS implements the plugin.Handler interface.</span><span class="hljs-function"><span class="hljs-keyword">func</span> <span class="hljs-params">(gDNS *GDNS)</span> <span class="hljs-title">ServeDNS</span><span class="hljs-params">(ctx context.Context, w dns.ResponseWriter, r *dns.Msg)</span> <span class="hljs-params">(<span class="hljs-keyword">int</span>, error)</span></span> &#123;<span class="hljs-comment">// ...... 这里应当实现你的业务逻辑，查找相应的 DNS 记录</span><span class="hljs-comment">// 最后通过 new 一个 dns.Msg 作为返回结果</span>resp := <span class="hljs-built_in">new</span>(dns.Msg)resp.SetReply(r)resp.Authoritative = <span class="hljs-literal">true</span><span class="hljs-comment">// records 是真正的记录结果，应当在业务逻辑区准备好</span>resp.Answer = <span class="hljs-built_in">append</span>(resp.Answer, records...)<span class="hljs-comment">// 返回结果</span>err = w.WriteMsg(resp)<span class="hljs-keyword">if</span> err != <span class="hljs-literal">nil</span> &#123;log.Error(err)&#125;   <span class="hljs-comment">// 告诉 CoreDNS 是否处理成功</span><span class="hljs-keyword">return</span> dns.RcodeSuccess, <span class="hljs-literal">nil</span>&#125;</code></pre></div><p><strong>需要注意的是，无论根据业务逻辑是否查询到 DNS 记录，都要返回响应结果(没有就返回空)，错误或者未返回将会导致 Client 端查询 DNS 超时，然后不断重试，最终可能导致 Client 端服务故障</strong></p><h3 id="2-6、Name-方法"><a href="#2-6、Name-方法" class="headerlink" title="2.6、Name 方法"></a>2.6、Name 方法</h3><p><code>Name</code> 方法非常简单，只需要返回当前插件名称既可；该方法的作用是为了其他插件判断本插件是否加载等情况</p><div class="hljs"><pre><code class="hljs go"><span class="hljs-comment">// Name implements the Handler interface.</span><span class="hljs-function"><span class="hljs-keyword">func</span> <span class="hljs-params">(gDNS *GDNS)</span> <span class="hljs-title">Name</span><span class="hljs-params">()</span> <span class="hljs-title">string</span></span> &#123; <span class="hljs-keyword">return</span> <span class="hljs-string">"gdns"</span> &#125;</code></pre></div><h2 id="三、CoreDNS-插件处理"><a href="#三、CoreDNS-插件处理" class="headerlink" title="三、CoreDNS 插件处理"></a>三、CoreDNS 插件处理</h2><p>对于实际的业务处理，可以通过 <code>case</code> 请求 <code>QType</code> 来做具体的业务实现</p><div class="hljs"><pre><code class="hljs go"><span class="hljs-comment">// ServeDNS implements the plugin.Handler interface.</span><span class="hljs-function"><span class="hljs-keyword">func</span> <span class="hljs-params">(gDNS *GDNS)</span> <span class="hljs-title">ServeDNS</span><span class="hljs-params">(ctx context.Context, w dns.ResponseWriter, r *dns.Msg)</span> <span class="hljs-params">(<span class="hljs-keyword">int</span>, error)</span></span> &#123;state := request.Request&#123;W: w, Req: r&#125;zone := plugin.Zones(gDNS.Zones).Matches(state.Name())<span class="hljs-keyword">if</span> zone == <span class="hljs-string">""</span> &#123;<span class="hljs-keyword">return</span> plugin.NextOrFailure(gDNS.Name(), gDNS.Next, ctx, w, r)&#125;<span class="hljs-comment">// ...业务处理</span><span class="hljs-keyword">switch</span> state.QType() &#123;<span class="hljs-keyword">case</span> dns.TypeA:<span class="hljs-comment">// A 记录查询业务逻辑</span><span class="hljs-keyword">case</span> dns.TypeAAAA:<span class="hljs-comment">// AAAA 记录查询业务逻辑</span><span class="hljs-keyword">default</span>:<span class="hljs-keyword">return</span> <span class="hljs-literal">false</span>resp := <span class="hljs-built_in">new</span>(dns.Msg)resp.SetReply(r)resp.Authoritative = <span class="hljs-literal">true</span>resp.Answer = <span class="hljs-built_in">append</span>(resp.Answer, records...)err = w.WriteMsg(resp)<span class="hljs-keyword">if</span> err != <span class="hljs-literal">nil</span> &#123;log.Error(err)&#125;<span class="hljs-keyword">return</span> dns.RcodeSuccess, <span class="hljs-literal">nil</span>&#125;</code></pre></div><h2 id="四、插件编译及测试"><a href="#四、插件编译及测试" class="headerlink" title="四、插件编译及测试"></a>四、插件编译及测试</h2><h3 id="4-1、官方标准操作"><a href="#4-1、官方标准操作" class="headerlink" title="4.1、官方标准操作"></a>4.1、官方标准操作</h3><p>根据官方文档的描述，当你编写好插件以后，<strong>你的插件应当提交到一个 Git 仓库中，可以使 Github 等(保证可以 <code>go get</code> 拉取就行)，然后修改 <code>plugin.cfg</code>，最后执行 <code>make</code> 既可</strong>；具体修改如下所示</p><p><img src="https://cdn.oss.link/markdown/vey4u.png" srcset="/img/loading.gif" alt="plugin.cfg"></p><p><strong>值得注意的是: 插件配置在 <code>plugin.cfg</code> 内的顺序决定了插件的执行顺序；通俗的讲，如果 Client 的一个 DNS 请求进来，CoreDNS 根据你在 <code>plugin.cfg</code> 内书写的顺序依次调用，而并非 <code>Corefile</code> 内的配置顺序</strong></p><p>配置好以后直接执行 <code>make</code> 既可编译成功一个包含自定义插件的 CoreDNS 二进制文件(编译过程的 <code>go mod</code> 下载加速问题不在本文讨论范围内)；你可以直接通过这个二进制测试插件的处理情况，当然这种测试不够直观，而且频繁修改由于 <code>go mod</code> 缓存等原因并不一定能保证每次编译的都包含最新插件代码，所以另一种方式请看下一章节</p><h3 id="4-2、经验性的操作"><a href="#4-2、经验性的操作" class="headerlink" title="4.2、经验性的操作"></a>4.2、经验性的操作</h3><p>根据个人测试以及对源码的分析，在修改 <code>plugin.cfg</code> 然后执行 <code>make</code> 命令后，实际上是进行了代码生成；当你通过 git 命令查看相关修改文件时，整个插件加载体系便没什么秘密可言了；<strong>在整个插件体系中，插件加载是通过 <code>init</code> 方法注册的，那么既然用 go 写插件，那么应该清楚 <code>init</code> 方法只有在包引用之后才会执行，所以整个插件体系实际上是这样事儿的:</strong></p><p>首先 <code>make</code> 以后会修改 <code>core/plugin/zplugin.go</code> 文件，这个文件啥也不干，就是 <code>import</code> 来实现调用对应包的 <code>init</code> 方法</p><p><img src="https://cdn.oss.link/markdown/ny1rz.png" srcset="/img/loading.gif" alt="zplugin.go"></p><p>当 <code>init</code> 执行后你去追源码，实际上就是 Caddy 维护了一个 <code>map[string]Plugin</code>，<code>init</code> 会把你的插件 func 塞进去然后后面再调用，实现一个懒加载或者说延迟初始化</p><p><img src="https://cdn.oss.link/markdown/idno4.png" srcset="/img/loading.gif" alt="caddy_plugin"></p><p>接着修改了一下 <code>core/dnsserver/zdirectives.go</code>，这个里面也没啥，就是一个 <code>[]string</code>，<strong>但是 <code>[]string</code> 这玩意有顺序啊，这就是为什么你在 <code>plugin.cfg</code> 里写的顺序决定了插件处理顺序的原因(因为生成的这个切片有顺序)</strong></p><p><img src="https://cdn.oss.link/markdown/bixos.png" srcset="/img/loading.gif" alt="zdirectives.go"></p><p>综上所述，实际上 <code>make</code> 命令一共修改了两个文件，如果想在 IDE 内直接 debug CoreDNS + Plugin 源码，那么只需要这样做:</p><p>复制自己编写的插件目录到 <code>plugin</code> 目录，类似这样</p><p><img src="https://cdn.oss.link/markdown/whwuy.png" srcset="/img/loading.gif" alt="gdns"></p><p>手动修改 <code>core/plugin/zplugin.go</code>，加入自己插件的 <code>import</code>(此时你直接复制系统其他插件，改一下目录名既可)</p><p><img src="https://cdn.oss.link/markdown/g7wp0.png" srcset="/img/loading.gif" alt="update_zplugin"></p><p>手动修改 <code>core/dnsserver/zdirectives.go</code> 把自己插件名称写进去(自己控制顺序)，然后 debug 启动 <code>coredns.go</code> 里面的 main 方法测试既可</p><p><img src="https://cdn.oss.link/markdown/4ucqg.png" srcset="/img/loading.gif" alt="coredns.go"></p><h2 id="五、本文参考"><a href="#五、本文参考" class="headerlink" title="五、本文参考"></a>五、本文参考</h2><ul><li>Writing Plugins for CoreDNS: <a href="https://coredns.io/2016/12/19/writing-plugins-for-coredns" target="_blank" rel="noopener">https://coredns.io/2016/12/19/writing-plugins-for-coredns</a></li><li>how-to-add-plugins.md: <a href="https://github.com/coredns/coredns.io/blob/master/content/blog/how-to-add-plugins.md" target="_blank" rel="noopener">https://github.com/coredns/coredns.io/blob/master/content/blog/how-to-add-plugins.md</a></li><li>example plugin: <a href="https://github.com/coredns/example" target="_blank" rel="noopener">https://github.com/coredns/example</a></li></ul>]]></content:encoded>
      
      
      <category domain="https://mritd.com/categories/golang/">Golang</category>
      
      
      <category domain="https://mritd.com/tags/kubernetes/">Kubernetes</category>
      
      <category domain="https://mritd.com/tags/golang/">Golang</category>
      
      <category domain="https://mritd.com/tags/coredns/">CoreDNS</category>
      
      
      <comments>https://mritd.com/2019/11/05/writing-plugin-for-coredns/#disqus_thread</comments>
      
    </item>
    
    <item>
      <title>Golang Etcd Client Example</title>
      <link>https://mritd.com/2019/10/15/golang-etcd-client-example/</link>
      <guid>https://mritd.com/2019/10/15/golang-etcd-client-example/</guid>
      <pubDate>Tue, 15 Oct 2019 04:21:07 GMT</pubDate>
      
      <description>准备开发点东西，需要用到 Etcd，由于生产 Etcd 全部开启了 TLS 加密，所以客户端需要相应修改，以下为 Golang 链接 Etcd 并且使用客户端证书验证的样例代码</description>
      
      
      
      <content:encoded><![CDATA[<blockquote><p>准备开发点东西，需要用到 Etcd，由于生产 Etcd 全部开启了 TLS 加密，所以客户端需要相应修改，以下为 Golang 链接 Etcd 并且使用客户端证书验证的样例代码</p></blockquote><h2 id="API-V2"><a href="#API-V2" class="headerlink" title="API V2"></a>API V2</h2><div class="hljs"><pre><code class="hljs go"><span class="hljs-keyword">package</span> main<span class="hljs-keyword">import</span> (<span class="hljs-string">"context"</span><span class="hljs-string">"crypto/tls"</span><span class="hljs-string">"crypto/x509"</span><span class="hljs-string">"io/ioutil"</span><span class="hljs-string">"log"</span><span class="hljs-string">"net"</span><span class="hljs-string">"net/http"</span><span class="hljs-string">"time"</span><span class="hljs-string">"go.etcd.io/etcd/client"</span>)<span class="hljs-function"><span class="hljs-keyword">func</span> <span class="hljs-title">main</span><span class="hljs-params">()</span></span> &#123;<span class="hljs-comment">// 为了保证 HTTPS 链接可信，需要预先加载目标证书签发机构的 CA 根证书</span>etcdCA, err := ioutil.ReadFile(<span class="hljs-string">"/Users/mritd/tmp/etcd_ssl/etcd-root-ca.pem"</span>)<span class="hljs-keyword">if</span> err != <span class="hljs-literal">nil</span> &#123;log.Fatal(err)&#125;<span class="hljs-comment">// etcd 启用了双向 TLS 认证，所以客户端证书同样需要加载</span>etcdClientCert, err := tls.LoadX509KeyPair(<span class="hljs-string">"/Users/mritd/tmp/etcd_ssl/etcd.pem"</span>, <span class="hljs-string">"/Users/mritd/tmp/etcd_ssl/etcd-key.pem"</span>)<span class="hljs-keyword">if</span> err != <span class="hljs-literal">nil</span> &#123;log.Fatal(err)&#125;<span class="hljs-comment">// 创建一个空的 CA Pool</span><span class="hljs-comment">// 因为后续只会链接 Etcd 的 api 端点，所以此处选择使用空的 CA Pool，然后只加入 Etcd CA 既可</span><span class="hljs-comment">// 如果期望链接其他 TLS 端点，那么最好使用 x509.SystemCertPool() 方法先 copy 一份系统根 CA</span><span class="hljs-comment">// 然后再向这个 Pool 中添加自定义 CA</span>rootCertPool := x509.NewCertPool()rootCertPool.AppendCertsFromPEM(etcdCA)cfg := client.Config&#123;<span class="hljs-comment">// Etcd HTTPS api 端点</span>Endpoints: []<span class="hljs-keyword">string</span>&#123;<span class="hljs-string">"https://172.16.14.114:2379"</span>&#125;,<span class="hljs-comment">// 自定义 Transport 实现自签 CA 加载以及 Client Cert 加载</span><span class="hljs-comment">// 其他参数最好从 client.DefaultTranspor copy，以保证与默认 client 相同的行为</span>Transport: &amp;http.Transport&#123;Proxy: http.ProxyFromEnvironment,<span class="hljs-comment">// Dial 方法已被启用，采用新的 DialContext 设置超时</span>DialContext: (&amp;net.Dialer&#123;KeepAlive: <span class="hljs-number">30</span> * time.Second,Timeout:   <span class="hljs-number">30</span> * time.Second,&#125;).DialContext,<span class="hljs-comment">// 自定义 CA 及 Client Cert 配置</span>TLSClientConfig: &amp;tls.Config&#123;RootCAs:      rootCertPool,Certificates: []tls.Certificate&#123;etcdClientCert&#125;,&#125;,TLSHandshakeTimeout: <span class="hljs-number">10</span> * time.Second,&#125;,<span class="hljs-comment">// set timeout per request to fail fast when the target endpoint is unavailable</span>HeaderTimeoutPerRequest: time.Second,&#125;c, err := client.New(cfg)<span class="hljs-keyword">if</span> err != <span class="hljs-literal">nil</span> &#123;log.Fatal(err)&#125;kapi := client.NewKeysAPI(c)<span class="hljs-comment">// set "/foo" key with "bar" value</span>log.Print(<span class="hljs-string">"Setting '/foo' key with 'bar' value"</span>)resp, err := kapi.Set(context.Background(), <span class="hljs-string">"/foo"</span>, <span class="hljs-string">"bar"</span>, <span class="hljs-literal">nil</span>)<span class="hljs-keyword">if</span> err != <span class="hljs-literal">nil</span> &#123;log.Fatal(err)&#125; <span class="hljs-keyword">else</span> &#123;<span class="hljs-comment">// print common key info</span>log.Printf(<span class="hljs-string">"Set is done. Metadata is %q\n"</span>, resp)&#125;<span class="hljs-comment">// get "/foo" key's value</span>log.Print(<span class="hljs-string">"Getting '/foo' key value"</span>)resp, err = kapi.Get(context.Background(), <span class="hljs-string">"/foo"</span>, <span class="hljs-literal">nil</span>)<span class="hljs-keyword">if</span> err != <span class="hljs-literal">nil</span> &#123;log.Fatal(err)&#125; <span class="hljs-keyword">else</span> &#123;<span class="hljs-comment">// print common key info</span>log.Printf(<span class="hljs-string">"Get is done. Metadata is %q\n"</span>, resp)<span class="hljs-comment">// print value</span>log.Printf(<span class="hljs-string">"%q key has %q value\n"</span>, resp.Node.Key, resp.Node.Value)&#125;&#125;</code></pre></div><h2 id="API-V3"><a href="#API-V3" class="headerlink" title="API V3"></a>API V3</h2><div class="hljs"><pre><code class="hljs go"><span class="hljs-keyword">package</span> main<span class="hljs-keyword">import</span> (<span class="hljs-string">"context"</span><span class="hljs-string">"crypto/tls"</span><span class="hljs-string">"crypto/x509"</span><span class="hljs-string">"io/ioutil"</span><span class="hljs-string">"log"</span><span class="hljs-string">"time"</span><span class="hljs-string">"go.etcd.io/etcd/clientv3"</span>)<span class="hljs-function"><span class="hljs-keyword">func</span> <span class="hljs-title">main</span><span class="hljs-params">()</span></span> &#123;<span class="hljs-comment">// 为了保证 HTTPS 链接可信，需要预先加载目标证书签发机构的 CA 根证书</span>etcdCA, err := ioutil.ReadFile(<span class="hljs-string">"/Users/mritd/tmp/etcd_ssl/etcd-root-ca.pem"</span>)<span class="hljs-keyword">if</span> err != <span class="hljs-literal">nil</span> &#123;log.Fatal(err)&#125;<span class="hljs-comment">// etcd 启用了双向 TLS 认证，所以客户端证书同样需要加载</span>etcdClientCert, err := tls.LoadX509KeyPair(<span class="hljs-string">"/Users/mritd/tmp/etcd_ssl/etcd.pem"</span>, <span class="hljs-string">"/Users/mritd/tmp/etcd_ssl/etcd-key.pem"</span>)<span class="hljs-keyword">if</span> err != <span class="hljs-literal">nil</span> &#123;log.Fatal(err)&#125;<span class="hljs-comment">// 创建一个空的 CA Pool</span><span class="hljs-comment">// 因为后续只会链接 Etcd 的 api 端点，所以此处选择使用空的 CA Pool，然后只加入 Etcd CA 既可</span><span class="hljs-comment">// 如果期望链接其他 TLS 端点，那么最好使用 x509.SystemCertPool() 方法先 copy 一份系统根 CA</span><span class="hljs-comment">// 然后再向这个 Pool 中添加自定义 CA</span>rootCertPool := x509.NewCertPool()rootCertPool.AppendCertsFromPEM(etcdCA)<span class="hljs-comment">// 创建 api v3 的 client</span>cli, err := clientv3.New(clientv3.Config&#123;<span class="hljs-comment">// etcd https api 端点</span>Endpoints:   []<span class="hljs-keyword">string</span>&#123;<span class="hljs-string">"https://172.16.14.114:2379"</span>&#125;,DialTimeout: <span class="hljs-number">5</span> * time.Second,<span class="hljs-comment">// 自定义 CA 及 Client Cert 配置</span>TLS: &amp;tls.Config&#123;RootCAs:      rootCertPool,Certificates: []tls.Certificate&#123;etcdClientCert&#125;,&#125;,&#125;)<span class="hljs-keyword">if</span> err != <span class="hljs-literal">nil</span> &#123;log.Fatal(err)&#125;<span class="hljs-keyword">defer</span> <span class="hljs-function"><span class="hljs-keyword">func</span><span class="hljs-params">()</span></span> &#123; _ = cli.Close() &#125;()ctx, cancel := context.WithTimeout(context.Background(), <span class="hljs-number">3</span>*time.Second)putResp, err := cli.Put(ctx, <span class="hljs-string">"sample_key"</span>, <span class="hljs-string">"sample_value"</span>)<span class="hljs-keyword">if</span> err != <span class="hljs-literal">nil</span> &#123;log.Fatal(err)&#125; <span class="hljs-keyword">else</span> &#123;log.Println(putResp)&#125;cancel()ctx, cancel = context.WithTimeout(context.Background(), <span class="hljs-number">3</span>*time.Second)delResp, err := cli.Delete(ctx, <span class="hljs-string">"sample_key"</span>)<span class="hljs-keyword">if</span> err != <span class="hljs-literal">nil</span> &#123;log.Fatal(err)&#125; <span class="hljs-keyword">else</span> &#123;log.Println(delResp)&#125;cancel()&#125;</code></pre></div>]]></content:encoded>
      
      
      <category domain="https://mritd.com/categories/golang/">Golang</category>
      
      
      <category domain="https://mritd.com/tags/golang/">Golang</category>
      
      <category domain="https://mritd.com/tags/etcd/">etcd</category>
      
      
      <comments>https://mritd.com/2019/10/15/golang-etcd-client-example/#disqus_thread</comments>
      
    </item>
    
    <item>
      <title>Podman 初试 - 容器发展史</title>
      <link>https://mritd.com/2019/06/26/podman-history-of-container/</link>
      <guid>https://mritd.com/2019/06/26/podman-history-of-container/</guid>
      <pubDate>Wed, 26 Jun 2019 15:22:49 GMT</pubDate>
      
      <description>这是一篇纯介绍性文章，本文不包含任何技术层面的操作，本文仅作为后续 Podman 文章铺垫；本文细节部份并未阐述，很多地方并不详实(一家只谈，不可轻信)。</description>
      
      
      
      <content:encoded><![CDATA[<blockquote><p>这是一篇纯介绍性文章，本文不包含任何技术层面的操作，本文仅作为后续 Podman 文章铺垫；本文细节部份并未阐述，很多地方并不详实(一家只谈，不可轻信)。</p></blockquote><h2 id="一、缘起"><a href="#一、缘起" class="headerlink" title="一、缘起"></a>一、缘起</h2><h3 id="1-1、鸿蒙"><a href="#1-1、鸿蒙" class="headerlink" title="1.1、鸿蒙"></a>1.1、鸿蒙</h3><p>在上古时期，天地初开，一群称之为 “运维” 的人们每天在一种叫作 “服务器” 的神秘盒子中创造属于他们的世界；他们在这个世界中每日劳作，一遍又一遍的写入他们的历史，比如搭建一个 nginx、布署一个 java web 应用…</p><p>大多数人其实并没有那么聪明，他们所 “创造” 的事实上可能是有人已经创造过的东西，他们可能每天都在做着重复的劳动；久而久之，一些人厌倦了、疲惫了…又过了一段时间，一些功力深厚的老前辈创造了一些批量布署工具来帮助人们做一些重复性的劳动，这些工具被起名为 “Asible”、”Chef”、”Puppet” 等等…</p><p>而随着时代的发展，”世界” 变得越来越复杂，运维们需要处理的事情越来越多，比如各种网络、磁盘环境的隔离，各种应用服务的高可用…在时代的洪流下，运维们急需要一种简单高效的布署工具，既能有一定的隔离性，又能方便使用，并且最大程度降低重复劳动来提升效率。</p><h3 id="1-2、创世"><a href="#1-2、创世" class="headerlink" title="1.2、创世"></a>1.2、创世</h3><p>在时代洪流的冲击下，一位名为 “Solomon Hykes” 的人异军突起，他创造了一个称之为 Docker 的工具，Docker 被创造以后就以灭世之威向运维们展示了它的强大；一个战斗力只有 5 的运维只需要学习 Docker 很短时间就可以完成资深运维们才能完成的事情，在某些情况下以前需要 1 天才能完成的工作使用 Docker 后几分钟就可以完成；此时运维们已经意识到 “新的时代” 开启了，接下来 Docker 开源并被整个运维界人们使用，Docker 也不断地完善增加各种各样的功能，此后世界正式进入 “容器纪元”。</p><h2 id="二、纷争"><a href="#二、纷争" class="headerlink" title="二、纷争"></a>二、纷争</h2><h3 id="2-1、发展"><a href="#2-1、发展" class="headerlink" title="2.1、发展"></a>2.1、发展</h3><p>随着 Docker 的日益成熟，一些人开始在 Docker 之上创造更加强大的工具，一些人开始在 Docker 之下为其提供更稳定的运行环境…</p><p>其中一个叫作 Google 的公司在 Docker 之上创建了名为 “Kubernetes” 的工具，Kubernetes 操纵 Docker 完成更加复杂的任务；Kubernetes 的出现更加印证了 Docker 的强大，以及 “容器纪元” 的发展正确性。</p><h3 id="2-2、野心"><a href="#2-2、野心" class="headerlink" title="2.2、野心"></a>2.2、野心</h3><p>当然这是一个充满利益的世界，Google 公司创造 Kubernetes 是可以为他们带来利益的，比如他们可以让 Kubernetes 深度适配他们的云平台，以此来增加云平台的销量等；此时 Docker 创始人也成立了一个公司，提供 Docker 的付费服务以及深度定制等；不过值得一提的是 Docker 公司提供的付费服务始终没有 Kubernetes 为 Google 公司带来的利益高，所以在利益的驱使下，Docker 公司开始动起了歪心思: <strong>创造一个 Kubernetes 的替代品，利用用户粘度复制 Kubernetes 的成功，从 Google 嘴里抢下这块蛋糕！</strong>此时 Docker 公司只想把蛋糕抢过来，但是他们根本没有在意到暗中一群人创造了一个叫 “rkt” 的东西也在妄图夺走他们嘴里的蛋糕。</p><h3 id="2-3、冲突"><a href="#2-3、冲突" class="headerlink" title="2.3、冲突"></a>2.3、冲突</h3><p>在一段时间的沉默后，Docker 公司又创造了 “Swarm” 这个工具，妄图夺走 Google 公司利用 Kubernetes 赢来的蛋糕；当然，Google 这个公司极其庞大，人数众多，而且在这个社会有很大的影响地位…</p><p>终于，巨人苏醒了，Google 联合了 Redhat、Microsoft、IBM、Intel、Cisco 等公司决定对这个爱动歪脑筋的 Docker 公司进行制裁；当然制裁的手段不能过于暴力，那样会让别人落下把柄，成为别人的笑料，被人所不耻；<strong>最总他们决定制订规范，成立组织，明确规定 Docker 的角色，以及它应当拥有的能力，这些规范包括但不限于 <code>CRI</code>、<code>CNI</code> 等；自此之后各大公司宣布他们容器相关的工具只兼容 CRI 等相关标准，无论是 Docker 还是 rkt 等工具，只要实现了这些标准，就可以配合这些容器工具进行使用</strong>。</p><h2 id="三、成败"><a href="#三、成败" class="headerlink" title="三、成败"></a>三、成败</h2><p>自此之后，Docker 跌下神坛，各路大神纷纷创造满足 CRI 等规范的工具用来取代 Docker，Docker 丢失了往日一家独大的场面，最终为了顺应时代发展，拆分自己成为模块化组件；这些模块化组件被放置在 <a href="https://mobyproject.org/" target="_blank" rel="noopener">mobyproject</a> 中方便其他人重复利用。</p><p>时至今日，虽然 Docker 已经不负以前，但是仍然是容器化首选工具，因为 Docker 是一个完整的产品，它可以提供除了满足 CRI 等标准以外更加方便的功能；但是制裁并非没有结果，Google 公司借此创造了 cri-o 用来满足 CRI 标准，其他公司也相应创建了对应的 CRI 实现；<strong>为了进一步分化 Docker 势力，一个叫作 Podman 的工具被创建，它以 cri-o 为基础，兼容大部份 Docker 命令的方式开始抢夺 Dcoker 用户</strong>；到目前为止 Podman 已经可以在大部份功能上替代 Docker。</p>]]></content:encoded>
      
      
      <category domain="https://mritd.com/categories/docker/">Docker</category>
      
      
      <category domain="https://mritd.com/tags/docker/">Docker</category>
      
      <category domain="https://mritd.com/tags/podman/">Podman</category>
      
      
      <comments>https://mritd.com/2019/06/26/podman-history-of-container/#disqus_thread</comments>
      
    </item>
    
    <item>
      <title>Calico 3.6 转发外部流量到集群 Pod</title>
      <link>https://mritd.com/2019/06/18/calico-3.6-forward-network-traffic/</link>
      <guid>https://mritd.com/2019/06/18/calico-3.6-forward-network-traffic/</guid>
      <pubDate>Tue, 18 Jun 2019 14:20:54 GMT</pubDate>
      
      <description>由于开发有部份服务使用 GRPC 进行通讯，同时采用 Consul 进行服务发现；在微服务架构下可能会导致一些访问问题，目前解决方案就是打通开发环境网络与测试环境 Kubernetes 内部 Pod 网络；翻了好多资料发现都是 2.x 的，而目前测试集群 Calico 版本为 3.6.3，很多文档都不适用只能自己折腾，目前折腾完了这里记录一下</description>
      
      
      
      <content:encoded><![CDATA[<blockquote><p>由于开发有部份服务使用 GRPC 进行通讯，同时采用 Consul 进行服务发现；在微服务架构下可能会导致一些访问问题，目前解决方案就是打通开发环境网络与测试环境 Kubernetes 内部 Pod 网络；翻了好多资料发现都是 2.x 的，而目前测试集群 Calico 版本为 3.6.3，很多文档都不适用只能自己折腾，目前折腾完了这里记录一下</p></blockquote><p><strong>本文默认为读者已经存在一个运行正常的 Kubernetes 集群，并且采用 Calico 作为 CNI 组件，且 Calico 工作正常；同时应当在某个节点完成了 calicoctl 命令行工具的配置</strong></p><h2 id="一、问题描述"><a href="#一、问题描述" class="headerlink" title="一、问题描述"></a>一、问题描述</h2><p>在微服务架构下，由于服务组件很多，开发在本地机器想测试应用需要启动整套服务，这对开发机器的性能确实是个考验；但如果直接连接测试环境的服务，由于服务发现问题最终得到的具体服务 IP 是 Kubernetes Pod IP，此 IP 由集群内部 Calico 维护与分配，外部不可访问；最终目标为打通开发环境与集群内部网络，实现开发网络下直连 Pod IP，这或许在以后对生产服务暴露负载均衡有一定帮助意义；目前网络环境如下:</p><p>开发网段: <code>10.10.0.0/24</code><br>测试网段: <code>172.16.0.0/24</code><br>Kubernetes Pod 网段: <code>10.20.0.0/16</code></p><h2 id="二、打通网络"><a href="#二、打通网络" class="headerlink" title="二、打通网络"></a>二、打通网络</h2><p>首先面临的第一个问题是 Calico 处理，因为<strong>如果想要让数据包能从开发网络到达 Pod 网络，那么必然需要测试环境宿主机上的 Calico Node 帮忙转发</strong>；因为 Pod 网络由 Calico 维护，只要 Calico Node 帮忙转发那么数据一定可以到达 Pod IP 上；</p><p>一开始我很天真的认为这就是个 <code>ip route add 10.20.0.0/16 via 172.16.0.13</code> 的问题… 后来发现</p><p><img src="https://cdn.oss.link/markdown/hwp9s.jpg" srcset="/img/loading.gif" alt="没那么简单"></p><p>经过翻文档、issue、blog 等最终发现需要进行以下步骤</p><h3 id="2-1、关闭全互联模式"><a href="#2-1、关闭全互联模式" class="headerlink" title="2.1、关闭全互联模式"></a>2.1、关闭全互联模式</h3><p><strong>注意: 关闭全互联时可能导致网络暂时中断，请在夜深人静时操作</strong></p><p>首先执行以下命令查看是否存在默认的 BGP 配置</p><div class="hljs"><pre><code class="hljs sh">calicoctl get bgpconfig default</code></pre></div><p>如果存在则将其保存为配置文件</p><div class="hljs"><pre><code class="hljs sh">calicoctl get bgpconfig default -o yaml &gt; bgp.yaml</code></pre></div><p>修改其中的 <code>spec.nodeToNodeMeshEnabled</code> 为 <code>false</code>，然后进行替换</p><div class="hljs"><pre><code class="hljs sh">calicoctl apply -f bgp.yaml</code></pre></div><p>如果不存在则手动创建一个配置，然后应用</p><div class="hljs"><pre><code class="hljs sh"> cat &lt;&lt; EOF | calicoctl create -f - apiVersion: projectcalico.org/v3 kind: BGPConfiguration metadata:   name: default spec:   logSeverityScreen: Info   nodeToNodeMeshEnabled: <span class="hljs-literal">false</span>   asNumber: 63400EOF</code></pre></div><p>本部分参考: </p><ul><li><a href="https://docs.projectcalico.org/v3.6/networking/bgp" target="_blank" rel="noopener">Disabling the full node-to-node BGP mesh</a></li></ul><h3 id="2-2、开启集群内-RR-模式"><a href="#2-2、开启集群内-RR-模式" class="headerlink" title="2.2、开启集群内 RR 模式"></a>2.2、开启集群内 RR 模式</h3><p>在 Calico 3.3 后支持了集群内节点的 RR 模式，即将某个集群内的 Calico Node 转变为 RR 节点；将某个节点设置为 RR 节点只需要增加 <code>routeReflectorClusterID</code> 既可，为了后面方便配置同时增加了一个 lable 字段 <code>route-reflector: &quot;true&quot;</code></p><div class="hljs"><pre><code class="hljs sh">calicoctl get node CALICO_NODE_NAME -o yaml &gt; node.yaml</code></pre></div><p>然后增加 <code>routeReflectorClusterID</code> 字段，样例如下</p><div class="hljs"><pre><code class="hljs yaml"><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">projectcalico.org/v3</span><span class="hljs-attr">kind:</span> <span class="hljs-string">Node</span><span class="hljs-attr">metadata:</span>  <span class="hljs-attr">annotations:</span>    <span class="hljs-attr">projectcalico.org/kube-labels:</span> <span class="hljs-string">'&#123;"beta.kubernetes.io/arch":"amd64","beta.kubernetes.io/os":"linux","kubernetes.io/hostname":"d13.node","node-role.kubernetes.io/k8s-master":"true"&#125;'</span>  <span class="hljs-attr">creationTimestamp:</span> <span class="hljs-number">2019</span><span class="hljs-number">-06</span><span class="hljs-string">-17T13:55:44Z</span>  <span class="hljs-attr">labels:</span>    <span class="hljs-attr">beta.kubernetes.io/arch:</span> <span class="hljs-string">amd64</span>    <span class="hljs-attr">beta.kubernetes.io/os:</span> <span class="hljs-string">linux</span>    <span class="hljs-attr">kubernetes.io/hostname:</span> <span class="hljs-string">d13.node</span>    <span class="hljs-attr">node-role.kubernetes.io/k8s-master:</span> <span class="hljs-string">"true"</span>    <span class="hljs-attr">route-reflector:</span> <span class="hljs-string">"true"</span>  <span class="hljs-comment"># 增加 lable</span>  <span class="hljs-attr">name:</span> <span class="hljs-string">d13.node</span>  <span class="hljs-attr">resourceVersion:</span> <span class="hljs-string">"61822269"</span>  <span class="hljs-attr">uid:</span> <span class="hljs-string">9a1897e0-9107-11e9-bc1c-90b11c53d1e3</span><span class="hljs-attr">spec:</span>  <span class="hljs-attr">bgp:</span>    <span class="hljs-attr">ipv4Address:</span> <span class="hljs-number">172.16</span><span class="hljs-number">.0</span><span class="hljs-number">.13</span><span class="hljs-string">/19</span>    <span class="hljs-attr">ipv4IPIPTunnelAddr:</span> <span class="hljs-number">10.20</span><span class="hljs-number">.73</span><span class="hljs-number">.82</span>    <span class="hljs-attr">routeReflectorClusterID:</span> <span class="hljs-number">172.16</span><span class="hljs-number">.20</span><span class="hljs-number">.1</span> <span class="hljs-comment"># 添加集群 ID</span>  <span class="hljs-attr">orchRefs:</span>  <span class="hljs-bullet">-</span> <span class="hljs-attr">nodeName:</span> <span class="hljs-string">d13.node</span>    <span class="hljs-attr">orchestrator:</span> <span class="hljs-string">k8s</span></code></pre></div><p><strong>事实上我们应当导出多个 Calico Node 的配置，并将其配置为 RR 节点以进行冗余；对于 <code>routeReflectorClusterID</code> 目前测试只是作为一个 ID(至少在本文是这样的)，所以理论上可以是任何 IP，个人猜测最好在同一集群网络下采用相同的 IP，由于这是真正的测试环境我没有对 ID 做过多的测试(怕玩挂)</strong></p><p>修改完成后只需要应用一下就行</p><div class="hljs"><pre><code class="hljs sh">calicoctl apply -f node.yaml</code></pre></div><p>接下来需要创建对等规则，规则文件如下</p><div class="hljs"><pre><code class="hljs yaml"><span class="hljs-attr">kind:</span> <span class="hljs-string">BGPPeer</span><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">projectcalico.org/v3</span><span class="hljs-attr">metadata:</span>  <span class="hljs-attr">name:</span> <span class="hljs-string">peer-to-rrs</span><span class="hljs-attr">spec:</span>  <span class="hljs-attr">nodeSelector:</span> <span class="hljs-string">"!has(route-reflector)"</span>  <span class="hljs-attr">peerSelector:</span> <span class="hljs-string">has(route-reflector)</span><span class="hljs-meta">---</span><span class="hljs-attr">kind:</span> <span class="hljs-string">BGPPeer</span><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">projectcalico.org/v3</span><span class="hljs-attr">metadata:</span>  <span class="hljs-attr">name:</span> <span class="hljs-string">rr-mesh</span><span class="hljs-attr">spec:</span>  <span class="hljs-attr">nodeSelector:</span> <span class="hljs-string">has(route-reflector)</span>  <span class="hljs-attr">peerSelector:</span> <span class="hljs-string">has(route-reflector)</span></code></pre></div><p>假定规则文件名称为 <code>rr.yaml</code>，则创建命令为 <code>calicoctl create -f rr.yaml</code>；此时在 RR 节点上使用 <code>calicoctl node status</code> 应该能看到类似如下输出</p><div class="hljs"><pre><code class="hljs sh">Calico process is running.IPv4 BGP status+--------------+---------------+-------+----------+-------------+| PEER ADDRESS |   PEER TYPE   | STATE |  SINCE   |    INFO     |+--------------+---------------+-------+----------+-------------+| 172.16.0.19  | node specific | up    | 05:43:51 | Established || 172.16.0.16  | node specific | up    | 05:43:51 | Established || 172.16.0.17  | node specific | up    | 05:43:51 | Established || 172.16.0.13  | node specific | up    | 13:01:17 | Established |+--------------+---------------+-------+----------+-------------+IPv6 BGP statusNo IPv6 peers found.</code></pre></div><p><strong><code>PEER ADDRESS</code> 应当包含所有非 RR 节点 IP(由于真实测试环境，以上输出已人为修改)</strong></p><p>同时在非 RR 节点上使用 <code>calicoctl node status</code> 应该能看到以下输出</p><div class="hljs"><pre><code class="hljs sh">Calico process is running.IPv4 BGP status+--------------+---------------+-------+----------+-------------+| PEER ADDRESS |   PEER TYPE   | STATE |  SINCE   |    INFO     |+--------------+---------------+-------+----------+-------------+| 172.16.0.10  | node specific | up    | 05:43:51 | Established || 172.16.0.13  | node specific | up    | 13:01:20 | Established |+--------------+---------------+-------+----------+-------------+IPv6 BGP statusNo IPv6 peers found.</code></pre></div><p><strong><code>PEER ADDRESS</code> 应当包含所有 RR 节点 IP，此时原本的 Pod 网络连接应当已经恢复</strong></p><p>本部分参考:</p><ul><li><a href="https://www.projectcalico.org/how-does-in-cluster-route-reflection-work/" target="_blank" rel="noopener">In-cluster Route Reflection</a></li><li><a href="https://docs.projectcalico.org/v3.6/networking/bgp" target="_blank" rel="noopener">Configuring in-cluster route reflectors</a></li></ul><h3 id="2-3、调整-IPIP-规则"><a href="#2-3、调整-IPIP-规则" class="headerlink" title="2.3、调整 IPIP 规则"></a>2.3、调整 IPIP 规则</h3><p>先说一下 Calico IPIP 模式的三个可选项:</p><ul><li><code>Always</code>: 永远进行 IPIP 封装(默认)</li><li><code>CrossSubnet</code>: 只在跨网段时才进行 IPIP 封装，适合有 Kubernetes 节点在其他网段的情况，属于中肯友好方案</li><li><code>Never</code>: 从不进行 IPIP 封装，适合确认所有 Kubernetes 节点都在同一个网段下的情况</li></ul><p>在默认情况下，默认的 ipPool 启用了 IPIP 封装(至少通过官方安装文档安装的 Calico 是这样)，并且封装模式为 <code>Always</code>；这也就意味着任何时候都会在原报文上封装新 IP 地址，<strong>在这种情况下将外部流量路由到 RR 节点，RR 节点再转发进行 IPIP 封装时，可能出现网络无法联通的情况(没仔细追查，网络渣，猜测是 Pod 那边得到的源 IP 不对导致的)；</strong>此时我们应当调整 IPIP 封装策略为 <code>CrossSubnet</code></p><p>导出 ipPool 配置</p><div class="hljs"><pre><code class="hljs sh">calicoctl get ippool default-ipv4-ippool -o yaml &gt; ippool.yaml</code></pre></div><p>修改 <code>ipipMode</code> 值为 <code>CrossSubnet</code></p><div class="hljs"><pre><code class="hljs yaml"><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">projectcalico.org/v3</span><span class="hljs-attr">kind:</span> <span class="hljs-string">IPPool</span><span class="hljs-attr">metadata:</span>  <span class="hljs-attr">creationTimestamp:</span> <span class="hljs-number">2019</span><span class="hljs-number">-06</span><span class="hljs-string">-17T13:55:44Z</span>  <span class="hljs-attr">name:</span> <span class="hljs-string">default-ipv4-ippool</span>  <span class="hljs-attr">resourceVersion:</span> <span class="hljs-string">"61858741"</span>  <span class="hljs-attr">uid:</span> <span class="hljs-string">99a82055-9107-11e9-815b-b82a72dffa9f</span><span class="hljs-attr">spec:</span>  <span class="hljs-attr">blockSize:</span> <span class="hljs-number">26</span>  <span class="hljs-attr">cidr:</span> <span class="hljs-number">10.20</span><span class="hljs-number">.0</span><span class="hljs-number">.0</span><span class="hljs-string">/16</span>  <span class="hljs-attr">ipipMode:</span> <span class="hljs-string">CrossSubnet</span>  <span class="hljs-attr">natOutgoing:</span> <span class="hljs-literal">true</span>  <span class="hljs-attr">nodeSelector:</span> <span class="hljs-string">all()</span></code></pre></div><p>重新使用 <code>calicoctl apply -f ippool.yaml</code> 应用既可</p><p>本部分参考:</p><ul><li><a href="https://docs.projectcalico.org/v3.6/networking/ip-in-ip" target="_blank" rel="noopener">Configuring IP-in-IP</a></li><li><a href="https://docs.projectcalico.org/v3.6/reference/calicoctl/resources/ippool" target="_blank" rel="noopener">IP pool resource</a></li></ul><h3 id="2-4、增加路由联通网络"><a href="#2-4、增加路由联通网络" class="headerlink" title="2.4、增加路由联通网络"></a>2.4、增加路由联通网络</h3><p>万事俱备只欠东风，最后只需要在开发机器添加路由既可</p><p>将 Pod IP <code>10.20.0.0/16</code> 和 Service IP <code>10.254.0.0/16</code> 路由到 RR 节点 <code>172.16.0.13</code></p><div class="hljs"><pre><code class="hljs sh"><span class="hljs-comment"># Pod IP</span>ip route add 10.20.0.0/16 via 172.16.0.13<span class="hljs-comment"># Service IP</span>ip route add 10.254.0.0/16 via 172.16.0.13</code></pre></div><p>当然最方便的肯定是将这一步在开发网络的路由上做，设置完成后开发网络就可以直连集群内的 Pod IP 和 Service IP 了；至于想直接访问 Service Name 只需要调整上游 DNS 解析既可</p>]]></content:encoded>
      
      
      <category domain="https://mritd.com/categories/kubernetes/">Kubernetes</category>
      
      
      <category domain="https://mritd.com/tags/kubernetes/">Kubernetes</category>
      
      
      <comments>https://mritd.com/2019/06/18/calico-3.6-forward-network-traffic/#disqus_thread</comments>
      
    </item>
    
    <item>
      <title>Dockerfile 目前可扩展的语法</title>
      <link>https://mritd.com/2019/05/13/dockerfile-extended-syntax/</link>
      <guid>https://mritd.com/2019/05/13/dockerfile-extended-syntax/</guid>
      <pubDate>Mon, 13 May 2019 14:57:07 GMT</pubDate>
      
      <description>最近在调整公司项目的 CI，目前主要使用 GitLab CI，在尝试多阶段构建中踩了点坑，然后发现了一些有意思的玩意</description>
      
      
      
      <content:encoded><![CDATA[<blockquote><p>最近在调整公司项目的 CI，目前主要使用 GitLab CI，在尝试多阶段构建中踩了点坑，然后发现了一些有意思的玩意</p></blockquote><p>本文参考:</p><ul><li><a href="https://github.com/moby/buildkit/blob/master/frontend/dockerfile/docs/experimental.md" target="_blank" rel="noopener">Dockerfile frontend experimental syntaxes</a></li><li><a href="https://medium.com/@tonistiigi/advanced-multi-stage-build-patterns-6f741b852fae" target="_blank" rel="noopener">Advanced multi-stage build patterns</a></li><li><a href="https://docs.docker.com/engine/reference/commandline/build/" target="_blank" rel="noopener">docker build Document</a></li></ul><h2 id="一、起因"><a href="#一、起因" class="headerlink" title="一、起因"></a>一、起因</h2><p>公司目前主要使用 GitLab CI 作为主力 CI 构建工具，而且由于机器有限，我们对一些包管理器的本地 cache 直接持久化到了本机；比如 maven 的 <code>.m2</code> 目录，nodejs 的 <code>.npm</code> 目录等；虽然我们创建了对应的私服，但是在 build 时毕竟会下载，所以当时索性调整 GitLab Runner 在每个由 GitLab Runner 启动的容器中挂载这些缓存目录(GitLab CI 在 build 时会新启动容器运行 build 任务)；今天调整 nodejs 项目浪了一下，直接采用 Dockerfile 的 multi-stage build 功能进行 “Build =&gt; Package(docker image)” 的实现，基本 Dockerfile 如下</p><div class="hljs"><pre><code class="hljs sh">FROM gozap/build as builderCOPY . /xxxxWORKDIR /xxxxRUN <span class="hljs-built_in">source</span> ~/.bashrc \    &amp;&amp; cnpm install \    &amp;&amp; cnpm run buildFROM gozap/nginx-react:v1.0.0LABEL maintainer=<span class="hljs-string">"mritd &lt;mritd@linux.com&gt;"</span>COPY --from=builder /xxxx/public /usr/share/nginx/htmlEXPOSE 80STOPSIGNAL SIGTERMCMD [<span class="hljs-string">"nginx"</span>, <span class="hljs-string">"-g"</span>, <span class="hljs-string">"daemon off;"</span>]</code></pre></div><p>本来这个 <code>cnpm</code> 命令是带有 cache 的(<a href="https://github.com/Gozap/dockerfile/blob/master/build/cnpm" target="_blank" rel="noopener">见这里</a>)，不过运行完 build 以后发现很慢，检查宿主机 cache 目录发现根本没有 cache…然后突然感觉</p><p><img src="https://cdn.oss.link/markdown/6ieh4.jpg" srcset="/img/loading.gif" alt="事情并没有这么简单"></p><p>仔细想想，情况应该是这样事儿的…</p><div class="hljs"><pre><code class="hljs sh">+------------+                +-------------+            +----------------+|            |                |             |            |                ||            |                |    build    |            |   Multi-stage  ||   Runner   +---------------&gt;+  conatiner  +-----------&gt;+     Build      ||            |                |             |            |                ||            |                |             |            |                |+------------+                +------+------+            +----------------+                                     ^                                     |                                     |                                     |                                     |                              +------+------+                              |             |                              |    Cache    |                              |             |                              +-------------+</code></pre></div><p><img src="https://cdn.oss.link/markdown/9ov8m.jpg" srcset="/img/loading.gif" alt="挂载不管用"></p><p>后来经过查阅文档，发现 Dockerfile 是有扩展语法的(当然最终我还是没用)，具体请见<del>下篇文章</del>(我怕被打死)下面，<strong>先说好，下面的内容无法完美的解决上面的问题，目前只是支持了一部分功能，当然未来很可能支持类似 <code>IF ELSE</code> 语法、直接挂载宿主机目录等功能</strong></p><h2 id="二、开启-Dockerfile-扩展语法"><a href="#二、开启-Dockerfile-扩展语法" class="headerlink" title="二、开启 Dockerfile 扩展语法"></a>二、开启 Dockerfile 扩展语法</h2><h3 id="2-1、开启实验性功能"><a href="#2-1、开启实验性功能" class="headerlink" title="2.1、开启实验性功能"></a>2.1、开启实验性功能</h3><p>目前这个扩展语法还处于实验性功能，所以需要配置 dockerd 守护进程，修改如下</p><div class="hljs"><pre><code class="hljs sh">ExecStart=/usr/bin/dockerd  -H unix:// \                            --init \                            --live-restore \                            --data-root=/data/docker \                            --experimental \                            --<span class="hljs-built_in">log</span>-driver json-file \                            --<span class="hljs-built_in">log</span>-opt max-size=30m \                            --<span class="hljs-built_in">log</span>-opt max-file=3</code></pre></div><p>主要是 <code>--experimental</code> 参数，参考<a href="https://docs.docker.com/engine/reference/commandline/dockerd/#description" target="_blank" rel="noopener">官方文档</a>；<strong>同时在 build 前声明 <code>export DOCKER_BUILDKIT=1</code> 变量</strong></p><h3 id="2-2、修改-Dockerfile"><a href="#2-2、修改-Dockerfile" class="headerlink" title="2.2、修改 Dockerfile"></a>2.2、修改 Dockerfile</h3><p>开启实验性功能后，只需要在 Dockerfile 头部增加 <code># syntax=docker/dockerfile:experimental</code> 既可；为了保证稳定性，你也可以指定具体的版本号，类似这样</p><div class="hljs"><pre><code class="hljs sh"><span class="hljs-comment"># syntax=docker/dockerfile:1.1.1-experimental</span>FROM tomcat</code></pre></div><h3 id="2-3、可用的扩展语法"><a href="#2-3、可用的扩展语法" class="headerlink" title="2.3、可用的扩展语法"></a>2.3、可用的扩展语法</h3><ul><li><code>RUN --mount=type=bind</code></li></ul><p>这个是默认的挂载模式，这个允许将上下文或者镜像以可都可写/只读模式挂载到 build 容器中，可选参数如下(不翻译了)</p><table><thead><tr><th>Option</th><th>Description</th></tr></thead><tbody><tr><td><code>target</code> (required)</td><td>Mount path.</td></tr><tr><td><code>source</code></td><td>Source path in the <code>from</code>. Defaults to the root of the <code>from</code>.</td></tr><tr><td><code>from</code></td><td>Build stage or image name for the root of the source. Defaults to the build context.</td></tr><tr><td><code>rw</code>,<code>readwrite</code></td><td>Allow writes on the mount. Written data will be discarded.</td></tr></tbody></table><ul><li><code>RUN --mount=type=cache</code></li></ul><p>专用于作为 cache 的挂载位置，一般用于 cache 包管理器的下载等</p><table><thead><tr><th>Option</th><th>Description</th></tr></thead><tbody><tr><td><code>id</code></td><td>Optional ID to identify separate/different caches</td></tr><tr><td><code>target</code> (required)</td><td>Mount path.</td></tr><tr><td><code>ro</code>,<code>readonly</code></td><td>Read-only if set.</td></tr><tr><td><code>sharing</code></td><td>One of <code>shared</code>, <code>private</code>, or <code>locked</code>. Defaults to <code>shared</code>. A <code>shared</code> cache mount can be used concurrently by multiple writers. <code>private</code> creates a new mount if there are multiple writers. <code>locked</code> pauses the second writer until the first one releases the mount.</td></tr><tr><td><code>from</code></td><td>Build stage to use as a base of the cache mount. Defaults to empty directory.</td></tr><tr><td><code>source</code></td><td>Subpath in the <code>from</code> to mount. Defaults to the root of the <code>from</code>.</td></tr></tbody></table><p><strong>Example: cache Go packages</strong></p><div class="hljs"><pre><code class="hljs sh"><span class="hljs-comment"># syntax = docker/dockerfile:experimental</span>FROM golang...RUN --mount=<span class="hljs-built_in">type</span>=cache,target=/root/.cache/go-build go build ...</code></pre></div><p><strong>Example: cache apt packages</strong></p><div class="hljs"><pre><code class="hljs sh"><span class="hljs-comment"># syntax = docker/dockerfile:experimental</span>FROM ubuntuRUN rm -f /etc/apt/apt.conf.d/docker-clean; <span class="hljs-built_in">echo</span> <span class="hljs-string">'Binary::apt::APT::Keep-Downloaded-Packages "true";'</span> &gt; /etc/apt/apt.conf.d/keep-cacheRUN --mount=<span class="hljs-built_in">type</span>=cache,target=/var/cache/apt --mount=<span class="hljs-built_in">type</span>=cache,target=/var/lib/apt \  apt update &amp;&amp; apt install -y gcc</code></pre></div><ul><li><code>RUN --mount=type=tmpfs</code></li></ul><p>专用于挂载 tmpfs 的选项</p><table><thead><tr><th>Option</th><th>Description</th></tr></thead><tbody><tr><td><code>target</code> (required)</td><td>Mount path.</td></tr></tbody></table><ul><li><code>RUN --mount=type=secret</code></li></ul><p>这个类似 k8s 的 secret，用来挂载一些不想打入镜像，但是构建时想使用的密钥等，例如 docker 的 <code>config.json</code>，S3 的 <code>credentials</code></p><table><thead><tr><th>Option</th><th>Description</th></tr></thead><tbody><tr><td><code>id</code></td><td>ID of the secret. Defaults to basename of the target path.</td></tr><tr><td><code>target</code></td><td>Mount path. Defaults to <code>/run/secrets/</code> + <code>id</code>.</td></tr><tr><td><code>required</code></td><td>If set to <code>true</code>, the instruction errors out when the secret is unavailable. Defaults to <code>false</code>.</td></tr><tr><td><code>mode</code></td><td>File mode for secret file in octal. Default 0400.</td></tr><tr><td><code>uid</code></td><td>User ID for secret file. Default 0.</td></tr><tr><td><code>gid</code></td><td>Group ID for secret file. Default 0.</td></tr></tbody></table><p><strong>Example: access to S3</strong></p><div class="hljs"><pre><code class="hljs sh"><span class="hljs-comment"># syntax = docker/dockerfile:experimental</span>FROM python:3RUN pip install awscliRUN --mount=<span class="hljs-built_in">type</span>=secret,id=aws,target=/root/.aws/credentials aws s3 cp s3://... ...</code></pre></div><p><strong>注意: <code>buildctl</code> 是 BuildKit 的命令，你要测试的话自己换成 <code>docker build</code> 相关参数</strong></p><div class="hljs"><pre><code class="hljs console"><span class="hljs-meta">$</span><span class="bash"> buildctl build --frontend=dockerfile.v0 --<span class="hljs-built_in">local</span> context=. --<span class="hljs-built_in">local</span> dockerfile=. \</span>  --secret id=aws,src=$HOME/.aws/credentials</code></pre></div><ul><li><code>RUN --mount=type=ssh</code></li></ul><p>允许 build 容器通过 SSH agent 访问 SSH key，并且支持 <code>passphrases</code></p><table><thead><tr><th>Option</th><th>Description</th></tr></thead><tbody><tr><td><code>id</code></td><td>ID of SSH agent socket or key. Defaults to “default”.</td></tr><tr><td><code>target</code></td><td>SSH agent socket path. Defaults to <code>/run/buildkit/ssh_agent.${N}</code>.</td></tr><tr><td><code>required</code></td><td>If set to <code>true</code>, the instruction errors out when the key is unavailable. Defaults to <code>false</code>.</td></tr><tr><td><code>mode</code></td><td>File mode for socket in octal. Default 0600.</td></tr><tr><td><code>uid</code></td><td>User ID for socket. Default 0.</td></tr><tr><td><code>gid</code></td><td>Group ID for socket. Default 0.</td></tr></tbody></table><p><strong>Example: access to Gitlab</strong></p><div class="hljs"><pre><code class="hljs sh"><span class="hljs-comment"># syntax = docker/dockerfile:experimental</span>FROM alpineRUN apk add --no-cache openssh-clientRUN mkdir -p -m 0700 ~/.ssh &amp;&amp; ssh-keyscan gitlab.com &gt;&gt; ~/.ssh/known_hostsRUN --mount=<span class="hljs-built_in">type</span>=ssh ssh -q -T git@gitlab.com 2&gt;&amp;1 | tee /hello<span class="hljs-comment"># "Welcome to GitLab, @GITLAB_USERNAME_ASSOCIATED_WITH_SSHKEY" should be printed here</span><span class="hljs-comment"># with the type of build progress is defined as `plain`.</span></code></pre></div><div class="hljs"><pre><code class="hljs sh">$ <span class="hljs-built_in">eval</span> $(ssh-agent)$ ssh-add ~/.ssh/id_rsa(Input your passphrase here)$ buildctl build --frontend=dockerfile.v0 --<span class="hljs-built_in">local</span> context=. --<span class="hljs-built_in">local</span> dockerfile=. \  --ssh default=<span class="hljs-variable">$SSH_AUTH_SOCK</span></code></pre></div><p>你也可以直接使用宿主机目录的 pem 文件，但是带有密码的 pem 目前不支持</p><p><strong>目前根据文档测试，当前的挂载类型比如 <code>cache</code> 类型，仅用于 multi-stage 内的挂载，比如你有 2+ 个构建步骤，<code>cache</code> 挂载类型能帮你在各个阶段内共享文件；但是它目前无法解决直接将宿主机目录挂载到 multi-stage 的问题(可以采取些曲线救国方案，但是很不优雅)；但是未来还是很有展望的，可以关注一下</strong></p>]]></content:encoded>
      
      
      <category domain="https://mritd.com/categories/docker/">Docker</category>
      
      
      <category domain="https://mritd.com/tags/docker/">Docker</category>
      
      
      <comments>https://mritd.com/2019/05/13/dockerfile-extended-syntax/#disqus_thread</comments>
      
    </item>
    
    <item>
      <title>Mac 下调校 Rime</title>
      <link>https://mritd.com/2019/03/23/oh-my-rime/</link>
      <guid>https://mritd.com/2019/03/23/oh-my-rime/</guid>
      <pubDate>Sat, 23 Mar 2019 13:03:43 GMT</pubDate>
      
      <description>由于对国内输入法隐私问题的担忧，决定放弃搜狗等输入法；为了更加 Geek 一些，最终决定了折腾 Rime(鼠须管) 输入法，以下为一些折腾的过程</description>
      
      
      
      <content:encoded><![CDATA[<blockquote><p>由于对国内输入法隐私问题的担忧，决定放弃搜狗等输入法；为了更加 Geek 一些，最终决定了折腾 Rime(鼠须管) 输入法，以下为一些折腾的过程</p></blockquote><p><strong>国际惯例先放点图压压惊</strong></p><p><img src="https://cdn.oss.link/markdown/t3otb.jpg" srcset="/img/loading.gif" alt="example1"><br><img src="https://cdn.oss.link/markdown/ep8sl.jpg" srcset="/img/loading.gif" alt="example2"><br><img src="https://cdn.oss.link/markdown/wth6n.jpg" srcset="/img/loading.gif" alt="example3"><br><img src="https://cdn.oss.link/markdown/5b85o.jpg" srcset="/img/loading.gif" alt="example4"></p><h2 id="一、安装"><a href="#一、安装" class="headerlink" title="一、安装"></a>一、安装</h2><p>安装 Rime 没啥好说的，直接从<a href="https://rime.im" target="_blank" rel="noopener">官网</a>下载最新版本的安装包既可；安装完成后配置文件位于 <code>~/Library/Rime</code> 位置；在进行后续折腾之前我建议还是先 <code>cp -r ~/Library/Rime ~/Library/Rime.bak</code> 备份一下配置文件，以防制后续折腾挂了还可以还原；安装完成以后按 <code>⌘ + 反引号(~)</code> 切换到 <code>朙月拼音-简化字</code> 既可开启简体中文输入</p><h2 id="二、乱码解决"><a href="#二、乱码解决" class="headerlink" title="二、乱码解决"></a>二、乱码解决</h2><p>安装完成后在打字时可能出现乱码情况(俗称豆腐块)，这是由于 Rime 默认 UTF-8 字符集比较大，预选词内会出现生僻字，而 mac 字体内又不包含这些字体，从而导致乱码；解决方案很简单，下载 <a href="https://github.com/mritd/rime/tree/master/fonts" target="_blank" rel="noopener">花园明朝</a> A、B 两款字体安装既可，安装后重启一下就不会出现乱码了</p><p><img src="https://cdn.oss.link/markdown/yfbis.png" srcset="/img/loading.gif" alt="fonts"></p><h2 id="三、配置文件"><a href="#三、配置文件" class="headerlink" title="三、配置文件"></a>三、配置文件</h2><p>官方并不建议直接修改原始的配置文件，因为输入法更新时会重新覆盖默认配置，可能导致某些自定义配置丢失；推荐作法是创建一系列的 patch 配置，通过类似打补丁替换这种方式来实现无感的增加自定义配置；</p><p>由于使用的是 <code>朙月拼音-简化字</code> 输入方案，所以需要创建 <code>luna_pinyin_simp.custom.yaml</code> 等配置文件，后面就是查文档 + 各种 Google 一顿魔改了；目前我将我自己用的配置放在了 <a href="https://github.com/mritd/rime" target="_blank" rel="noopener">Github</a> 上，有需要的可以直接 clone 下来，用里面的配置文件直接覆盖 <code>~/Library/Rime</code> 下的文件，然后重新部署既可，关于具体配置细节在下面写</p><h2 id="四、自定义配色"><a href="#四、自定义配色" class="headerlink" title="四、自定义配色"></a>四、自定义配色</h2><p>皮肤配色配置方案位于 <code>squirrel.custom.yaml</code> 配置文件中，我的配置目前是参考搜狗输入法皮肤自己调试的；官方也提供了一些皮肤外观配置，详见 <a href="https://gist.github.com/lotem/2290714" target="_blank" rel="noopener">Gist</a>；想要切换皮肤配色只需要修改 <code>style/color_scheme</code> 为相应的皮肤配色名称既可</p><div class="hljs"><pre><code class="hljs yaml"><span class="hljs-attr">patch:</span>  <span class="hljs-attr">show_notifications_when:</span> <span class="hljs-string">appropriate</span>          <span class="hljs-comment"># 状态通知，适当，也可设为全开（always）全关（never）</span>  <span class="hljs-attr">style/color_scheme:</span> <span class="hljs-string">mritd_dark</span>                <span class="hljs-comment"># 方案命名，不能有空格</span>  <span class="hljs-attr">preset_color_schemes:</span>    <span class="hljs-attr">mritd_dark:</span>      <span class="hljs-attr">name:</span> <span class="hljs-string">漠然／mritd</span> <span class="hljs-string">dark</span>      <span class="hljs-attr">author:</span> <span class="hljs-string">mritd</span> <span class="hljs-string">&lt;mritd1234@gmail.com&gt;</span>      <span class="hljs-attr">horizontal:</span> <span class="hljs-literal">true</span>                          <span class="hljs-comment"># 水平排列</span>      <span class="hljs-attr">inline_preedit:</span> <span class="hljs-literal">true</span>                      <span class="hljs-comment"># 单行显示，false双行显示</span>      <span class="hljs-attr">candidate_format:</span> <span class="hljs-string">"%c\u2005%@"</span>            <span class="hljs-comment"># 用 1/6 em 空格 U+2005 来控制编号 %c 和候选词 %@ 前后的空间。</span>      <span class="hljs-attr">corner_radius:</span> <span class="hljs-number">5</span>                          <span class="hljs-comment"># 候选条圆角</span>      <span class="hljs-attr">hilited_corner_radius:</span> <span class="hljs-number">3</span>                  <span class="hljs-comment"># 高亮圆角</span>      <span class="hljs-attr">border_height:</span> <span class="hljs-number">6</span>                          <span class="hljs-comment"># 窗口边界高度，大于圆角半径才生效</span>      <span class="hljs-attr">border_width:</span> <span class="hljs-number">6</span>                           <span class="hljs-comment"># 窗口边界宽度，大于圆角半径才生效</span>      <span class="hljs-attr">border_color_width:</span> <span class="hljs-number">0</span>      <span class="hljs-comment">#font_face: "PingFangSC"                   # 候选词字体</span>      <span class="hljs-attr">font_point:</span> <span class="hljs-number">16</span>                            <span class="hljs-comment"># 候选字词大小</span>      <span class="hljs-attr">label_font_point:</span> <span class="hljs-number">14</span>                      <span class="hljs-comment"># 候选编号大小</span>      <span class="hljs-attr">text_color:</span> <span class="hljs-number">0xdedddd</span>                      <span class="hljs-comment"># 拼音行文字颜色，24位色值，16进制，BGR顺序</span>      <span class="hljs-attr">back_color:</span> <span class="hljs-number">0x4b4b4b</span>                      <span class="hljs-comment"># 候选条背景色</span>      <span class="hljs-attr">label_color:</span> <span class="hljs-number">0x888785</span>                     <span class="hljs-comment"># 预选栏编号颜色</span>      <span class="hljs-attr">border_color:</span> <span class="hljs-number">0x4b4b4b</span>                    <span class="hljs-comment"># 边框色</span>      <span class="hljs-attr">candidate_text_color:</span> <span class="hljs-number">0xffffff</span>            <span class="hljs-comment"># 预选项文字颜色</span>      <span class="hljs-attr">hilited_text_color:</span> <span class="hljs-number">0xdedddd</span>              <span class="hljs-comment"># 高亮拼音 (需要开启内嵌编码)</span>      <span class="hljs-attr">hilited_back_color:</span> <span class="hljs-number">0x252320</span>              <span class="hljs-comment"># 高亮拼音 (需要开启内嵌编码)</span>      <span class="hljs-attr">hilited_candidate_text_color:</span> <span class="hljs-number">0xFFE696</span>    <span class="hljs-comment"># 第一候选项文字颜色</span>      <span class="hljs-attr">hilited_candidate_back_color:</span> <span class="hljs-number">0x4b4b4b</span>    <span class="hljs-comment"># 第一候选项背景背景色</span>      <span class="hljs-attr">hilited_candidate_label_color:</span> <span class="hljs-number">0xffffff</span>   <span class="hljs-comment"># 第一候选项编号颜色</span>      <span class="hljs-attr">comment_text_color:</span> <span class="hljs-number">0xdedddd</span>              <span class="hljs-comment"># 拼音等提示文字颜色</span></code></pre></div><h2 id="五、增加自定义快捷字符"><a href="#五、增加自定义快捷字符" class="headerlink" title="五、增加自定义快捷字符"></a>五、增加自定义快捷字符</h2><p>快捷字符例如在中文输入法状态下可以直接输入 <code>/dn</code> 来调出特殊符号输入；这些配置位于 <code>luna_pinyin_simp.custom.yaml</code> 的 <code>punctuator</code> 配置中，我目前自行定义了一些，有需要的可以依葫芦画瓢直接修改</p><div class="hljs"><pre><code class="hljs yaml"><span class="hljs-attr">punctuator:</span>    <span class="hljs-attr">import_preset:</span> <span class="hljs-string">symbols</span>    <span class="hljs-attr">symbols:</span>      <span class="hljs-string">"/fs"</span><span class="hljs-string">:</span> <span class="hljs-string">[½,‰,¼,⅓,⅔,¾,⅒]</span>      <span class="hljs-string">"/dq"</span><span class="hljs-string">:</span> <span class="hljs-string">[🌍,🌎,🌏,🌐,🌑,🌒,🌓,🌔,🌕,🌖,🌗,🌘,🌙,🌚,🌛,🌜,🌝,🌞,⭐,🌟,🌠,⛅,⚡,❄,🔥,💧,🌊]</span>      <span class="hljs-string">"/jt"</span><span class="hljs-string">:</span> <span class="hljs-string">[⬆,↗,➡,↘,⬇,↙,⬅,↖,↕,↔,↩,↪,⤴,⤵,🔃,🔄,🔙,🔚,🔛,🔜,🔝]</span>      <span class="hljs-string">"/sg"</span><span class="hljs-string">:</span> <span class="hljs-string">[🍇,🍈,🍉,🍊,🍋,🍌,🍍,🍎,🍏,🍐,🍑,🍒,🍓,🍅,🍆,🌽,🍄,🌰,🍞,🍖,🍗,🍔,🍟,🍕,🍳,🍲,🍱,🍘,🍙,🍚,🍛,🍜,🍝,🍠,🍢,🍣,🍤,🍥,🍡,🍦,🍧,🍨,🍩,🍪,🎂,🍰,🍫,🍬,🍭,🍮,🍯,🍼,🍵,🍶,🍷,🍸,🍹,🍺,🍻,🍴]</span>      <span class="hljs-string">"/dw"</span><span class="hljs-string">:</span> <span class="hljs-string">[🙈,🙉,🙊,🐵,🐒,🐶,🐕,🐩,🐺,🐱,😺,😸,😹,😻,😼,😽,🙀,😿,😾,🐈,🐯,🐅,🐆,🐴,🐎,🐮,🐂,🐃,🐄,🐷,🐖,🐗,🐽,🐏,🐑,🐐,🐪,🐫,🐘,🐭,🐁,🐀,🐹,🐰,🐇,🐻,🐨,🐼,🐾,🐔,🐓,🐣,🐤,🐥,🐦,🐧,🐸,🐊,🐢,🐍,🐲,🐉,🐳,🐋,🐬,🐟,🐠,🐡,🐙,🐚,🐌,🐛,🐜,🐝,🐞,🦋]</span>      <span class="hljs-string">"/bq"</span><span class="hljs-string">:</span> <span class="hljs-string">[😀,😁,😂,😃,😄,😅,😆,😉,😊,😋,😎,😍,😘,😗,😙,😚,😇,😐,😑,😶,😏,😣,😥,😮,😯,😪,😫,😴,😌,😛,😜,😝,😒,😓,😔,😕,😲,😷,😖,😞,😟,😤,😢,😭,😦,😧,😨,😬,😰,😱,😳,😵,😡,😠]</span>      <span class="hljs-string">"/ss"</span><span class="hljs-string">:</span> <span class="hljs-string">[💪,👈,👉,👆,👇,✋,👌,👍,👎,✊,👊,👋,👏,👐]</span>      <span class="hljs-string">"/dn"</span><span class="hljs-string">:</span> <span class="hljs-string">[⌘,</span> <span class="hljs-string">⌥,</span> <span class="hljs-string">⇧,</span> <span class="hljs-string">⌃,</span> <span class="hljs-string">⎋,</span> <span class="hljs-string">⇪,</span> <span class="hljs-string">,</span> <span class="hljs-string">⌫,</span> <span class="hljs-string">⌦,</span> <span class="hljs-string">↩︎,</span> <span class="hljs-string">⏎,</span> <span class="hljs-string">↑,</span> <span class="hljs-string">↓,</span> <span class="hljs-string">←,</span> <span class="hljs-string">→,</span> <span class="hljs-string">↖,</span> <span class="hljs-string">↘,</span> <span class="hljs-string">⇟,</span> <span class="hljs-string">⇞]</span>      <span class="hljs-string">"/fh"</span><span class="hljs-string">:</span> <span class="hljs-string">[©,®,℗,ⓘ,℠,™,℡,␡,♂,♀,☉,☊,☋,☌,☍,☑︎,☒,☜,☝,☞,☟,✎,✄,♻,⚐,⚑,⚠]</span>      <span class="hljs-string">"/xh"</span><span class="hljs-string">:</span> <span class="hljs-string">[＊,×,✱,★,☆,✩,✧,❋,❊,❉,❈,❅,✿,✲]</span></code></pre></div><h2 id="六、设置输入方案"><a href="#六、设置输入方案" class="headerlink" title="六、设置输入方案"></a>六、设置输入方案</h2><p>在第一次按 <code>⌘ + 反引号(~)</code> 设置输入法时实际上我们可以看到很多的输入方案，而事实上很多方案我们根本用不上；想要删除和修改方案可以调整 <code>default.custom.yaml</code> 中的 <code>schema_list</code> 字段</p><div class="hljs"><pre><code class="hljs yaml"><span class="hljs-attr">patch:</span>  <span class="hljs-attr">menu:</span>    <span class="hljs-attr">page_size:</span> <span class="hljs-number">8</span>  <span class="hljs-attr">schema_list:</span>  <span class="hljs-bullet">-</span> <span class="hljs-attr">schema:</span> <span class="hljs-string">luna_pinyin_simp</span>      <span class="hljs-comment"># 朙月拼音 简化字</span>  <span class="hljs-bullet">-</span> <span class="hljs-attr">schema:</span> <span class="hljs-string">luna_pinyin</span>           <span class="hljs-comment"># 朙月拼音</span>  <span class="hljs-bullet">-</span> <span class="hljs-attr">schema:</span> <span class="hljs-string">luna_pinyin_fluency</span>   <span class="hljs-comment"># 语句流</span><span class="hljs-comment">#  - schema: double_pinyin         # 自然碼雙拼</span><span class="hljs-comment">#  - schema: double_pinyin_flypy   # 小鹤雙拼</span><span class="hljs-comment">#  - schema: double_pinyin_pyjj    # 拼音加加双拼</span><span class="hljs-comment">#  - schema: wubi_pinyin           # 五笔拼音混合輸入</span></code></pre></div><p><strong>实际上我只能用上第一个…毕竟写了好几年代码还得看键盘的人也只能这样了…</strong></p><h2 id="七、调整特殊键行为"><a href="#七、调整特殊键行为" class="headerlink" title="七、调整特殊键行为"></a>七、调整特殊键行为</h2><p>在刚安装完以后发现在中文输入法状态下输入英文，按 <code>shift</code> 键后字符上屏，然后还得回车一下，这就很让我难受…最后找到了这篇 <a href="https://gist.github.com/lotem/2981316" target="_blank" rel="noopener">Gist</a>，目前将大写锁定、<code>shift</code> 键调整为了跟搜狗一致的配置，有需要调整的可以自行编辑 <code>default.custom.yaml</code> 中的 <code>ascii_composer/switch_key</code> 部分</p><div class="hljs"><pre><code class="hljs yaml"><span class="hljs-comment"># capslock 键切换英文并输出大写</span><span class="hljs-attr">ascii_composer/good_old_caps_lock:</span> <span class="hljs-literal">true</span><span class="hljs-comment"># 输入法中英文状态快捷键</span><span class="hljs-attr">ascii_composer/switch_key:</span>  <span class="hljs-attr">Caps_Lock:</span> <span class="hljs-string">commit_code</span>  <span class="hljs-attr">Control_L:</span> <span class="hljs-string">noop</span>  <span class="hljs-attr">Control_R:</span> <span class="hljs-string">noop</span>  <span class="hljs-comment"># 按下左 shift 英文字符直接上屏，不需要再次回车，输入法保持英文状态</span>  <span class="hljs-attr">Shift_L:</span> <span class="hljs-string">commit_code</span>  <span class="hljs-attr">Shift_R:</span> <span class="hljs-string">noop</span></code></pre></div><h2 id="八、自定义词库"><a href="#八、自定义词库" class="headerlink" title="八、自定义词库"></a>八、自定义词库</h2><p>Rime 默认的词库稍为有点弱，我们可以下载一些搜狗词库来进行扩展；不过搜狗词库格式默认是无法解析的，好在有人开发了工具可以方便的将搜狗细胞词库转化为 Rime 的格式(工具<a href="https://github.com/studyzy/imewlconverter/releases" target="_blank" rel="noopener">点击这里</a>下载)；目前该工具只支持 Windows(也有些别人写的 py 脚本啥的，但是我没用)，所以词库转换这种操作还得需要一个 Windows 虚拟机；</p><p>转换过程很简单，先从<a href="https://pinyin.sogou.com/dict/" target="_blank" rel="noopener">搜狗词库</a>下载一系列的 <code>scel</code> 文件，然后批量选中，接着调整一下输入和输出格式点击转换，最后保存成一个 <code>txt</code> 文本</p><p><img src="https://cdn.oss.link/markdown/jtv97.png" srcset="/img/loading.gif" alt="input-setting"></p><p><img src="https://cdn.oss.link/markdown/p7qha.png" srcset="/img/loading.gif" alt="convert"></p><p>光有这个文本还不够，我们要将它塞到词库的 <code>yaml</code> 配置里，所以新建一个词库配置文件 <code>luna_pinyin.sougou.dict.yaml</code>，然后写上头部说明(<strong>注意最后三个点后面加一个换行</strong>)</p><div class="hljs"><pre><code class="hljs yaml"><span class="hljs-comment"># Rime dictionary</span><span class="hljs-comment"># encoding: utf-8</span><span class="hljs-comment"># 搜狗词库 目前包含如下:</span><span class="hljs-comment"># IT计算机 实用IT词汇 亲戚称呼 化学品名 数字时间 数学词汇 淘宝词库 编程语言 软件专业 颜色名称 程序猿词库 开发专用词库 搜狗标准词库</span><span class="hljs-comment"># 摄影专业名词 计算机专业词库 计算机词汇大全 保险词汇 最详细的全国地名大全 饮食大全 常见花卉名称 房地产词汇大全 中国传统节日大全 财经金融词汇大全</span><span class="hljs-meta">---</span><span class="hljs-attr">name:</span> <span class="hljs-string">luna_pinyin.sougou</span><span class="hljs-attr">version:</span> <span class="hljs-string">"1.0"</span><span class="hljs-attr">sort:</span> <span class="hljs-string">by_weight</span><span class="hljs-attr">use_preset_vocabulary:</span> <span class="hljs-literal">true</span><span class="hljs-string">...</span></code></pre></div><p>接着只需要把生成好的词库 <code>txt</code> 文件内容粘贴到三个点下面既可；但是词库太多的话你会发现这个文本有好几十 M，一般编辑器打开都会卡死，解决这种情况只需要用命令行 <code>cat</code> 一下就行</p><div class="hljs"><pre><code class="hljs sh">cat sougou.txt &gt;&gt; luna_pinyin.sougou.dict.yaml</code></pre></div><p>最后修改 <code>luna_pinyin.extended.dict.yaml</code> 中的 <code>import_tables</code> 字段，加入刚刚新建的词库既可</p><div class="hljs"><pre><code class="hljs yaml"><span class="hljs-meta">---</span><span class="hljs-attr">name:</span> <span class="hljs-string">luna_pinyin.extended</span><span class="hljs-attr">version:</span> <span class="hljs-string">"2016.06.26"</span><span class="hljs-attr">sort:</span> <span class="hljs-string">by_weight</span>  <span class="hljs-comment">#字典初始排序，可選original或by_weight</span><span class="hljs-attr">use_preset_vocabulary:</span> <span class="hljs-literal">true</span><span class="hljs-comment">#此處爲明月拼音擴充詞庫（基本）默認鏈接載入的詞庫，有朙月拼音官方詞庫、明月拼音擴充詞庫（漢語大詞典）、明月拼音擴充詞庫（詩詞）、明月拼音擴充詞庫（含西文的詞彙）。如果不需要加載某个詞庫請將其用「#」註釋掉。</span><span class="hljs-comment">#雙拼不支持 luna_pinyin.cn_en 詞庫，請用戶手動禁用。</span><span class="hljs-attr">import_tables:</span>  <span class="hljs-bullet">-</span> <span class="hljs-string">luna_pinyin</span>  <span class="hljs-comment"># 加入搜狗词库</span>  <span class="hljs-bullet">-</span> <span class="hljs-string">luna_pinyin.sougou</span>  <span class="hljs-bullet">-</span> <span class="hljs-string">luna_pinyin.poetry</span>  <span class="hljs-bullet">-</span> <span class="hljs-string">luna_pinyin.cn_en</span>  <span class="hljs-bullet">-</span> <span class="hljs-string">luna_pinyin.kaomoji</span></code></pre></div><h2 id="九、定制特殊单词"><a href="#九、定制特殊单词" class="headerlink" title="九、定制特殊单词"></a>九、定制特殊单词</h2><p>由于长期撸码，24 小时离不开命令行，偶尔在中文输入法下输入了一些命令导致汉字直接出现在 terminal 上就很尴尬…这时候我们可以在 <code>luna_pinyin.cn_en.dict.yaml</code> 加入一些我们自己的专属词库，比如这样</p><div class="hljs"><pre><code class="hljs yaml"><span class="hljs-meta">---</span><span class="hljs-attr">name:</span> <span class="hljs-string">luna_pinyin.cn_en</span><span class="hljs-attr">version:</span> <span class="hljs-string">"2017.9.13"</span><span class="hljs-attr">sort:</span> <span class="hljs-string">by_weight</span><span class="hljs-attr">use_preset_vocabulary:</span> <span class="hljs-literal">true</span><span class="hljs-string">...</span><span class="hljs-string">git</span><span class="hljs-string">git</span><span class="hljs-string">ls</span><span class="hljs-string">ls</span><span class="hljs-string">cd</span><span class="hljs-string">cd</span><span class="hljs-string">pwd</span><span class="hljs-string">pwd</span><span class="hljs-string">git</span> <span class="hljs-string">ps</span><span class="hljs-string">gitps</span><span class="hljs-string">kubernetes</span><span class="hljs-string">kubernetes</span><span class="hljs-string">kubernetes</span><span class="hljs-string">kuber</span><span class="hljs-string">kubectl</span><span class="hljs-string">kubectl</span><span class="hljs-string">kubectl</span><span class="hljs-string">kubec</span><span class="hljs-string">docker</span><span class="hljs-string">docker</span><span class="hljs-string">docker</span><span class="hljs-string">dock</span><span class="hljs-string">ipvs</span><span class="hljs-string">ipvs</span><span class="hljs-string">ps</span><span class="hljs-string">ps</span><span class="hljs-string">bash</span><span class="hljs-string">bash</span><span class="hljs-string">source</span><span class="hljs-string">source</span><span class="hljs-string">source</span><span class="hljs-string">sou</span><span class="hljs-string">rm</span><span class="hljs-string">rm</span></code></pre></div><p>配置后如果我在中文输入法下输入 git 则会自动匹配 git 这个单词，避免错误的键入中文字符；<strong>需要注意的是第一列代表上屏的字符，第二列代表输入的单词，即 “当输入第二列时候选词为第一列”；两列之间要用 tag 制表符隔开，记住不是空格</strong></p>]]></content:encoded>
      
      
      <category domain="https://mritd.com/categories/mac/">Mac</category>
      
      
      <category domain="https://mritd.com/tags/mac/">Mac</category>
      
      <category domain="https://mritd.com/tags/rime/">Rime</category>
      
      
      <comments>https://mritd.com/2019/03/23/oh-my-rime/#disqus_thread</comments>
      
    </item>
    
    <item>
      <title>Ubuntu 设置多个源</title>
      <link>https://mritd.com/2019/03/19/how-to-set-multiple-apt-mirrors-for-ubuntu/</link>
      <guid>https://mritd.com/2019/03/19/how-to-set-multiple-apt-mirrors-for-ubuntu/</guid>
      <pubDate>Tue, 19 Mar 2019 13:43:23 GMT</pubDate>
      
      <description>介绍通过 mirror 方式来设置 Ubuntu 源，从而实现自动切换 apt 源下载</description>
      
      
      
      <content:encoded><![CDATA[<h2 id="一、源起"><a href="#一、源起" class="headerlink" title="一、源起"></a>一、源起</h2><p>使用 Ubuntu 作为生产容器系统好久了，但是 apt 源问题一致有点困扰: <strong>由于众所周知的原因，官方源执行 <code>apt update</code> 等命令会非常慢；而国内有很多镜像服务，但是某些偶尔也会抽风(比如清华大源)，最后的结果就是日常修改 apt 源…</strong>Google 查了了好久发现事实上 apt 源是支持 <code>mirror</code> 协议的，从而自动选择可用的一个</p><h2 id="二、使用-mirror-协议"><a href="#二、使用-mirror-协议" class="headerlink" title="二、使用 mirror 协议"></a>二、使用 mirror 协议</h2><p>废话不说多直接上代码，编辑 <code>/etc/apt/sources.list</code>，替换为如下内容</p><div class="hljs"><pre><code class="hljs sh"><span class="hljs-comment">#------------------------------------------------------------------------------#</span><span class="hljs-comment">#                            OFFICIAL UBUNTU REPOS                             #</span><span class="hljs-comment">#------------------------------------------------------------------------------#</span><span class="hljs-comment">###### Ubuntu Main Repos</span>deb mirror://mirrors.ubuntu.com/mirrors.txt bionic main restricted universe multiversedeb-src mirror://mirrors.ubuntu.com/mirrors.txt bionic main restricted universe multiverse<span class="hljs-comment">###### Ubuntu Update Repos</span>deb mirror://mirrors.ubuntu.com/mirrors.txt bionic-security main restricted universe multiversedeb mirror://mirrors.ubuntu.com/mirrors.txt bionic-updates main restricted universe multiversedeb mirror://mirrors.ubuntu.com/mirrors.txt bionic-backports main restricted universe multiversedeb-src mirror://mirrors.ubuntu.com/mirrors.txt bionic-security main restricted universe multiversedeb-src mirror://mirrors.ubuntu.com/mirrors.txt bionic-updates main restricted universe multiversedeb-src mirror://mirrors.ubuntu.com/mirrors.txt bionic-backports main restricted universe multiverse</code></pre></div><p>当使用 <code>mirror</code> 协议后，执行 <code>apt update</code> 时会首先<strong>通过 http 访问</strong> <code>mirrors.ubuntu.com/mirrors.txt</code> 文本；文本内容实际上就是当前可用的镜像源列表，如下所示</p><div class="hljs"><pre><code class="hljs sh">http://ftp.sjtu.edu.cn/ubuntu/http://mirrors.nju.edu.cn/ubuntu/http://mirrors.nwafu.edu.cn/ubuntu/http://mirrors.sohu.com/ubuntu/http://mirrors.aliyun.com/ubuntu/http://mirrors.shu.edu.cn/ubuntu/http://mirrors.cqu.edu.cn/ubuntu/http://mirrors.huaweicloud.com/repository/ubuntu/http://mirrors.cn99.com/ubuntu/http://mirrors.yun-idc.com/ubuntu/http://mirrors.tuna.tsinghua.edu.cn/ubuntu/http://mirrors.ustc.edu.cn/ubuntu/http://mirrors.njupt.edu.cn/ubuntu/http://mirror.lzu.edu.cn/ubuntu/http://archive.ubuntu.com/ubuntu/</code></pre></div><p>得到列表后 apt 会自动选择一个(选择规则暂不清楚，国外有文章说是选择最快的，但是不清楚这个最快是延迟还是网速)进行下载；<strong>同时根据地区不通，官方也提供指定国家的 <code>mirror.txt</code></strong>，比如中国的实际上可以设置为 <code>mirrors.ubuntu.com/CN.txt</code>(我测试跟官方一样，推测可能是使用了类似 DNS 选优的策略)</p><h2 id="三、自定义-mirror-地址"><a href="#三、自定义-mirror-地址" class="headerlink" title="三、自定义 mirror 地址"></a>三、自定义 mirror 地址</h2><p>现在已经解决了能同时使用多个源的问题，但是有些时候你会发现源的可用性检测并不是很精准，比如某个源只有 40k 的下载速度…不巧你某个下载还命中了，这就很尴尬；<strong>所以有时候我们可能需要自定义 <code>mirror.txt</code> 这个源列表</strong>，经过测试证明<strong>只需要开启一个标准的 <code>http server</code> 能返回一个文本即可，不过需要注意只能是 <code>http</code>，而不是 <code>https</code></strong>；所以我们首先下载一下这个文本，把不想要的删掉；然后弄个 nginx，甚至 <code>python -m http.server</code> 把文本文件暴露出去就可以；我比较懒…扔 CDN 上了: <a href="http://oss.link/config/apt-mirrors.txt" target="_blank" rel="noopener">http://oss.link/config/apt-mirrors.txt</a></p><p>关于源的精简，我建议将一些 <code>edu</code> 的删掉，因为敏感时期他们很不稳定；优选阿里云、网易、华为这种大公司的，比较有名的清华大的什么的可以留着，其他的可以考虑都删掉</p>]]></content:encoded>
      
      
      <category domain="https://mritd.com/categories/linux/">Linux</category>
      
      
      <category domain="https://mritd.com/tags/linux/">Linux</category>
      
      
      <comments>https://mritd.com/2019/03/19/how-to-set-multiple-apt-mirrors-for-ubuntu/#disqus_thread</comments>
      
    </item>
    
    <item>
      <title>Kubernetes 1.13.4 搭建</title>
      <link>https://mritd.com/2019/03/16/set-up-kubernetes-1.13.4-cluster/</link>
      <guid>https://mritd.com/2019/03/16/set-up-kubernetes-1.13.4-cluster/</guid>
      <pubDate>Sat, 16 Mar 2019 09:36:52 GMT</pubDate>
      
      <description>年后回来有点懒，也有点忙；1.13 出来好久了，周末还是决定折腾一下吧</description>
      
      
      
      <content:encoded><![CDATA[<blockquote><p>年后回来有点懒，也有点忙；1.13 出来好久了，周末还是决定折腾一下吧</p></blockquote><h2 id="一、环境准备"><a href="#一、环境准备" class="headerlink" title="一、环境准备"></a>一、环境准备</h2><p>老样子，安装环境为 5 台 Ubuntu 18.04.2 LTS 虚拟机，其他详细信息如下</p><table><thead><tr><th>System OS</th><th>IP Address</th><th>Docker</th><th>Kernel</th><th>Application</th></tr></thead><tbody><tr><td>Ubuntu 18.04.2 LTS</td><td>192.168.1.51</td><td>18.09.2</td><td>4.15.0-46-generic</td><td>k8s-master、etcd</td></tr><tr><td>Ubuntu 18.04.2 LTS</td><td>192.168.1.52</td><td>18.09.2</td><td>4.15.0-46-generic</td><td>k8s-master、etcd</td></tr><tr><td>Ubuntu 18.04.2 LTS</td><td>192.168.1.53</td><td>18.09.2</td><td>4.15.0-46-generic</td><td>k8s-master、etcd</td></tr><tr><td>Ubuntu 18.04.2 LTS</td><td>192.168.1.54</td><td>18.09.2</td><td>4.15.0-46-generic</td><td>k8s-node</td></tr><tr><td>Ubuntu 18.04.2 LTS</td><td>192.168.1.55</td><td>18.09.2</td><td>4.15.0-46-generic</td><td>k8s-node</td></tr></tbody></table><p><strong>所有配置生成将在第一个节点上完成，第一个节点与其他节点 root 用户免密码登录，用于分发文件；为了方便搭建弄了一点小脚本，仓库地址 <a href="https://github.com/mritd/ktool" target="_blank" rel="noopener">ktool</a>，本文后续所有脚本、配置都可以在此仓库找到；关于 <a href="https://github.com/cloudflare/cfssl" target="_blank" rel="noopener">cfssl</a> 等基本工具使用，本文不再阐述</strong></p><h2 id="二、安装-Etcd"><a href="#二、安装-Etcd" class="headerlink" title="二、安装 Etcd"></a>二、安装 Etcd</h2><h3 id="2-1、生成证书"><a href="#2-1、生成证书" class="headerlink" title="2.1、生成证书"></a>2.1、生成证书</h3><p>Etcd 仍然开启 TLS 认证，所以先使用 cfssl 生成相关证书</p><ul><li>etcd-root-ca-csr.json</li></ul><div class="hljs"><pre><code class="hljs json">&#123;    <span class="hljs-attr">"CN"</span>: <span class="hljs-string">"etcd-root-ca"</span>,    <span class="hljs-attr">"key"</span>: &#123;        <span class="hljs-attr">"algo"</span>: <span class="hljs-string">"rsa"</span>,        <span class="hljs-attr">"size"</span>: <span class="hljs-number">4096</span>    &#125;,    <span class="hljs-attr">"names"</span>: [        &#123;            <span class="hljs-attr">"O"</span>: <span class="hljs-string">"etcd"</span>,            <span class="hljs-attr">"OU"</span>: <span class="hljs-string">"etcd Security"</span>,            <span class="hljs-attr">"L"</span>: <span class="hljs-string">"Beijing"</span>,            <span class="hljs-attr">"ST"</span>: <span class="hljs-string">"Beijing"</span>,            <span class="hljs-attr">"C"</span>: <span class="hljs-string">"CN"</span>        &#125;    ],    <span class="hljs-attr">"ca"</span>: &#123;        <span class="hljs-attr">"expiry"</span>: <span class="hljs-string">"87600h"</span>    &#125;&#125;</code></pre></div><ul><li>etcd-gencert.json</li></ul><div class="hljs"><pre><code class="hljs json">&#123;  <span class="hljs-attr">"signing"</span>: &#123;    <span class="hljs-attr">"default"</span>: &#123;        <span class="hljs-attr">"usages"</span>: [          <span class="hljs-string">"signing"</span>,          <span class="hljs-string">"key encipherment"</span>,          <span class="hljs-string">"server auth"</span>,          <span class="hljs-string">"client auth"</span>        ],        <span class="hljs-attr">"expiry"</span>: <span class="hljs-string">"87600h"</span>    &#125;  &#125;&#125;</code></pre></div><ul><li>etcd-csr.json</li></ul><div class="hljs"><pre><code class="hljs json">&#123;    <span class="hljs-attr">"key"</span>: &#123;        <span class="hljs-attr">"algo"</span>: <span class="hljs-string">"rsa"</span>,        <span class="hljs-attr">"size"</span>: <span class="hljs-number">2048</span>    &#125;,    <span class="hljs-attr">"names"</span>: [        &#123;            <span class="hljs-attr">"O"</span>: <span class="hljs-string">"etcd"</span>,            <span class="hljs-attr">"OU"</span>: <span class="hljs-string">"etcd Security"</span>,            <span class="hljs-attr">"L"</span>: <span class="hljs-string">"Beijing"</span>,            <span class="hljs-attr">"ST"</span>: <span class="hljs-string">"Beijing"</span>,            <span class="hljs-attr">"C"</span>: <span class="hljs-string">"CN"</span>        &#125;    ],    <span class="hljs-attr">"CN"</span>: <span class="hljs-string">"etcd"</span>,    <span class="hljs-attr">"hosts"</span>: [        <span class="hljs-string">"127.0.0.1"</span>,        <span class="hljs-string">"localhost"</span>,        <span class="hljs-string">"192.168.1.51"</span>,        <span class="hljs-string">"192.168.1.52"</span>,        <span class="hljs-string">"192.168.1.53"</span>    ]&#125;</code></pre></div><p>接下来执行生成即可；<strong>我建议在生产环境在证书内预留几个 IP，已防止意外故障迁移时还需要重新生成证书；证书默认期限为 10 年(包括 CA 证书)，有需要加强安全性的可以适当减小</strong></p><div class="hljs"><pre><code class="hljs sh">cfssl gencert --initca=<span class="hljs-literal">true</span> etcd-root-ca-csr.json | cfssljson --bare etcd-root-cacfssl gencert --ca etcd-root-ca.pem --ca-key etcd-root-ca-key.pem --config etcd-gencert.json etcd-csr.json | cfssljson --bare etcd</code></pre></div><h3 id="2-2、安装-Etcd"><a href="#2-2、安装-Etcd" class="headerlink" title="2.2、安装 Etcd"></a>2.2、安装 Etcd</h3><h4 id="2-2-1、安装脚本"><a href="#2-2-1、安装脚本" class="headerlink" title="2.2.1、安装脚本"></a>2.2.1、安装脚本</h4><p>安装 Etcd 只需要将二进制文件放在可执行目录下，然后修改配置增加 systemd service 配置文件即可；为了安全性起见最好使用单独的用户启动 Etcd</p><div class="hljs"><pre><code class="hljs sh"><span class="hljs-meta">#!/bin/bash</span><span class="hljs-built_in">set</span> -eETCD_DEFAULT_VERSION=<span class="hljs-string">"3.3.12"</span><span class="hljs-keyword">if</span> [ <span class="hljs-string">"<span class="hljs-variable">$1</span>"</span> != <span class="hljs-string">""</span> ]; <span class="hljs-keyword">then</span>  ETCD_VERSION=<span class="hljs-variable">$1</span><span class="hljs-keyword">else</span>  <span class="hljs-built_in">echo</span> -e <span class="hljs-string">"\033[33mWARNING: ETCD_VERSION is blank,use default version: <span class="hljs-variable">$&#123;ETCD_DEFAULT_VERSION&#125;</span>\033[0m"</span>  ETCD_VERSION=<span class="hljs-variable">$&#123;ETCD_DEFAULT_VERSION&#125;</span><span class="hljs-keyword">fi</span><span class="hljs-comment"># 下载 Etcd 二进制文件</span><span class="hljs-keyword">function</span> <span class="hljs-function"><span class="hljs-title">download</span></span>()&#123;    <span class="hljs-keyword">if</span> [ ! -f <span class="hljs-string">"etcd-v<span class="hljs-variable">$&#123;ETCD_VERSION&#125;</span>-linux-amd64.tar.gz"</span> ]; <span class="hljs-keyword">then</span>        wget https://github.com/coreos/etcd/releases/download/v<span class="hljs-variable">$&#123;ETCD_VERSION&#125;</span>/etcd-v<span class="hljs-variable">$&#123;ETCD_VERSION&#125;</span>-linux-amd64.tar.gz        tar -zxvf etcd-v<span class="hljs-variable">$&#123;ETCD_VERSION&#125;</span>-linux-amd64.tar.gz    <span class="hljs-keyword">fi</span>&#125;<span class="hljs-comment"># 为 Etcd 创建单独的用户</span><span class="hljs-keyword">function</span> <span class="hljs-function"><span class="hljs-title">preinstall</span></span>()&#123;getent group etcd &gt;/dev/null || groupadd -r etcdgetent passwd etcd &gt;/dev/null || useradd -r -g etcd -d /var/lib/etcd -s /sbin/nologin -c <span class="hljs-string">"etcd user"</span> etcd&#125;<span class="hljs-comment"># 安装(复制文件)</span><span class="hljs-keyword">function</span> <span class="hljs-function"><span class="hljs-title">install</span></span>()&#123;    <span class="hljs-comment"># 释放 Etcd 二进制文件</span>    <span class="hljs-built_in">echo</span> -e <span class="hljs-string">"\033[32mINFO: Copy etcd...\033[0m"</span>    tar -zxvf etcd-v<span class="hljs-variable">$&#123;ETCD_VERSION&#125;</span>-linux-amd64.tar.gz    cp etcd-v<span class="hljs-variable">$&#123;ETCD_VERSION&#125;</span>-linux-amd64/etcd* /usr/<span class="hljs-built_in">local</span>/bin    rm -rf etcd-v<span class="hljs-variable">$&#123;ETCD_VERSION&#125;</span>-linux-amd64    <span class="hljs-comment"># 复制 配置文件 到 /etc/etcd(目录内文件结构在下面)</span>    <span class="hljs-built_in">echo</span> -e <span class="hljs-string">"\033[32mINFO: Copy etcd config...\033[0m"</span>    cp -r conf /etc/etcd    chown -R etcd:etcd /etc/etcd    chmod -R 755 /etc/etcd/ssl    <span class="hljs-comment"># 复制 systemd service 配置</span>    <span class="hljs-built_in">echo</span> -e <span class="hljs-string">"\033[32mINFO: Copy etcd systemd config...\033[0m"</span>    cp systemd/*.service /lib/systemd/system    systemctl daemon-reload&#125;<span class="hljs-comment"># 创建 Etcd 存储目录(如需要更改，请求改 /etc/etcd/etcd.conf 配置文件)</span><span class="hljs-keyword">function</span> <span class="hljs-function"><span class="hljs-title">postinstall</span></span>()&#123;    <span class="hljs-keyword">if</span> [ ! -d <span class="hljs-string">"/var/lib/etcd"</span> ]; <span class="hljs-keyword">then</span>        mkdir /var/lib/etcd        chown -R etcd:etcd /var/lib/etcd    <span class="hljs-keyword">fi</span>&#125;<span class="hljs-comment"># 依次执行</span>downloadpreinstallinstallpostinstall</code></pre></div><h4 id="2-2-2、配置文件"><a href="#2-2-2、配置文件" class="headerlink" title="2.2.2、配置文件"></a>2.2.2、配置文件</h4><p><strong>关于配置文件目录结构如下(请自行复制证书)</strong></p><div class="hljs"><pre><code class="hljs sh">conf├── etcd.conf├── etcd.conf.cluster.example├── etcd.conf.single.example└── ssl    ├── etcd-key.pem    ├── etcd.pem    ├── etcd-root-ca-key.pem    └── etcd-root-ca.pem1 directory, 7 files</code></pre></div><ul><li>etcd.conf</li></ul><div class="hljs"><pre><code class="hljs sh"><span class="hljs-comment"># [member]</span>ETCD_NAME=etcd1ETCD_DATA_DIR=<span class="hljs-string">"/var/lib/etcd/data"</span>ETCD_WAL_DIR=<span class="hljs-string">"/var/lib/etcd/wal"</span>ETCD_SNAPSHOT_COUNT=<span class="hljs-string">"100"</span>ETCD_HEARTBEAT_INTERVAL=<span class="hljs-string">"100"</span>ETCD_ELECTION_TIMEOUT=<span class="hljs-string">"1000"</span>ETCD_LISTEN_PEER_URLS=<span class="hljs-string">"https://192.168.1.51:2380"</span>ETCD_LISTEN_CLIENT_URLS=<span class="hljs-string">"https://192.168.1.51:2379,http://127.0.0.1:2379"</span>ETCD_MAX_SNAPSHOTS=<span class="hljs-string">"5"</span>ETCD_MAX_WALS=<span class="hljs-string">"5"</span><span class="hljs-comment">#ETCD_CORS=""</span><span class="hljs-comment"># [cluster]</span>ETCD_INITIAL_ADVERTISE_PEER_URLS=<span class="hljs-string">"https://192.168.1.51:2380"</span><span class="hljs-comment"># if you use different ETCD_NAME (e.g. test), set ETCD_INITIAL_CLUSTER value for this name, i.e. "test=http://..."</span>ETCD_INITIAL_CLUSTER=<span class="hljs-string">"etcd1=https://192.168.1.51:2380,etcd2=https://192.168.1.52:2380,etcd3=https://192.168.1.53:2380"</span>ETCD_INITIAL_CLUSTER_STATE=<span class="hljs-string">"new"</span>ETCD_INITIAL_CLUSTER_TOKEN=<span class="hljs-string">"etcd-cluster"</span>ETCD_ADVERTISE_CLIENT_URLS=<span class="hljs-string">"https://192.168.1.51:2379"</span><span class="hljs-comment">#ETCD_DISCOVERY=""</span><span class="hljs-comment">#ETCD_DISCOVERY_SRV=""</span><span class="hljs-comment">#ETCD_DISCOVERY_FALLBACK="proxy"</span><span class="hljs-comment">#ETCD_DISCOVERY_PROXY=""</span><span class="hljs-comment">#ETCD_STRICT_RECONFIG_CHECK="false"</span><span class="hljs-comment">#ETCD_AUTO_COMPACTION_RETENTION="0"</span><span class="hljs-comment"># [proxy]</span><span class="hljs-comment">#ETCD_PROXY="off"</span><span class="hljs-comment">#ETCD_PROXY_FAILURE_WAIT="5000"</span><span class="hljs-comment">#ETCD_PROXY_REFRESH_INTERVAL="30000"</span><span class="hljs-comment">#ETCD_PROXY_DIAL_TIMEOUT="1000"</span><span class="hljs-comment">#ETCD_PROXY_WRITE_TIMEOUT="5000"</span><span class="hljs-comment">#ETCD_PROXY_READ_TIMEOUT="0"</span><span class="hljs-comment"># [security]</span>ETCD_CERT_FILE=<span class="hljs-string">"/etc/etcd/ssl/etcd.pem"</span>ETCD_KEY_FILE=<span class="hljs-string">"/etc/etcd/ssl/etcd-key.pem"</span>ETCD_CLIENT_CERT_AUTH=<span class="hljs-string">"true"</span>ETCD_TRUSTED_CA_FILE=<span class="hljs-string">"/etc/etcd/ssl/etcd-root-ca.pem"</span>ETCD_AUTO_TLS=<span class="hljs-string">"true"</span>ETCD_PEER_CERT_FILE=<span class="hljs-string">"/etc/etcd/ssl/etcd.pem"</span>ETCD_PEER_KEY_FILE=<span class="hljs-string">"/etc/etcd/ssl/etcd-key.pem"</span>ETCD_PEER_CLIENT_CERT_AUTH=<span class="hljs-string">"true"</span>ETCD_PEER_TRUSTED_CA_FILE=<span class="hljs-string">"/etc/etcd/ssl/etcd-root-ca.pem"</span>ETCD_PEER_AUTO_TLS=<span class="hljs-string">"true"</span><span class="hljs-comment"># [logging]</span><span class="hljs-comment">#ETCD_DEBUG="false"</span><span class="hljs-comment"># examples for -log-package-levels etcdserver=WARNING,security=DEBUG</span><span class="hljs-comment">#ETCD_LOG_PACKAGE_LEVELS=""</span></code></pre></div><ul><li>etcd.service</li></ul><div class="hljs"><pre><code class="hljs sh">[Unit]Description=Etcd ServerAfter=network.targetAfter=network-online.targetWants=network-online.target[Service]Type=notifyWorkingDirectory=/var/lib/etcd/EnvironmentFile=-/etc/etcd/etcd.confUser=etcd<span class="hljs-comment"># set GOMAXPROCS to number of processors</span>ExecStart=/bin/bash -c <span class="hljs-string">"GOMAXPROCS=<span class="hljs-variable">$(nproc)</span> /usr/local/bin/etcd --name=\"<span class="hljs-variable">$&#123;ETCD_NAME&#125;</span>\" --data-dir=\"<span class="hljs-variable">$&#123;ETCD_DATA_DIR&#125;</span>\" --listen-client-urls=\"<span class="hljs-variable">$&#123;ETCD_LISTEN_CLIENT_URLS&#125;</span>\""</span>Restart=on-failureLimitNOFILE=65536[Install]WantedBy=multi-user.target</code></pre></div><p>最后三台机器依次修改 <code>IP</code>、<code>ETCD_NAME</code> 然后启动即可，<strong>生产环境请不要忘记修改集群 Token 为真实随机字符串 (<code>ETCD_INITIAL_CLUSTER_TOKEN</code> 变量)</strong>启动后可以通过以下命令测试集群联通性</p><div class="hljs"><pre><code class="hljs sh">docker1.node ➜  ~ <span class="hljs-built_in">export</span> ETCDCTL_API=3docker1.node ➜  ~ etcdctl member list238b72cdd26e304f, started, etcd2, https://192.168.1.52:2380, https://192.168.1.52:23798034142cf01c5d1c, started, etcd3, https://192.168.1.53:2380, https://192.168.1.53:23798da171dbef9ded69, started, etcd1, https://192.168.1.51:2380, https://192.168.1.51:2379</code></pre></div><h2 id="三、安装-Kubernetes"><a href="#三、安装-Kubernetes" class="headerlink" title="三、安装 Kubernetes"></a>三、安装 Kubernetes</h2><h3 id="3-1、生成证书及配置"><a href="#3-1、生成证书及配置" class="headerlink" title="3.1、生成证书及配置"></a>3.1、生成证书及配置</h3><h4 id="3-1-1、生成证书"><a href="#3-1-1、生成证书" class="headerlink" title="3.1.1、生成证书"></a>3.1.1、生成证书</h4><p>新版本已经越来越趋近全面 TLS + RBAC 配置，<strong>所以本次安装将会启动大部分 TLS + RBAC 配置，包括 <code>kube-controler-manager</code>、<code>kube-scheduler</code> 组件不再连接本地 <code>kube-apiserver</code> 的 8080 非认证端口，<code>kubelet</code> 等组件 API 端点关闭匿名访问，启动 RBAC 认证等</strong>；为了满足这些认证，需要签署以下证书</p><ul><li>k8s-root-ca-csr.json 集群 CA 根证书</li></ul><div class="hljs"><pre><code class="hljs json">&#123;    <span class="hljs-attr">"CN"</span>: <span class="hljs-string">"kubernetes"</span>,    <span class="hljs-attr">"key"</span>: &#123;        <span class="hljs-attr">"algo"</span>: <span class="hljs-string">"rsa"</span>,        <span class="hljs-attr">"size"</span>: <span class="hljs-number">4096</span>    &#125;,    <span class="hljs-attr">"names"</span>: [        &#123;            <span class="hljs-attr">"C"</span>: <span class="hljs-string">"CN"</span>,            <span class="hljs-attr">"ST"</span>: <span class="hljs-string">"BeiJing"</span>,            <span class="hljs-attr">"L"</span>: <span class="hljs-string">"BeiJing"</span>,            <span class="hljs-attr">"O"</span>: <span class="hljs-string">"kubernetes"</span>,            <span class="hljs-attr">"OU"</span>: <span class="hljs-string">"System"</span>        &#125;    ],    <span class="hljs-attr">"ca"</span>: &#123;        <span class="hljs-attr">"expiry"</span>: <span class="hljs-string">"87600h"</span>    &#125;&#125;</code></pre></div><ul><li>k8s-gencert.json 用于生成其他证书的标准配置</li></ul><div class="hljs"><pre><code class="hljs json">&#123;    <span class="hljs-attr">"signing"</span>: &#123;        <span class="hljs-attr">"default"</span>: &#123;            <span class="hljs-attr">"expiry"</span>: <span class="hljs-string">"87600h"</span>        &#125;,        <span class="hljs-attr">"profiles"</span>: &#123;            <span class="hljs-attr">"kubernetes"</span>: &#123;                <span class="hljs-attr">"usages"</span>: [                    <span class="hljs-string">"signing"</span>,                    <span class="hljs-string">"key encipherment"</span>,                    <span class="hljs-string">"server auth"</span>,                    <span class="hljs-string">"client auth"</span>                ],                <span class="hljs-attr">"expiry"</span>: <span class="hljs-string">"87600h"</span>            &#125;        &#125;    &#125;&#125;</code></pre></div><ul><li>kube-apiserver-csr.json apiserver TLS 认证端口需要的证书</li></ul><div class="hljs"><pre><code class="hljs json">&#123;    <span class="hljs-attr">"CN"</span>: <span class="hljs-string">"kubernetes"</span>,    <span class="hljs-attr">"hosts"</span>: [        <span class="hljs-string">"127.0.0.1"</span>,        <span class="hljs-string">"10.254.0.1"</span>,        <span class="hljs-string">"localhost"</span>,        <span class="hljs-string">"*.master.kubernetes.node"</span>,        <span class="hljs-string">"kubernetes"</span>,        <span class="hljs-string">"kubernetes.default"</span>,        <span class="hljs-string">"kubernetes.default.svc"</span>,        <span class="hljs-string">"kubernetes.default.svc.cluster"</span>,        <span class="hljs-string">"kubernetes.default.svc.cluster.local"</span>    ],    <span class="hljs-attr">"key"</span>: &#123;        <span class="hljs-attr">"algo"</span>: <span class="hljs-string">"rsa"</span>,        <span class="hljs-attr">"size"</span>: <span class="hljs-number">2048</span>    &#125;,    <span class="hljs-attr">"names"</span>: [        &#123;            <span class="hljs-attr">"C"</span>: <span class="hljs-string">"CN"</span>,            <span class="hljs-attr">"ST"</span>: <span class="hljs-string">"BeiJing"</span>,            <span class="hljs-attr">"L"</span>: <span class="hljs-string">"BeiJing"</span>,            <span class="hljs-attr">"O"</span>: <span class="hljs-string">"kubernetes"</span>,            <span class="hljs-attr">"OU"</span>: <span class="hljs-string">"System"</span>        &#125;    ]&#125;</code></pre></div><ul><li>kube-controller-manager-csr.json controller manager 连接 apiserver 需要使用的证书，同时本身 <code>10257</code> 端口也会使用此证书</li></ul><div class="hljs"><pre><code class="hljs json">&#123;  <span class="hljs-attr">"CN"</span>: <span class="hljs-string">"system:kube-controller-manager"</span>,  <span class="hljs-attr">"hosts"</span>: [    <span class="hljs-string">"127.0.0.1"</span>,    <span class="hljs-string">"localhost"</span>,    <span class="hljs-string">"*.master.kubernetes.node"</span>  ],  <span class="hljs-attr">"key"</span>: &#123;    <span class="hljs-attr">"algo"</span>: <span class="hljs-string">"rsa"</span>,    <span class="hljs-attr">"size"</span>: <span class="hljs-number">2048</span>  &#125;,  <span class="hljs-attr">"names"</span>: [    &#123;      <span class="hljs-attr">"C"</span>: <span class="hljs-string">"CN"</span>,      <span class="hljs-attr">"ST"</span>: <span class="hljs-string">"BeiJing"</span>,      <span class="hljs-attr">"L"</span>: <span class="hljs-string">"BeiJing"</span>,      <span class="hljs-attr">"O"</span>: <span class="hljs-string">"system:kube-controller-manager"</span>,      <span class="hljs-attr">"OU"</span>: <span class="hljs-string">"System"</span>    &#125;  ]&#125;</code></pre></div><ul><li>kube-scheduler-csr.json scheduler 连接 apiserver 需要使用的证书，同时本身 <code>10259</code> 端口也会使用此证书</li></ul><div class="hljs"><pre><code class="hljs json">&#123;  <span class="hljs-attr">"CN"</span>: <span class="hljs-string">"system:kube-scheduler"</span>,  <span class="hljs-attr">"hosts"</span>: [    <span class="hljs-string">"127.0.0.1"</span>,    <span class="hljs-string">"localhost"</span>,    <span class="hljs-string">"*.master.kubernetes.node"</span>  ],  <span class="hljs-attr">"key"</span>: &#123;    <span class="hljs-attr">"algo"</span>: <span class="hljs-string">"rsa"</span>,    <span class="hljs-attr">"size"</span>: <span class="hljs-number">2048</span>  &#125;,  <span class="hljs-attr">"names"</span>: [    &#123;      <span class="hljs-attr">"C"</span>: <span class="hljs-string">"CN"</span>,      <span class="hljs-attr">"ST"</span>: <span class="hljs-string">"BeiJing"</span>,      <span class="hljs-attr">"L"</span>: <span class="hljs-string">"BeiJing"</span>,      <span class="hljs-attr">"O"</span>: <span class="hljs-string">"system:kube-scheduler"</span>,      <span class="hljs-attr">"OU"</span>: <span class="hljs-string">"System"</span>    &#125;  ]&#125;</code></pre></div><ul><li>kube-proxy-csr.json proxy 组件连接 apiserver 需要使用的证书</li></ul><div class="hljs"><pre><code class="hljs json">&#123;    <span class="hljs-attr">"CN"</span>: <span class="hljs-string">"system:kube-proxy"</span>,    <span class="hljs-attr">"hosts"</span>: [],    <span class="hljs-attr">"key"</span>: &#123;        <span class="hljs-attr">"algo"</span>: <span class="hljs-string">"rsa"</span>,        <span class="hljs-attr">"size"</span>: <span class="hljs-number">2048</span>    &#125;,    <span class="hljs-attr">"names"</span>: [        &#123;            <span class="hljs-attr">"C"</span>: <span class="hljs-string">"CN"</span>,            <span class="hljs-attr">"ST"</span>: <span class="hljs-string">"BeiJing"</span>,            <span class="hljs-attr">"L"</span>: <span class="hljs-string">"BeiJing"</span>,            <span class="hljs-attr">"O"</span>: <span class="hljs-string">"system:kube-proxy"</span>,            <span class="hljs-attr">"OU"</span>: <span class="hljs-string">"System"</span>        &#125;    ]&#125;</code></pre></div><ul><li>kubelet-api-admin-csr.json apiserver 反向连接 kubelet 组件 <code>10250</code> 端口需要使用的证书(例如执行 <code>kubectl logs</code>)</li></ul><div class="hljs"><pre><code class="hljs json">&#123;    <span class="hljs-attr">"CN"</span>: <span class="hljs-string">"system:kubelet-api-admin"</span>,    <span class="hljs-attr">"hosts"</span>: [],    <span class="hljs-attr">"key"</span>: &#123;        <span class="hljs-attr">"algo"</span>: <span class="hljs-string">"rsa"</span>,        <span class="hljs-attr">"size"</span>: <span class="hljs-number">2048</span>    &#125;,    <span class="hljs-attr">"names"</span>: [        &#123;            <span class="hljs-attr">"C"</span>: <span class="hljs-string">"CN"</span>,            <span class="hljs-attr">"ST"</span>: <span class="hljs-string">"BeiJing"</span>,            <span class="hljs-attr">"L"</span>: <span class="hljs-string">"BeiJing"</span>,            <span class="hljs-attr">"O"</span>: <span class="hljs-string">"system:kubelet-api-admin"</span>,            <span class="hljs-attr">"OU"</span>: <span class="hljs-string">"System"</span>        &#125;    ]&#125;</code></pre></div><ul><li>admin-csr.json 集群管理员(kubectl)连接 apiserver 需要使用的证书</li></ul><div class="hljs"><pre><code class="hljs json">&#123;    <span class="hljs-attr">"CN"</span>: <span class="hljs-string">"system:masters"</span>,    <span class="hljs-attr">"hosts"</span>: [],    <span class="hljs-attr">"key"</span>: &#123;        <span class="hljs-attr">"algo"</span>: <span class="hljs-string">"rsa"</span>,        <span class="hljs-attr">"size"</span>: <span class="hljs-number">2048</span>    &#125;,    <span class="hljs-attr">"names"</span>: [        &#123;            <span class="hljs-attr">"C"</span>: <span class="hljs-string">"CN"</span>,            <span class="hljs-attr">"ST"</span>: <span class="hljs-string">"BeiJing"</span>,            <span class="hljs-attr">"L"</span>: <span class="hljs-string">"BeiJing"</span>,            <span class="hljs-attr">"O"</span>: <span class="hljs-string">"system:masters"</span>,            <span class="hljs-attr">"OU"</span>: <span class="hljs-string">"System"</span>        &#125;    ]&#125;</code></pre></div><p><strong>注意: 请不要修改证书配置的 <code>CN</code>、<code>O</code> 字段，这两个字段名称比较特殊，大多数为 <code>system:</code> 开头，实际上是为了匹配 RBAC 规则，具体请参考 <a href="https://kubernetes.io/docs/reference/access-authn-authz/rbac/#default-roles-and-role-bindings" target="_blank" rel="noopener">Default Roles and Role Bindings</a></strong></p><p>最后使用如下命令生成即可:</p><div class="hljs"><pre><code class="hljs sh">cfssl gencert --initca=<span class="hljs-literal">true</span> k8s-root-ca-csr.json | cfssljson --bare k8s-root-ca<span class="hljs-keyword">for</span> targetName <span class="hljs-keyword">in</span> kube-apiserver kube-controller-manager kube-scheduler kube-proxy kubelet-api-admin admin; <span class="hljs-keyword">do</span>    cfssl gencert --ca k8s-root-ca.pem --ca-key k8s-root-ca-key.pem --config k8s-gencert.json --profile kubernetes <span class="hljs-variable">$targetName</span>-csr.json | cfssljson --bare <span class="hljs-variable">$targetName</span><span class="hljs-keyword">done</span></code></pre></div><h4 id="3-1-2、生成配置文件"><a href="#3-1-2、生成配置文件" class="headerlink" title="3.1.2、生成配置文件"></a>3.1.2、生成配置文件</h4><p>集群搭建需要预先生成一系列配置文件，生成配置需要预先安装 <code>kubectl</code> 命令，请自行根据文档安装 <a href="https://kubernetes.io/docs/tasks/tools/install-kubectl/#install-kubectl-binary-using-curl" target="_blank" rel="noopener">Install kubectl binary using curl</a>；其中配置文件及其作用如下:</p><ul><li><code>bootstrap.kubeconfig</code> kubelet TLS Bootstarp 引导阶段需要使用的配置文件</li><li><code>kube-controller-manager.kubeconfig</code> controller manager 组件开启安全端口及 RBAC 认证所需配置</li><li><code>kube-scheduler.kubeconfig</code> scheduler 组件开启安全端口及 RBAC 认证所需配置</li><li><code>kube-proxy.kubeconfig</code> proxy 组件连接 apiserver 所需配置文件</li><li><code>audit-policy.yaml</code> apiserver RBAC 审计日志配置文件</li><li><code>bootstrap.secret.yaml</code> kubelet TLS Bootstarp 引导阶段使用 Bootstrap Token 方式引导，需要预先创建此 Token</li></ul><p>生成这些配置文件的脚本如下</p><div class="hljs"><pre><code class="hljs sh"><span class="hljs-comment"># 指定 apiserver 地址</span>KUBE_APISERVER=<span class="hljs-string">"https://127.0.0.1:6443"</span><span class="hljs-comment"># 生成 Bootstrap Token</span>BOOTSTRAP_TOKEN_ID=$(head -c 6 /dev/urandom | md5sum | head -c 6)BOOTSTRAP_TOKEN_SECRET=$(head -c 16 /dev/urandom | md5sum | head -c 16)BOOTSTRAP_TOKEN=<span class="hljs-string">"<span class="hljs-variable">$&#123;BOOTSTRAP_TOKEN_ID&#125;</span>.<span class="hljs-variable">$&#123;BOOTSTRAP_TOKEN_SECRET&#125;</span>"</span><span class="hljs-built_in">echo</span> <span class="hljs-string">"Bootstrap Tokne: <span class="hljs-variable">$&#123;BOOTSTRAP_TOKEN&#125;</span>"</span><span class="hljs-comment"># 生成 kubelet tls bootstrap 配置</span><span class="hljs-built_in">echo</span> <span class="hljs-string">"Create kubelet bootstrapping kubeconfig..."</span>kubectl config <span class="hljs-built_in">set</span>-cluster kubernetes \  --certificate-authority=k8s-root-ca.pem \  --embed-certs=<span class="hljs-literal">true</span> \  --server=<span class="hljs-variable">$&#123;KUBE_APISERVER&#125;</span> \  --kubeconfig=bootstrap.kubeconfigkubectl config <span class="hljs-built_in">set</span>-credentials <span class="hljs-string">"system:bootstrap:<span class="hljs-variable">$&#123;BOOTSTRAP_TOKEN_ID&#125;</span>"</span> \  --token=<span class="hljs-variable">$&#123;BOOTSTRAP_TOKEN&#125;</span> \  --kubeconfig=bootstrap.kubeconfigkubectl config <span class="hljs-built_in">set</span>-context default \  --cluster=kubernetes \  --user=<span class="hljs-string">"system:bootstrap:<span class="hljs-variable">$&#123;BOOTSTRAP_TOKEN_ID&#125;</span>"</span> \  --kubeconfig=bootstrap.kubeconfigkubectl config use-context default --kubeconfig=bootstrap.kubeconfig<span class="hljs-comment"># 生成 kube-controller-manager 配置文件</span><span class="hljs-built_in">echo</span> <span class="hljs-string">"Create kube-controller-manager kubeconfig..."</span>kubectl config <span class="hljs-built_in">set</span>-cluster kubernetes \  --certificate-authority=k8s-root-ca.pem \  --embed-certs=<span class="hljs-literal">true</span> \  --server=<span class="hljs-variable">$&#123;KUBE_APISERVER&#125;</span> \  --kubeconfig=kube-controller-manager.kubeconfigkubectl config <span class="hljs-built_in">set</span>-credentials <span class="hljs-string">"system:kube-controller-manager"</span> \  --client-certificate=kube-controller-manager.pem \  --client-key=kube-controller-manager-key.pem \  --embed-certs=<span class="hljs-literal">true</span> \  --kubeconfig=kube-controller-manager.kubeconfigkubectl config <span class="hljs-built_in">set</span>-context default \  --cluster=kubernetes \  --user=system:kube-controller-manager \  --kubeconfig=kube-controller-manager.kubeconfigkubectl config use-context default --kubeconfig=kube-controller-manager.kubeconfig <span class="hljs-comment"># 生成 kube-scheduler 配置文件</span><span class="hljs-built_in">echo</span> <span class="hljs-string">"Create kube-scheduler kubeconfig..."</span>kubectl config <span class="hljs-built_in">set</span>-cluster kubernetes \  --certificate-authority=k8s-root-ca.pem \  --embed-certs=<span class="hljs-literal">true</span> \  --server=<span class="hljs-variable">$&#123;KUBE_APISERVER&#125;</span> \  --kubeconfig=kube-scheduler.kubeconfigkubectl config <span class="hljs-built_in">set</span>-credentials <span class="hljs-string">"system:kube-scheduler"</span> \  --client-certificate=kube-scheduler.pem \  --client-key=kube-scheduler-key.pem \  --embed-certs=<span class="hljs-literal">true</span> \  --kubeconfig=kube-scheduler.kubeconfigkubectl config <span class="hljs-built_in">set</span>-context default \  --cluster=kubernetes \  --user=system:kube-scheduler \  --kubeconfig=kube-scheduler.kubeconfigkubectl config use-context default --kubeconfig=kube-scheduler.kubeconfig <span class="hljs-comment"># 生成 kube-proxy 配置文件</span><span class="hljs-built_in">echo</span> <span class="hljs-string">"Create kube-proxy kubeconfig..."</span>kubectl config <span class="hljs-built_in">set</span>-cluster kubernetes \  --certificate-authority=k8s-root-ca.pem \  --embed-certs=<span class="hljs-literal">true</span> \  --server=<span class="hljs-variable">$&#123;KUBE_APISERVER&#125;</span> \  --kubeconfig=kube-proxy.kubeconfigkubectl config <span class="hljs-built_in">set</span>-credentials <span class="hljs-string">"system:kube-proxy"</span> \  --client-certificate=kube-proxy.pem \  --client-key=kube-proxy-key.pem \  --embed-certs=<span class="hljs-literal">true</span> \  --kubeconfig=kube-proxy.kubeconfigkubectl config <span class="hljs-built_in">set</span>-context default \  --cluster=kubernetes \  --user=system:kube-proxy \  --kubeconfig=kube-proxy.kubeconfigkubectl config use-context default --kubeconfig=kube-proxy.kubeconfig <span class="hljs-comment"># 生成 apiserver RBAC 审计配置文件 </span>cat &gt;&gt; audit-policy.yaml &lt;&lt;EOF<span class="hljs-comment"># Log all requests at the Metadata level.</span>apiVersion: audit.k8s.io/v1kind: Policyrules:- level: MetadataEOF<span class="hljs-comment"># 生成 tls bootstrap token secret 配置文件</span>cat &gt;&gt; bootstrap.secret.yaml &lt;&lt;EOFapiVersion: v1kind: Secretmetadata:  <span class="hljs-comment"># Name MUST be of form "bootstrap-token-&lt;token id&gt;"</span>  name: bootstrap-token-<span class="hljs-variable">$&#123;BOOTSTRAP_TOKEN_ID&#125;</span>  namespace: kube-system<span class="hljs-comment"># Type MUST be 'bootstrap.kubernetes.io/token'</span><span class="hljs-built_in">type</span>: bootstrap.kubernetes.io/tokenstringData:  <span class="hljs-comment"># Human readable description. Optional.</span>  description: <span class="hljs-string">"The default bootstrap token."</span>  <span class="hljs-comment"># Token ID and secret. Required.</span>  token-id: <span class="hljs-variable">$&#123;BOOTSTRAP_TOKEN_ID&#125;</span>  token-secret: <span class="hljs-variable">$&#123;BOOTSTRAP_TOKEN_SECRET&#125;</span>  <span class="hljs-comment"># Expiration. Optional.</span>  expiration: $(date -d<span class="hljs-string">'+2 day'</span> -u +<span class="hljs-string">"%Y-%m-%dT%H:%M:%SZ"</span>)  <span class="hljs-comment"># Allowed usages.</span>  usage-bootstrap-authentication: <span class="hljs-string">"true"</span>  usage-bootstrap-signing: <span class="hljs-string">"true"</span>  <span class="hljs-comment"># Extra groups to authenticate the token as. Must start with "system:bootstrappers:"</span><span class="hljs-comment">#  auth-extra-groups: system:bootstrappers:worker,system:bootstrappers:ingress</span>EOF</code></pre></div><h3 id="3-2、处理-ipvs-及依赖"><a href="#3-2、处理-ipvs-及依赖" class="headerlink" title="3.2、处理 ipvs 及依赖"></a>3.2、处理 ipvs 及依赖</h3><p>新版本目前 <code>kube-proxy</code> 组件全部采用 ipvs 方式负载，所以为了 <code>kube-proxy</code> 能正常工作需要预先处理一下 ipvs 配置以及相关依赖(每台 node 都要处理)</p><div class="hljs"><pre><code class="hljs sh">cat &gt;&gt; /etc/sysctl.conf &lt;&lt;EOFnet.ipv4.ip_forward=1net.bridge.bridge-nf-call-iptables=1net.bridge.bridge-nf-call-ip6tables=1EOFsysctl -pcat &gt;&gt; /etc/modules &lt;&lt;EOFip_vsip_vs_lcip_vs_wlcip_vs_rrip_vs_wrrip_vs_lblcip_vs_lblcrip_vs_dhip_vs_ship_vs_foip_vs_nqip_vs_sedip_vs_ftpEOFapt install -y conntrack ipvsadm</code></pre></div><h3 id="3-3、部署-Master"><a href="#3-3、部署-Master" class="headerlink" title="3.3、部署 Master"></a>3.3、部署 Master</h3><h4 id="3-3-1、安装脚本"><a href="#3-3-1、安装脚本" class="headerlink" title="3.3.1、安装脚本"></a>3.3.1、安装脚本</h4><p>master 节点上需要三个组件: <code>kube-apiserver</code>、<code>kube-controller-manager</code>、<code>kube-scheduler</code></p><p><strong>安装流程整体为以下几步</strong></p><ul><li><strong>创建单独的 <code>kube</code> 用户</strong></li><li><strong>复制相关二进制文件到 <code>/usr/bin</code>，可以采用 <code>all in one</code> 的 <code>hyperkube</code></strong></li><li><strong>复制配置文件到 <code>/etc/kubernetes</code></strong></li><li><strong>复制证书文件到 <code>/etc/kubernetes/ssl</code></strong></li><li><strong>修改配置并启动</strong></li></ul><p>安装脚本如下所示:</p><div class="hljs"><pre><code class="hljs sh">KUBE_DEFAULT_VERSION=<span class="hljs-string">"1.13.4"</span><span class="hljs-keyword">if</span> [ <span class="hljs-string">"<span class="hljs-variable">$1</span>"</span> != <span class="hljs-string">""</span> ]; <span class="hljs-keyword">then</span>  KUBE_VERSION=<span class="hljs-variable">$1</span><span class="hljs-keyword">else</span>  <span class="hljs-built_in">echo</span> -e <span class="hljs-string">"\033[33mWARNING: KUBE_VERSION is blank,use default version: <span class="hljs-variable">$&#123;KUBE_DEFAULT_VERSION&#125;</span>\033[0m"</span>  KUBE_VERSION=<span class="hljs-variable">$&#123;KUBE_DEFAULT_VERSION&#125;</span><span class="hljs-keyword">fi</span><span class="hljs-comment"># 下载 hyperkube</span><span class="hljs-keyword">function</span> <span class="hljs-function"><span class="hljs-title">download_k8s</span></span>()&#123;    <span class="hljs-keyword">if</span> [ ! -f <span class="hljs-string">"hyperkube_v<span class="hljs-variable">$&#123;KUBE_VERSION&#125;</span>"</span> ]; <span class="hljs-keyword">then</span>        wget https://storage.googleapis.com/kubernetes-release/release/v<span class="hljs-variable">$&#123;KUBE_VERSION&#125;</span>/bin/linux/amd64/hyperkube -O hyperkube_v<span class="hljs-variable">$&#123;KUBE_VERSION&#125;</span>        chmod +x hyperkube_v<span class="hljs-variable">$&#123;KUBE_VERSION&#125;</span>    <span class="hljs-keyword">fi</span>&#125;<span class="hljs-comment"># 创建专用用户 kube</span><span class="hljs-keyword">function</span> <span class="hljs-function"><span class="hljs-title">preinstall</span></span>()&#123;    getent group kube &gt;/dev/null || groupadd -r kube    getent passwd kube &gt;/dev/null || useradd -r -g kube -d / -s /sbin/nologin -c <span class="hljs-string">"Kubernetes user"</span> kube&#125;<span class="hljs-comment"># 复制可执行文件和配置以及证书</span><span class="hljs-keyword">function</span> <span class="hljs-function"><span class="hljs-title">install_k8s</span></span>()&#123;    <span class="hljs-built_in">echo</span> -e <span class="hljs-string">"\033[32mINFO: Copy hyperkube...\033[0m"</span>    cp hyperkube_v<span class="hljs-variable">$&#123;KUBE_VERSION&#125;</span> /usr/bin/hyperkube    <span class="hljs-built_in">echo</span> -e <span class="hljs-string">"\033[32mINFO: Create symbolic link...\033[0m"</span>    (<span class="hljs-built_in">cd</span> /usr/bin &amp;&amp; hyperkube --make-symlinks)    <span class="hljs-built_in">echo</span> -e <span class="hljs-string">"\033[32mINFO: Copy kubernetes config...\033[0m"</span>    cp -r conf /etc/kubernetes    <span class="hljs-keyword">if</span> [ -d <span class="hljs-string">"/etc/kubernetes/ssl"</span> ]; <span class="hljs-keyword">then</span>        chown -R kube:kube /etc/kubernetes/ssl    <span class="hljs-keyword">fi</span>    <span class="hljs-built_in">echo</span> -e <span class="hljs-string">"\033[32mINFO: Copy kubernetes systemd config...\033[0m"</span>    cp systemd/*.service /lib/systemd/system    systemctl daemon-reload&#125;<span class="hljs-comment"># 创建必要的目录并修改权限</span><span class="hljs-keyword">function</span> <span class="hljs-function"><span class="hljs-title">postinstall</span></span>()&#123;    <span class="hljs-keyword">if</span> [ ! -d <span class="hljs-string">"/var/log/kube-audit"</span> ]; <span class="hljs-keyword">then</span>        mkdir /var/<span class="hljs-built_in">log</span>/kube-audit    <span class="hljs-keyword">fi</span>        <span class="hljs-keyword">if</span> [ ! -d <span class="hljs-string">"/var/lib/kubelet"</span> ]; <span class="hljs-keyword">then</span>        mkdir /var/lib/kubelet    <span class="hljs-keyword">fi</span>    <span class="hljs-keyword">if</span> [ ! -d <span class="hljs-string">"/usr/libexec"</span> ]; <span class="hljs-keyword">then</span>        mkdir /usr/libexec    <span class="hljs-keyword">fi</span>    chown -R kube:kube /etc/kubernetes /var/<span class="hljs-built_in">log</span>/kube-audit /var/lib/kubelet /usr/libexec&#125;<span class="hljs-comment"># 执行</span>download_k8spreinstallinstall_k8spostinstall</code></pre></div><p><strong>hyperkube 是一个多合一的可执行文件，通过 <code>--make-symlinks</code> 会在当前目录生成 kubernetes 各个组件的软连接</strong></p><p>被复制的 conf 目录结构如下(最终被复制到 <code>/etc/kubernetes</code>)</p><div class="hljs"><pre><code class="hljs sh">.├── apiserver├── audit-policy.yaml├── bootstrap.kubeconfig├── bootstrap.secret.yaml├── controller-manager├── kube-controller-manager.kubeconfig├── kubelet├── kube-proxy.kubeconfig├── kube-scheduler.kubeconfig├── proxy├── scheduler└── ssl    ├── admin-key.pem    ├── admin.pem    ├── k8s-root-ca-key.pem    ├── k8s-root-ca.pem    ├── kube-apiserver-key.pem    ├── kube-apiserver.pem    ├── kube-controller-manager-key.pem    ├── kube-controller-manager.pem    ├── kubelet-api-admin-key.pem    ├── kubelet-api-admin.pem    ├── kube-proxy-key.pem    ├── kube-proxy.pem    ├── kube-scheduler-key.pem    └── kube-scheduler.pem1 directory, 25 files</code></pre></div><h4 id="3-3-2、配置文件"><a href="#3-3-2、配置文件" class="headerlink" title="3.3.2、配置文件"></a>3.3.2、配置文件</h4><p>以下为相关配置文件内容</p><p><strong>systemd 配置如下</strong></p><ul><li>kube-apiserver.service</li></ul><div class="hljs"><pre><code class="hljs sh">[Unit]Description=Kubernetes API ServerDocumentation=https://github.com/GoogleCloudPlatform/kubernetesAfter=network.targetAfter=etcd.service[Service]EnvironmentFile=-/etc/kubernetes/apiserverUser=kubeExecStart=/usr/bin/kube-apiserver \    <span class="hljs-variable">$KUBE_LOGTOSTDERR</span> \    <span class="hljs-variable">$KUBE_LOG_LEVEL</span> \    <span class="hljs-variable">$KUBE_ETCD_SERVERS</span> \    <span class="hljs-variable">$KUBE_API_ADDRESS</span> \    <span class="hljs-variable">$KUBE_API_PORT</span> \    <span class="hljs-variable">$KUBELET_PORT</span> \    <span class="hljs-variable">$KUBE_ALLOW_PRIV</span> \    <span class="hljs-variable">$KUBE_SERVICE_ADDRESSES</span> \    <span class="hljs-variable">$KUBE_ADMISSION_CONTROL</span> \    <span class="hljs-variable">$KUBE_API_ARGS</span>Restart=on-failureType=notifyLimitNOFILE=65536[Install]WantedBy=multi-user.target</code></pre></div><ul><li>kube-controller-manager.service</li></ul><div class="hljs"><pre><code class="hljs sh">[Unit]Description=Kubernetes Controller ManagerDocumentation=https://github.com/GoogleCloudPlatform/kubernetes[Service]EnvironmentFile=-/etc/kubernetes/controller-managerUser=kubeExecStart=/usr/bin/kube-controller-manager \    <span class="hljs-variable">$KUBE_LOGTOSTDERR</span> \    <span class="hljs-variable">$KUBE_LOG_LEVEL</span> \    <span class="hljs-variable">$KUBE_MASTER</span> \    <span class="hljs-variable">$KUBE_CONTROLLER_MANAGER_ARGS</span>Restart=on-failureLimitNOFILE=65536[Install]WantedBy=multi-user.target</code></pre></div><ul><li>kube-scheduler.service</li></ul><div class="hljs"><pre><code class="hljs sh">[Unit]Description=Kubernetes Scheduler PluginDocumentation=https://github.com/GoogleCloudPlatform/kubernetes[Service]EnvironmentFile=-/etc/kubernetes/schedulerUser=kubeExecStart=/usr/bin/kube-scheduler \    <span class="hljs-variable">$KUBE_LOGTOSTDERR</span> \    <span class="hljs-variable">$KUBE_LOG_LEVEL</span> \    <span class="hljs-variable">$KUBE_MASTER</span> \    <span class="hljs-variable">$KUBE_SCHEDULER_ARGS</span>Restart=on-failureLimitNOFILE=65536[Install]WantedBy=multi-user.target</code></pre></div><p><strong>核心配置文件</strong></p><ul><li>apiserver</li></ul><div class="hljs"><pre><code class="hljs sh"><span class="hljs-comment">###</span><span class="hljs-comment"># kubernetes system config</span><span class="hljs-comment">#</span><span class="hljs-comment"># The following values are used to configure the kube-apiserver</span><span class="hljs-comment">#</span><span class="hljs-comment"># The address on the local server to listen to.</span>KUBE_API_ADDRESS=<span class="hljs-string">"--advertise-address=192.168.1.51 --bind-address=0.0.0.0"</span><span class="hljs-comment"># The port on the local server to listen on.</span>KUBE_API_PORT=<span class="hljs-string">"--secure-port=6443"</span><span class="hljs-comment"># Port minions listen on</span><span class="hljs-comment"># KUBELET_PORT="--kubelet-port=10250"</span><span class="hljs-comment"># Comma separated list of nodes in the etcd cluster</span>KUBE_ETCD_SERVERS=<span class="hljs-string">"--etcd-servers=https://192.168.1.51:2379,https://192.168.1.52:2379,https://192.168.1.53:2379"</span><span class="hljs-comment"># Address range to use for services</span>KUBE_SERVICE_ADDRESSES=<span class="hljs-string">"--service-cluster-ip-range=10.254.0.0/16"</span><span class="hljs-comment"># default admission control policies</span>KUBE_ADMISSION_CONTROL=<span class="hljs-string">"--enable-admission-plugins=NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,Priority,ResourceQuota"</span><span class="hljs-comment"># Add your own!</span>KUBE_API_ARGS=<span class="hljs-string">" --allow-privileged=true \</span><span class="hljs-string">                --anonymous-auth=false \</span><span class="hljs-string">                --alsologtostderr \</span><span class="hljs-string">                --apiserver-count=3 \</span><span class="hljs-string">                --audit-log-maxage=30 \</span><span class="hljs-string">                --audit-log-maxbackup=3 \</span><span class="hljs-string">                --audit-log-maxsize=100 \</span><span class="hljs-string">                --audit-log-path=/var/log/kube-audit/audit.log \</span><span class="hljs-string">                --audit-policy-file=/etc/kubernetes/audit-policy.yaml \</span><span class="hljs-string">                --authorization-mode=Node,RBAC \</span><span class="hljs-string">                --client-ca-file=/etc/kubernetes/ssl/k8s-root-ca.pem \</span><span class="hljs-string">                --enable-bootstrap-token-auth \</span><span class="hljs-string">                --enable-garbage-collector \</span><span class="hljs-string">                --enable-logs-handler \</span><span class="hljs-string">                --endpoint-reconciler-type=lease \</span><span class="hljs-string">                --etcd-cafile=/etc/etcd/ssl/etcd-root-ca.pem \</span><span class="hljs-string">                --etcd-certfile=/etc/etcd/ssl/etcd.pem \</span><span class="hljs-string">                --etcd-keyfile=/etc/etcd/ssl/etcd-key.pem \</span><span class="hljs-string">                --etcd-compaction-interval=0s \</span><span class="hljs-string">                --event-ttl=168h0m0s \</span><span class="hljs-string">                --kubelet-https=true \</span><span class="hljs-string">                --kubelet-certificate-authority=/etc/kubernetes/ssl/k8s-root-ca.pem \</span><span class="hljs-string">                --kubelet-client-certificate=/etc/kubernetes/ssl/kubelet-api-admin.pem \</span><span class="hljs-string">                --kubelet-client-key=/etc/kubernetes/ssl/kubelet-api-admin-key.pem \</span><span class="hljs-string">                --kubelet-timeout=3s \</span><span class="hljs-string">                --runtime-config=api/all=true \</span><span class="hljs-string">                --service-node-port-range=30000-50000 \</span><span class="hljs-string">                --service-account-key-file=/etc/kubernetes/ssl/k8s-root-ca.pem \</span><span class="hljs-string">                --tls-cert-file=/etc/kubernetes/ssl/kube-apiserver.pem \</span><span class="hljs-string">                --tls-private-key-file=/etc/kubernetes/ssl/kube-apiserver-key.pem \</span><span class="hljs-string">                --v=2"</span></code></pre></div><p>配置解释:</p><table><thead><tr><th>选项</th><th>作用</th></tr></thead><tbody><tr><td><code>--client-ca-file</code></td><td>定义客户端 CA</td></tr><tr><td><code>--endpoint-reconciler-type</code></td><td>master endpoint 策略</td></tr><tr><td><code>--kubelet-client-certificate</code>、<code>--kubelet-client-key</code></td><td>master 反向连接 kubelet 使用的证书</td></tr><tr><td><code>--service-account-key-file</code></td><td>service account 签名 key(用于有效性验证)</td></tr><tr><td><code>--tls-cert-file</code>、<code>--tls-private-key-file</code></td><td>master apiserver <code>6443</code> 端口证书</td></tr></tbody></table><ul><li>controller-manager</li></ul><div class="hljs"><pre><code class="hljs sh"><span class="hljs-comment">###</span><span class="hljs-comment"># The following values are used to configure the kubernetes controller-manager</span><span class="hljs-comment"># defaults from config and apiserver should be adequate</span><span class="hljs-comment"># Add your own!</span>KUBE_CONTROLLER_MANAGER_ARGS=<span class="hljs-string">"  --address=127.0.0.1 \</span><span class="hljs-string">                                --authentication-kubeconfig=/etc/kubernetes/kube-controller-manager.kubeconfig \</span><span class="hljs-string">                                --authorization-kubeconfig=/etc/kubernetes/kube-controller-manager.kubeconfig \</span><span class="hljs-string">                                --bind-address=0.0.0.0 \</span><span class="hljs-string">                                --cluster-name=kubernetes \</span><span class="hljs-string">                                --cluster-signing-cert-file=/etc/kubernetes/ssl/k8s-root-ca.pem \</span><span class="hljs-string">                                --cluster-signing-key-file=/etc/kubernetes/ssl/k8s-root-ca-key.pem \</span><span class="hljs-string">                                --client-ca-file=/etc/kubernetes/ssl/k8s-root-ca.pem \</span><span class="hljs-string">                                --controllers=*,bootstrapsigner,tokencleaner \</span><span class="hljs-string">                                --deployment-controller-sync-period=10s \</span><span class="hljs-string">                                --experimental-cluster-signing-duration=87600h0m0s \</span><span class="hljs-string">                                --enable-garbage-collector=true \</span><span class="hljs-string">                                --kubeconfig=/etc/kubernetes/kube-controller-manager.kubeconfig \</span><span class="hljs-string">                                --leader-elect=true \</span><span class="hljs-string">                                --node-monitor-grace-period=20s \</span><span class="hljs-string">                                --node-monitor-period=5s \</span><span class="hljs-string">                                --port=10252 \</span><span class="hljs-string">                                --pod-eviction-timeout=2m0s \</span><span class="hljs-string">                                --requestheader-client-ca-file=/etc/kubernetes/ssl/k8s-root-ca.pem \</span><span class="hljs-string">                                --terminated-pod-gc-threshold=50 \</span><span class="hljs-string">                                --tls-cert-file=/etc/kubernetes/ssl/kube-controller-manager.pem \</span><span class="hljs-string">                                --tls-private-key-file=/etc/kubernetes/ssl/kube-controller-manager-key.pem \</span><span class="hljs-string">                                --root-ca-file=/etc/kubernetes/ssl/k8s-root-ca.pem \</span><span class="hljs-string">                                --secure-port=10257 \</span><span class="hljs-string">                                --service-cluster-ip-range=10.254.0.0/16 \</span><span class="hljs-string">                                --service-account-private-key-file=/etc/kubernetes/ssl/k8s-root-ca-key.pem \</span><span class="hljs-string">                                --use-service-account-credentials=true \</span><span class="hljs-string">                                --v=2"</span></code></pre></div><p>controller manager 将不安全端口 <code>10252</code> 绑定到 127.0.0.1 确保 <code>kuebctl get cs</code> 有正确返回；将安全端口 <code>10257</code> 绑定到 0.0.0.0 公开，提供服务调用；<strong>由于 controller manager 开始连接 apiserver 的 <code>6443</code> 认证端口，所以需要 <code>--use-service-account-credentials</code> 选项来让 controller manager 创建单独的 service account(默认 <code>system:kube-controller-manager</code> 用户没有那么高权限)</strong></p><ul><li>scheduler</li></ul><div class="hljs"><pre><code class="hljs sh"><span class="hljs-comment">###</span><span class="hljs-comment"># kubernetes scheduler config</span><span class="hljs-comment"># default config should be adequate</span><span class="hljs-comment"># Add your own!</span>KUBE_SCHEDULER_ARGS=<span class="hljs-string">"   --address=127.0.0.1 \</span><span class="hljs-string">                        --authentication-kubeconfig=/etc/kubernetes/kube-scheduler.kubeconfig \</span><span class="hljs-string">                        --authorization-kubeconfig=/etc/kubernetes/kube-scheduler.kubeconfig \</span><span class="hljs-string">                        --bind-address=0.0.0.0 \</span><span class="hljs-string">                        --client-ca-file=/etc/kubernetes/ssl/k8s-root-ca.pem \</span><span class="hljs-string">                        --kubeconfig=/etc/kubernetes/kube-scheduler.kubeconfig \</span><span class="hljs-string">                        --requestheader-client-ca-file=/etc/kubernetes/ssl/k8s-root-ca.pem \</span><span class="hljs-string">                        --secure-port=10259 \</span><span class="hljs-string">                        --leader-elect=true \</span><span class="hljs-string">                        --port=10251 \</span><span class="hljs-string">                        --tls-cert-file=/etc/kubernetes/ssl/kube-scheduler.pem \</span><span class="hljs-string">                        --tls-private-key-file=/etc/kubernetes/ssl/kube-scheduler-key.pem \</span><span class="hljs-string">                        --v=2"</span></code></pre></div><p>shceduler 同  controller manager 一样将不安全端口绑定在本地，安全端口对外公开</p><p><strong>最后在三台节点上调整一下 IP 配置，启动即可</strong></p><h3 id="3-4、部署-Node"><a href="#3-4、部署-Node" class="headerlink" title="3.4、部署 Node"></a>3.4、部署 Node</h3><h4 id="3-4-1、安装脚本"><a href="#3-4-1、安装脚本" class="headerlink" title="3.4.1、安装脚本"></a>3.4.1、安装脚本</h4><p>node 安装与 master 安装过程一致，这里不再阐述</p><h4 id="3-4-2、配置文件"><a href="#3-4-2、配置文件" class="headerlink" title="3.4.2、配置文件"></a>3.4.2、配置文件</h4><p><strong>systemd 配置文件</strong></p><ul><li>kubelet.service</li></ul><div class="hljs"><pre><code class="hljs sh">[Unit]Description=Kubernetes Kubelet ServerDocumentation=https://github.com/GoogleCloudPlatform/kubernetesAfter=docker.serviceRequires=docker.service[Service]WorkingDirectory=/var/lib/kubeletEnvironmentFile=-/etc/kubernetes/kubeletExecStart=/usr/bin/kubelet \    <span class="hljs-variable">$KUBE_LOGTOSTDERR</span> \    <span class="hljs-variable">$KUBE_LOG_LEVEL</span> \    <span class="hljs-variable">$KUBELET_API_SERVER</span> \    <span class="hljs-variable">$KUBELET_ADDRESS</span> \    <span class="hljs-variable">$KUBELET_PORT</span> \    <span class="hljs-variable">$KUBELET_HOSTNAME</span> \    <span class="hljs-variable">$KUBE_ALLOW_PRIV</span> \    <span class="hljs-variable">$KUBELET_ARGS</span>Restart=on-failureKillMode=process[Install]WantedBy=multi-user.target</code></pre></div><ul><li>kube-proxy.service</li></ul><div class="hljs"><pre><code class="hljs sh">[Unit]Description=Kubernetes Kube-Proxy ServerDocumentation=https://github.com/GoogleCloudPlatform/kubernetesAfter=network.target[Service]EnvironmentFile=-/etc/kubernetes/proxyExecStart=/usr/bin/kube-proxy \    <span class="hljs-variable">$KUBE_LOGTOSTDERR</span> \    <span class="hljs-variable">$KUBE_LOG_LEVEL</span> \    <span class="hljs-variable">$KUBE_MASTER</span> \    <span class="hljs-variable">$KUBE_PROXY_ARGS</span>Restart=on-failureLimitNOFILE=65536[Install]WantedBy=multi-user.target</code></pre></div><p><strong>核心配置文件</strong></p><ul><li>kubelet</li></ul><div class="hljs"><pre><code class="hljs sh"><span class="hljs-comment">###</span><span class="hljs-comment"># kubernetes kubelet (minion) config</span><span class="hljs-comment"># The address for the info server to serve on (set to 0.0.0.0 or "" for all interfaces)</span>KUBELET_ADDRESS=<span class="hljs-string">"--node-ip=192.168.1.54"</span><span class="hljs-comment"># The port for the info server to serve on</span><span class="hljs-comment"># KUBELET_PORT="--port=10250"</span><span class="hljs-comment"># You may leave this blank to use the actual hostname</span>KUBELET_HOSTNAME=<span class="hljs-string">"--hostname-override=docker4.node"</span><span class="hljs-comment"># location of the api-server</span><span class="hljs-comment"># KUBELET_API_SERVER=""</span><span class="hljs-comment"># Add your own!</span>KUBELET_ARGS=<span class="hljs-string">"  --address=0.0.0.0 \</span><span class="hljs-string">                --allow-privileged \</span><span class="hljs-string">                --anonymous-auth=false \</span><span class="hljs-string">                --authorization-mode=Webhook \</span><span class="hljs-string">                --bootstrap-kubeconfig=/etc/kubernetes/bootstrap.kubeconfig \</span><span class="hljs-string">                --client-ca-file=/etc/kubernetes/ssl/k8s-root-ca.pem \</span><span class="hljs-string">                --network-plugin=cni \</span><span class="hljs-string">                --cgroup-driver=cgroupfs \</span><span class="hljs-string">                --cert-dir=/etc/kubernetes/ssl \</span><span class="hljs-string">                --cluster-dns=10.254.0.2 \</span><span class="hljs-string">                --cluster-domain=cluster.local \</span><span class="hljs-string">                --cni-conf-dir=/etc/cni/net.d \</span><span class="hljs-string">                --eviction-soft=imagefs.available&lt;15%,memory.available&lt;512Mi,nodefs.available&lt;15%,nodefs.inodesFree&lt;10% \</span><span class="hljs-string">                --eviction-soft-grace-period=imagefs.available=3m,memory.available=1m,nodefs.available=3m,nodefs.inodesFree=1m \</span><span class="hljs-string">                --eviction-hard=imagefs.available&lt;10%,memory.available&lt;256Mi,nodefs.available&lt;10%,nodefs.inodesFree&lt;5% \</span><span class="hljs-string">                --eviction-max-pod-grace-period=30 \</span><span class="hljs-string">                --image-gc-high-threshold=80 \</span><span class="hljs-string">                --image-gc-low-threshold=70 \</span><span class="hljs-string">                --image-pull-progress-deadline=30s \</span><span class="hljs-string">                --kube-reserved=cpu=500m,memory=512Mi,ephemeral-storage=1Gi \</span><span class="hljs-string">                --kubeconfig=/etc/kubernetes/kubelet.kubeconfig \</span><span class="hljs-string">                --max-pods=100 \</span><span class="hljs-string">                --minimum-image-ttl-duration=720h0m0s \</span><span class="hljs-string">                --node-labels=node.kubernetes.io/k8s-node=true \</span><span class="hljs-string">                --pod-infra-container-image=gcr.azk8s.cn/google_containers/pause-amd64:3.1 \</span><span class="hljs-string">                --port=10250 \</span><span class="hljs-string">                --read-only-port=0 \</span><span class="hljs-string">                --rotate-certificates \</span><span class="hljs-string">                --rotate-server-certificates \</span><span class="hljs-string">                --resolv-conf=/run/systemd/resolve/resolv.conf \</span><span class="hljs-string">                --system-reserved=cpu=500m,memory=512Mi,ephemeral-storage=1Gi \</span><span class="hljs-string">                --fail-swap-on=false \</span><span class="hljs-string">                --v=2"</span></code></pre></div><p><strong>当 kubelet 组件设置了 <code>--rotate-certificates</code>，<code>--rotate-server-certificates</code> 后会自动更新其使用的相关证书，同时指定 <code>--authorization-mode=Webhook</code> 后 <code>10250</code> 端口 RBAC 授权将会委托给 apiserver</strong></p><ul><li>proxy</li></ul><div class="hljs"><pre><code class="hljs sh"><span class="hljs-comment">###</span><span class="hljs-comment"># kubernetes proxy config</span><span class="hljs-comment"># default config should be adequate</span><span class="hljs-comment"># Add your own!</span>KUBE_PROXY_ARGS=<span class="hljs-string">"   --bind-address=0.0.0.0 \</span><span class="hljs-string">                    --cleanup-ipvs=true \</span><span class="hljs-string">                    --cluster-cidr=10.254.0.0/16 \</span><span class="hljs-string">                    --hostname-override=docker4.node \</span><span class="hljs-string">                    --healthz-bind-address=0.0.0.0 \</span><span class="hljs-string">                    --healthz-port=10256 \</span><span class="hljs-string">                    --masquerade-all=true \</span><span class="hljs-string">                    --proxy-mode=ipvs \</span><span class="hljs-string">                    --ipvs-min-sync-period=5s \</span><span class="hljs-string">                    --ipvs-sync-period=5s \</span><span class="hljs-string">                    --ipvs-scheduler=wrr \</span><span class="hljs-string">                    --kubeconfig=/etc/kubernetes/kube-proxy.kubeconfig \</span><span class="hljs-string">                    --logtostderr=true \</span><span class="hljs-string">                    --v=2"</span></code></pre></div><p>由于 <code>kubelet</code> 组件是采用 TLS Bootstrap 启动，所以需要预先创建相关配置</p><div class="hljs"><pre><code class="hljs sh"><span class="hljs-comment"># 创建用于 tls bootstrap 的 token secret</span>kubectl create -f bootstrap.secret.yaml<span class="hljs-comment"># 为了能让 kubelet 实现自动更新证书，需要配置相关 clusterrolebinding</span><span class="hljs-comment"># 允许 kubelet tls bootstrap 创建 csr 请求</span>kubectl create clusterrolebinding create-csrs-for-bootstrapping \    --clusterrole=system:node-bootstrapper \    --group=system:bootstrappers<span class="hljs-comment"># 自动批准 system:bootstrappers 组用户 TLS bootstrapping 首次申请证书的 CSR 请求</span>kubectl create clusterrolebinding auto-approve-csrs-for-group \    --clusterrole=system:certificates.k8s.io:certificatesigningrequests:nodeclient \    --group=system:bootstrappers<span class="hljs-comment"># 自动批准 system:nodes 组用户更新 kubelet 自身与 apiserver 通讯证书的 CSR 请求</span>kubectl create clusterrolebinding auto-approve-renewals-for-nodes \    --clusterrole=system:certificates.k8s.io:certificatesigningrequests:selfnodeclient \    --group=system:nodes<span class="hljs-comment"># 在 kubelet server 开启 api 认证的情况下，apiserver 反向访问 kubelet 10250 需要此授权(eg: kubectl logs)</span>kubectl create clusterrolebinding system:kubelet-api-admin \    --clusterrole=system:kubelet-api-admin \    --user=system:kubelet-api-admin</code></pre></div><h4 id="3-4-3、Nginx-代理"><a href="#3-4-3、Nginx-代理" class="headerlink" title="3.4.3、Nginx 代理"></a>3.4.3、Nginx 代理</h4><p>为了保证 apiserver 的 HA，需要在每个 node 上部署 nginx 来反向代理(tcp)所有 apiserver；然后 kubelet、kube-proxy 组件连接本地 <code>127.0.0.1:6443</code> 访问 apiserver，以确保任何 master 挂掉以后 node 都不会受到影响</p><ul><li>nginx.conf</li></ul><div class="hljs"><pre><code class="hljs sh">error_log stderr notice;worker_processes auto;events &#123;  multi_accept on;  use epoll;  worker_connections 1024;&#125;stream &#123;    upstream kube_apiserver &#123;        least_conn;        server 192.168.1.51:6443;        server 192.168.1.52:6443;        server 192.168.1.53:6443;    &#125;    server &#123;        listen        0.0.0.0:6443;        proxy_pass    kube_apiserver;        proxy_timeout 10m;        proxy_connect_timeout 1s;    &#125;&#125;</code></pre></div><ul><li>nginx-proxy.service</li></ul><div class="hljs"><pre><code class="hljs sh">[Unit]Description=kubernetes apiserver docker wrapperWants=docker.socketAfter=docker.service[Service]User=rootPermissionsStartOnly=<span class="hljs-literal">true</span>ExecStart=/usr/bin/docker run -p 127.0.0.1:6443:6443 \                              -v /etc/nginx:/etc/nginx \                              --name nginx-proxy \                              --net=host \                              --restart=on-failure:5 \                              --memory=512M \                              nginx:1.14.2-alpineExecStartPre=-/usr/bin/docker rm -f nginx-proxyExecStop=/usr/bin/docker stop nginx-proxyRestart=alwaysRestartSec=15sTimeoutStartSec=30s[Install]WantedBy=multi-user.target</code></pre></div><p>然后在每个 node 上先启动 nginx-proxy，接着启动 kubelet 与 kube-proxy 即可(master 上的 kubelet、kube-proxy 只需要修改 ip 和 node name)</p><h4 id="3-4-4、kubelet-server-证书"><a href="#3-4-4、kubelet-server-证书" class="headerlink" title="3.4.4、kubelet server 证书"></a>3.4.4、kubelet server 证书</h4><p><strong>注意: 新版本 kubelet server 的证书自动签发已经被关闭(看 issue 好像是由于安全原因)，所以对于 kubelet server 的证书仍需要手动签署</strong></p><div class="hljs"><pre><code class="hljs sh">docker1.node ➜  ~ kubectl get csrNAME                                                   AGE   REQUESTOR                  CONDITIONcsr-99l77                                              10s   system:node:docker4.node   Pendingnode-csr-aGwaNKorMc0MZBYOuJsJGCB8Bg8ds97rmE3oKBTV-_E   11s   system:bootstrap:5d820b    Approved,Issueddocker1.node ➜  ~ kubectl certificate approve csr-99l77certificatesigningrequest.certificates.k8s.io/csr-99l77 approved</code></pre></div><h3 id="3-5、部署-Calico"><a href="#3-5、部署-Calico" class="headerlink" title="3.5、部署 Calico"></a>3.5、部署 Calico</h3><p>当 node 全部启动后，由于网络组件(CNI)未安装会显示为 NotReady 状态；下面将部署 Calico 作为网络组件，完成跨节点网络通讯；具体安装文档可以参考 <a href="https://docs.projectcalico.org/v3.6/getting-started/kubernetes/installation/calico#installing-with-the-etcd-datastore" target="_blank" rel="noopener">Installing with the etcd datastore</a></p><p>以下为 calico 的配置文件</p><ul><li>calico.yaml</li></ul><div class="hljs"><pre><code class="hljs yaml"><span class="hljs-meta">---</span><span class="hljs-comment"># Source: calico/templates/calico-etcd-secrets.yaml</span><span class="hljs-comment"># The following contains k8s Secrets for use with a TLS enabled etcd cluster.</span><span class="hljs-comment"># For information on populating Secrets, see http://kubernetes.io/docs/user-guide/secrets/</span><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">v1</span><span class="hljs-attr">kind:</span> <span class="hljs-string">Secret</span><span class="hljs-attr">type:</span> <span class="hljs-string">Opaque</span><span class="hljs-attr">metadata:</span>  <span class="hljs-attr">name:</span> <span class="hljs-string">calico-etcd-secrets</span>  <span class="hljs-attr">namespace:</span> <span class="hljs-string">kube-system</span><span class="hljs-attr">data:</span>  <span class="hljs-comment"># Populate the following with etcd TLS configuration if desired, but leave blank if</span>  <span class="hljs-comment"># not using TLS for etcd.</span>  <span class="hljs-comment"># The keys below should be uncommented and the values populated with the base64</span>  <span class="hljs-comment"># encoded contents of each file that would be associated with the TLS data.</span>  <span class="hljs-comment"># Example command for encoding a file contents: cat &lt;file&gt; | base64 -w 0</span>  <span class="hljs-attr">etcd-key:</span> <span class="hljs-string">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</span>  <span class="hljs-attr">etcd-cert:</span> <span class="hljs-string">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</span>  <span class="hljs-attr">etcd-ca:</span> <span class="hljs-string">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</span><span class="hljs-meta">---</span><span class="hljs-comment"># Source: calico/templates/calico-config.yaml</span><span class="hljs-comment"># This ConfigMap is used to configure a self-hosted Calico installation.</span><span class="hljs-attr">kind:</span> <span class="hljs-string">ConfigMap</span><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">v1</span><span class="hljs-attr">metadata:</span>  <span class="hljs-attr">name:</span> <span class="hljs-string">calico-config</span>  <span class="hljs-attr">namespace:</span> <span class="hljs-string">kube-system</span><span class="hljs-attr">data:</span>  <span class="hljs-comment"># Configure this with the location of your etcd cluster.</span>  <span class="hljs-attr">etcd_endpoints:</span> <span class="hljs-string">"https://192.168.1.51:2379,https://192.168.1.52:2379,https://192.168.1.53:2379"</span>  <span class="hljs-comment"># If you're using TLS enabled etcd uncomment the following.</span>  <span class="hljs-comment"># You must also populate the Secret below with these files.</span>  <span class="hljs-attr">etcd_ca:</span> <span class="hljs-string">"/calico-secrets/etcd-ca"</span>  <span class="hljs-attr">etcd_cert:</span> <span class="hljs-string">"/calico-secrets/etcd-cert"</span>  <span class="hljs-attr">etcd_key:</span> <span class="hljs-string">"/calico-secrets/etcd-key"</span>  <span class="hljs-comment"># Typha is disabled.</span>  <span class="hljs-attr">typha_service_name:</span> <span class="hljs-string">"none"</span>  <span class="hljs-comment"># Configure the Calico backend to use.</span>  <span class="hljs-attr">calico_backend:</span> <span class="hljs-string">"bird"</span>  <span class="hljs-comment"># Configure the MTU to use</span>  <span class="hljs-attr">veth_mtu:</span> <span class="hljs-string">"1440"</span>  <span class="hljs-comment"># The CNI network configuration to install on each node.  The special</span>  <span class="hljs-comment"># values in this config will be automatically populated.</span>  <span class="hljs-attr">cni_network_config:</span> <span class="hljs-string">|-</span>    <span class="hljs-string">&#123;</span>      <span class="hljs-attr">"name":</span> <span class="hljs-string">"k8s-pod-network"</span><span class="hljs-string">,</span>      <span class="hljs-attr">"cniVersion":</span> <span class="hljs-string">"0.3.0"</span><span class="hljs-string">,</span>      <span class="hljs-attr">"plugins":</span> <span class="hljs-string">[</span>        <span class="hljs-string">&#123;</span>          <span class="hljs-attr">"type":</span> <span class="hljs-string">"calico"</span><span class="hljs-string">,</span>          <span class="hljs-attr">"log_level":</span> <span class="hljs-string">"info"</span><span class="hljs-string">,</span>          <span class="hljs-attr">"etcd_endpoints":</span> <span class="hljs-string">"__ETCD_ENDPOINTS__"</span><span class="hljs-string">,</span>          <span class="hljs-attr">"etcd_key_file":</span> <span class="hljs-string">"__ETCD_KEY_FILE__"</span><span class="hljs-string">,</span>          <span class="hljs-attr">"etcd_cert_file":</span> <span class="hljs-string">"__ETCD_CERT_FILE__"</span><span class="hljs-string">,</span>          <span class="hljs-attr">"etcd_ca_cert_file":</span> <span class="hljs-string">"__ETCD_CA_CERT_FILE__"</span><span class="hljs-string">,</span>          <span class="hljs-attr">"mtu":</span> <span class="hljs-string">__CNI_MTU__,</span>          <span class="hljs-attr">"ipam":</span> <span class="hljs-string">&#123;</span>              <span class="hljs-attr">"type":</span> <span class="hljs-string">"calico-ipam"</span>          <span class="hljs-string">&#125;,</span>          <span class="hljs-attr">"policy":</span> <span class="hljs-string">&#123;</span>              <span class="hljs-attr">"type":</span> <span class="hljs-string">"k8s"</span>          <span class="hljs-string">&#125;,</span>          <span class="hljs-attr">"kubernetes":</span> <span class="hljs-string">&#123;</span>              <span class="hljs-attr">"kubeconfig":</span> <span class="hljs-string">"__KUBECONFIG_FILEPATH__"</span>          <span class="hljs-string">&#125;</span>        <span class="hljs-string">&#125;,</span>        <span class="hljs-string">&#123;</span>          <span class="hljs-attr">"type":</span> <span class="hljs-string">"portmap"</span><span class="hljs-string">,</span>          <span class="hljs-attr">"snat":</span> <span class="hljs-literal">true</span><span class="hljs-string">,</span>          <span class="hljs-attr">"capabilities":</span> <span class="hljs-string">&#123;"portMappings":</span> <span class="hljs-literal">true</span><span class="hljs-string">&#125;</span>        <span class="hljs-string">&#125;</span>      <span class="hljs-string">]</span>    <span class="hljs-string">&#125;</span><span class="hljs-meta">---</span><span class="hljs-comment"># Source: calico/templates/rbac.yaml</span><span class="hljs-comment"># Include a clusterrole for the kube-controllers component,</span><span class="hljs-comment"># and bind it to the calico-kube-controllers serviceaccount.</span><span class="hljs-attr">kind:</span> <span class="hljs-string">ClusterRole</span><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">rbac.authorization.k8s.io/v1beta1</span><span class="hljs-attr">metadata:</span>  <span class="hljs-attr">name:</span> <span class="hljs-string">calico-kube-controllers</span><span class="hljs-attr">rules:</span>  <span class="hljs-comment"># Pods are monitored for changing labels.</span>  <span class="hljs-comment"># The node controller monitors Kubernetes nodes.</span>  <span class="hljs-comment"># Namespace and serviceaccount labels are used for policy.</span>  <span class="hljs-bullet">-</span> <span class="hljs-attr">apiGroups:</span> <span class="hljs-string">[""]</span>    <span class="hljs-attr">resources:</span>      <span class="hljs-bullet">-</span> <span class="hljs-string">pods</span>      <span class="hljs-bullet">-</span> <span class="hljs-string">nodes</span>      <span class="hljs-bullet">-</span> <span class="hljs-string">namespaces</span>      <span class="hljs-bullet">-</span> <span class="hljs-string">serviceaccounts</span>    <span class="hljs-attr">verbs:</span>      <span class="hljs-bullet">-</span> <span class="hljs-string">watch</span>      <span class="hljs-bullet">-</span> <span class="hljs-string">list</span>  <span class="hljs-comment"># Watch for changes to Kubernetes NetworkPolicies.</span>  <span class="hljs-bullet">-</span> <span class="hljs-attr">apiGroups:</span> <span class="hljs-string">["networking.k8s.io"]</span>    <span class="hljs-attr">resources:</span>      <span class="hljs-bullet">-</span> <span class="hljs-string">networkpolicies</span>    <span class="hljs-attr">verbs:</span>      <span class="hljs-bullet">-</span> <span class="hljs-string">watch</span>      <span class="hljs-bullet">-</span> <span class="hljs-string">list</span><span class="hljs-meta">---</span><span class="hljs-attr">kind:</span> <span class="hljs-string">ClusterRoleBinding</span><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">rbac.authorization.k8s.io/v1beta1</span><span class="hljs-attr">metadata:</span>  <span class="hljs-attr">name:</span> <span class="hljs-string">calico-kube-controllers</span><span class="hljs-attr">roleRef:</span>  <span class="hljs-attr">apiGroup:</span> <span class="hljs-string">rbac.authorization.k8s.io</span>  <span class="hljs-attr">kind:</span> <span class="hljs-string">ClusterRole</span>  <span class="hljs-attr">name:</span> <span class="hljs-string">calico-kube-controllers</span><span class="hljs-attr">subjects:</span><span class="hljs-bullet">-</span> <span class="hljs-attr">kind:</span> <span class="hljs-string">ServiceAccount</span>  <span class="hljs-attr">name:</span> <span class="hljs-string">calico-kube-controllers</span>  <span class="hljs-attr">namespace:</span> <span class="hljs-string">kube-system</span><span class="hljs-meta">---</span><span class="hljs-comment"># Include a clusterrole for the calico-node DaemonSet,</span><span class="hljs-comment"># and bind it to the calico-node serviceaccount.</span><span class="hljs-attr">kind:</span> <span class="hljs-string">ClusterRole</span><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">rbac.authorization.k8s.io/v1beta1</span><span class="hljs-attr">metadata:</span>  <span class="hljs-attr">name:</span> <span class="hljs-string">calico-node</span><span class="hljs-attr">rules:</span>  <span class="hljs-comment"># The CNI plugin needs to get pods, nodes, and namespaces.</span>  <span class="hljs-bullet">-</span> <span class="hljs-attr">apiGroups:</span> <span class="hljs-string">[""]</span>    <span class="hljs-attr">resources:</span>      <span class="hljs-bullet">-</span> <span class="hljs-string">pods</span>      <span class="hljs-bullet">-</span> <span class="hljs-string">nodes</span>      <span class="hljs-bullet">-</span> <span class="hljs-string">namespaces</span>    <span class="hljs-attr">verbs:</span>      <span class="hljs-bullet">-</span> <span class="hljs-string">get</span>  <span class="hljs-bullet">-</span> <span class="hljs-attr">apiGroups:</span> <span class="hljs-string">[""]</span>    <span class="hljs-attr">resources:</span>      <span class="hljs-bullet">-</span> <span class="hljs-string">endpoints</span>      <span class="hljs-bullet">-</span> <span class="hljs-string">services</span>    <span class="hljs-attr">verbs:</span>      <span class="hljs-comment"># Used to discover service IPs for advertisement.</span>      <span class="hljs-bullet">-</span> <span class="hljs-string">watch</span>      <span class="hljs-bullet">-</span> <span class="hljs-string">list</span>  <span class="hljs-bullet">-</span> <span class="hljs-attr">apiGroups:</span> <span class="hljs-string">[""]</span>    <span class="hljs-attr">resources:</span>      <span class="hljs-bullet">-</span> <span class="hljs-string">nodes/status</span>    <span class="hljs-attr">verbs:</span>      <span class="hljs-comment"># Needed for clearing NodeNetworkUnavailable flag.</span>      <span class="hljs-bullet">-</span> <span class="hljs-string">patch</span><span class="hljs-meta">---</span><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">rbac.authorization.k8s.io/v1beta1</span><span class="hljs-attr">kind:</span> <span class="hljs-string">ClusterRoleBinding</span><span class="hljs-attr">metadata:</span>  <span class="hljs-attr">name:</span> <span class="hljs-string">calico-node</span><span class="hljs-attr">roleRef:</span>  <span class="hljs-attr">apiGroup:</span> <span class="hljs-string">rbac.authorization.k8s.io</span>  <span class="hljs-attr">kind:</span> <span class="hljs-string">ClusterRole</span>  <span class="hljs-attr">name:</span> <span class="hljs-string">calico-node</span><span class="hljs-attr">subjects:</span><span class="hljs-bullet">-</span> <span class="hljs-attr">kind:</span> <span class="hljs-string">ServiceAccount</span>  <span class="hljs-attr">name:</span> <span class="hljs-string">calico-node</span>  <span class="hljs-attr">namespace:</span> <span class="hljs-string">kube-system</span><span class="hljs-meta">---</span><span class="hljs-meta">---</span><span class="hljs-comment"># Source: calico/templates/calico-node.yaml</span><span class="hljs-comment"># This manifest installs the calico/node container, as well</span><span class="hljs-comment"># as the Calico CNI plugins and network config on</span><span class="hljs-comment"># each master and worker node in a Kubernetes cluster.</span><span class="hljs-attr">kind:</span> <span class="hljs-string">DaemonSet</span><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">extensions/v1beta1</span><span class="hljs-attr">metadata:</span>  <span class="hljs-attr">name:</span> <span class="hljs-string">calico-node</span>  <span class="hljs-attr">namespace:</span> <span class="hljs-string">kube-system</span>  <span class="hljs-attr">labels:</span>    <span class="hljs-attr">k8s-app:</span> <span class="hljs-string">calico-node</span><span class="hljs-attr">spec:</span>  <span class="hljs-attr">selector:</span>    <span class="hljs-attr">matchLabels:</span>      <span class="hljs-attr">k8s-app:</span> <span class="hljs-string">calico-node</span>  <span class="hljs-attr">updateStrategy:</span>    <span class="hljs-attr">type:</span> <span class="hljs-string">RollingUpdate</span>    <span class="hljs-attr">rollingUpdate:</span>      <span class="hljs-attr">maxUnavailable:</span> <span class="hljs-number">1</span>  <span class="hljs-attr">template:</span>    <span class="hljs-attr">metadata:</span>      <span class="hljs-attr">labels:</span>        <span class="hljs-attr">k8s-app:</span> <span class="hljs-string">calico-node</span>      <span class="hljs-attr">annotations:</span>        <span class="hljs-comment"># This, along with the CriticalAddonsOnly toleration below,</span>        <span class="hljs-comment"># marks the pod as a critical add-on, ensuring it gets</span>        <span class="hljs-comment"># priority scheduling and that its resources are reserved</span>        <span class="hljs-comment"># if it ever gets evicted.</span>        <span class="hljs-attr">scheduler.alpha.kubernetes.io/critical-pod:</span> <span class="hljs-string">''</span>    <span class="hljs-attr">spec:</span>      <span class="hljs-attr">nodeSelector:</span>        <span class="hljs-attr">beta.kubernetes.io/os:</span> <span class="hljs-string">linux</span>      <span class="hljs-attr">hostNetwork:</span> <span class="hljs-literal">true</span>      <span class="hljs-attr">tolerations:</span>        <span class="hljs-comment"># Make sure calico-node gets scheduled on all nodes.</span>        <span class="hljs-bullet">-</span> <span class="hljs-attr">effect:</span> <span class="hljs-string">NoSchedule</span>          <span class="hljs-attr">operator:</span> <span class="hljs-string">Exists</span>        <span class="hljs-comment"># Mark the pod as a critical add-on for rescheduling.</span>        <span class="hljs-bullet">-</span> <span class="hljs-attr">key:</span> <span class="hljs-string">CriticalAddonsOnly</span>          <span class="hljs-attr">operator:</span> <span class="hljs-string">Exists</span>        <span class="hljs-bullet">-</span> <span class="hljs-attr">effect:</span> <span class="hljs-string">NoExecute</span>          <span class="hljs-attr">operator:</span> <span class="hljs-string">Exists</span>      <span class="hljs-attr">serviceAccountName:</span> <span class="hljs-string">calico-node</span>      <span class="hljs-comment"># Minimize downtime during a rolling upgrade or deletion; tell Kubernetes to do a "force</span>      <span class="hljs-comment"># deletion": https://kubernetes.io/docs/concepts/workloads/pods/pod/#termination-of-pods.</span>      <span class="hljs-attr">terminationGracePeriodSeconds:</span> <span class="hljs-number">0</span>      <span class="hljs-attr">initContainers:</span>        <span class="hljs-comment"># This container installs the Calico CNI binaries</span>        <span class="hljs-comment"># and CNI network config file on each node.</span>        <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">install-cni</span>          <span class="hljs-attr">image:</span> <span class="hljs-string">calico/cni:v3.6.0</span>          <span class="hljs-attr">command:</span> <span class="hljs-string">["/install-cni.sh"]</span>          <span class="hljs-attr">env:</span>            <span class="hljs-comment"># Name of the CNI config file to create.</span>            <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">CNI_CONF_NAME</span>              <span class="hljs-attr">value:</span> <span class="hljs-string">"10-calico.conflist"</span>            <span class="hljs-comment"># The CNI network config to install on each node.</span>            <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">CNI_NETWORK_CONFIG</span>              <span class="hljs-attr">valueFrom:</span>                <span class="hljs-attr">configMapKeyRef:</span>                  <span class="hljs-attr">name:</span> <span class="hljs-string">calico-config</span>                  <span class="hljs-attr">key:</span> <span class="hljs-string">cni_network_config</span>            <span class="hljs-comment"># The location of the Calico etcd cluster.</span>            <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">ETCD_ENDPOINTS</span>              <span class="hljs-attr">valueFrom:</span>                <span class="hljs-attr">configMapKeyRef:</span>                  <span class="hljs-attr">name:</span> <span class="hljs-string">calico-config</span>                  <span class="hljs-attr">key:</span> <span class="hljs-string">etcd_endpoints</span>            <span class="hljs-comment"># CNI MTU Config variable</span>            <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">CNI_MTU</span>              <span class="hljs-attr">valueFrom:</span>                <span class="hljs-attr">configMapKeyRef:</span>                  <span class="hljs-attr">name:</span> <span class="hljs-string">calico-config</span>                  <span class="hljs-attr">key:</span> <span class="hljs-string">veth_mtu</span>            <span class="hljs-comment"># Prevents the container from sleeping forever.</span>            <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">SLEEP</span>              <span class="hljs-attr">value:</span> <span class="hljs-string">"false"</span>          <span class="hljs-attr">volumeMounts:</span>            <span class="hljs-bullet">-</span> <span class="hljs-attr">mountPath:</span> <span class="hljs-string">/host/opt/cni/bin</span>              <span class="hljs-attr">name:</span> <span class="hljs-string">cni-bin-dir</span>            <span class="hljs-bullet">-</span> <span class="hljs-attr">mountPath:</span> <span class="hljs-string">/host/etc/cni/net.d</span>              <span class="hljs-attr">name:</span> <span class="hljs-string">cni-net-dir</span>            <span class="hljs-bullet">-</span> <span class="hljs-attr">mountPath:</span> <span class="hljs-string">/calico-secrets</span>              <span class="hljs-attr">name:</span> <span class="hljs-string">etcd-certs</span>      <span class="hljs-attr">containers:</span>        <span class="hljs-comment"># Runs calico/node container on each Kubernetes node.  This</span>        <span class="hljs-comment"># container programs network policy and routes on each</span>        <span class="hljs-comment"># host.</span>        <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">calico-node</span>          <span class="hljs-attr">image:</span> <span class="hljs-string">calico/node:v3.6.0</span>          <span class="hljs-attr">env:</span>            <span class="hljs-comment"># The location of the Calico etcd cluster.</span>            <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">ETCD_ENDPOINTS</span>              <span class="hljs-attr">valueFrom:</span>                <span class="hljs-attr">configMapKeyRef:</span>                  <span class="hljs-attr">name:</span> <span class="hljs-string">calico-config</span>                  <span class="hljs-attr">key:</span> <span class="hljs-string">etcd_endpoints</span>            <span class="hljs-comment"># Location of the CA certificate for etcd.</span>            <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">ETCD_CA_CERT_FILE</span>              <span class="hljs-attr">valueFrom:</span>                <span class="hljs-attr">configMapKeyRef:</span>                  <span class="hljs-attr">name:</span> <span class="hljs-string">calico-config</span>                  <span class="hljs-attr">key:</span> <span class="hljs-string">etcd_ca</span>            <span class="hljs-comment"># Location of the client key for etcd.</span>            <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">ETCD_KEY_FILE</span>              <span class="hljs-attr">valueFrom:</span>                <span class="hljs-attr">configMapKeyRef:</span>                  <span class="hljs-attr">name:</span> <span class="hljs-string">calico-config</span>                  <span class="hljs-attr">key:</span> <span class="hljs-string">etcd_key</span>            <span class="hljs-comment"># Location of the client certificate for etcd.</span>            <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">ETCD_CERT_FILE</span>              <span class="hljs-attr">valueFrom:</span>                <span class="hljs-attr">configMapKeyRef:</span>                  <span class="hljs-attr">name:</span> <span class="hljs-string">calico-config</span>                  <span class="hljs-attr">key:</span> <span class="hljs-string">etcd_cert</span>            <span class="hljs-comment"># Set noderef for node controller.</span>            <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">CALICO_K8S_NODE_REF</span>              <span class="hljs-attr">valueFrom:</span>                <span class="hljs-attr">fieldRef:</span>                  <span class="hljs-attr">fieldPath:</span> <span class="hljs-string">spec.nodeName</span>            <span class="hljs-comment"># Choose the backend to use.</span>            <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">CALICO_NETWORKING_BACKEND</span>              <span class="hljs-attr">valueFrom:</span>                <span class="hljs-attr">configMapKeyRef:</span>                  <span class="hljs-attr">name:</span> <span class="hljs-string">calico-config</span>                  <span class="hljs-attr">key:</span> <span class="hljs-string">calico_backend</span>            <span class="hljs-comment"># Cluster type to identify the deployment type</span>            <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">CLUSTER_TYPE</span>              <span class="hljs-attr">value:</span> <span class="hljs-string">"k8s,bgp"</span>            <span class="hljs-comment"># Auto-detect the BGP IP address.</span>            <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">IP</span>              <span class="hljs-attr">value:</span> <span class="hljs-string">"autodetect"</span>            <span class="hljs-comment"># Enable IPIP</span>            <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">CALICO_IPV4POOL_IPIP</span>              <span class="hljs-attr">value:</span> <span class="hljs-string">"Always"</span>            <span class="hljs-comment"># Set MTU for tunnel device used if ipip is enabled</span>            <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">FELIX_IPINIPMTU</span>              <span class="hljs-attr">valueFrom:</span>                <span class="hljs-attr">configMapKeyRef:</span>                  <span class="hljs-attr">name:</span> <span class="hljs-string">calico-config</span>                  <span class="hljs-attr">key:</span> <span class="hljs-string">veth_mtu</span>            <span class="hljs-comment"># The default IPv4 pool to create on startup if none exists. Pod IPs will be</span>            <span class="hljs-comment"># chosen from this range. Changing this value after installation will have</span>            <span class="hljs-comment"># no effect. This should fall within `--cluster-cidr`.</span>            <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">CALICO_IPV4POOL_CIDR</span>              <span class="hljs-attr">value:</span> <span class="hljs-string">"10.20.0.0/16"</span>            <span class="hljs-comment"># Disable file logging so `kubectl logs` works.</span>            <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">CALICO_DISABLE_FILE_LOGGING</span>              <span class="hljs-attr">value:</span> <span class="hljs-string">"true"</span>            <span class="hljs-comment"># Set Felix endpoint to host default action to ACCEPT.</span>            <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">FELIX_DEFAULTENDPOINTTOHOSTACTION</span>              <span class="hljs-attr">value:</span> <span class="hljs-string">"ACCEPT"</span>            <span class="hljs-comment"># Disable IPv6 on Kubernetes.</span>            <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">FELIX_IPV6SUPPORT</span>              <span class="hljs-attr">value:</span> <span class="hljs-string">"false"</span>            <span class="hljs-comment"># Set Felix logging to "info"</span>            <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">FELIX_LOGSEVERITYSCREEN</span>              <span class="hljs-attr">value:</span> <span class="hljs-string">"info"</span>            <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">FELIX_HEALTHENABLED</span>              <span class="hljs-attr">value:</span> <span class="hljs-string">"true"</span>            <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">IP_AUTODETECTION_METHOD</span>              <span class="hljs-attr">value:</span> <span class="hljs-string">can-reach=192.168.1.51</span>          <span class="hljs-attr">securityContext:</span>            <span class="hljs-attr">privileged:</span> <span class="hljs-literal">true</span>          <span class="hljs-attr">resources:</span>            <span class="hljs-attr">requests:</span>              <span class="hljs-attr">cpu:</span> <span class="hljs-string">250m</span>          <span class="hljs-attr">livenessProbe:</span>            <span class="hljs-attr">httpGet:</span>              <span class="hljs-attr">path:</span> <span class="hljs-string">/liveness</span>              <span class="hljs-attr">port:</span> <span class="hljs-number">9099</span>              <span class="hljs-attr">host:</span> <span class="hljs-string">localhost</span>            <span class="hljs-attr">periodSeconds:</span> <span class="hljs-number">10</span>            <span class="hljs-attr">initialDelaySeconds:</span> <span class="hljs-number">10</span>            <span class="hljs-attr">failureThreshold:</span> <span class="hljs-number">6</span>          <span class="hljs-attr">readinessProbe:</span>            <span class="hljs-attr">exec:</span>              <span class="hljs-attr">command:</span>              <span class="hljs-bullet">-</span> <span class="hljs-string">/bin/calico-node</span>              <span class="hljs-bullet">-</span> <span class="hljs-string">-bird-ready</span>              <span class="hljs-bullet">-</span> <span class="hljs-string">-felix-ready</span>            <span class="hljs-attr">periodSeconds:</span> <span class="hljs-number">10</span>          <span class="hljs-attr">volumeMounts:</span>            <span class="hljs-bullet">-</span> <span class="hljs-attr">mountPath:</span> <span class="hljs-string">/lib/modules</span>              <span class="hljs-attr">name:</span> <span class="hljs-string">lib-modules</span>              <span class="hljs-attr">readOnly:</span> <span class="hljs-literal">true</span>            <span class="hljs-bullet">-</span> <span class="hljs-attr">mountPath:</span> <span class="hljs-string">/run/xtables.lock</span>              <span class="hljs-attr">name:</span> <span class="hljs-string">xtables-lock</span>              <span class="hljs-attr">readOnly:</span> <span class="hljs-literal">false</span>            <span class="hljs-bullet">-</span> <span class="hljs-attr">mountPath:</span> <span class="hljs-string">/var/run/calico</span>              <span class="hljs-attr">name:</span> <span class="hljs-string">var-run-calico</span>              <span class="hljs-attr">readOnly:</span> <span class="hljs-literal">false</span>            <span class="hljs-bullet">-</span> <span class="hljs-attr">mountPath:</span> <span class="hljs-string">/var/lib/calico</span>              <span class="hljs-attr">name:</span> <span class="hljs-string">var-lib-calico</span>              <span class="hljs-attr">readOnly:</span> <span class="hljs-literal">false</span>            <span class="hljs-bullet">-</span> <span class="hljs-attr">mountPath:</span> <span class="hljs-string">/calico-secrets</span>              <span class="hljs-attr">name:</span> <span class="hljs-string">etcd-certs</span>      <span class="hljs-attr">volumes:</span>        <span class="hljs-comment"># Used by calico/node.</span>        <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">lib-modules</span>          <span class="hljs-attr">hostPath:</span>            <span class="hljs-attr">path:</span> <span class="hljs-string">/lib/modules</span>        <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">var-run-calico</span>          <span class="hljs-attr">hostPath:</span>            <span class="hljs-attr">path:</span> <span class="hljs-string">/var/run/calico</span>        <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">var-lib-calico</span>          <span class="hljs-attr">hostPath:</span>            <span class="hljs-attr">path:</span> <span class="hljs-string">/var/lib/calico</span>        <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">xtables-lock</span>          <span class="hljs-attr">hostPath:</span>            <span class="hljs-attr">path:</span> <span class="hljs-string">/run/xtables.lock</span>            <span class="hljs-attr">type:</span> <span class="hljs-string">FileOrCreate</span>        <span class="hljs-comment"># Used to install CNI.</span>        <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">cni-bin-dir</span>          <span class="hljs-attr">hostPath:</span>            <span class="hljs-attr">path:</span> <span class="hljs-string">/opt/cni/bin</span>        <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">cni-net-dir</span>          <span class="hljs-attr">hostPath:</span>            <span class="hljs-attr">path:</span> <span class="hljs-string">/etc/cni/net.d</span>        <span class="hljs-comment"># Mount in the etcd TLS secrets with mode 400.</span>        <span class="hljs-comment"># See https://kubernetes.io/docs/concepts/configuration/secret/</span>        <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">etcd-certs</span>          <span class="hljs-attr">secret:</span>            <span class="hljs-attr">secretName:</span> <span class="hljs-string">calico-etcd-secrets</span>            <span class="hljs-attr">defaultMode:</span> <span class="hljs-number">0400</span><span class="hljs-meta">---</span><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">v1</span><span class="hljs-attr">kind:</span> <span class="hljs-string">ServiceAccount</span><span class="hljs-attr">metadata:</span>  <span class="hljs-attr">name:</span> <span class="hljs-string">calico-node</span>  <span class="hljs-attr">namespace:</span> <span class="hljs-string">kube-system</span><span class="hljs-meta">---</span><span class="hljs-comment"># Source: calico/templates/calico-kube-controllers.yaml</span><span class="hljs-comment"># This manifest deploys the Calico Kubernetes controllers.</span><span class="hljs-comment"># See https://github.com/projectcalico/kube-controllers</span><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">extensions/v1beta1</span><span class="hljs-attr">kind:</span> <span class="hljs-string">Deployment</span><span class="hljs-attr">metadata:</span>  <span class="hljs-attr">name:</span> <span class="hljs-string">calico-kube-controllers</span>  <span class="hljs-attr">namespace:</span> <span class="hljs-string">kube-system</span>  <span class="hljs-attr">labels:</span>    <span class="hljs-attr">k8s-app:</span> <span class="hljs-string">calico-kube-controllers</span>  <span class="hljs-attr">annotations:</span>    <span class="hljs-attr">scheduler.alpha.kubernetes.io/critical-pod:</span> <span class="hljs-string">''</span><span class="hljs-attr">spec:</span>  <span class="hljs-comment"># The controllers can only have a single active instance.</span>  <span class="hljs-attr">replicas:</span> <span class="hljs-number">1</span>  <span class="hljs-attr">strategy:</span>    <span class="hljs-attr">type:</span> <span class="hljs-string">Recreate</span>  <span class="hljs-attr">template:</span>    <span class="hljs-attr">metadata:</span>      <span class="hljs-attr">name:</span> <span class="hljs-string">calico-kube-controllers</span>      <span class="hljs-attr">namespace:</span> <span class="hljs-string">kube-system</span>      <span class="hljs-attr">labels:</span>        <span class="hljs-attr">k8s-app:</span> <span class="hljs-string">calico-kube-controllers</span>    <span class="hljs-attr">spec:</span>      <span class="hljs-attr">nodeSelector:</span>        <span class="hljs-attr">beta.kubernetes.io/os:</span> <span class="hljs-string">linux</span>      <span class="hljs-comment"># The controllers must run in the host network namespace so that</span>      <span class="hljs-comment"># it isn't governed by policy that would prevent it from working.</span>      <span class="hljs-attr">hostNetwork:</span> <span class="hljs-literal">true</span>      <span class="hljs-attr">tolerations:</span>        <span class="hljs-comment"># Mark the pod as a critical add-on for rescheduling.</span>        <span class="hljs-bullet">-</span> <span class="hljs-attr">key:</span> <span class="hljs-string">CriticalAddonsOnly</span>          <span class="hljs-attr">operator:</span> <span class="hljs-string">Exists</span>        <span class="hljs-bullet">-</span> <span class="hljs-attr">key:</span> <span class="hljs-string">node-role.kubernetes.io/master</span>          <span class="hljs-attr">effect:</span> <span class="hljs-string">NoSchedule</span>      <span class="hljs-attr">serviceAccountName:</span> <span class="hljs-string">calico-kube-controllers</span>      <span class="hljs-attr">containers:</span>        <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">calico-kube-controllers</span>          <span class="hljs-attr">image:</span> <span class="hljs-string">calico/kube-controllers:v3.6.0</span>          <span class="hljs-attr">env:</span>            <span class="hljs-comment"># The location of the Calico etcd cluster.</span>            <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">ETCD_ENDPOINTS</span>              <span class="hljs-attr">valueFrom:</span>                <span class="hljs-attr">configMapKeyRef:</span>                  <span class="hljs-attr">name:</span> <span class="hljs-string">calico-config</span>                  <span class="hljs-attr">key:</span> <span class="hljs-string">etcd_endpoints</span>            <span class="hljs-comment"># Location of the CA certificate for etcd.</span>            <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">ETCD_CA_CERT_FILE</span>              <span class="hljs-attr">valueFrom:</span>                <span class="hljs-attr">configMapKeyRef:</span>                  <span class="hljs-attr">name:</span> <span class="hljs-string">calico-config</span>                  <span class="hljs-attr">key:</span> <span class="hljs-string">etcd_ca</span>            <span class="hljs-comment"># Location of the client key for etcd.</span>            <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">ETCD_KEY_FILE</span>              <span class="hljs-attr">valueFrom:</span>                <span class="hljs-attr">configMapKeyRef:</span>                  <span class="hljs-attr">name:</span> <span class="hljs-string">calico-config</span>                  <span class="hljs-attr">key:</span> <span class="hljs-string">etcd_key</span>            <span class="hljs-comment"># Location of the client certificate for etcd.</span>            <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">ETCD_CERT_FILE</span>              <span class="hljs-attr">valueFrom:</span>                <span class="hljs-attr">configMapKeyRef:</span>                  <span class="hljs-attr">name:</span> <span class="hljs-string">calico-config</span>                  <span class="hljs-attr">key:</span> <span class="hljs-string">etcd_cert</span>            <span class="hljs-comment"># Choose which controllers to run.</span>            <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">ENABLED_CONTROLLERS</span>              <span class="hljs-attr">value:</span> <span class="hljs-string">policy,namespace,serviceaccount,workloadendpoint,node</span>          <span class="hljs-attr">volumeMounts:</span>            <span class="hljs-comment"># Mount in the etcd TLS secrets.</span>            <span class="hljs-bullet">-</span> <span class="hljs-attr">mountPath:</span> <span class="hljs-string">/calico-secrets</span>              <span class="hljs-attr">name:</span> <span class="hljs-string">etcd-certs</span>          <span class="hljs-attr">readinessProbe:</span>            <span class="hljs-attr">exec:</span>              <span class="hljs-attr">command:</span>              <span class="hljs-bullet">-</span> <span class="hljs-string">/usr/bin/check-status</span>              <span class="hljs-bullet">-</span> <span class="hljs-string">-r</span>      <span class="hljs-attr">volumes:</span>        <span class="hljs-comment"># Mount in the etcd TLS secrets with mode 400.</span>        <span class="hljs-comment"># See https://kubernetes.io/docs/concepts/configuration/secret/</span>        <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">etcd-certs</span>          <span class="hljs-attr">secret:</span>            <span class="hljs-attr">secretName:</span> <span class="hljs-string">calico-etcd-secrets</span>            <span class="hljs-attr">defaultMode:</span> <span class="hljs-number">0400</span><span class="hljs-meta">---</span><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">v1</span><span class="hljs-attr">kind:</span> <span class="hljs-string">ServiceAccount</span><span class="hljs-attr">metadata:</span>  <span class="hljs-attr">name:</span> <span class="hljs-string">calico-kube-controllers</span>  <span class="hljs-attr">namespace:</span> <span class="hljs-string">kube-system</span></code></pre></div><p><strong>需要注意的是我们添加了 <code>IP_AUTODETECTION_METHOD</code> 变量，这个变量会设置 calcio 获取 node ip 的方式；默认情况下采用 <a href="https://docs.projectcalico.org/v3.6/reference/node/configuration#ip-autodetection-methods" target="_blank" rel="noopener">first-found</a> 方式获取，即获取第一个有效网卡的 IP 作为 node ip；在某些多网卡机器上可能会出现问题；这里将值设置为 <code>can-reach=192.168.1.51</code>，即使用第一个能够访问 master <code>192.168.1.51</code> 的网卡地址作为 node ip</strong></p><p>最后执行创建即可，创建成功后如下所示</p><div class="hljs"><pre><code class="hljs sh">docker1.node ➜  ~ kubectl get pod -o wide -n kube-systemNAME                                      READY   STATUS    RESTARTS   AGE   IP             NODE           NOMINATED NODE   READINESS GATEScalico-kube-controllers-65bc6b9f9-cn27f   1/1     Running   0          85s   192.168.1.53   docker3.node   &lt;none&gt;           &lt;none&gt;calico-node-c5nl8                         1/1     Running   0          85s   192.168.1.53   docker3.node   &lt;none&gt;           &lt;none&gt;calico-node-fqknv                         1/1     Running   0          85s   192.168.1.51   docker1.node   &lt;none&gt;           &lt;none&gt;calico-node-ldfzs                         1/1     Running   0          85s   192.168.1.55   docker5.node   &lt;none&gt;           &lt;none&gt;calico-node-ngjxc                         1/1     Running   0          85s   192.168.1.52   docker2.node   &lt;none&gt;           &lt;none&gt;calico-node-vj8np                         1/1     Running   0          85s   192.168.1.54   docker4.node   &lt;none&gt;           &lt;none&gt;</code></pre></div><p>此时所有 node 应当变为 Ready 状态</p><h3 id="3-5、部署-DNS"><a href="#3-5、部署-DNS" class="headerlink" title="3.5、部署 DNS"></a>3.5、部署 DNS</h3><p>其他组件全部完成后我们应当部署集群 DNS 使 service 等能够正常解析；集群 DNS 这里采用 coredns，具体安装文档参考 <a href="https://github.com/coredns/deployment/tree/master/kubernetes" target="_blank" rel="noopener">coredns/deployment</a>；coredns 完整配置如下</p><ul><li>coredns.yaml</li></ul><div class="hljs"><pre><code class="hljs yaml"><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">v1</span><span class="hljs-attr">kind:</span> <span class="hljs-string">ServiceAccount</span><span class="hljs-attr">metadata:</span>  <span class="hljs-attr">name:</span> <span class="hljs-string">coredns</span>  <span class="hljs-attr">namespace:</span> <span class="hljs-string">kube-system</span><span class="hljs-meta">---</span><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">rbac.authorization.k8s.io/v1</span><span class="hljs-attr">kind:</span> <span class="hljs-string">ClusterRole</span><span class="hljs-attr">metadata:</span>  <span class="hljs-attr">labels:</span>    <span class="hljs-attr">kubernetes.io/bootstrapping:</span> <span class="hljs-string">rbac-defaults</span>  <span class="hljs-attr">name:</span> <span class="hljs-string">system:coredns</span><span class="hljs-attr">rules:</span><span class="hljs-bullet">-</span> <span class="hljs-attr">apiGroups:</span>  <span class="hljs-bullet">-</span> <span class="hljs-string">""</span>  <span class="hljs-attr">resources:</span>  <span class="hljs-bullet">-</span> <span class="hljs-string">endpoints</span>  <span class="hljs-bullet">-</span> <span class="hljs-string">services</span>  <span class="hljs-bullet">-</span> <span class="hljs-string">pods</span>  <span class="hljs-bullet">-</span> <span class="hljs-string">namespaces</span>  <span class="hljs-attr">verbs:</span>  <span class="hljs-bullet">-</span> <span class="hljs-string">list</span>  <span class="hljs-bullet">-</span> <span class="hljs-string">watch</span><span class="hljs-bullet">-</span> <span class="hljs-attr">apiGroups:</span>  <span class="hljs-bullet">-</span> <span class="hljs-string">""</span>  <span class="hljs-attr">resources:</span>  <span class="hljs-bullet">-</span> <span class="hljs-string">nodes</span>  <span class="hljs-attr">verbs:</span>  <span class="hljs-bullet">-</span> <span class="hljs-string">get</span><span class="hljs-meta">---</span><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">rbac.authorization.k8s.io/v1</span><span class="hljs-attr">kind:</span> <span class="hljs-string">ClusterRoleBinding</span><span class="hljs-attr">metadata:</span>  <span class="hljs-attr">annotations:</span>    <span class="hljs-attr">rbac.authorization.kubernetes.io/autoupdate:</span> <span class="hljs-string">"true"</span>  <span class="hljs-attr">labels:</span>    <span class="hljs-attr">kubernetes.io/bootstrapping:</span> <span class="hljs-string">rbac-defaults</span>  <span class="hljs-attr">name:</span> <span class="hljs-string">system:coredns</span><span class="hljs-attr">roleRef:</span>  <span class="hljs-attr">apiGroup:</span> <span class="hljs-string">rbac.authorization.k8s.io</span>  <span class="hljs-attr">kind:</span> <span class="hljs-string">ClusterRole</span>  <span class="hljs-attr">name:</span> <span class="hljs-string">system:coredns</span><span class="hljs-attr">subjects:</span><span class="hljs-bullet">-</span> <span class="hljs-attr">kind:</span> <span class="hljs-string">ServiceAccount</span>  <span class="hljs-attr">name:</span> <span class="hljs-string">coredns</span>  <span class="hljs-attr">namespace:</span> <span class="hljs-string">kube-system</span><span class="hljs-meta">---</span><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">v1</span><span class="hljs-attr">kind:</span> <span class="hljs-string">ConfigMap</span><span class="hljs-attr">metadata:</span>  <span class="hljs-attr">name:</span> <span class="hljs-string">coredns</span>  <span class="hljs-attr">namespace:</span> <span class="hljs-string">kube-system</span><span class="hljs-attr">data:</span>  <span class="hljs-attr">Corefile:</span> <span class="hljs-string">|</span>    <span class="hljs-string">.:53</span> <span class="hljs-string">&#123;</span>        <span class="hljs-string">errors</span>        <span class="hljs-string">health</span>        <span class="hljs-string">kubernetes</span> <span class="hljs-string">cluster.local</span> <span class="hljs-string">in-addr.arpa</span> <span class="hljs-string">ip6.arpa</span> <span class="hljs-string">&#123;</span>          <span class="hljs-string">pods</span> <span class="hljs-string">insecure</span>          <span class="hljs-string">upstream</span>          <span class="hljs-string">fallthrough</span> <span class="hljs-string">in-addr.arpa</span> <span class="hljs-string">ip6.arpa</span>        <span class="hljs-string">&#125;</span>        <span class="hljs-string">prometheus</span> <span class="hljs-string">:9153</span>        <span class="hljs-string">forward</span> <span class="hljs-string">.</span> <span class="hljs-string">/etc/resolv.conf</span>        <span class="hljs-string">cache</span> <span class="hljs-number">30</span>        <span class="hljs-string">loop</span>        <span class="hljs-string">reload</span>        <span class="hljs-string">loadbalance</span>    <span class="hljs-string">&#125;</span><span class="hljs-meta">---</span><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">apps/v1</span><span class="hljs-attr">kind:</span> <span class="hljs-string">Deployment</span><span class="hljs-attr">metadata:</span>  <span class="hljs-attr">name:</span> <span class="hljs-string">coredns</span>  <span class="hljs-attr">namespace:</span> <span class="hljs-string">kube-system</span>  <span class="hljs-attr">labels:</span>    <span class="hljs-attr">k8s-app:</span> <span class="hljs-string">kube-dns</span>    <span class="hljs-attr">kubernetes.io/name:</span> <span class="hljs-string">"CoreDNS"</span><span class="hljs-attr">spec:</span>  <span class="hljs-attr">replicas:</span> <span class="hljs-number">2</span>  <span class="hljs-attr">strategy:</span>    <span class="hljs-attr">type:</span> <span class="hljs-string">RollingUpdate</span>    <span class="hljs-attr">rollingUpdate:</span>      <span class="hljs-attr">maxUnavailable:</span> <span class="hljs-number">1</span>  <span class="hljs-attr">selector:</span>    <span class="hljs-attr">matchLabels:</span>      <span class="hljs-attr">k8s-app:</span> <span class="hljs-string">kube-dns</span>  <span class="hljs-attr">template:</span>    <span class="hljs-attr">metadata:</span>      <span class="hljs-attr">labels:</span>        <span class="hljs-attr">k8s-app:</span> <span class="hljs-string">kube-dns</span>    <span class="hljs-attr">spec:</span>      <span class="hljs-attr">priorityClassName:</span> <span class="hljs-string">system-cluster-critical</span>      <span class="hljs-attr">serviceAccountName:</span> <span class="hljs-string">coredns</span>      <span class="hljs-attr">tolerations:</span>        <span class="hljs-bullet">-</span> <span class="hljs-attr">key:</span> <span class="hljs-string">"CriticalAddonsOnly"</span>          <span class="hljs-attr">operator:</span> <span class="hljs-string">"Exists"</span>      <span class="hljs-attr">nodeSelector:</span>        <span class="hljs-attr">beta.kubernetes.io/os:</span> <span class="hljs-string">linux</span>      <span class="hljs-attr">containers:</span>      <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">coredns</span>        <span class="hljs-attr">image:</span> <span class="hljs-string">coredns/coredns:1.3.1</span>        <span class="hljs-attr">imagePullPolicy:</span> <span class="hljs-string">IfNotPresent</span>        <span class="hljs-attr">resources:</span>          <span class="hljs-attr">limits:</span>            <span class="hljs-attr">memory:</span> <span class="hljs-string">170Mi</span>          <span class="hljs-attr">requests:</span>            <span class="hljs-attr">cpu:</span> <span class="hljs-string">100m</span>            <span class="hljs-attr">memory:</span> <span class="hljs-string">70Mi</span>        <span class="hljs-attr">args:</span> <span class="hljs-string">[</span> <span class="hljs-string">"-conf"</span><span class="hljs-string">,</span> <span class="hljs-string">"/etc/coredns/Corefile"</span> <span class="hljs-string">]</span>        <span class="hljs-attr">volumeMounts:</span>        <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">config-volume</span>          <span class="hljs-attr">mountPath:</span> <span class="hljs-string">/etc/coredns</span>          <span class="hljs-attr">readOnly:</span> <span class="hljs-literal">true</span>        <span class="hljs-attr">ports:</span>        <span class="hljs-bullet">-</span> <span class="hljs-attr">containerPort:</span> <span class="hljs-number">53</span>          <span class="hljs-attr">name:</span> <span class="hljs-string">dns</span>          <span class="hljs-attr">protocol:</span> <span class="hljs-string">UDP</span>        <span class="hljs-bullet">-</span> <span class="hljs-attr">containerPort:</span> <span class="hljs-number">53</span>          <span class="hljs-attr">name:</span> <span class="hljs-string">dns-tcp</span>          <span class="hljs-attr">protocol:</span> <span class="hljs-string">TCP</span>        <span class="hljs-bullet">-</span> <span class="hljs-attr">containerPort:</span> <span class="hljs-number">9153</span>          <span class="hljs-attr">name:</span> <span class="hljs-string">metrics</span>          <span class="hljs-attr">protocol:</span> <span class="hljs-string">TCP</span>        <span class="hljs-attr">securityContext:</span>          <span class="hljs-attr">allowPrivilegeEscalation:</span> <span class="hljs-literal">false</span>          <span class="hljs-attr">capabilities:</span>            <span class="hljs-attr">add:</span>            <span class="hljs-bullet">-</span> <span class="hljs-string">NET_BIND_SERVICE</span>            <span class="hljs-attr">drop:</span>            <span class="hljs-bullet">-</span> <span class="hljs-string">all</span>          <span class="hljs-attr">readOnlyRootFilesystem:</span> <span class="hljs-literal">true</span>        <span class="hljs-attr">livenessProbe:</span>          <span class="hljs-attr">httpGet:</span>            <span class="hljs-attr">path:</span> <span class="hljs-string">/health</span>            <span class="hljs-attr">port:</span> <span class="hljs-number">8080</span>            <span class="hljs-attr">scheme:</span> <span class="hljs-string">HTTP</span>          <span class="hljs-attr">initialDelaySeconds:</span> <span class="hljs-number">60</span>          <span class="hljs-attr">timeoutSeconds:</span> <span class="hljs-number">5</span>          <span class="hljs-attr">successThreshold:</span> <span class="hljs-number">1</span>          <span class="hljs-attr">failureThreshold:</span> <span class="hljs-number">5</span>        <span class="hljs-attr">readinessProbe:</span>          <span class="hljs-attr">httpGet:</span>            <span class="hljs-attr">path:</span> <span class="hljs-string">/health</span>            <span class="hljs-attr">port:</span> <span class="hljs-number">8080</span>            <span class="hljs-attr">scheme:</span> <span class="hljs-string">HTTP</span>      <span class="hljs-attr">dnsPolicy:</span> <span class="hljs-string">Default</span>      <span class="hljs-attr">volumes:</span>        <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">config-volume</span>          <span class="hljs-attr">configMap:</span>            <span class="hljs-attr">name:</span> <span class="hljs-string">coredns</span>            <span class="hljs-attr">items:</span>            <span class="hljs-bullet">-</span> <span class="hljs-attr">key:</span> <span class="hljs-string">Corefile</span>              <span class="hljs-attr">path:</span> <span class="hljs-string">Corefile</span><span class="hljs-meta">---</span><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">v1</span><span class="hljs-attr">kind:</span> <span class="hljs-string">Service</span><span class="hljs-attr">metadata:</span>  <span class="hljs-attr">name:</span> <span class="hljs-string">kube-dns</span>  <span class="hljs-attr">namespace:</span> <span class="hljs-string">kube-system</span>  <span class="hljs-attr">annotations:</span>    <span class="hljs-attr">prometheus.io/port:</span> <span class="hljs-string">"9153"</span>    <span class="hljs-attr">prometheus.io/scrape:</span> <span class="hljs-string">"true"</span>  <span class="hljs-attr">labels:</span>    <span class="hljs-attr">k8s-app:</span> <span class="hljs-string">kube-dns</span>    <span class="hljs-attr">kubernetes.io/cluster-service:</span> <span class="hljs-string">"true"</span>    <span class="hljs-attr">kubernetes.io/name:</span> <span class="hljs-string">"CoreDNS"</span><span class="hljs-attr">spec:</span>  <span class="hljs-attr">selector:</span>    <span class="hljs-attr">k8s-app:</span> <span class="hljs-string">kube-dns</span>  <span class="hljs-attr">clusterIP:</span> <span class="hljs-number">10.254</span><span class="hljs-number">.0</span><span class="hljs-number">.2</span>  <span class="hljs-attr">ports:</span>  <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">dns</span>    <span class="hljs-attr">port:</span> <span class="hljs-number">53</span>    <span class="hljs-attr">protocol:</span> <span class="hljs-string">UDP</span>  <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">dns-tcp</span>    <span class="hljs-attr">port:</span> <span class="hljs-number">53</span>    <span class="hljs-attr">protocol:</span> <span class="hljs-string">TCP</span>  <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">metrics</span>    <span class="hljs-attr">port:</span> <span class="hljs-number">9153</span>    <span class="hljs-attr">protocol:</span> <span class="hljs-string">TCP</span></code></pre></div><h3 id="3-5、部署-DNS-自动扩容"><a href="#3-5、部署-DNS-自动扩容" class="headerlink" title="3.5、部署 DNS 自动扩容"></a>3.5、部署 DNS 自动扩容</h3><p>在大规模集群的情况下，可能需要集群 DNS 自动扩容，具体文档请参考 <a href="https://github.com/kubernetes/kubernetes/tree/master/cluster/addons/dns-horizontal-autoscaler" target="_blank" rel="noopener">DNS Horizontal Autoscaler</a>，DNS 扩容算法可参考 <a href="https://github.com/kubernetes-incubator/cluster-proportional-autoscaler/" target="_blank" rel="noopener">Github</a>，如有需要请自行修改；以下为具体配置</p><ul><li>dns-horizontal-autoscaler.yaml</li></ul><div class="hljs"><pre><code class="hljs yaml"><span class="hljs-comment"># Copyright 2016 The Kubernetes Authors.</span><span class="hljs-comment">#</span><span class="hljs-comment"># Licensed under the Apache License, Version 2.0 (the "License");</span><span class="hljs-comment"># you may not use this file except in compliance with the License.</span><span class="hljs-comment"># You may obtain a copy of the License at</span><span class="hljs-comment">#</span><span class="hljs-comment">#     http://www.apache.org/licenses/LICENSE-2.0</span><span class="hljs-comment">#</span><span class="hljs-comment"># Unless required by applicable law or agreed to in writing, software</span><span class="hljs-comment"># distributed under the License is distributed on an "AS IS" BASIS,</span><span class="hljs-comment"># WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.</span><span class="hljs-comment"># See the License for the specific language governing permissions and</span><span class="hljs-comment"># limitations under the License.</span><span class="hljs-attr">kind:</span> <span class="hljs-string">ServiceAccount</span><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">v1</span><span class="hljs-attr">metadata:</span>  <span class="hljs-attr">name:</span> <span class="hljs-string">kube-dns-autoscaler</span>  <span class="hljs-attr">namespace:</span> <span class="hljs-string">kube-system</span>  <span class="hljs-attr">labels:</span>    <span class="hljs-attr">addonmanager.kubernetes.io/mode:</span> <span class="hljs-string">Reconcile</span><span class="hljs-meta">---</span><span class="hljs-attr">kind:</span> <span class="hljs-string">ClusterRole</span><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">rbac.authorization.k8s.io/v1</span><span class="hljs-attr">metadata:</span>  <span class="hljs-attr">name:</span> <span class="hljs-string">system:kube-dns-autoscaler</span>  <span class="hljs-attr">labels:</span>    <span class="hljs-attr">addonmanager.kubernetes.io/mode:</span> <span class="hljs-string">Reconcile</span><span class="hljs-attr">rules:</span>  <span class="hljs-bullet">-</span> <span class="hljs-attr">apiGroups:</span> <span class="hljs-string">[""]</span>    <span class="hljs-attr">resources:</span> <span class="hljs-string">["nodes"]</span>    <span class="hljs-attr">verbs:</span> <span class="hljs-string">["list"]</span>  <span class="hljs-bullet">-</span> <span class="hljs-attr">apiGroups:</span> <span class="hljs-string">[""]</span>    <span class="hljs-attr">resources:</span> <span class="hljs-string">["replicationcontrollers/scale"]</span>    <span class="hljs-attr">verbs:</span> <span class="hljs-string">["get",</span> <span class="hljs-string">"update"</span><span class="hljs-string">]</span>  <span class="hljs-bullet">-</span> <span class="hljs-attr">apiGroups:</span> <span class="hljs-string">["extensions"]</span>    <span class="hljs-attr">resources:</span> <span class="hljs-string">["deployments/scale",</span> <span class="hljs-string">"replicasets/scale"</span><span class="hljs-string">]</span>    <span class="hljs-attr">verbs:</span> <span class="hljs-string">["get",</span> <span class="hljs-string">"update"</span><span class="hljs-string">]</span><span class="hljs-comment"># Remove the configmaps rule once below issue is fixed:</span><span class="hljs-comment"># kubernetes-incubator/cluster-proportional-autoscaler#16</span>  <span class="hljs-bullet">-</span> <span class="hljs-attr">apiGroups:</span> <span class="hljs-string">[""]</span>    <span class="hljs-attr">resources:</span> <span class="hljs-string">["configmaps"]</span>    <span class="hljs-attr">verbs:</span> <span class="hljs-string">["get",</span> <span class="hljs-string">"create"</span><span class="hljs-string">]</span><span class="hljs-meta">---</span><span class="hljs-attr">kind:</span> <span class="hljs-string">ClusterRoleBinding</span><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">rbac.authorization.k8s.io/v1</span><span class="hljs-attr">metadata:</span>  <span class="hljs-attr">name:</span> <span class="hljs-string">system:kube-dns-autoscaler</span>  <span class="hljs-attr">labels:</span>    <span class="hljs-attr">addonmanager.kubernetes.io/mode:</span> <span class="hljs-string">Reconcile</span><span class="hljs-attr">subjects:</span>  <span class="hljs-bullet">-</span> <span class="hljs-attr">kind:</span> <span class="hljs-string">ServiceAccount</span>    <span class="hljs-attr">name:</span> <span class="hljs-string">kube-dns-autoscaler</span>    <span class="hljs-attr">namespace:</span> <span class="hljs-string">kube-system</span><span class="hljs-attr">roleRef:</span>  <span class="hljs-attr">kind:</span> <span class="hljs-string">ClusterRole</span>  <span class="hljs-attr">name:</span> <span class="hljs-string">system:kube-dns-autoscaler</span>  <span class="hljs-attr">apiGroup:</span> <span class="hljs-string">rbac.authorization.k8s.io</span><span class="hljs-meta">---</span><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">apps/v1</span><span class="hljs-attr">kind:</span> <span class="hljs-string">Deployment</span><span class="hljs-attr">metadata:</span>  <span class="hljs-attr">name:</span> <span class="hljs-string">kube-dns-autoscaler</span>  <span class="hljs-attr">namespace:</span> <span class="hljs-string">kube-system</span>  <span class="hljs-attr">labels:</span>    <span class="hljs-attr">k8s-app:</span> <span class="hljs-string">kube-dns-autoscaler</span>    <span class="hljs-attr">kubernetes.io/cluster-service:</span> <span class="hljs-string">"true"</span>    <span class="hljs-attr">addonmanager.kubernetes.io/mode:</span> <span class="hljs-string">Reconcile</span><span class="hljs-attr">spec:</span>  <span class="hljs-attr">selector:</span>    <span class="hljs-attr">matchLabels:</span>      <span class="hljs-attr">k8s-app:</span> <span class="hljs-string">kube-dns-autoscaler</span>  <span class="hljs-attr">template:</span>    <span class="hljs-attr">metadata:</span>      <span class="hljs-attr">labels:</span>        <span class="hljs-attr">k8s-app:</span> <span class="hljs-string">kube-dns-autoscaler</span>      <span class="hljs-attr">annotations:</span>        <span class="hljs-attr">scheduler.alpha.kubernetes.io/critical-pod:</span> <span class="hljs-string">''</span>    <span class="hljs-attr">spec:</span>      <span class="hljs-attr">priorityClassName:</span> <span class="hljs-string">system-cluster-critical</span>      <span class="hljs-attr">containers:</span>      <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">autoscaler</span>        <span class="hljs-attr">image:</span> <span class="hljs-string">gcr.azk8s.cn/google_containers/cluster-proportional-autoscaler-amd64:1.1.2-r2</span>        <span class="hljs-attr">resources:</span>            <span class="hljs-attr">requests:</span>                <span class="hljs-attr">cpu:</span> <span class="hljs-string">"20m"</span>                <span class="hljs-attr">memory:</span> <span class="hljs-string">"10Mi"</span>        <span class="hljs-attr">command:</span>          <span class="hljs-bullet">-</span> <span class="hljs-string">/cluster-proportional-autoscaler</span>          <span class="hljs-bullet">-</span> <span class="hljs-string">--namespace=kube-system</span>          <span class="hljs-bullet">-</span> <span class="hljs-string">--configmap=kube-dns-autoscaler</span>          <span class="hljs-comment"># Should keep target in sync with cluster/addons/dns/kube-dns.yaml.base</span>          <span class="hljs-bullet">-</span> <span class="hljs-string">--target=Deployment/coredns</span>          <span class="hljs-comment"># When cluster is using large nodes(with more cores), "coresPerReplica" should dominate.</span>          <span class="hljs-comment"># If using small nodes, "nodesPerReplica" should dominate.</span>          <span class="hljs-bullet">-</span> <span class="hljs-string">--default-params=&#123;"linear":&#123;"coresPerReplica":256,"nodesPerReplica":16,"preventSinglePointFailure":true&#125;&#125;</span>          <span class="hljs-bullet">-</span> <span class="hljs-string">--logtostderr=true</span>          <span class="hljs-bullet">-</span> <span class="hljs-string">--v=2</span>      <span class="hljs-attr">tolerations:</span>      <span class="hljs-bullet">-</span> <span class="hljs-attr">key:</span> <span class="hljs-string">"CriticalAddonsOnly"</span>        <span class="hljs-attr">operator:</span> <span class="hljs-string">"Exists"</span>      <span class="hljs-attr">serviceAccountName:</span> <span class="hljs-string">kube-dns-autoscaler</span></code></pre></div><h2 id="四、其他"><a href="#四、其他" class="headerlink" title="四、其他"></a>四、其他</h2><h3 id="4-1、集群测试"><a href="#4-1、集群测试" class="headerlink" title="4.1、集群测试"></a>4.1、集群测试</h3><p>为测试集群工作正常，我们创建一个 deployment 和一个 service，用于测试联通性和 DNS 工作是否正常；测试配置如下</p><ul><li>test.yaml</li></ul><div class="hljs"><pre><code class="hljs yaml"><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">apps/v1</span><span class="hljs-attr">kind:</span> <span class="hljs-string">Deployment</span><span class="hljs-attr">metadata:</span>  <span class="hljs-attr">name:</span> <span class="hljs-string">test</span>  <span class="hljs-attr">labels:</span>    <span class="hljs-attr">app:</span> <span class="hljs-string">test</span><span class="hljs-attr">spec:</span>  <span class="hljs-attr">replicas:</span> <span class="hljs-number">5</span>  <span class="hljs-attr">selector:</span>    <span class="hljs-attr">matchLabels:</span>      <span class="hljs-attr">app:</span> <span class="hljs-string">test</span>  <span class="hljs-attr">template:</span>    <span class="hljs-attr">metadata:</span>      <span class="hljs-attr">labels:</span>        <span class="hljs-attr">app:</span> <span class="hljs-string">test</span>    <span class="hljs-attr">spec:</span>      <span class="hljs-attr">containers:</span>      <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">test</span>        <span class="hljs-attr">image:</span> <span class="hljs-string">nginx:1.14.2-alpine</span>        <span class="hljs-attr">ports:</span>        <span class="hljs-bullet">-</span> <span class="hljs-attr">containerPort:</span> <span class="hljs-number">80</span><span class="hljs-meta">---</span><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">v1</span><span class="hljs-attr">kind:</span> <span class="hljs-string">Service</span><span class="hljs-attr">metadata:</span>  <span class="hljs-attr">name:</span> <span class="hljs-string">test-service</span><span class="hljs-attr">spec:</span>  <span class="hljs-attr">selector:</span>    <span class="hljs-attr">app:</span> <span class="hljs-string">test</span>  <span class="hljs-attr">ports:</span>  <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">nginx</span>    <span class="hljs-attr">port:</span> <span class="hljs-number">80</span>    <span class="hljs-attr">nodePort:</span> <span class="hljs-number">30001</span>    <span class="hljs-attr">targetPort:</span> <span class="hljs-number">80</span>    <span class="hljs-attr">protocol:</span> <span class="hljs-string">TCP</span>  <span class="hljs-attr">type:</span> <span class="hljs-string">NodePort</span></code></pre></div><p>测试方式很简单，进入某一个 pod ping 其他 pod ip 确认网络是否正常，直接访问 service 名称测试 DNS 是否工作正常，这里不再演示</p><h3 id="4-2、其他说明"><a href="#4-2、其他说明" class="headerlink" title="4.2、其他说明"></a>4.2、其他说明</h3><p>此次搭建开启了大部分认证，限于篇幅原因没有将每个选项作用做完整解释，推荐搭建完成后仔细阅读以下 <code>--help</code> 中的描述(官方文档页面有时候更新不完整)；目前 apiserver 仍然保留了 8080 端口(因为直接使用 kubectl 方便)，但是在高安全性环境请关闭 8080 端口，因为即使绑定在 127.0.0.1 上，对于任何能够登录 master 机器的用户仍然能够不经验证操作整个集群</p>]]></content:encoded>
      
      
      <category domain="https://mritd.com/categories/kubernetes/">Kubernetes</category>
      
      
      <category domain="https://mritd.com/tags/kubernetes/">Kubernetes</category>
      
      
      <comments>https://mritd.com/2019/03/16/set-up-kubernetes-1.13.4-cluster/#disqus_thread</comments>
      
    </item>
    
    <item>
      <title>Kubernetes sample-cli-plugin 源码分析</title>
      <link>https://mritd.com/2019/01/16/understand-kubernetes-sample-cli-plugin-source-code/</link>
      <guid>https://mritd.com/2019/01/16/understand-kubernetes-sample-cli-plugin-source-code/</guid>
      <pubDate>Wed, 16 Jan 2019 04:16:42 GMT</pubDate>
      
      <description>写这篇文章的目的是为了继续上篇 [Kubernetes 1.12 新的插件机制](https://mritd.me/2018/11/30/kubectl-plugin-new-solution-on-kubernetes-1.12/) 中最后部分对 `Golang 的插件辅助库` 说明；以及为后续使用 Golang 编写自己的 Kubernetes 插件做一个基础铺垫；顺边说一下 **sample-cli-plugin 这个项目是官方为 Golang 开发者编写的一个用于快速切换配置文件中 Namespace 的一个插件样例**</description>
      
      
      
      <content:encoded><![CDATA[<blockquote><p>写这篇文章的目的是为了继续上篇 <a href="https://mritd.me/2018/11/30/kubectl-plugin-new-solution-on-kubernetes-1.12/" target="_blank" rel="noopener">Kubernetes 1.12 新的插件机制</a> 中最后部分对 <code>Golang 的插件辅助库</code> 说明；以及为后续使用 Golang 编写自己的 Kubernetes 插件做一个基础铺垫；顺边说一下 <strong>sample-cli-plugin 这个项目是官方为 Golang 开发者编写的一个用于快速切换配置文件中 Namespace 的一个插件样例</strong></p></blockquote><h2 id="一、基础准备"><a href="#一、基础准备" class="headerlink" title="一、基础准备"></a>一、基础准备</h2><p>在开始分析源码之前，<strong>我们假设读者已经熟悉 Golang 语言，至少对基本语法、指针、依赖管理工具有一定认知</strong>；下面介绍一下 <a href="https://github.com/kubernetes/sample-cli-plugin" target="_blank" rel="noopener">sample-cli-plugin</a> 这个项目一些基础核心的依赖:</p><h3 id="1-1、Cobra-终端库"><a href="#1-1、Cobra-终端库" class="headerlink" title="1.1、Cobra 终端库"></a>1.1、Cobra 终端库</h3><p>这是一个强大的 Golang 的 command line interface 库，其支持用非常简单的代码创建出符合 Unix 风格的 cli 程序；甚至官方提供了用于创建 cli 工程脚手架的 cli 命令工具；Cobra 官方 Github 地址 <a href="https://github.com/spf13/cobra" target="_blank" rel="noopener">点击这里</a>，具体用法请自行 Google，以下只做一个简单的命令定义介绍(docker、kubernetes 终端 cli 都基于这个库)</p><div class="hljs"><pre><code class="hljs golang"># 每一个命令(不论是子命令还是主命令)都会是一个 cobra.Command 对象<span class="hljs-keyword">var</span> lsCmd = &amp;cobra.Command&#123;    <span class="hljs-comment">// 一些命令帮助文档有关的描述信息</span>    Use:   <span class="hljs-string">"ls"</span>,    Short: <span class="hljs-string">"A brief description of your command"</span>,    Long: <span class="hljs-string">`A longer description that spans multiple lines and likely contains examples</span><span class="hljs-string">and usage of using your command. For example:</span><span class="hljs-string"></span><span class="hljs-string">Cobra is a CLI library for Go that empowers applications.</span><span class="hljs-string">This application is a tool to generate the needed files</span><span class="hljs-string">to quickly create a Cobra application.`</span>,    <span class="hljs-comment">// 命令运行时真正执行逻辑，如果需要返回 Error 信息，我们一般设置 RunE</span>    Run: <span class="hljs-function"><span class="hljs-keyword">func</span><span class="hljs-params">(cmd *cobra.Command, args []<span class="hljs-keyword">string</span>)</span></span> &#123;        fmt.Println(<span class="hljs-string">"ls called"</span>)    &#125;,&#125;<span class="hljs-comment">// 为这个命令添加 flag，比如 `--help`、`-p`</span><span class="hljs-comment">// PersistentFlags() 方法添加的 flag 在所有子 command 也会生效</span><span class="hljs-comment">// Cobra 的 command 可以无限级联，比如 `kubectl get pod` 就是在 `kubectl` command 下增加了子 `get` command</span>lsCmd.PersistentFlags().String(<span class="hljs-string">"foo"</span>, <span class="hljs-string">""</span>, <span class="hljs-string">"A help for foo"</span>)<span class="hljs-comment">// Flags() 方法添加的 flag 仅在直接调用此子命令时生效</span>lsCmd.Flags().BoolP(<span class="hljs-string">"toggle"</span>, <span class="hljs-string">"t"</span>, <span class="hljs-literal">false</span>, <span class="hljs-string">"Help message for toggle"</span>)</code></pre></div><h3 id="1-2、vendor-依赖"><a href="#1-2、vendor-依赖" class="headerlink" title="1.2、vendor 依赖"></a>1.2、vendor 依赖</h3><p>vendor 目录用于存放 Golang 的依赖库，sample-cli-plugin 这个项目采用 <a href="https://github.com/tools/godep" target="_blank" rel="noopener">godep</a> 工具管理依赖；依赖配置信息被保存在 <code>Godeps/Godeps.json</code> 中，<strong>一般项目不会上传 vendor 目录，因为它的依赖信息已经在 Godeps.json 中存在，只需要在项目下使用 <code>godep restore</code> 命令恢复就可自动重新下载</strong>；这里上传了 vendor 目录的原因应该是为了方便开发者直接使用 <code>go get</code> 命令安装；顺边说一下在 Golang 新版本已经开始转换到 <code>go mod</code> 依赖管理工具，标志就是项目下会有 <code>go.mod</code> 文件</p><h2 id="二、源码分析"><a href="#二、源码分析" class="headerlink" title="二、源码分析"></a>二、源码分析</h2><h3 id="2-1、环境搭建"><a href="#2-1、环境搭建" class="headerlink" title="2.1、环境搭建"></a>2.1、环境搭建</h3><p>这里准备一笔带过了，基本就是 clone 源码到 <code>$GOPATH/src/k8s.io/sample-cli-plugin</code> 目录，然后在 GoLand 中打开；目前我使用的 Go 版本为最新的 1.11.4；以下时导入源码后的截图</p><p><img src="https://cdn.oss.link/markdown/sn8o8.png" srcset="/img/loading.gif" alt="GoLand"></p><h3 id="2-2、定位核心运行方法"><a href="#2-2、定位核心运行方法" class="headerlink" title="2.2、定位核心运行方法"></a>2.2、定位核心运行方法</h3><p>熟悉过 Cobra 库以后，再从整个项目包名上分析，首先想到的启动入口应该在 <code>cmd</code> 包下(一般 <code>cmd</code> 包下的文件都会编译成最终可执行文件名，Kubernetes 也是一样)</p><p><img src="https://cdn.oss.link/markdown/rafeq.png" srcset="/img/loading.gif" alt="main"></p><p>从以上截图中可以看出，首先通过 <code>cmd.NewCmdNamespace</code> 方法创建了一个 Command 对象 <code>root</code>，然后调用了 <code>root.Execute</code> 就结束了；那么也就说明 <code>root</code> 这个 Command 是唯一的核心命令对象，整个插件实现都在这个 <code>root</code> 里；所以我们需要查看一下这个 <code>cmd.NewCmdNamespace</code> 是如何对它初始化的，找到 Cobra 中的 <code>Run</code> 或者 <code>RunE</code> 设置</p><p><img src="https://cdn.oss.link/markdown/77krg.png" srcset="/img/loading.gif" alt="NewCmdNamespace"></p><p>定位到 <code>NewCmdNamespace</code> 方法以后，基本上就是标准的 Cobra 库的使用方式了；<strong>从截图上可以看到，<code>RunE</code> 设置的函数总共运行了 3 个动作: <code>o.Complete</code>、<code>o.Validate</code>、<code>o.Run</code></strong>；所以接下来我们主要分析这三个方法就行了</p><h3 id="2-3、NamespaceOptions-结构体"><a href="#2-3、NamespaceOptions-结构体" class="headerlink" title="2.3、NamespaceOptions 结构体"></a>2.3、NamespaceOptions 结构体</h3><p>在分析上面说的这三个方法之前，我们还应当了解一下这个 <code>o</code> 是什么玩意</p><p><img src="https://cdn.oss.link/markdown/4b3cc.png" srcset="/img/loading.gif" alt="NamespaceOptions"></p><p>从源码中可以看到，<code>o</code> 这个对象由 <code>NewNamespaceOptions</code> 创建，而 <code>NewNamespaceOptions</code> 方法返回的实际上是一个 <code>NamespaceOptions</code> 结构体；接下来我们需要研究一下这个结构体都是由什么组成的，换句话说要基本大致上整明白结构体的基本结构，比如里面的属性都是干啥的</p><h4 id="2-3-1、-genericclioptions-ConfigFlags"><a href="#2-3-1、-genericclioptions-ConfigFlags" class="headerlink" title="2.3.1、*genericclioptions.ConfigFlags"></a>2.3.1、*genericclioptions.ConfigFlags</h4><p>首先看下第一个属性 <code>configFlags</code>，它的实际类型是 <code>*genericclioptions.ConfigFlags</code>，点击查看以后如下</p><p><img src="https://cdn.oss.link/markdown/li6s4.png" srcset="/img/loading.gif" alt="genericclioptions.ConfigFlags"></p><p>从这些字段上来看，我们可以暂且模糊的推测出这应该是个基础配置型的字段，负责存储一些全局基本设置，比如 API Server 认证信息等</p><h4 id="2-3-2、-api-Context"><a href="#2-3-2、-api-Context" class="headerlink" title="2.3.2、*api.Context"></a>2.3.2、*api.Context</h4><p>下面这两个 <code>resultingContext</code>、<code>resultingContextName</code> 就很好理解了，从名字上看就可以知道它们应该是用来存储结果集的 Context 信息的；当然这个 <code>*api.Context</code> 就是 Kubernetes 配置文件中 Context 的 Go 结构体</p><h4 id="2-3-3、userSpecified"><a href="#2-3-3、userSpecified" class="headerlink" title="2.3.3、userSpecified*"></a>2.3.3、userSpecified*</h4><p>这几个字段从名字上就可以区分出，他们应该用于存储用户设置的或者说是通过命令行选项输入的一些指定配置信息，比如 Cluster、Context 等</p><h4 id="2-3-4、rawConfig"><a href="#2-3-4、rawConfig" class="headerlink" title="2.3.4、rawConfig"></a>2.3.4、rawConfig</h4><p>rawConfig 这个变量名字有点子奇怪，不过它实际上是个 <code>api.Config</code>；里面保存了与 API Server 通讯的配置信息；<strong>至于为什么要有这玩意，是因为配置信息输入源有两个: cli 命令行选项(eg: <code>--namespace</code>)和用户配置文件(eg: <code>~/.kube/config</code>)；最终这两个地方的配置合并后会存储在这个 rawConfig 里</strong></p><h4 id="2-3-5、listNamespaces"><a href="#2-3-5、listNamespaces" class="headerlink" title="2.3.5、listNamespaces"></a>2.3.5、listNamespaces</h4><p>这个变量实际上相当于一个 flag，用于存储插件是否使用了 <code>--list</code> 选项；在分析结构体这里没法看出来；不过只要稍稍的多看一眼代码就能看在 <code>NewCmdNamespace</code> 方法中有这么一行代码</p><p><img src="https://cdn.oss.link/markdown/f07l3.png" srcset="/img/loading.gif" alt="listNamespaces"></p><h3 id="2-4、核心处理逻辑"><a href="#2-4、核心处理逻辑" class="headerlink" title="2.4、核心处理逻辑"></a>2.4、核心处理逻辑</h3><p>介绍完了结构体的基本属性，最后我们只需要弄明白在核心 Command 方法内运行的这三个核心方法就行了</p><p><img src="https://cdn.oss.link/markdown/8lm4b.png" srcset="/img/loading.gif" alt="core func"></p><h4 id="2-4-1、-NamespaceOptions-Complete"><a href="#2-4-1、-NamespaceOptions-Complete" class="headerlink" title="2.4.1、*NamespaceOptions.Complete"></a>2.4.1、*NamespaceOptions.Complete</h4><p>这个方法代码稍微有点多，这里不会对每一行代码都做解释，只要大体明白都在干什么就行了；我们的目的是理解它，后续模仿它创造自己的插件；下面是代码截图</p><p><img src="https://cdn.oss.link/markdown/qqf0f.png" srcset="/img/loading.gif" alt="NamespaceOptions.Complete"></p><p>从截图上可以看到，首先弄出了 <code>rawConfig</code> 这个玩意，<code>rawConfig</code> 上面也提到了，它就是终端选项和用户配置文件的最终合并，至于为什么可以查看 <code>ToRawKubeConfigLoader().RawConfig()</code> 这两个方法的注释和实现即可；</p><p>接下来就是各种获取插件执行所需要的变量信息，比如获取用户指定的 <code>Namespace</code>、<code>Cluster</code>、<code>Context</code> 等，其中还包含了一些必要的校验；比如不允许使用 <code>kubectl ns NS_NAME1 --namespace NS_NAME2</code> 这种操作(因为这么干很让人难以理解 “你到底是要切换到 <code>NS_NAME1</code> 还是 <code>NS_NAME2</code>“)</p><p>最后从 <code>153</code> 行 <code>o.resultingContext = api.NewContext()</code> 开始就是创建最终的 <code>resultingContext</code> 对象，把获取到的用户指定的 <code>Namespace</code> 等各种信息赋值好，为下一步将其持久化到配置文件中做准备</p><h4 id="2-4-2、-NamespaceOptions-Validate"><a href="#2-4-2、-NamespaceOptions-Validate" class="headerlink" title="2.4.2、*NamespaceOptions.Validate"></a>2.4.2、*NamespaceOptions.Validate</h4><p>这个方法看名字就知道，里面全是对最终结果的校验；比如检查一下 <code>rawConfig</code> 中的 <code>CurrentContext</code> 是否获取到了，看看命令行参数是否正确，确保你不会瞎鸡儿输入 <code>kubectl ns NS_NAME1 NS_NAME2</code> 这种命令</p><p><img src="https://cdn.oss.link/markdown/frqpb.png" srcset="/img/loading.gif" alt="NamespaceOptions.Validate"></p><h4 id="2-4-3、-NamespaceOptions-Run"><a href="#2-4-3、-NamespaceOptions-Run" class="headerlink" title="2.4.3、*NamespaceOptions.Run"></a>2.4.3、*NamespaceOptions.Run</h4><p>第一步合并配置信息并获取到用户设置(输入)的配置，第二部做参数校验；可以说前面的两步操作都是为这一步做准备，<code>Run</code> 方法真正的做了配置文件写入、终端返回结果打印操作</p><p><img src="https://cdn.oss.link/markdown/6tkjz.png" srcset="/img/loading.gif" alt="NamespaceOptions.Run"></p><p>可以看到，<code>Run</code> 方法第一步就是更加谨慎的检查了一下参数是否正常，然后调用了 <code>o.setNamespace</code>；这个方法截图如下</p><p><img src="https://cdn.oss.link/markdown/1jc3k.png" srcset="/img/loading.gif" alt="NamespaceOptions.setNamespace"></p><p>这个 <code>setNamespace</code>是真正的做了配置文件写入动作的，实际写入方法就是 <code>clientcmd.ModifyConfig</code>；这个是 <code>Kubernetes</code> <code>client-go</code> 提供的方法，这些库的作用就是提供给我们非常方便的 API 操作；比如修改配置文件，你不需要关心配置文件在哪，你更不需要关系文件句柄是否被释放</p><p>从 <code>o.setNamespace</code> 方法以后其实就没什么看头了，毕竟插件的核心功能就是快速修改 <code>Namespace</code>；下面的各种 <code>for</code> 循环遍历其实就是在做打印输出；比如当你没有设置 <code>Namespace</code> 而使用了 <code>--list</code> 选项，插件就通过这里帮你打印设置过那些 <code>Namespace</code></p><h2 id="三、插件总结"><a href="#三、插件总结" class="headerlink" title="三、插件总结"></a>三、插件总结</h2><p>分析完了这个官方的插件，然后想一下自己以后写插件可能的需求，最后对比一下，可以为以后写插件做个总结:</p><ul><li>我们最好也弄个 <code>xxxOptions</code> 这种结构体存存一些配置</li><li>结构体内至少我们应当存储 <code>configFlags</code>、<code>rawConfig</code> 这两个基础配置信息</li><li>结构体内其它参数都应当是跟自己实际业务有关的</li><li>最后在在结构体上增加适当的方法完成自己的业务逻辑并保持好适当的校验</li></ul><p>转载请注明出n，本文采用 [CC4.0](<a href="http://c" target="_blank" rel="noopener">http://c</a> 1.12 新的插件机制](<a href="https://mritd.me/2018/11/30/kubectl-plugin-new-solution-on-kubernetes-1.12/" target="_blank" rel="noopener">https://mritd.me/2018/11/30/kubectl-plugin-new-solution-on-kubernetes-1.12/</a>) 中最后部分对 <code>Golang 的插件辅助库</code> 说明；以及为后续使用 Golang 编写自己的 Kubernetes 插件做一个基础铺垫；顺边说一下 <strong>sample-cli-plugin 这个项目是官方为 Golang 开发者编写的一个用于快速切换配置文件中 Namespace 的一个插件样例</strong></p>]]></content:encoded>
      
      
      <category domain="https://mritd.com/categories/kubernetes/">Kubernetes</category>
      
      
      <category domain="https://mritd.com/tags/kubernetes/">Kubernetes</category>
      
      
      <comments>https://mritd.com/2019/01/16/understand-kubernetes-sample-cli-plugin-source-code/#disqus_thread</comments>
      
    </item>
    
    <item>
      <title>Kubernetes 1.12 新的插件机制</title>
      <link>https://mritd.com/2018/11/30/kubectl-plugin-new-solution-on-kubernetes-1.12/</link>
      <guid>https://mritd.com/2018/11/30/kubectl-plugin-new-solution-on-kubernetes-1.12/</guid>
      <pubDate>Thu, 29 Nov 2018 16:05:34 GMT</pubDate>
      
      <description>在很久以前的版本研究过 kubernetes 的插件机制，当时弄了一个快速切换 `namespace` 的小插件；最近把自己本机的 kubectl 升级到了 1.12，突然发现插件不能用了；撸了一下文档发现插件机制彻底改了...</description>
      
      
      
      <content:encoded><![CDATA[<blockquote><p>在很久以前的版本研究过 kubernetes 的插件机制，当时弄了一个快速切换 <code>namespace</code> 的小插件；最近把自己本机的 kubectl 升级到了 1.12，突然发现插件不能用了；撸了一下文档发现插件机制彻底改了…</p></blockquote><h2 id="一、插件编写语言"><a href="#一、插件编写语言" class="headerlink" title="一、插件编写语言"></a>一、插件编写语言</h2><p>kubernetes 1.12 新的插件机制在编写语言上同以前一样，<strong>可以以任意语言编写，只要能弄一个可执行的文件出来就行</strong>，插件可以是一个 <code>bash</code>、<code>python</code> 脚本，也可以是 <code>Go</code> 等编译语言最终编译的二进制；以下是一个 Copy 自官方文档的 <code>bash</code> 编写的插件样例</p><div class="hljs"><pre><code class="hljs sh"><span class="hljs-meta">#!/bin/bash</span><span class="hljs-comment"># optional argument handling</span><span class="hljs-keyword">if</span> [[ <span class="hljs-string">"<span class="hljs-variable">$1</span>"</span> == <span class="hljs-string">"version"</span> ]]<span class="hljs-keyword">then</span>    <span class="hljs-built_in">echo</span> <span class="hljs-string">"1.0.0"</span>    <span class="hljs-built_in">exit</span> 0<span class="hljs-keyword">fi</span><span class="hljs-comment"># optional argument handling</span><span class="hljs-keyword">if</span> [[ <span class="hljs-string">"<span class="hljs-variable">$1</span>"</span> == <span class="hljs-string">"config"</span> ]]<span class="hljs-keyword">then</span>    <span class="hljs-built_in">echo</span> <span class="hljs-variable">$KUBECONFIG</span>    <span class="hljs-built_in">exit</span> 0<span class="hljs-keyword">fi</span><span class="hljs-built_in">echo</span> <span class="hljs-string">"I am a plugin named kubectl-foo"</span></code></pre></div><h2 id="二、插件加载方式"><a href="#二、插件加载方式" class="headerlink" title="二、插件加载方式"></a>二、插件加载方式</h2><h3 id="2-1、插件位置"><a href="#2-1、插件位置" class="headerlink" title="2.1、插件位置"></a>2.1、插件位置</h3><p>1.12 kubectl 插件最大的变化就是加载方式变了，由原来的放置在指定位置，还要为其编写 yaml 配置变成了现在的类似 git 扩展命令的方式: <strong>只要放置在 PATH 下，并以 <code>kubectl-</code> 开头的可执行文件都被认为是 <code>kubectl</code> 的插件</strong>；所以你可以随便弄个小脚本(比如上面的代码)，然后改好名字赋予可执行权限，扔到 PATH 下即可</p><p><img src="https://cdn.oss.link/markdown/s64v6.png" srcset="/img/loading.gif" alt="test-plugin"></p><h3 id="2-2、插件变量"><a href="#2-2、插件变量" class="headerlink" title="2.2、插件变量"></a>2.2、插件变量</h3><p>同以前不通，<strong>以前版本的执行插件时，<code>kubectl</code> 会向插件传递一些特定的与 <code>kubectl</code> 相关的变量，现在则只会传递标准变量；即 <code>kubectl</code> 能读到什么变量，插件就能读到，其他的私有化变量(比如 <code>KUBECTL_PLUGINS_CURRENT_NAMESPACE</code>)不会再提供</strong></p><p><img src="https://cdn.oss.link/markdown/vs1c3.png" srcset="/img/loading.gif" alt="plugin env"></p><p><strong>并且新版本的插件体系，所有选项(<code>flag</code>) 将全部交由插件本身处理，kubectl 不会再解析</strong>，比如下面的 <code>--help</code> 交给了自定义插件处理，由于脚本内没有处理这个选项，所以相当于选项无效了</p><p><img src="https://cdn.oss.link/markdown/8ch88.png" srcset="/img/loading.gif" alt="plugin flag"></p><p>还有就是 <strong>传递给插件的第一个参数永远是插件自己的绝对位置，比如这个 <code>test</code> 插件在执行时的 <code>$0</code> 是 <code>/usr/local/bin/kubectl-test</code></strong></p><h3 id="2-3、插件命名及查找"><a href="#2-3、插件命名及查找" class="headerlink" title="2.3、插件命名及查找"></a>2.3、插件命名及查找</h3><p>目前在插件命名及查找顺序上官方文档写的非常详尽，不给过对于普通使用者来说，实际上命名规则和查找与常规的 Linux 下的命令查找机制相同，只不过还做了增强；增强后的基本规则如下</p><ul><li><code>PATH</code> 优先匹配原则</li><li>短横线 <code>-</code> 自动分割匹配以及智能转义</li><li>以最精确匹配为首要目标</li><li>查找失败自动转换参数</li></ul><p><code>PATH</code> 优先匹配原则跟传统的命令查找一致，即当多个路径下存在同名的插件时，则采用最先查找到的插件</p><p><img src="https://cdn.oss.link/markdown/ljyp5.png" srcset="/img/loading.gif" alt="plugin path"></p><p>当你的插件文件名中包含 <code>-</code> ，并且 <code>kubectl</code> 在无法精确找到插件时会尝试自动拼接命令来尝试匹配；如下所示，在没有找到 <code>kubectl-test</code> 这个命令时会尝试拼接参数查找</p><p><img src="https://cdn.oss.link/markdown/l85bp.png" srcset="/img/loading.gif" alt="auto merge"></p><p>由于以上这种查找机制，<strong>当命令中确实包含 <code>-</code> 时，必须进行转义以 <code>_</code> 替换，否则 <code>kubectl</code> 会提示命令未找到错误</strong>；替换后可直接使用 <code>kubectl 插件命令(包含-)</code> 执行，同时也支持以原始插件名称执行(使用 <code>_</code>)</p><p><img src="https://cdn.oss.link/markdown/7vm0l.png" srcset="/img/loading.gif" alt="name contains dash"></p><p>在复杂插件体系下，多个插件可能包含同样的前缀，此时将遵序最精确查找原则；即当两个插件 <code>kubectl-test-aaa</code>、<code>kubectl-test-aaa-bbb</code> 同时存在，并且执行 <code>kubectl test aaa bbb</code> 命令时，优先匹配最精确的插件 <code>kubectl-test-aaa-bbb</code>，<strong>而不是将 <code>bbb</code> 作为参数传递给 <code>kubectl-test-aaa</code> 插件</strong></p><p><img src="https://cdn.oss.link/markdown/god8q.png" srcset="/img/loading.gif" alt="precise search"></p><h3 id="2-4、总结"><a href="#2-4、总结" class="headerlink" title="2.4、总结"></a>2.4、总结</h3><p>插件查找机制在一般情况下与传统 PATH 查找方式相同，同时 <code>kubectl</code> 实现了智能的 <code>-</code> 自动匹配查找、更精确的命令命中功能；这两种机制的实现主要为了方便编写插件的命令树(插件命令的子命令…)，类似下面这种</p><div class="hljs"><pre><code class="hljs sh">$ ls ./plugin_command_treekubectl-parentkubectl-parent-subcommandkubectl-parent-subcommand-subsubcommand</code></pre></div><p>当出现多个位置有同名插件时，执行 <code>kubectl plugin list</code> 能够检测出哪些插件由于 PATH 查找顺序原因导致永远不会被执行问题</p><div class="hljs"><pre><code class="hljs sh">$ kubectl plugin listThe following kubectl-compatible plugins are available:<span class="hljs-built_in">test</span>/fixtures/pkg/kubectl/plugins/kubectl-foo/usr/<span class="hljs-built_in">local</span>/bin/kubectl-foo  - warning: /usr/<span class="hljs-built_in">local</span>/bin/kubectl-foo is overshadowed by a similarly named plugin: <span class="hljs-built_in">test</span>/fixtures/pkg/kubectl/plugins/kubectl-fooplugins/kubectl-invalid  - warning: plugins/kubectl-invalid identified as a kubectl plugin, but it is not executableerror: 2 plugin warnings were found</code></pre></div><h3 id="三、Golang-的插件辅助库"><a href="#三、Golang-的插件辅助库" class="headerlink" title="三、Golang 的插件辅助库"></a>三、Golang 的插件辅助库</h3><p>由于插件机制的变更，导致其他语言编写的插件在实时获取某些配置信息、动态修改 <code>kubectl</code> 配置方面可能造成一定的阻碍；为此 kubernetes 提供了一个 <a href="https://github.com/kubernetes/cli-runtime" target="_blank" rel="noopener">command line runtime package</a>，使用 Go 编写插件，配合这个库可以更加方便的解析和调整 <code>kubectl</code> 的配置信息</p><p>官方为了演示如何使用这个 <a href="https://github.com/kubernetes/cli-runtime" target="_blank" rel="noopener">cli-runtime</a> 库编写了一个 <code>namespace</code> 切换的插件(自己白写了…)，仓库地址在 <a href="https://github.com/kubernetes/sample-cli-plugin" target="_blank" rel="noopener">Github</a> 上，基本编译使用如下(直接 <code>go get</code> 后编译文件默认为目录名 <code>cmd</code>)</p><div class="hljs"><pre><code class="hljs sh">➜  ~ go get k8s.io/sample-cli-plugin/cmd➜  ~ sudo mv gopath/bin/cmd /usr/<span class="hljs-built_in">local</span>/bin/kubectl-ns➜  ~ kubectl nsdefault➜  ~ kubectl ns --<span class="hljs-built_in">help</span>View or <span class="hljs-built_in">set</span> the current namespaceUsage:  ns [new-namespace] [flags]Examples:        <span class="hljs-comment"># view the current namespace in your KUBECONFIG</span>        kubectl ns        <span class="hljs-comment"># view all of the namespaces in use by contexts in your KUBECONFIG</span>        kubectl ns --list        <span class="hljs-comment"># switch your current-context to one that contains the desired namespace</span>        kubectl ns fooFlags:      --as string                      Username to impersonate <span class="hljs-keyword">for</span> the operation      --as-group stringArray           Group to impersonate <span class="hljs-keyword">for</span> the operation, this flag can be repeated to specify multiple groups.      --cache-dir string               Default HTTP cache directory (default <span class="hljs-string">"/Users/mritd/.kube/http-cache"</span>)      --certificate-authority string   Path to a cert file <span class="hljs-keyword">for</span> the certificate authority      --client-certificate string      Path to a client certificate file <span class="hljs-keyword">for</span> TLS      --client-key string              Path to a client key file <span class="hljs-keyword">for</span> TLS      --cluster string                 The name of the kubeconfig cluster to use      --context string                 The name of the kubeconfig context to use  -h, --<span class="hljs-built_in">help</span>                           <span class="hljs-built_in">help</span> <span class="hljs-keyword">for</span> ns      --insecure-skip-tls-verify       If <span class="hljs-literal">true</span>, the server<span class="hljs-string">'s certificate will not be checked for validity. This will make your HTTPS connections insecure</span><span class="hljs-string">      --kubeconfig string              Path to the kubeconfig file to use for CLI requests.</span><span class="hljs-string">      --list                           if true, print the list of all namespaces in the current KUBECONFIG</span><span class="hljs-string">  -n, --namespace string               If present, the namespace scope for this CLI request</span><span class="hljs-string">      --request-timeout string         The length of time to wait before giving up on a single server request. Non-zero values should contain a corresponding time unit (e.g. 1s, 2m, 3h). A value of zero means don'</span>t timeout requests. (default <span class="hljs-string">"0"</span>)  -s, --server string                  The address and port of the Kubernetes API server      --token string                   Bearer token <span class="hljs-keyword">for</span> authentication to the API server      --user string                    The name of the kubeconfig user to use</code></pre></div><p>限于篇幅原因，具体这个 <code>cli-runtime</code> 包怎么用请自行参考官方写的这个 <code>sample-cli-plugin</code> (其实并不怎么 “simple”…)</p><p>本文参考文档:</p><ul><li><a href="https://kubernetes.io/docs/tasks/extend-kubectl/kubectl-plugins/" target="_blank" rel="noopener">Extend kubectl with plugins</a></li><li><a href="https://github.com/kubernetes/cli-runtime" target="_blank" rel="noopener">cli-runtime</a></li><li><a href="https://github.com/kubernetes/sample-cli-plugin" target="_blank" rel="noopener">sample-cli-plugin</a></li></ul>]]></content:encoded>
      
      
      <category domain="https://mritd.com/categories/kubernetes/">Kubernetes</category>
      
      
      <category domain="https://mritd.com/tags/kubernetes/">Kubernetes</category>
      
      
      <comments>https://mritd.com/2018/11/30/kubectl-plugin-new-solution-on-kubernetes-1.12/#disqus_thread</comments>
      
    </item>
    
    <item>
      <title>Go 编写的一些常用小工具</title>
      <link>https://mritd.com/2018/11/27/simple-tool-written-in-golang/</link>
      <guid>https://mritd.com/2018/11/27/simple-tool-written-in-golang/</guid>
      <pubDate>Tue, 27 Nov 2018 04:45:46 GMT</pubDate>
      
      <description>迫于 Github 上 Star 的项目有点多，今天整理一下一些有意思的 Go 编写的小工具；大多数为终端下的实用工具，装逼的比如天气预报啥的就不写了</description>
      
      
      
      <content:encoded><![CDATA[<blockquote><p>迫于 Github 上 Star 的项目有点多，今天整理一下一些有意思的 Go 编写的小工具；大多数为终端下的实用工具，装逼的比如天气预报啥的就不写了</p></blockquote><h3 id="syncthing"><a href="#syncthing" class="headerlink" title="syncthing"></a>syncthing</h3><p>强大的文件同步工具，构建私人同步盘 👉 <a href="https://github.com/syncthing、syncthing" target="_blank" rel="noopener">Github</a></p><p><img src="https://cdn.oss.link/markdown/er3tj.jpg" srcset="/img/loading.gif" alt="syncthing"></p><h3 id="fzf"><a href="#fzf" class="headerlink" title="fzf"></a>fzf</h3><p>一个强大的终端文件浏览器 👉 <a href="https://github.com/junegunn/fzf" target="_blank" rel="noopener">Github</a></p><p><img src="https://cdn.oss.link/markdown/ihhqy.jpg" srcset="/img/loading.gif" alt="fzf"></p><h3 id="hey"><a href="#hey" class="headerlink" title="hey"></a>hey</h3><p>http 负载测试工具，简单好用 👉 <a href="https://github.com/rakyll/hey" target="_blank" rel="noopener">Github</a></p><div class="hljs"><pre><code class="hljs sh">Usage: hey [options...] &lt;url&gt;Options:  -n  Number of requests to run. Default is 200.  -c  Number of requests to run concurrently. Total number of requests cannot      be smaller than the concurrency level. Default is 50.  -q  Rate <span class="hljs-built_in">limit</span>, <span class="hljs-keyword">in</span> queries per second (QPS). Default is no rate <span class="hljs-built_in">limit</span>.  -z  Duration of application to send requests. When duration is reached,      application stops and exits. If duration is specified, n is ignored.      Examples: -z 10s -z 3m.  -o  Output <span class="hljs-built_in">type</span>. If none provided, a summary is printed.      <span class="hljs-string">"csv"</span> is the only supported alternative. Dumps the response      metrics <span class="hljs-keyword">in</span> comma-separated values format.  -m  HTTP method, one of GET, POST, PUT, DELETE, HEAD, OPTIONS.  -H  Custom HTTP header. You can specify as many as needed by repeating the flag.      For example, -H <span class="hljs-string">"Accept: text/html"</span> -H <span class="hljs-string">"Content-Type: application/xml"</span> .  -t  Timeout <span class="hljs-keyword">for</span> each request <span class="hljs-keyword">in</span> seconds. Default is 20, use 0 <span class="hljs-keyword">for</span> infinite.  -A  HTTP Accept header.  -d  HTTP request body.  -D  HTTP request body from file. For example, /home/user/file.txt or ./file.txt.  -T  Content-type, defaults to <span class="hljs-string">"text/html"</span>.  -a  Basic authentication, username:password.  -x  HTTP Proxy address as host:port.  -h2 Enable HTTP/2.  -host    HTTP Host header.  -<span class="hljs-built_in">disable</span>-compression  Disable compression.  -<span class="hljs-built_in">disable</span>-keepalive    Disable keep-alive, prevents re-use of TCP                        connections between different HTTP requests.  -<span class="hljs-built_in">disable</span>-redirects    Disable following of HTTP redirects  -cpus                 Number of used cpu cores.                        (default <span class="hljs-keyword">for</span> current machine is 8 cores)</code></pre></div><h3 id="vegeta"><a href="#vegeta" class="headerlink" title="vegeta"></a>vegeta</h3><p>http 负载测试工具，功能强大 👉 <a href="https://github.com/tsenart/vegeta" target="_blank" rel="noopener">Github</a></p><div class="hljs"><pre><code class="hljs sh">Usage: vegeta [global flags] &lt;<span class="hljs-built_in">command</span>&gt; [<span class="hljs-built_in">command</span> flags]global flags:  -cpus int        Number of CPUs to use (default 8)  -profile string        Enable profiling of [cpu, heap]  -version        Print version and <span class="hljs-built_in">exit</span>attack <span class="hljs-built_in">command</span>:  -body string        Requests body file  -cert string        TLS client PEM encoded certificate file  -connections int        Max open idle connections per target host (default 10000)  -duration duration        Duration of the <span class="hljs-built_in">test</span> [0 = forever]  -format string        Targets format [http, json] (default <span class="hljs-string">"http"</span>)  -h2c        Send HTTP/2 requests without TLS encryption  -header value        Request header  -http2        Send HTTP/2 requests when supported by the server (default <span class="hljs-literal">true</span>)  -insecure        Ignore invalid server TLS certificates  -keepalive        Use persistent connections (default <span class="hljs-literal">true</span>)  -key string        TLS client PEM encoded private key file  -laddr value        Local IP address (default 0.0.0.0)  -lazy        Read targets lazily  -max-body value        Maximum number of bytes to capture from response bodies. [-1 = no <span class="hljs-built_in">limit</span>] (default -1)  -name string        Attack name  -output string        Output file (default <span class="hljs-string">"stdout"</span>)  -rate value        Number of requests per time unit (default 50/1s)  -redirects int        Number of redirects to follow. -1 will not follow but marks as success (default 10)  -resolvers value        List of addresses (ip:port) to use <span class="hljs-keyword">for</span> DNS resolution. Disables use of <span class="hljs-built_in">local</span> system DNS. (comma separated list)  -root-certs value        TLS root certificate files (comma separated list)  -targets string        Targets file (default <span class="hljs-string">"stdin"</span>)  -timeout duration        Requests timeout (default 30s)  -workers uint        Initial number of workers (default 10)encode <span class="hljs-built_in">command</span>:  -output string        Output file (default <span class="hljs-string">"stdout"</span>)  -to string        Output encoding [csv, gob, json] (default <span class="hljs-string">"json"</span>)plot <span class="hljs-built_in">command</span>:  -output string        Output file (default <span class="hljs-string">"stdout"</span>)  -threshold int        Threshold of data points above <span class="hljs-built_in">which</span> series are downsampled. (default 4000)  -title string        Title and header of the resulting HTML page (default <span class="hljs-string">"Vegeta Plot"</span>)report <span class="hljs-built_in">command</span>:  -every duration        Report interval  -output string        Output file (default <span class="hljs-string">"stdout"</span>)  -<span class="hljs-built_in">type</span> string        Report <span class="hljs-built_in">type</span> to generate [text, json, hist[buckets]] (default <span class="hljs-string">"text"</span>)examples:  <span class="hljs-built_in">echo</span> <span class="hljs-string">"GET http://localhost/"</span> | vegeta attack -duration=5s | tee results.bin | vegeta report  vegeta report -<span class="hljs-built_in">type</span>=json results.bin &gt; metrics.json  cat results.bin | vegeta plot &gt; plot.html  cat results.bin | vegeta report -<span class="hljs-built_in">type</span>=<span class="hljs-string">"hist[0,100ms,200ms,300ms]"</span></code></pre></div><h3 id="dive"><a href="#dive" class="headerlink" title="dive"></a>dive</h3><p>功能强大的 Docker 镜像分析工具，可以查看每层镜像的具体差异等 👉 <a href="https://github.com/wagoodman/dive" target="_blank" rel="noopener">Github</a></p><p><img src="https://cdn.oss.link/markdown/ik3ng.gif" srcset="/img/loading.gif" alt="dive"></p><h3 id="ctop"><a href="#ctop" class="headerlink" title="ctop"></a>ctop</h3><p>容器运行时资源分析，如 CPU、内存消耗等 👉 <a href="https://github.com/bcicen/ctop" target="_blank" rel="noopener">Github</a></p><p><img src="https://cdn.oss.link/markdown/mr3x3.gif" srcset="/img/loading.gif" alt="ctop"></p><h3 id="container-diff"><a href="#container-diff" class="headerlink" title="container-diff"></a>container-diff</h3><p>Google 推出的工具，功能就顾名思义了 👉 <a href="https://github.com/GoogleContainerTools/container-diff" target="_blank" rel="noopener">Github</a></p><p><img src="https://cdn.oss.link/markdown/dtapx.png" srcset="/img/loading.gif" alt="container-diff"></p><h3 id="transfer-sh"><a href="#transfer-sh" class="headerlink" title="transfer.sh"></a>transfer.sh</h3><p>快捷的终端文件分享工具 👉 <a href="https://github.com/dutchcoders/transfer.sh" target="_blank" rel="noopener">Github</a></p><p><img src="https://cdn.oss.link/markdown/76vh0.png" srcset="/img/loading.gif" alt="transfer.sh"></p><h3 id="vuls"><a href="#vuls" class="headerlink" title="vuls"></a>vuls</h3><p> Linux/FreeBSD 漏洞扫描工具 👉 <a href="https://github.com/future-architect/vuls" target="_blank" rel="noopener">Github</a></p><p> <img src="https://cdn.oss.link/markdown/bpsps.jpg" srcset="/img/loading.gif" alt="vuls"></p><h3 id="restic"><a href="#restic" class="headerlink" title="restic"></a>restic</h3><p>高性能安全的文件备份工具 👉 <a href="https://github.com/restic/restic" target="_blank" rel="noopener">Github</a></p><p><img src="https://cdn.oss.link/markdown/g51z4.png" srcset="/img/loading.gif" alt="restic"></p><h3 id="gitql"><a href="#gitql" class="headerlink" title="gitql"></a>gitql</h3><p>使用 sql 的方式查询 git 提交 👉 <a href="https://github.com/cloudson/gitql" target="_blank" rel="noopener">Github</a></p><p><img src="https://cdn.oss.link/markdown/4h095.gif" srcset="/img/loading.gif" alt="gitql"></p><h3 id="gitflow-toolkit"><a href="#gitflow-toolkit" class="headerlink" title="gitflow-toolkit"></a>gitflow-toolkit</h3><p>帮助生成满足 Gitflow 格式 commit message 的小工具(自己写的) 👉 <a href="https://github.com/mritd/gitflow-toolkit" target="_blank" rel="noopener">Github</a></p><p><img src="https://cdn.oss.link/markdown/1e2v1.gif" srcset="/img/loading.gif" alt="gitflow-toolkit"></p><h3 id="git-chglog"><a href="#git-chglog" class="headerlink" title="git-chglog"></a>git-chglog</h3><p>对主流的 Gitflow 格式的 commit message 生成 CHANGELOG 👉 <a href="https://github.com/git-chglog/git-chglog" target="_blank" rel="noopener">Github</a></p><p><img src="https://cdn.oss.link/markdown/zphxd.gif" srcset="/img/loading.gif" alt="git-chglog"></p><h3 id="grv"><a href="#grv" class="headerlink" title="grv"></a>grv</h3><p>一个 git 终端图形化浏览工具 👉 <a href="https://github.com/rgburke/grv" target="_blank" rel="noopener">Github</a></p><p><img src="https://cdn.oss.link/markdown/k1vh2.jpg" srcset="/img/loading.gif" alt="grv"></p><h3 id="jid"><a href="#jid" class="headerlink" title="jid"></a>jid</h3><p>命令行 json 格式化处理工具，类似 jq，不过感觉更加强大 👉 <a href="https://github.com/simeji/jid" target="_blank" rel="noopener">Github</a></p><p><img src="https://cdn.oss.link/markdown/3k4ue.gif" srcset="/img/loading.gif" alt="jid"></p><h3 id="annie"><a href="#annie" class="headerlink" title="annie"></a>annie</h3><p>类似 youget 的一个视频下载工具，可以解析大部分视频网站直接下载 👉 <a href="https://github.com/iawia002/annie" target="_blank" rel="noopener">Github</a></p><div class="hljs"><pre><code class="hljs sh">$ annie -i https://www.youtube.com/watch?v=dQw4w9WgXcQ Site:      YouTube youtube.com Title:     Rick Astley - Never Gonna Give You Up (Video) Type:      video Streams:   <span class="hljs-comment"># All available quality</span>     [248]  -------------------     Quality:         1080p video/webm; codecs=<span class="hljs-string">"vp9"</span>     Size:            49.29 MiB (51687554 Bytes)     <span class="hljs-comment"># download with: annie -f 248 ...</span>     [137]  -------------------     Quality:         1080p video/mp4; codecs=<span class="hljs-string">"avc1.640028"</span>     Size:            43.45 MiB (45564306 Bytes)     <span class="hljs-comment"># download with: annie -f 137 ...</span>     [398]  -------------------     Quality:         720p video/mp4; codecs=<span class="hljs-string">"av01.0.05M.08"</span>     Size:            37.12 MiB (38926432 Bytes)     <span class="hljs-comment"># download with: annie -f 398 ...</span>     [136]  -------------------     Quality:         720p video/mp4; codecs=<span class="hljs-string">"avc1.4d401f"</span>     Size:            31.34 MiB (32867324 Bytes)     <span class="hljs-comment"># download with: annie -f 136 ...</span>     [247]  -------------------     Quality:         720p video/webm; codecs=<span class="hljs-string">"vp9"</span>     Size:            31.03 MiB (32536181 Bytes)     <span class="hljs-comment"># download with: annie -f 247 ...</span></code></pre></div><h3 id="up"><a href="#up" class="headerlink" title="up"></a>up</h3><p>Linux 下管道式终端搜索工具 👉 <a href="https://github.com/akavel/up" target="_blank" rel="noopener">Github</a></p><p><img src="https://cdn.oss.link/markdown/n8zdj.gif" srcset="/img/loading.gif" alt="up"></p><h3 id="lego"><a href="#lego" class="headerlink" title="lego"></a>lego</h3><p>Let’s Encrypt 证书申请工具 👉 <a href="https://github.com/xenolf/lego" target="_blank" rel="noopener">Github</a></p><div class="hljs"><pre><code class="hljs sh">NAME:   lego - Let<span class="hljs-string">'s Encrypt client written in Go</span><span class="hljs-string"></span><span class="hljs-string">USAGE:</span><span class="hljs-string">   lego [global options] command [command options] [arguments...]</span><span class="hljs-string"></span><span class="hljs-string">COMMANDS:</span><span class="hljs-string">     run      Register an account, then create and install a certificate</span><span class="hljs-string">     revoke   Revoke a certificate</span><span class="hljs-string">     renew    Renew a certificate</span><span class="hljs-string">     dnshelp  Shows additional help for the --dns global option</span><span class="hljs-string">     help, h  Shows a list of commands or help for one command</span><span class="hljs-string"></span><span class="hljs-string">GLOBAL OPTIONS:</span><span class="hljs-string">   --domains value, -d value   Add a domain to the process. Can be specified multiple times.</span><span class="hljs-string">   --csr value, -c value       Certificate signing request filename, if an external CSR is to be used</span><span class="hljs-string">   --server value, -s value    CA hostname (and optionally :port). The server certificate must be trusted in order to avoid further modifications to the client. (default: "https://acme-v02.api.letsencrypt.org/directory")</span><span class="hljs-string">   --email value, -m value     Email used for registration and recovery contact.</span><span class="hljs-string">   --filename value            Filename of the generated certificate</span><span class="hljs-string">   --accept-tos, -a            By setting this flag to true you indicate that you accept the current Let'</span>s Encrypt terms of service.   --eab                       Use External Account Binding <span class="hljs-keyword">for</span> account registration. Requires --kid and --hmac.   --kid value                 Key identifier from External CA. Used <span class="hljs-keyword">for</span> External Account Binding.   --hmac value                MAC key from External CA. Should be <span class="hljs-keyword">in</span> Base64 URL Encoding without padding format. Used <span class="hljs-keyword">for</span> External Account Binding.   --key-type value, -k value  Key <span class="hljs-built_in">type</span> to use <span class="hljs-keyword">for</span> private keys. Supported: rsa2048, rsa4096, rsa8192, ec256, ec384 (default: <span class="hljs-string">"rsa2048"</span>)   --path value                Directory to use <span class="hljs-keyword">for</span> storing the data (default: <span class="hljs-string">"./.lego"</span>)   --exclude value, -x value   Explicitly disallow solvers by name from being used. Solvers: <span class="hljs-string">"http-01"</span>, <span class="hljs-string">"dns-01"</span>, <span class="hljs-string">"tls-alpn-01"</span>.   --webroot value             Set the webroot folder to use <span class="hljs-keyword">for</span> HTTP based challenges to write directly <span class="hljs-keyword">in</span> a file <span class="hljs-keyword">in</span> .well-known/acme-challenge   --memcached-host value      Set the memcached host(s) to use <span class="hljs-keyword">for</span> HTTP based challenges. Challenges will be written to all specified hosts.   --http value                Set the port and interface to use <span class="hljs-keyword">for</span> HTTP based challenges to listen on. Supported: interface:port or :port   --tls value                 Set the port and interface to use <span class="hljs-keyword">for</span> TLS based challenges to listen on. Supported: interface:port or :port   --dns value                 Solve a DNS challenge using the specified provider. Disables all other challenges. Run <span class="hljs-string">'lego dnshelp'</span> <span class="hljs-keyword">for</span> <span class="hljs-built_in">help</span> on usage.   --http-timeout value        Set the HTTP timeout value to a specific value <span class="hljs-keyword">in</span> seconds. The default is 10 seconds. (default: 0)   --dns-timeout value         Set the DNS timeout value to a specific value <span class="hljs-keyword">in</span> seconds. The default is 10 seconds. (default: 0)   --dns-resolvers value       Set the resolvers to use <span class="hljs-keyword">for</span> performing recursive DNS queries. Supported: host:port. The default is to use the system resolvers, or Google<span class="hljs-string">'s DNS resolvers if the system'</span>s cannot be determined.   --pem                       Generate a .pem file by concatenating the .key and .crt files together.   --<span class="hljs-built_in">help</span>, -h                  show <span class="hljs-built_in">help</span>   --version, -v               <span class="hljs-built_in">print</span> the version</code></pre></div><h3 id="noti"><a href="#noti" class="headerlink" title="noti"></a>noti</h3><p>贼好用的终端命令异步执行通知工具 👉 <a href="https://github.com/variadico/noti" target="_blank" rel="noopener">Github</a></p><p><img src="https://cdn.oss.link/markdown/m2r1e.jpg" srcset="/img/loading.gif" alt="noti"></p><h3 id="gosu"><a href="#gosu" class="headerlink" title="gosu"></a>gosu</h3><p>临时切换到指定用户运行特定命令，方便测试权限问题 👉 <a href="https://github.com/tianon/gosu" target="_blank" rel="noopener">Github</a></p><div class="hljs"><pre><code class="hljs sh">$ gosuUsage: ./gosu user-spec <span class="hljs-built_in">command</span> [args]   eg: ./gosu tianon bash       ./gosu nobody:root bash -c <span class="hljs-string">'whoami &amp;&amp; id'</span>       ./gosu 1000:1 id</code></pre></div><h3 id="sup"><a href="#sup" class="headerlink" title="sup"></a>sup</h3><p>类似 Ansible 的一个批量执行工具，暂且称之为低配版 Ansible 👉 <a href="https://github.com/pressly/sup" target="_blank" rel="noopener">Github</a></p><p><img src="https://cdn.oss.link/markdown/x0eaz.gif" srcset="/img/loading.gif" alt="sup"></p><h3 id="aptly"><a href="#aptly" class="headerlink" title="aptly"></a>aptly</h3><p>Debian 仓库管理工具 👉 <a href="https://github.com/aptly-dev/aptly" target="_blank" rel="noopener">Github</a></p><p><img src="https://cdn.oss.link/markdown/8e0ml.jpg" srcset="/img/loading.gif" alt="aptly"></p><h3 id="mmh"><a href="#mmh" class="headerlink" title="mmh"></a>mmh</h3><p>支持无限跳板机登录的 ssh 小工具(自己写的) 👉 <a href="https://github.com/mritd/mmh" target="_blank" rel="noopener">Github</a></p><p><img src="https://cdn.oss.link/markdown/37638.gif" srcset="/img/loading.gif" alt="mmh"></p>]]></content:encoded>
      
      
      <category domain="https://mritd.com/categories/golang/">Golang</category>
      
      
      <category domain="https://mritd.com/tags/golang/">Golang</category>
      
      
      <comments>https://mritd.com/2018/11/27/simple-tool-written-in-golang/#disqus_thread</comments>
      
    </item>
    
    <item>
      <title>远程 Debug kubeadm</title>
      <link>https://mritd.com/2018/11/25/kubeadm-remote-debug/</link>
      <guid>https://mritd.com/2018/11/25/kubeadm-remote-debug/</guid>
      <pubDate>Sun, 25 Nov 2018 03:11:28 GMT</pubDate>
      
      <description>最近在看 kubeadm 的源码，不过有些东西光看代码还是没法太清楚，还是需要实际运行才能看到具体代码怎么跑的，还得打断点 debug；无奈的是本机是 mac，debug 得在 Linux 下，so 研究了一下 remote debug</description>
      
      
      
      <content:encoded><![CDATA[<blockquote><p>最近在看 kubeadm 的源码，不过有些东西光看代码还是没法太清楚，还是需要实际运行才能看到具体代码怎么跑的，还得打断点 debug；无奈的是本机是 mac，debug 得在 Linux 下，so 研究了一下 remote debug</p></blockquote><h2 id="一、环境准备"><a href="#一、环境准备" class="headerlink" title="一、环境准备"></a>一、环境准备</h2><ul><li>GoLand 2018.2.4</li><li>Golang 1.11.2</li><li>delve v1.1.0</li><li>Kubernetest master</li><li>Ubuntu 18.04</li><li>能够高速访问外网(自行理解)</li></ul><p><strong>这里不会详细写如何安装 Go 开发环境以及 GoLand 安装，本文默认读者已经至少已经对 Go 开发环境以及代码有一定了解；顺便提一下 GoLand，这玩意属于 jetbrains 系列 IDE，在大约 2018.1 版本后在线激活服务器已经全部失效，不过网上还有其他本地离线激活工具，具体请自行 Google，如果后续工资能支撑得起，请补票支持正版(感恩节全家桶半价真香😂)</strong></p><h3 id="1-1、获取源码"><a href="#1-1、获取源码" class="headerlink" title="1.1、获取源码"></a>1.1、获取源码</h3><p>需要注意的是 Kubernetes 源码虽然托管在 Github，但是在使用 <code>go get</code> 的时候要使用 <code>k8s.io</code> 域名</p><div class="hljs"><pre><code class="hljs sh">go get -d k8s.io/kubernetes</code></pre></div><p><code>go get</code> 命令是接受标准的 http 代理的，这个源码下载会非常慢，源码大约 1G 左右，所以最好使用加速工具下载</p><div class="hljs"><pre><code class="hljs sh">➜  ~ <span class="hljs-built_in">which</span> proxy/usr/<span class="hljs-built_in">local</span>/bin/proxy➜  ~ cat /usr/<span class="hljs-built_in">local</span>/bin/proxy<span class="hljs-meta">#!/bin/bash</span>http_proxy=http://127.0.0.1:8123 https_proxy=http://127.0.0.1:8123 $*➜  ~ proxy go get -d k8s.io/kubernetes</code></pre></div><h3 id="1-2、安装-delve"><a href="#1-2、安装-delve" class="headerlink" title="1.2、安装 delve"></a>1.2、安装 delve</h3><p>delve 是一个 Golang 的 debug 工具，有点类似 gdb，不过是专门针对 Golang 的，GoLand 的 debug 实际上就是使用的这个开源工具；为了进行远程 debug，运行 kubeadm 的机器必须安装 delve，从而进行远程连接</p><div class="hljs"><pre><code class="hljs sh"><span class="hljs-comment"># 同样这里省略在 Linux 安装 go 环境操作</span>go get -u github.com/derekparker/delve/cmd/dlv</code></pre></div><h2 id="二、远程-Debug"><a href="#二、远程-Debug" class="headerlink" title="二、远程 Debug"></a>二、远程 Debug</h2><h3 id="2-1、重新编译-kubeadm"><a href="#2-1、重新编译-kubeadm" class="headerlink" title="2.1、重新编译 kubeadm"></a>2.1、重新编译 kubeadm</h3><p>默认情况下直接编译出的 kubeadm 是无法进行 debug 的，因为 Golang 的编译器会进行编译优化，比如进行内联等；所以要关闭编译优化和内联，方便 debug</p><div class="hljs"><pre><code class="hljs sh"><span class="hljs-built_in">cd</span> <span class="hljs-variable">$&#123;GOPATH&#125;</span>/src/k8s.io/kubernetes/cmd/kubeadmGOOS=<span class="hljs-string">"linux"</span> GOARCH=<span class="hljs-string">"amd64"</span> go build -gcflags <span class="hljs-string">"all=-N -l"</span></code></pre></div><h3 id="2-2、远程运行-kubeadm"><a href="#2-2、远程运行-kubeadm" class="headerlink" title="2.2、远程运行 kubeadm"></a>2.2、远程运行 kubeadm</h3><p>将编译好的 kubeadm 复制到远程，并且使用 delve 启动它，此时 delve 会监听 api 端口，GoLand 就可以远程连接过来了</p><div class="hljs"><pre><code class="hljs sh">dlv --listen=192.168.1.61:2345 --headless=<span class="hljs-literal">true</span> --api-version=2 <span class="hljs-built_in">exec</span> ./kubeadm init</code></pre></div><p><strong>注意: 要指定需要 debug 的 kubeadm 的子命令，否则可能出现连接上以后 GoLand 无反应的情况</strong></p><h3 id="2-3、运行-GoLand"><a href="#2-3、运行-GoLand" class="headerlink" title="2.3、运行 GoLand"></a>2.3、运行 GoLand</h3><p>在 GoLand 中打开 kubernetes 源码，在需要 debug 的代码中打上断点，这里以 init 子命令为例</p><p>首先新建一个远程 debug configuration</p><p><img src="https://cdn.oss.link/markdown/i6oed.png" srcset="/img/loading.gif" alt="create configuration"></p><p>名字可以随便写，主要是地址和端口</p><p><img src="https://cdn.oss.link/markdown/rmczj.png" srcset="/img/loading.gif" alt="conifg delve"></p><p>接下来在目标源码位置打断点，以下为 init 子命令的源码位置</p><p><img src="https://cdn.oss.link/markdown/ylf97.png" srcset="/img/loading.gif" alt="create breakpoint"></p><p>最后只需要点击 debug 按钮即可</p><p><img src="https://cdn.oss.link/markdown/ns2yw.png" srcset="/img/loading.gif" alt="debug"></p><p><strong>在没有运行 GoLand debug 之前，目标机器的实际指令是不会运行的，也就是说在 GoLand 没有连接到远程 delve 启动的 <code>kubeadm init</code> 命令之前，<code>kubeadm init</code> 并不会真正运行；当点击 GoLand 的终止 debug 按钮后，远程的 delve 也会随之退出</strong></p><p><img src="https://cdn.oss.link/markdown/lmdke.png" srcset="/img/loading.gif" alt="stop"></p>]]></content:encoded>
      
      
      <category domain="https://mritd.com/categories/kubernetes/">Kubernetes</category>
      
      <category domain="https://mritd.com/categories/golang/">Golang</category>
      
      
      <category domain="https://mritd.com/tags/kubernetes/">Kubernetes</category>
      
      <category domain="https://mritd.com/tags/golang/">Golang</category>
      
      
      <comments>https://mritd.com/2018/11/25/kubeadm-remote-debug/#disqus_thread</comments>
      
    </item>
    
    <item>
      <title>Mac: Extract JDK to folder, without running installer</title>
      <link>https://mritd.com/2018/11/23/extract-jdk-to-folder-on-mac/</link>
      <guid>https://mritd.com/2018/11/23/extract-jdk-to-folder-on-mac/</guid>
      <pubDate>Fri, 23 Nov 2018 04:33:20 GMT</pubDate>
      
      <description>重装了 mac 系统，由于一些公司项目必须使用 Oracle JDK(验证码等组件用了一些 Oracle 独有的 API) 所以又得重新安装；但是 Oracle 只提供了 pkg 的安装方式，研究半天找到了一个解包 pkg 的安装方式，这里记录一下</description>
      
      
      
      <content:encoded><![CDATA[<blockquote><p>重装了 mac 系统，由于一些公司项目必须使用 Oracle JDK(验证码等组件用了一些 Oracle 独有的 API) 所以又得重新安装；但是 Oracle 只提供了 pkg 的安装方式，研究半天找到了一个解包 pkg 的安装方式，这里记录一下</p></blockquote><p>不使用 pkg 的原因是每次更新版本都要各种安装，最烦人的是 IDEA 选择 JDK 时候弹出的文件浏览器没法进入到这种方式安装的 JDK 的系统目录…mmp，后来从国外网站找到了一篇文章，基本套路如下</p><ul><li>下载 Oracle JDK，从 dmg 中拷贝 pkg 到任意位置</li><li>解压 pkg 到任意位置 <code>pkgutil --expand your_jdk.pkg jdkdir</code></li><li>进入到目录中，解压主文件 <code>cd jdkdir/jdk_version.pkg &amp;&amp; cpio -idv &lt; Payload</code></li><li>移动 jdk 到任意位置 <code>mv Contents/Home ~/myjdk</code></li></ul><p>原文地址: <a href="https://augustl.com/blog/2014/extracting_java_to_folder_no_installer_osx/" target="_blank" rel="noopener">OS X: Extract JDK to folder, without running installer</a></p>]]></content:encoded>
      
      
      <category domain="https://mritd.com/categories/java/">Java</category>
      
      
      <category domain="https://mritd.com/tags/java/">Java</category>
      
      
      <comments>https://mritd.com/2018/11/23/extract-jdk-to-folder-on-mac/#disqus_thread</comments>
      
    </item>
    
    <item>
      <title>Go ssh 交互式执行命令</title>
      <link>https://mritd.com/2018/11/09/go-interactive-shell/</link>
      <guid>https://mritd.com/2018/11/09/go-interactive-shell/</guid>
      <pubDate>Fri, 09 Nov 2018 15:13:44 GMT</pubDate>
      
      <description>最近在写一个跳板机登录的小工具，其中涉及到了用 Go 来进行交互式执行命令，简单地说就是弄个终端出来；一开始随便 Google 了一下，copy 下来基本上就是能跑了...但是后来发现了一些各种各样的小问题，强迫症的我实在受不了，最后翻了一下 Teleport 的源码，从中学到了不少有用的知识，这里记录一下</description>
      
      
      
      <content:encoded><![CDATA[<blockquote><p>最近在写一个跳板机登录的小工具，其中涉及到了用 Go 来进行交互式执行命令，简单地说就是弄个终端出来；一开始随便 Google 了一下，copy 下来基本上就是能跑了…但是后来发现了一些各种各样的小问题，强迫症的我实在受不了，最后翻了一下 Teleport 的源码，从中学到了不少有用的知识，这里记录一下</p></blockquote><h2 id="一、原始版本"><a href="#一、原始版本" class="headerlink" title="一、原始版本"></a>一、原始版本</h2><blockquote><p>不想看太多可以直接跳转到 <a href="#三完整代码">第三部分</a> 拿代码</p></blockquote><h3 id="1-1、样例代码"><a href="#1-1、样例代码" class="headerlink" title="1.1、样例代码"></a>1.1、样例代码</h3><p>一开始随便 Google 出来的代码，copy 上就直接跑；代码基本如下:</p><div class="hljs"><pre><code class="hljs golang"><span class="hljs-function"><span class="hljs-keyword">func</span> <span class="hljs-title">main</span><span class="hljs-params">()</span></span> &#123;<span class="hljs-comment">// 创建 ssh 配置</span>sshConfig := &amp;ssh.ClientConfig&#123;User: <span class="hljs-string">"root"</span>,Auth: []ssh.AuthMethod&#123;ssh.Password(<span class="hljs-string">"password"</span>),&#125;,HostKeyCallback: ssh.InsecureIgnoreHostKey(),Timeout:         <span class="hljs-number">5</span> * time.Second,&#125;<span class="hljs-comment">// 创建 client</span>client, err := ssh.Dial(<span class="hljs-string">"tcp"</span>, <span class="hljs-string">"192.168.1.20:22"</span>, sshConfig)checkErr(err)<span class="hljs-keyword">defer</span> client.Close()<span class="hljs-comment">// 获取 session</span>session, err := client.NewSession()checkErr(err)<span class="hljs-keyword">defer</span> session.Close()<span class="hljs-comment">// 拿到当前终端文件描述符</span>fd := <span class="hljs-keyword">int</span>(os.Stdin.Fd())termWidth, termHeight, err := terminal.GetSize(fd)<span class="hljs-comment">// request pty</span>err = session.RequestPty(<span class="hljs-string">"xterm-256color"</span>, termHeight, termWidth, ssh.TerminalModes&#123;&#125;)checkErr(err)<span class="hljs-comment">// 对接 std</span>session.Stdout = os.Stdoutsession.Stderr = os.Stderrsession.Stdin = os.Stdinerr = session.Shell()checkErr(err)err = session.Wait()checkErr(err)&#125;<span class="hljs-function"><span class="hljs-keyword">func</span> <span class="hljs-title">checkErr</span><span class="hljs-params">(err error)</span></span> &#123;<span class="hljs-keyword">if</span> err != <span class="hljs-literal">nil</span> &#123;fmt.Println(err)os.Exit(<span class="hljs-number">1</span>)&#125;&#125;</code></pre></div><h3 id="1-2、遇到的问题"><a href="#1-2、遇到的问题" class="headerlink" title="1.2、遇到的问题"></a>1.2、遇到的问题</h3><p>以上代码跑起来后，基本上遇到了以下问题:</p><ul><li>执行命令有回显，表现为敲一个 <code>ls</code> 出现两行</li><li>本地终端大小调整，远端完全无反应，导致显示不全</li><li>Tmux 下终端连接后窗口标题显示的是原始命令，而不是目标机器 shell 环境的目录位置</li><li>首次连接一些刚装完系统的机器可能出现执行命令后回显不换行</li></ul><h2 id="二、改进代码"><a href="#二、改进代码" class="headerlink" title="二、改进代码"></a>二、改进代码</h2><h3 id="2-1、回显问题"><a href="#2-1、回显问题" class="headerlink" title="2.1、回显问题"></a>2.1、回显问题</h3><p>关于回显问题，实际上解决方案很简单，设置当前终端进入 <code>raw</code> 模式即可；代码如下:</p><div class="hljs"><pre><code class="hljs golang"><span class="hljs-comment">// 拿到当前终端文件描述符</span>fd := <span class="hljs-keyword">int</span>(os.Stdin.Fd())<span class="hljs-comment">// make raw</span>state, err := terminal.MakeRaw(fd)checkErr(err)<span class="hljs-keyword">defer</span> terminal.Restore(fd, state)</code></pre></div><p>代码很简单，网上一大堆，But…基本没有文章详细说这个 <code>raw</code> 模式到底是个啥玩意；好在万能的 StackOverflow 对于不熟悉 Linux 的人给出了一个很清晰的解释: <a href="https://unix.stackexchange.com/questions/21752/what-s-the-difference-between-a-raw-and-a-cooked-device-driver" target="_blank" rel="noopener">What’s the difference between a “raw” and a “cooked” device driver?</a></p><p>大致意思就是说 <strong>在终端处于 <code>Cooked</code> 模式时，当你输入一些字符后，默认是被当前终端 cache 住的，在你敲了回车之前这些文本都在 cache 中，这样允许应用程序做一些处理，比如捕获 <code>Cntl-D</code> 等按键，这时候就会出现敲回车后本地终端帮你打印了一下，导致出现类似回显的效果；当设置终端为 <code>raw</code> 模式后，所有的输入将不被 cache，而是发送到应用程序，在我们的代码中表现为通过 <code>io.Copy</code> 直接发送到了远端 shell 程序</strong></p><h3 id="2-2、终端大小问题"><a href="#2-2、终端大小问题" class="headerlink" title="2.2、终端大小问题"></a>2.2、终端大小问题</h3><p>当本地调整了终端大小后，远程终端毫无反应；后来发现在 <code>*ssh.Session</code> 上有一个 <code>WindowChange</code> 方法，用于向远端发送窗口调整事件；解决方案就是启动一个 <code>goroutine</code> 在后台不断监听窗口改变事件，然后调用 <code>WindowChange</code> 即可；代码如下:</p><div class="hljs"><pre><code class="hljs golang"><span class="hljs-keyword">go</span> <span class="hljs-function"><span class="hljs-keyword">func</span><span class="hljs-params">()</span></span> &#123;<span class="hljs-comment">// 监听窗口变更事件</span>sigwinchCh := <span class="hljs-built_in">make</span>(<span class="hljs-keyword">chan</span> os.Signal, <span class="hljs-number">1</span>)signal.Notify(sigwinchCh, syscall.SIGWINCH)fd := <span class="hljs-keyword">int</span>(os.Stdin.Fd())termWidth, termHeight, err := terminal.GetSize(fd)<span class="hljs-keyword">if</span> err != <span class="hljs-literal">nil</span> &#123;fmt.Println(err)&#125;<span class="hljs-keyword">for</span> &#123;<span class="hljs-keyword">select</span> &#123;<span class="hljs-comment">// 阻塞读取</span><span class="hljs-keyword">case</span> sigwinch := &lt;-sigwinchCh:<span class="hljs-keyword">if</span> sigwinch == <span class="hljs-literal">nil</span> &#123;<span class="hljs-keyword">return</span>&#125;currTermWidth, currTermHeight, err := terminal.GetSize(fd)<span class="hljs-comment">// 判断一下窗口尺寸是否有改变</span><span class="hljs-keyword">if</span> currTermHeight == termHeight &amp;&amp; currTermWidth == termWidth &#123;<span class="hljs-keyword">continue</span>&#125;<span class="hljs-comment">// 更新远端大小</span>session.WindowChange(currTermHeight, currTermWidth)<span class="hljs-keyword">if</span> err != <span class="hljs-literal">nil</span> &#123;fmt.Printf(<span class="hljs-string">"Unable to send window-change reqest: %s."</span>, err)<span class="hljs-keyword">continue</span>&#125;termWidth, termHeight = currTermWidth, currTermHeight&#125;&#125;&#125;()</code></pre></div><h3 id="2-3、Tmux-标题以及回显不换行"><a href="#2-3、Tmux-标题以及回显不换行" class="headerlink" title="2.3、Tmux 标题以及回显不换行"></a>2.3、Tmux 标题以及回显不换行</h3><p>这两个问题实际上都是由于我们直接对接了 <code>stderr</code>、<code>stdout</code> 和 <code>stdin</code> 造成的，实际上我们应当启动一个异步的管道式复制行为，并且最好带有 buf 的发送；代码如下:</p><div class="hljs"><pre><code class="hljs golang">stdin, err := session.StdinPipe()checkErr(err)stdout, err := session.StdoutPipe()checkErr(err)stderr, err := session.StderrPipe()checkErr(err)<span class="hljs-keyword">go</span> io.Copy(os.Stderr, stderr)<span class="hljs-keyword">go</span> io.Copy(os.Stdout, stdout)<span class="hljs-keyword">go</span> <span class="hljs-function"><span class="hljs-keyword">func</span><span class="hljs-params">()</span></span> &#123;buf := <span class="hljs-built_in">make</span>([]<span class="hljs-keyword">byte</span>, <span class="hljs-number">128</span>)<span class="hljs-keyword">for</span> &#123;n, err := os.Stdin.Read(buf)<span class="hljs-keyword">if</span> err != <span class="hljs-literal">nil</span> &#123;fmt.Println(err)<span class="hljs-keyword">return</span>&#125;<span class="hljs-keyword">if</span> n &gt; <span class="hljs-number">0</span> &#123;_, err = stdin.Write(buf[:n])<span class="hljs-keyword">if</span> err != <span class="hljs-literal">nil</span> &#123;checkErr(err)&#125;&#125;&#125;&#125;()</code></pre></div><h2 id="三、完整代码"><a href="#三、完整代码" class="headerlink" title="三、完整代码"></a>三、完整代码</h2><div class="hljs"><pre><code class="hljs golang"><span class="hljs-keyword">type</span> SSHTerminal <span class="hljs-keyword">struct</span> &#123;Session *ssh.SessionexitMsg <span class="hljs-keyword">string</span>stdout  io.Readerstdin   io.Writerstderr  io.Reader&#125;<span class="hljs-function"><span class="hljs-keyword">func</span> <span class="hljs-title">main</span><span class="hljs-params">()</span></span> &#123;sshConfig := &amp;ssh.ClientConfig&#123;User: <span class="hljs-string">"root"</span>,Auth: []ssh.AuthMethod&#123;ssh.Password(<span class="hljs-string">"password"</span>),&#125;,HostKeyCallback: ssh.InsecureIgnoreHostKey(),&#125;client, err := ssh.Dial(<span class="hljs-string">"tcp"</span>, <span class="hljs-string">"192.168.1.20:22"</span>, sshConfig)<span class="hljs-keyword">if</span> err != <span class="hljs-literal">nil</span> &#123;fmt.Println(err)&#125;<span class="hljs-keyword">defer</span> client.Close()err = New(client)<span class="hljs-keyword">if</span> err != <span class="hljs-literal">nil</span> &#123;fmt.Println(err)&#125;&#125;<span class="hljs-function"><span class="hljs-keyword">func</span> <span class="hljs-params">(t *SSHTerminal)</span> <span class="hljs-title">updateTerminalSize</span><span class="hljs-params">()</span></span> &#123;<span class="hljs-keyword">go</span> <span class="hljs-function"><span class="hljs-keyword">func</span><span class="hljs-params">()</span></span> &#123;<span class="hljs-comment">// SIGWINCH is sent to the process when the window size of the terminal has</span><span class="hljs-comment">// changed.</span>sigwinchCh := <span class="hljs-built_in">make</span>(<span class="hljs-keyword">chan</span> os.Signal, <span class="hljs-number">1</span>)signal.Notify(sigwinchCh, syscall.SIGWINCH)fd := <span class="hljs-keyword">int</span>(os.Stdin.Fd())termWidth, termHeight, err := terminal.GetSize(fd)<span class="hljs-keyword">if</span> err != <span class="hljs-literal">nil</span> &#123;fmt.Println(err)&#125;<span class="hljs-keyword">for</span> &#123;<span class="hljs-keyword">select</span> &#123;<span class="hljs-comment">// The client updated the size of the local PTY. This change needs to occur</span><span class="hljs-comment">// on the server side PTY as well.</span><span class="hljs-keyword">case</span> sigwinch := &lt;-sigwinchCh:<span class="hljs-keyword">if</span> sigwinch == <span class="hljs-literal">nil</span> &#123;<span class="hljs-keyword">return</span>&#125;currTermWidth, currTermHeight, err := terminal.GetSize(fd)<span class="hljs-comment">// Terminal size has not changed, don't do anything.</span><span class="hljs-keyword">if</span> currTermHeight == termHeight &amp;&amp; currTermWidth == termWidth &#123;<span class="hljs-keyword">continue</span>&#125;t.Session.WindowChange(currTermHeight, currTermWidth)<span class="hljs-keyword">if</span> err != <span class="hljs-literal">nil</span> &#123;fmt.Printf(<span class="hljs-string">"Unable to send window-change reqest: %s."</span>, err)<span class="hljs-keyword">continue</span>&#125;termWidth, termHeight = currTermWidth, currTermHeight&#125;&#125;&#125;()&#125;<span class="hljs-function"><span class="hljs-keyword">func</span> <span class="hljs-params">(t *SSHTerminal)</span> <span class="hljs-title">interactiveSession</span><span class="hljs-params">()</span> <span class="hljs-title">error</span></span> &#123;<span class="hljs-keyword">defer</span> <span class="hljs-function"><span class="hljs-keyword">func</span><span class="hljs-params">()</span></span> &#123;<span class="hljs-keyword">if</span> t.exitMsg == <span class="hljs-string">""</span> &#123;fmt.Fprintln(os.Stdout, <span class="hljs-string">"the connection was closed on the remote side on "</span>, time.Now().Format(time.RFC822))&#125; <span class="hljs-keyword">else</span> &#123;fmt.Fprintln(os.Stdout, t.exitMsg)&#125;&#125;()fd := <span class="hljs-keyword">int</span>(os.Stdin.Fd())state, err := terminal.MakeRaw(fd)<span class="hljs-keyword">if</span> err != <span class="hljs-literal">nil</span> &#123;<span class="hljs-keyword">return</span> err&#125;<span class="hljs-keyword">defer</span> terminal.Restore(fd, state)termWidth, termHeight, err := terminal.GetSize(fd)<span class="hljs-keyword">if</span> err != <span class="hljs-literal">nil</span> &#123;<span class="hljs-keyword">return</span> err&#125;termType := os.Getenv(<span class="hljs-string">"TERM"</span>)<span class="hljs-keyword">if</span> termType == <span class="hljs-string">""</span> &#123;termType = <span class="hljs-string">"xterm-256color"</span>&#125;err = t.Session.RequestPty(termType, termHeight, termWidth, ssh.TerminalModes&#123;&#125;)<span class="hljs-keyword">if</span> err != <span class="hljs-literal">nil</span> &#123;<span class="hljs-keyword">return</span> err&#125;t.updateTerminalSize()t.stdin, err = t.Session.StdinPipe()<span class="hljs-keyword">if</span> err != <span class="hljs-literal">nil</span> &#123;<span class="hljs-keyword">return</span> err&#125;t.stdout, err = t.Session.StdoutPipe()<span class="hljs-keyword">if</span> err != <span class="hljs-literal">nil</span> &#123;<span class="hljs-keyword">return</span> err&#125;t.stderr, err = t.Session.StderrPipe()<span class="hljs-keyword">go</span> io.Copy(os.Stderr, t.stderr)<span class="hljs-keyword">go</span> io.Copy(os.Stdout, t.stdout)<span class="hljs-keyword">go</span> <span class="hljs-function"><span class="hljs-keyword">func</span><span class="hljs-params">()</span></span> &#123;buf := <span class="hljs-built_in">make</span>([]<span class="hljs-keyword">byte</span>, <span class="hljs-number">128</span>)<span class="hljs-keyword">for</span> &#123;n, err := os.Stdin.Read(buf)<span class="hljs-keyword">if</span> err != <span class="hljs-literal">nil</span> &#123;fmt.Println(err)<span class="hljs-keyword">return</span>&#125;<span class="hljs-keyword">if</span> n &gt; <span class="hljs-number">0</span> &#123;_, err = t.stdin.Write(buf[:n])<span class="hljs-keyword">if</span> err != <span class="hljs-literal">nil</span> &#123;fmt.Println(err)t.exitMsg = err.Error()<span class="hljs-keyword">return</span>&#125;&#125;&#125;&#125;()err = t.Session.Shell()<span class="hljs-keyword">if</span> err != <span class="hljs-literal">nil</span> &#123;<span class="hljs-keyword">return</span> err&#125;err = t.Session.Wait()<span class="hljs-keyword">if</span> err != <span class="hljs-literal">nil</span> &#123;<span class="hljs-keyword">return</span> err&#125;<span class="hljs-keyword">return</span> <span class="hljs-literal">nil</span>&#125;<span class="hljs-function"><span class="hljs-keyword">func</span> <span class="hljs-title">New</span><span class="hljs-params">(client *ssh.Client)</span> <span class="hljs-title">error</span></span> &#123;session, err := client.NewSession()<span class="hljs-keyword">if</span> err != <span class="hljs-literal">nil</span> &#123;<span class="hljs-keyword">return</span> err&#125;<span class="hljs-keyword">defer</span> session.Close()s := SSHTerminal&#123;Session: session,&#125;<span class="hljs-keyword">return</span> s.interactiveSession()&#125;</code></pre></div>]]></content:encoded>
      
      
      <category domain="https://mritd.com/categories/golang/">Golang</category>
      
      
      <category domain="https://mritd.com/tags/golang/">Golang</category>
      
      
      <comments>https://mritd.com/2018/11/09/go-interactive-shell/#disqus_thread</comments>
      
    </item>
    
    <item>
      <title>Go 代码的扩展套路</title>
      <link>https://mritd.com/2018/10/23/golang-code-plugin/</link>
      <guid>https://mritd.com/2018/10/23/golang-code-plugin/</guid>
      <pubDate>Tue, 23 Oct 2018 13:32:13 GMT</pubDate>
      
      <description>折腾 Go 已经有一段时间了，最近在用 Go 写点 web 的东西；在搭建脚手架的过程中总是有点不适应，尤其对可扩展性上总是感觉没有 Java 那么顺手；索性看了下 coredns 的源码，最后追踪到 caddy 源码；突然发现他们对代码内的 plugin 机制有一些骚套路，这里索性记录一下</description>
      
      
      
      <content:encoded><![CDATA[<blockquote><p>折腾 Go 已经有一段时间了，最近在用 Go 写点 web 的东西；在搭建脚手架的过程中总是有点不适应，尤其对可扩展性上总是感觉没有 Java 那么顺手；索性看了下 coredns 的源码，最后追踪到 caddy 源码；突然发现他们对代码内的 plugin 机制有一些骚套路，这里索性记录一下</p></blockquote><h3 id="一、问题由来"><a href="#一、问题由来" class="headerlink" title="一、问题由来"></a>一、问题由来</h3><p>纵观现在所有的 Go web 框架，在文档上可以看到使用方式很简明；非常符合我对 Go 的一贯感受: “所写即所得”；就拿 Gin 这个来说，在 README.md 上可以很轻松的看到 <code>engine</code> 或者说 <code>router</code> 这玩意的使用，比如下面这样:</p><div class="hljs"><pre><code class="hljs golang"><span class="hljs-function"><span class="hljs-keyword">func</span> <span class="hljs-title">main</span><span class="hljs-params">()</span></span> &#123;<span class="hljs-comment">// Disable Console Color</span><span class="hljs-comment">// gin.DisableConsoleColor()</span><span class="hljs-comment">// Creates a gin router with default middleware:</span><span class="hljs-comment">// logger and recovery (crash-free) middleware</span>router := gin.Default()router.GET(<span class="hljs-string">"/someGet"</span>, getting)router.POST(<span class="hljs-string">"/somePost"</span>, posting)router.PUT(<span class="hljs-string">"/somePut"</span>, putting)router.DELETE(<span class="hljs-string">"/someDelete"</span>, deleting)router.PATCH(<span class="hljs-string">"/somePatch"</span>, patching)router.HEAD(<span class="hljs-string">"/someHead"</span>, head)router.OPTIONS(<span class="hljs-string">"/someOptions"</span>, options)<span class="hljs-comment">// By default it serves on :8080 unless a</span><span class="hljs-comment">// PORT environment variable was defined.</span>router.Run()<span class="hljs-comment">// router.Run(":3000") for a hard coded port</span>&#125;</code></pre></div><p>乍一看简单到爆，但实际使用中，在脚手架搭建上，我们需要规划好 <strong>包结构、配置文件、命令行参数、数据库连接、cache</strong> 等等；直到目前为止，至少我没有找到一种非常规范的后端 MVC 的标准架子结构；这点目前确实不如 Java 的生态；作为最初的脚手架搭建者，站在这个角度，我想我们更应当考虑如何做好适当的抽象、隔离；以防止后面开发者对系统基础功能可能造成的破坏。</p><p>综上所述，再配合 Gin 或者说 Go 的代码风格，这就形成了一种强烈的冲突；在 Java 中，由于有注解(<code>Annotation</code>)的存在，事实上你是可以有这种操作的: <strong>新建一个 Class，创建 func，在上面加上合适的注解，最终框架会通过注解扫描的方式以适当的形式进行初始化</strong>；而 Go 中并没有 <code>Annotation</code> 这玩意，我们很难实现在 <strong>代码运行时扫描自身做出一种策略性调整</strong>；从而下面这个需求很难实现: <strong>作为脚手架搭建者，我希望我的基础代码安全的放在一个特定位置，后续开发者开发应当以一种类似可热插拔的形式注入进来</strong>，比如 Gin 的 router 路由设置，我不希望每次有修改都会有人动我的 router 核心配置文件。</p><h3 id="二、Caddy-的套路"><a href="#二、Caddy-的套路" class="headerlink" title="二、Caddy 的套路"></a>二、Caddy 的套路</h3><p>在翻了 coredns 的源码后，我发现他是依赖于 Caddy 这框架运行的，coredns 的代码内的插件机制也是直接调用的 Caddy；所以接着我就翻到了 Caddy 源码，其中的代码如下(完整代码<a href="https://github.com/mholt/caddy/blob/master/plugins.go" target="_blank" rel="noopener">点击这里</a>):</p><div class="hljs"><pre><code class="hljs golang"><span class="hljs-comment">// RegisterPlugin plugs in plugin. All plugins should register</span><span class="hljs-comment">// themselves, even if they do not perform an action associated</span><span class="hljs-comment">// with a directive. It is important for the process to know</span><span class="hljs-comment">// which plugins are available.</span><span class="hljs-comment">//</span><span class="hljs-comment">// The plugin MUST have a name: lower case and one word.</span><span class="hljs-comment">// If this plugin has an action, it must be the name of</span><span class="hljs-comment">// the directive that invokes it. A name is always required</span><span class="hljs-comment">// and must be unique for the server type.</span><span class="hljs-function"><span class="hljs-keyword">func</span> <span class="hljs-title">RegisterPlugin</span><span class="hljs-params">(name <span class="hljs-keyword">string</span>, plugin Plugin)</span></span> &#123;<span class="hljs-keyword">if</span> name == <span class="hljs-string">""</span> &#123;<span class="hljs-built_in">panic</span>(<span class="hljs-string">"plugin must have a name"</span>)&#125;<span class="hljs-keyword">if</span> _, ok := plugins[plugin.ServerType]; !ok &#123;plugins[plugin.ServerType] = <span class="hljs-built_in">make</span>(<span class="hljs-keyword">map</span>[<span class="hljs-keyword">string</span>]Plugin)&#125;<span class="hljs-keyword">if</span> _, dup := plugins[plugin.ServerType][name]; dup &#123;<span class="hljs-built_in">panic</span>(<span class="hljs-string">"plugin named "</span> + name + <span class="hljs-string">" already registered for server type "</span> + plugin.ServerType)&#125;plugins[plugin.ServerType][name] = plugin&#125;</code></pre></div><p>套路很清奇，为了实现我上面说的那个需求: “后面开发不需要动我核心代码，我还能允许他们动态添加”，Caddy 套路就是<strong>定义一个 map，map 里用于存放一种特定形式的 func，并且暴露出一个方法用于向 map 内添加指定 func，然后在合适的时机遍历这个 map，并执行其中的 func。</strong>这种套路利用了 Go 函数式编程的特性，将行为先存储在容器中，然后后续再去调用这些行为。</p><h3 id="三、总结"><a href="#三、总结" class="headerlink" title="三、总结"></a>三、总结</h3><p>长篇大论这么久，实际上我也是在一边折腾 Go 的过程中一边总结和对比跟 Java 的差异；在 Java 中扫描自己注解的套路 Go 中没法实现，但是 Go 利用其函数式编程的优势也可以利用一些延迟加载方式实现对应的功能；总结来说，不同语言有其自己的特性，当有对比的时候，可能更加深刻。</p>]]></content:encoded>
      
      
      <category domain="https://mritd.com/categories/golang/">Golang</category>
      
      
      <category domain="https://mritd.com/tags/golang/">Golang</category>
      
      
      <comments>https://mritd.com/2018/10/23/golang-code-plugin/#disqus_thread</comments>
      
    </item>
    
    <item>
      <title>Google container registry 同步</title>
      <link>https://mritd.com/2018/09/17/google-container-registry-sync/</link>
      <guid>https://mritd.com/2018/09/17/google-container-registry-sync/</guid>
      <pubDate>Mon, 17 Sep 2018 13:19:40 GMT</pubDate>
      
      <description>Google container registry 同步</description>
      
      
      
      <content:encoded><![CDATA[<h2 id="一、起因"><a href="#一、起因" class="headerlink" title="一、起因"></a>一、起因</h2><p>玩 Kubenretes 的基本都很清楚，Kubernetes 很多组件的镜像全部托管在 <code>gcr.io</code> 这个域名下(现在换成了 <code>k8s.gcr.io</code>)；由于众所周知的原因，这个网站在国内是不可达的；当时由于 Docker Hub 提供了 <code>Auto Build</code> 功能，机智的想到一个解决办法；就是利用 Docker Hub 的 <code>Auto Build</code>，创建只有一行的 Dockerfile，里面就一句 <code>FROM gcr.io/xxxx</code>，然后让 Docker Hub 帮你构建完成后拉取即可</p><p>这种套路的基本方案就是利用一个第三方公共仓库，这个仓库可以访问不可达的 <code>gcr.io</code>，然后生成镜像，我们再从这个仓库 pull 即可；为此我创建了一个 Github 仓库(<a href="https://github.com/mritd/docker-library" target="_blank" rel="noopener">docker-library</a>)；时隔这么久以后，我猜想大家都已经有了这种自己的仓库…不过最近发现这个仓库仍然在有人 fork…</p><p>为了一劳永逸的解决这个问题，只能撸点代码解决这个问题了</p><h2 id="二、仓库使用"><a href="#二、仓库使用" class="headerlink" title="二、仓库使用"></a>二、仓库使用</h2><p>为了解决上述问题，我写了一个 <a href="https://github.com/mritd/gcrsync" target="_blank" rel="noopener">gcrsync</a> 工具，并且借助 <a href="https://travis-ci.org/mritd/gcrsync" target="_blank" rel="noopener">Travis CI</a> 让其每天自动运行，将所有用得到的 <code>gcr.io</code> 下的镜像同步到了 Docker Hub</p><p><strong>目前对于一个 <code>gcr.io</code> 下的镜像，可以直接替换为 <code>gcrxio</code> 用户名，然后从 Docker Hub 直接拉取</strong>，以下为一个示例:</p><div class="hljs"><pre><code class="hljs sh"><span class="hljs-comment"># 原始命令</span>docker pull k8s.gcr.io/kubernetes-dashboard-amd64:v1.10.0<span class="hljs-comment"># 使用同步仓库</span>docker pull gcrxio/kubernetes-dashboard-amd64:v1.10.0</code></pre></div><h2 id="三、同步细节说明"><a href="#三、同步细节说明" class="headerlink" title="三、同步细节说明"></a>三、同步细节说明</h2><p>为了保证同步镜像的安全性，同步工具已经开源在 <a href="https://github.com/mritd/gcrsync" target="_blank" rel="noopener">gcrsync</a> 仓库，同步细节如下:</p><ul><li>工具每天由 <a href="https://travis-ci.org/mritd/gcrsync" target="_blank" rel="noopener">Travis CI</a> 自动进行一次 build，然后进行推送</li><li>工具每次推送前首先 clone 元数据仓库 <a href="https://github.com/mritd/gcr" target="_blank" rel="noopener">gcr</a></li><li>工具每次推送首先获取 <code>gcr.io</code> 指定 <code>namespace</code> 下的所有镜像(<code>namesapce</code> 由 <a href="https://github.com/mritd/gcrsync/blob/master/.travis.yml" target="_blank" rel="noopener">.travis.yml</a> <code>script</code> 段定义)</li><li>获取 <code>gcr.io</code> 镜像后，再读取元数据仓库(<a href="https://github.com/mritd/gcr" target="_blank" rel="noopener">gcr</a>) 中与 <code>namesapce</code> 同名文件(实际是个 json)</li><li>接着对比双方差异，得出需要同步的镜像</li><li>最后通过 API 调用本地的 docker 进行 <code>pull</code>、<code>tag</code>、<code>push</code> 操作，完成镜像推送</li><li>所有镜像推送成功后，更新元数据仓库内 <code>namespace</code> 对应的 json 文件，最后在生成 <a href="https://github.com/mritd/gcr/blob/master/CHANGELOG.md" target="_blank" rel="noopener">CHANGELOG</a>，执行 <code>git push</code> 到远程元数据仓库</li></ul><p>综上所述，如果想得知<strong>具体 <code>gcrxio</code> 用户下都有那些镜像，可直接访问 <a href="https://github.com/mritd/gcr" target="_blank" rel="noopener">gcr</a> 元数据仓库，查看对应 <code>namesapce</code> 同名的 json 文件即可；每天增量同步的信息会追加到 <a href="https://github.com/mritd/gcr" target="_blank" rel="noopener">gcr</a> 仓库的 <code>CHANGELOG.md</code> 文件中</strong></p><h2 id="四、gcrsync"><a href="#四、gcrsync" class="headerlink" title="四、gcrsync"></a>四、gcrsync</h2><p>为方便审查镜像安全性，以下为 <a href="https://github.com/mritd/gcrsync" target="_blank" rel="noopener">gcrsync</a> 工具的代码简介，代码仓库文件如下:</p><div class="hljs"><pre><code class="hljs sh">➜  gcrsync git:(master) tree -I vendor.├── CHANGELOG.md├── Gopkg.lock├── Gopkg.toml├── LICENSE├── README.md├── cmd│   ├── compare.go│   ├── monitor.go│   ├── root.go│   ├── sync.go│   └── test.go├── dist│   ├── gcrsync_darwin_amd64│   ├── gcrsync_linux_386│   └── gcrsync_linux_amd64├── main.go└── pkg    ├── gcrsync    │   ├── docker.go    │   ├── gcr.go    │   ├── git.go    │   ├── registry.go    │   └── sync.go    └── utils        └── common.go</code></pre></div><p>cmd 目录下为标准的 <code>cobra</code> 框架生成的子命令文件，其中每个命令包含了对应的 flag 设置，如 <code>namesapce</code>、<code>proxy</code> 等；<code>pkg/gcrsync</code> 目录下的文件为核心代码:</p><ul><li><code>docker.go</code> 包含了对本地 docker daemon API 调用，包括 <code>pull</code>、<code>tag</code>、<code>push</code> 操作</li><li><code>gcr.go</code> 包含了对 <code>gcr.io</code> 指定 <code>namespace</code> 下镜像列表获取操作</li><li><code>registry.go</code> 包含了对 Docker Hub 下指定用户(默认 <code>gcrxio</code>)的镜像列表获取操作(其主要用于首次执行 <code>compare</code> 命令生成 json 文件)</li><li><code>sync.go</code> 为主要的程序入口，其中包含了对其他文件内方法的调用，设置并发池等</li></ul><h2 id="五、其他说明"><a href="#五、其他说明" class="headerlink" title="五、其他说明"></a>五、其他说明</h2><p>该仓库不保证镜像实时同步，默认每天同步一次(由 <a href="https://travis-ci.org/mritd/gcrsync" target="_blank" rel="noopener">Travis CI</a> 执行)，如有特殊需求，如增加 <code>namesapce</code> 等请开启 issue；最后，请不要再 fork <a href="https://github.com/mritd/docker-library" target="_blank" rel="noopener">docker-library</a> 这个仓库了</p>]]></content:encoded>
      
      
      <category domain="https://mritd.com/categories/kubernetes/">Kubernetes</category>
      
      
      <category domain="https://mritd.com/tags/docker/">Docker</category>
      
      <category domain="https://mritd.com/tags/kubernetes/">Kubernetes</category>
      
      
      <comments>https://mritd.com/2018/09/17/google-container-registry-sync/#disqus_thread</comments>
      
    </item>
    
    <item>
      <title>使用 Bootstrap Token 完成 TLS Bootstrapping</title>
      <link>https://mritd.com/2018/08/28/kubernetes-tls-bootstrapping-with-bootstrap-token/</link>
      <guid>https://mritd.com/2018/08/28/kubernetes-tls-bootstrapping-with-bootstrap-token/</guid>
      <pubDate>Tue, 28 Aug 2018 08:54:43 GMT</pubDate>
      
      <description>最近在测试 Kubernetes 1.11.2 新版本的相关东西，发现新版本的 Bootstrap Token 功能已经进入 Beta 阶段，索性便尝试了一下；虽说目前是为 kubeadm 设计的，不过手动挡用起来也不错，这里记录一下使用方式</description>
      
      
      
      <content:encoded><![CDATA[<blockquote><p>最近在测试 Kubernetes 1.11.2 新版本的相关东西，发现新版本的 Bootstrap Token 功能已经进入 Beta 阶段，索性便尝试了一下；虽说目前是为 kubeadm 设计的，不过手动挡用起来也不错，这里记录一下使用方式</p></blockquote><h2 id="一、环境准备"><a href="#一、环境准备" class="headerlink" title="一、环境准备"></a>一、环境准备</h2><p>首先需要有一个运行状态正常的 Master 节点，目前我测试的是版本是 1.11.2，低版本我没测试；其次本文默认 Node 节点 Docker、kubelet 二进制文件、systemd service 配置等都已经处理好，更具体的环境如下:</p><p><strong>Master 节点 IP 为 <code>192.168.1.61</code>，Node 节点 IP 为 <code>192.168.1.64</code></strong></p><div class="hljs"><pre><code class="hljs sh">docker1.node ➜  ~ kubectl versionClient Version: version.Info&#123;Major:<span class="hljs-string">"1"</span>, Minor:<span class="hljs-string">"11"</span>, GitVersion:<span class="hljs-string">"v1.11.2"</span>, GitCommit:<span class="hljs-string">"bb9ffb1654d4a729bb4cec18ff088eacc153c239"</span>, GitTreeState:<span class="hljs-string">"clean"</span>, BuildDate:<span class="hljs-string">"2018-08-07T23:08:19Z"</span>, GoVersion:<span class="hljs-string">"go1.10.3"</span>, Compiler:<span class="hljs-string">"gc"</span>, Platform:<span class="hljs-string">"linux/amd64"</span>&#125;Server Version: version.Info&#123;Major:<span class="hljs-string">"1"</span>, Minor:<span class="hljs-string">"11"</span>, GitVersion:<span class="hljs-string">"v1.11.2"</span>, GitCommit:<span class="hljs-string">"bb9ffb1654d4a729bb4cec18ff088eacc153c239"</span>, GitTreeState:<span class="hljs-string">"clean"</span>, BuildDate:<span class="hljs-string">"2018-08-07T23:08:19Z"</span>, GoVersion:<span class="hljs-string">"go1.10.3"</span>, Compiler:<span class="hljs-string">"gc"</span>, Platform:<span class="hljs-string">"linux/amd64"</span>&#125;docker1.node ➜  ~ docker infoContainers: 0 Running: 0 Paused: 0 Stopped: 0Images: 0Server Version: 18.06.1-ceStorage Driver: overlay2 Backing Filesystem: xfs Supports d_type: <span class="hljs-literal">true</span> Native Overlay Diff: <span class="hljs-literal">true</span>Logging Driver: json-fileCgroup Driver: cgroupfsPlugins: Volume: <span class="hljs-built_in">local</span> Network: bridge host macvlan null overlay Log: awslogs fluentd gcplogs gelf journald json-file logentries splunk syslogSwarm: inactiveRuntimes: runcDefault Runtime: runcInit Binary: docker-initcontainerd version: 468a545b9edcd5932818eb9de8e72413e616e86erunc version: 69663f0bd4b60df09991c08812a60108003fa340init version: fec3683Security Options: apparmor seccomp  Profile: defaultKernel Version: 4.15.0-33-genericOperating System: Ubuntu 18.04.1 LTSOSType: linuxArchitecture: x86_64CPUs: 2Total Memory: 3.847GiBName: docker1.nodeID: AJOD:RBJZ:YP3G:HCGV:KT4R:D4AF:SBDN:5B76:JM4M:OCJA:YJMJ:OCYQDocker Root Dir: /data/dockerDebug Mode (client): <span class="hljs-literal">false</span>Debug Mode (server): <span class="hljs-literal">false</span>Registry: https://index.docker.io/v1/Labels:Experimental: <span class="hljs-literal">false</span>Insecure Registries: 127.0.0.0/8Live Restore Enabled: <span class="hljs-literal">false</span></code></pre></div><h2 id="二、TLS-Bootstrapping-回顾"><a href="#二、TLS-Bootstrapping-回顾" class="headerlink" title="二、TLS Bootstrapping 回顾"></a>二、TLS Bootstrapping 回顾</h2><p>在正式进行 TLS Bootstrapping 操作之前，<strong>如果对 TLS Bootstrapping 完全没接触过的请先阅读 <a href="https://mritd.me/2018/01/07/kubernetes-tls-bootstrapping-note" target="_blank" rel="noopener">Kubernetes TLS bootstrapping 那点事</a></strong>；我想这里有必要简单说明下使用 Token 时整个启动引导过程:</p><ul><li>在集群内创建特定的 <code>Bootstrap Token Secret</code>，该 Secret 将替代以前的 <code>token.csv</code> 内置用户声明文件</li><li>在集群内创建首次 TLS Bootstrap 申请证书的 ClusterRole、后续 renew Kubelet client/server 的 ClusterRole，以及其相关对应的 ClusterRoleBinding；并绑定到对应的组或用户</li><li>调整 Controller Manager 配置，以使其能自动签署相关证书和自动清理过期的 TLS Bootstrapping Token</li><li>生成特定的包含 TLS Bootstrapping Token 的 <code>bootstrap.kubeconfig</code> 以供 kubelet 启动时使用</li><li>调整 Kubelet 配置，使其首次启动加载 <code>bootstrap.kubeconfig</code> 并使用其中的 TLS Bootstrapping Token 完成首次证书申请</li><li>证书被 Controller Manager 签署，成功下发，Kubelet 自动重载完成引导流程</li><li>后续 Kubelet 自动 renew 相关证书</li><li>可选的: 集群搭建成功后立即清除 <code>Bootstrap Token Secret</code>，或等待 Controller Manager 待其过期后删除，以防止被恶意利用</li></ul><h2 id="三、使用-Bootstrap-Token"><a href="#三、使用-Bootstrap-Token" class="headerlink" title="三、使用 Bootstrap Token"></a>三、使用 Bootstrap Token</h2><p>第二部分算作大纲了，这部分将会按照第二部分的总体流程来走，同时会对一些细节进行详细说明</p><h3 id="3-1、创建-Bootstrap-Token"><a href="#3-1、创建-Bootstrap-Token" class="headerlink" title="3.1、创建 Bootstrap Token"></a>3.1、创建 Bootstrap Token</h3><p>既然整个功能都时刻强调这个 Token，那么第一步肯定是生成一个 token，生成方式如下:</p><div class="hljs"><pre><code class="hljs sh">➜  ~ <span class="hljs-built_in">echo</span> <span class="hljs-string">"<span class="hljs-variable">$(head -c 6 /dev/urandom | md5sum | head -c 6)</span>"</span>.<span class="hljs-string">"<span class="hljs-variable">$(head -c 16 /dev/urandom | md5sum | head -c 16)</span>"</span>47f392.d22d04e89a65eb22</code></pre></div><p>这个 <code>47f392.d22d04e89a65eb22</code> 就是生成的 Bootstrap Token，保存好 token，因为后续要用；关于这个 token 解释如下:</p><p>Token 必须满足 <code>[a-z0-9]{6}\.[a-z0-9]{16}</code> 格式；以 <code>.</code> 分割，前面的部分被称作  <code>Token ID</code>，<code>Token ID</code> 并不是 “机密信息”，它可以暴露出去；相对的后面的部分称为 <code>Token Secret</code>，它应该是保密的</p><p>本部分官方文档地址 <a href="https://kubernetes.io/docs/reference/access-authn-authz/bootstrap-tokens/#token-format" target="_blank" rel="noopener">Token Format</a></p><h3 id="3-2、创建-Bootstrap-Token-Secret"><a href="#3-2、创建-Bootstrap-Token-Secret" class="headerlink" title="3.2、创建 Bootstrap Token Secret"></a>3.2、创建 Bootstrap Token Secret</h3><p>对于 Kubernetes 来说 <code>Bootstrap Token Secret</code> 也仅仅是一个特殊的 <code>Secret</code> 而已；对于这个特殊的 <code>Secret</code> 样例 yaml 配置如下:</p><div class="hljs"><pre><code class="hljs yaml"><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">v1</span><span class="hljs-attr">kind:</span> <span class="hljs-string">Secret</span><span class="hljs-attr">metadata:</span>  <span class="hljs-comment"># Name MUST be of form "bootstrap-token-&lt;token id&gt;"</span>  <span class="hljs-attr">name:</span> <span class="hljs-string">bootstrap-token-07401b</span>  <span class="hljs-attr">namespace:</span> <span class="hljs-string">kube-system</span><span class="hljs-comment"># Type MUST be 'bootstrap.kubernetes.io/token'</span><span class="hljs-attr">type:</span> <span class="hljs-string">bootstrap.kubernetes.io/token</span><span class="hljs-attr">stringData:</span>  <span class="hljs-comment"># Human readable description. Optional.</span>  <span class="hljs-attr">description:</span> <span class="hljs-string">"The default bootstrap token generated by 'kubeadm init'."</span>  <span class="hljs-comment"># Token ID and secret. Required.</span>  <span class="hljs-attr">token-id:</span> <span class="hljs-string">47f392</span>  <span class="hljs-attr">token-secret:</span> <span class="hljs-string">d22d04e89a65eb22</span>  <span class="hljs-comment"># Expiration. Optional.</span>  <span class="hljs-attr">expiration:</span> <span class="hljs-number">2018</span><span class="hljs-number">-09</span><span class="hljs-string">-10T00:00:11Z</span>  <span class="hljs-comment"># Allowed usages.</span>  <span class="hljs-attr">usage-bootstrap-authentication:</span> <span class="hljs-string">"true"</span>  <span class="hljs-attr">usage-bootstrap-signing:</span> <span class="hljs-string">"true"</span>  <span class="hljs-comment"># Extra groups to authenticate the token as. Must start with "system:bootstrappers:"</span>  <span class="hljs-attr">auth-extra-groups:</span> <span class="hljs-string">system:bootstrappers:worker,system:bootstrappers:ingress</span></code></pre></div><p>需要注意几点:</p><ul><li>作为 <code>Bootstrap Token Secret</code> 的 type 必须为 <code>bootstrap.kubernetes.io/token</code>，name 必须为 <code>bootstrap-token-&lt;token id&gt;</code> (Token ID 就是上一步创建的 Token 前一部分)</li><li><code>usage-bootstrap-authentication</code>、<code>usage-bootstrap-signing</code> 必须存才且设置为 <code>true</code> (我个人感觉 <code>usage-bootstrap-signing</code> 可以没有，具体见文章最后部分)</li><li><code>expiration</code> 字段是可选的，如果设置则 <code>Secret</code> 到期后将由 Controller Manager 中的 <code>tokencleaner</code> 自动清理</li><li><code>auth-extra-groups</code> 也是可选的，令牌的扩展认证组，组必须以 <code>system:bootstrappers:</code> 开头</li></ul><p>最后使用 <code>kubectl create -f bootstrap.secret.yaml</code> 创建即可</p><p>本部分官方文档地址 <a href="https://kubernetes.io/docs/reference/access-authn-authz/bootstrap-tokens/#bootstrap-token-secret-format" target="_blank" rel="noopener">Bootstrap Token Secret Format</a></p><h3 id="3-3、创建-ClusterRole-和-ClusterRoleBinding"><a href="#3-3、创建-ClusterRole-和-ClusterRoleBinding" class="headerlink" title="3.3、创建 ClusterRole 和 ClusterRoleBinding"></a>3.3、创建 ClusterRole 和 ClusterRoleBinding</h3><p>具体都有哪些 <code>ClusterRole</code> 和 <code>ClusterRoleBinding</code>，以及其作用请参考上一篇的 <a href="https://mritd.me/2018/01/07/kubernetes-tls-bootstrapping-note" target="_blank" rel="noopener">Kubernetes TLS bootstrapping 那点事</a>，不想在这里重复了</p><p>在 1.8 以后三个 <code>ClusterRole</code> 中有两个已经有了，我们只需要创建剩下的一个即可:</p><div class="hljs"><pre><code class="hljs yaml"><span class="hljs-comment"># A ClusterRole which instructs the CSR approver to approve a node requesting a</span><span class="hljs-comment"># serving cert matching its client cert.</span><span class="hljs-attr">kind:</span> <span class="hljs-string">ClusterRole</span><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">rbac.authorization.k8s.io/v1</span><span class="hljs-attr">metadata:</span>  <span class="hljs-attr">name:</span> <span class="hljs-string">system:certificates.k8s.io:certificatesigningrequests:selfnodeserver</span><span class="hljs-attr">rules:</span><span class="hljs-bullet">-</span> <span class="hljs-attr">apiGroups:</span> <span class="hljs-string">["certificates.k8s.io"]</span>  <span class="hljs-attr">resources:</span> <span class="hljs-string">["certificatesigningrequests/selfnodeserver"]</span>  <span class="hljs-attr">verbs:</span> <span class="hljs-string">["create"]</span></code></pre></div><p>然后是三个 <code>ClusterRole</code> 对应的 <code>ClusterRoleBinding</code>；需要注意的是 <strong>在使用 <code>Bootstrap Token</code> 进行引导时，Kubelet 组件使用 Token 发起的请求其用户名为 <code>system:bootstrap:&lt;token id&gt;</code>，用户组为 <code>system:bootstrappers</code>；so 我们在创建 <code>ClusterRoleBinding</code> 时要绑定到这个用户或者组上</strong>；当然我选择懒一点，全部绑定到组上</p><div class="hljs"><pre><code class="hljs sh"><span class="hljs-comment"># 允许 system:bootstrappers 组用户创建 CSR 请求</span>kubectl create clusterrolebinding kubelet-bootstrap --clusterrole=system:node-bootstrapper --group=system:bootstrappers<span class="hljs-comment"># 自动批准 system:bootstrappers 组用户 TLS bootstrapping 首次申请证书的 CSR 请求</span>kubectl create clusterrolebinding node-client-auto-approve-csr --clusterrole=system:certificates.k8s.io:certificatesigningrequests:nodeclient --group=system:bootstrappers<span class="hljs-comment"># 自动批准 system:nodes 组用户更新 kubelet 自身与 apiserver 通讯证书的 CSR 请求</span>kubectl create clusterrolebinding node-client-auto-renew-crt --clusterrole=system:certificates.k8s.io:certificatesigningrequests:selfnodeclient --group=system:nodes<span class="hljs-comment"># 自动批准 system:nodes 组用户更新 kubelet 10250 api 端口证书的 CSR 请求</span>kubectl create clusterrolebinding node-server-auto-renew-crt --clusterrole=system:certificates.k8s.io:certificatesigningrequests:selfnodeserver --group=system:nodes</code></pre></div><p>关于本部分首次请求用户名变为 <code>system:bootstrap:&lt;token id&gt;</code> 官方文档原文如下:</p><blockquote><p>Tokens authenticate as the username system:bootstrap:<token id> and are members of the group system:bootstrappers. Additional groups may be specified in the token’s Secret.</p></blockquote><h3 id="3-4、调整-Controller-Manager"><a href="#3-4、调整-Controller-Manager" class="headerlink" title="3.4、调整 Controller Manager"></a>3.4、调整 Controller Manager</h3><p>根据官方文档描述，Controller Manager 需要启用 <code>tokencleaner</code> 和 <code>bootstrapsigner</code> (目测这个 <code>bootstrapsigner</code> 实际上并不需要，顺便加着吧)，完整配置如下(为什么贴完整配置? 文章凑数啊…):</p><div class="hljs"><pre><code class="hljs sh">KUBE_CONTROLLER_MANAGER_ARGS=<span class="hljs-string">"  --address=127.0.0.1 \</span><span class="hljs-string">                                --bind-address=192.168.1.61 \</span><span class="hljs-string">                                --port=10252 \</span><span class="hljs-string">                                --secure-port=10258 \</span><span class="hljs-string">                                --cluster-name=kubernetes \</span><span class="hljs-string">                                --cluster-signing-cert-file=/etc/kubernetes/ssl/k8s-root-ca.pem \</span><span class="hljs-string">                                --cluster-signing-key-file=/etc/kubernetes/ssl/k8s-root-ca-key.pem \</span><span class="hljs-string">                                --controllers=*,bootstrapsigner,tokencleaner \</span><span class="hljs-string">                                --deployment-controller-sync-period=10s \</span><span class="hljs-string">                                --experimental-cluster-signing-duration=86700h0m0s \</span><span class="hljs-string">                                --enable-garbage-collector=true \</span><span class="hljs-string">                                --leader-elect=true \</span><span class="hljs-string">                                --master=http://127.0.0.1:8080 \</span><span class="hljs-string">                                --node-monitor-grace-period=40s \</span><span class="hljs-string">                                --node-monitor-period=5s \</span><span class="hljs-string">                                --pod-eviction-timeout=5m0s \</span><span class="hljs-string">                                --terminated-pod-gc-threshold=50 \</span><span class="hljs-string">                                --root-ca-file=/etc/kubernetes/ssl/k8s-root-ca.pem \</span><span class="hljs-string">                                --service-account-private-key-file=/etc/kubernetes/ssl/k8s-root-ca-key.pem \</span><span class="hljs-string">                                --feature-gates=RotateKubeletServerCertificate=true"</span></code></pre></div><h3 id="3-5、生成-bootstrap-kubeconfig"><a href="#3-5、生成-bootstrap-kubeconfig" class="headerlink" title="3.5、生成 bootstrap.kubeconfig"></a>3.5、生成 bootstrap.kubeconfig</h3><p>前面所有步骤实际上都是在处理 Api Server、Controller Manager 这一块，为的就是 “老子启动后 TLS Bootstarpping 发证书申请你两个要立马允许，不能拒绝老子”；接下来就是比较重要的 <code>bootstrap.kubeconfig</code> 配置生成，这个 <code>bootstrap.kubeconfig</code> 是最终被 Kubelet 使用的，里面包含了相关的 Token，以帮助 Kubelet 在第一次通讯时能成功沟通 Api Server；生成方式如下:</p><div class="hljs"><pre><code class="hljs sh"><span class="hljs-comment"># 设置集群参数</span>kubectl config <span class="hljs-built_in">set</span>-cluster kubernetes \  --certificate-authority=/etc/kubernetes/ssl/k8s-root-ca.pem \  --embed-certs=<span class="hljs-literal">true</span> \  --server=https://127.0.0.1:6443 \  --kubeconfig=bootstrap.kubeconfig<span class="hljs-comment"># 设置客户端认证参数</span>kubectl config <span class="hljs-built_in">set</span>-credentials system:bootstrap:47f392 \  --token=47f392.d22d04e89a65eb22 \  --kubeconfig=bootstrap.kubeconfig<span class="hljs-comment"># 设置上下文参数</span>kubectl config <span class="hljs-built_in">set</span>-context default \  --cluster=kubernetes \  --user=system:bootstrap:47f392 \  --kubeconfig=bootstrap.kubeconfig<span class="hljs-comment"># 设置默认上下文</span>kubectl config use-context default --kubeconfig=bootstrap.kubeconfig</code></pre></div><h3 id="3-6、调整-Kubelet"><a href="#3-6、调整-Kubelet" class="headerlink" title="3.6、调整 Kubelet"></a>3.6、调整 Kubelet</h3><p>Kubelet 启动参数需要做一些相应调整，以使其能正确的使用 <code>Bootstartp Token</code>，完整配置如下(与使用 token.csv 配置没什么变化，因为主要变更在 bootstrap.kubeconfig 中):</p><div class="hljs"><pre><code class="hljs sh">KUBELET_ARGS=<span class="hljs-string">"  --address=192.168.1.64 \</span><span class="hljs-string">                --allow-privileged=true \</span><span class="hljs-string">                --alsologtostderr \</span><span class="hljs-string">                --anonymous-auth=true \</span><span class="hljs-string">                --bootstrap-kubeconfig=/etc/kubernetes/bootstrap.kubeconfig \</span><span class="hljs-string">                --cert-dir=/etc/kubernetes/ssl \</span><span class="hljs-string">                --cgroup-driver=cgroupfs \</span><span class="hljs-string">                --cluster-dns=10.254.0.2 \</span><span class="hljs-string">                --cluster-domain=cluster.local. \</span><span class="hljs-string">                --fail-swap-on=false \</span><span class="hljs-string">                --healthz-port=10248 \</span><span class="hljs-string">                --healthz-bind-address=192.168.1.64 \</span><span class="hljs-string">                --feature-gates=RotateKubeletClientCertificate=true,RotateKubeletServerCertificate=true \</span><span class="hljs-string">                --node-labels=node-role.kubernetes.io/k8s-master=true \</span><span class="hljs-string">                --image-gc-high-threshold=70 \</span><span class="hljs-string">                --image-gc-low-threshold=50 \</span><span class="hljs-string">                --kube-reserved=cpu=500m,memory=512Mi,ephemeral-storage=1Gi \</span><span class="hljs-string">                --kubeconfig=/etc/kubernetes/kubelet.kubeconfig \</span><span class="hljs-string">                --system-reserved=cpu=1000m,memory=1024Mi,ephemeral-storage=1Gi \</span><span class="hljs-string">                --serialize-image-pulls=false \</span><span class="hljs-string">                --sync-frequency=30s \</span><span class="hljs-string">                --pod-infra-container-image=k8s.gcr.io/pause:3.1 \</span><span class="hljs-string">                --resolv-conf=/etc/resolv.conf \</span><span class="hljs-string">                --rotate-certificates"</span></code></pre></div><p><strong>一切准备就绪后，执行 <code>systemctl daemon-reload &amp;&amp; systemctl start kubelet</code> 启动即可</strong></p><h2 id="四、其他说明"><a href="#四、其他说明" class="headerlink" title="四、其他说明"></a>四、其他说明</h2><p>可能有人已经注意到，在官方文档中最后部分有关于 <a href="https://kubernetes.io/docs/reference/access-authn-authz/bootstrap-tokens/#configmap-signing" target="_blank" rel="noopener">ConfigMap Signing</a> 的相关描述，同时要求了启用 <code>bootstrapsigner</code> 这个 controller，而且在上文创建 <code>Bootstrap Token Secret</code> 中我也说 <code>usage-bootstrap-signing</code> 这个可以不设置；其中官方文档上的描述我们能看到的大致只说了这么两段稍微有点用的话:</p><blockquote><p>In addition to authentication, the tokens can be used to sign a ConfigMap. This is used early in a cluster bootstrap process before the client trusts the API server. The signed ConfigMap can be authenticated by the shared token.</p></blockquote><blockquote><p>The ConfigMap that is signed is cluster-info in the kube-public namespace. The typical flow is that a client reads this ConfigMap while unauthenticated and ignoring TLS errors. It then validates the payload of the ConfigMap by looking at a signature embedded in the ConfigMap.</p></blockquote><p>从这两段话中我们只能得出两个结论:</p><ul><li>Bootstrap Token 能对 ConfigMap 签名</li><li>可以签名一个 <code>kube-public</code> NameSpace 下的名字叫 <code>cluster-info</code> 的 ConfigMap，并且这个 ConfigMap 可以在没进行引导之前强行读取</li></ul><p>说实话这两段话搞得我百思不得<del>骑姐</del>其解，最终我在 kubeadm 的相关文档中找到了真正的说明及作用:</p><ul><li>在使用 <code>kubeadm init</code> 时创建 <code>cluster-info</code> 这个 ConfigMap，ConfigMap 中包含了集群基本信息</li><li>在使用 <code>kubeadm join</code> 时目标节点强行读取 ConfigMap 以得知集群基本信息，然后进行 <code>join</code></li></ul><p><strong>综上所述，我个人认为手动部署下，在仅仅使用 Bootstrap Token 进行 TLS Bootstrapping 时，<code>bootstrapsigner</code> 这个 controller 和 <code>Bootstrap Token Secret</code> 中的 <code>usage-bootstrap-signing</code> 选项是没有必要的，当然我还没测试(胡吹谁不会)…</strong></p><p>最后附上 <code>kubeadm</code> 的文档说明: <a href="https://kubernetes.io/docs/reference/setup-tools/kubeadm/implementation-details/#create-the-public-cluster-info-configmap" target="_blank" rel="noopener">Create the public cluster-info ConfigMap</a>、<a href="https://kubernetes.io/docs/reference/setup-tools/kubeadm/implementation-details/#discovery-cluster-info" target="_blank" rel="noopener">Discovery cluster-info</a></p>]]></content:encoded>
      
      
      <category domain="https://mritd.com/categories/kubernetes/">Kubernetes</category>
      
      
      <category domain="https://mritd.com/tags/kubernetes/">Kubernetes</category>
      
      
      <comments>https://mritd.com/2018/08/28/kubernetes-tls-bootstrapping-with-bootstrap-token/#disqus_thread</comments>
      
    </item>
    
    <item>
      <title>Kubernetes 证书配置</title>
      <link>https://mritd.com/2018/08/26/kubernetes-certificate-configuration/</link>
      <guid>https://mritd.com/2018/08/26/kubernetes-certificate-configuration/</guid>
      <pubDate>Sun, 26 Aug 2018 14:54:16 GMT</pubDate>
      
      <description>一直以来自己的 Kubernetes 集群大部分证书配置全部都在使用一个 CA，而事实上很多教程也没有具体的解释过这些证书代表的作用以及含义；今天索性仔细的翻了翻，顺便看到了一篇老外的文章，感觉写的不错，这里顺带着自己的理解总结一下。</description>
      
      
      
      <content:encoded><![CDATA[<blockquote><p>一直以来自己的 Kubernetes 集群大部分证书配置全部都在使用一个 CA，而事实上很多教程也没有具体的解释过这些证书代表的作用以及含义；今天索性仔细的翻了翻，顺便看到了一篇老外的文章，感觉写的不错，这里顺带着自己的理解总结一下。</p></blockquote><h2 id="一、Kubernetes-证书分类"><a href="#一、Kubernetes-证书分类" class="headerlink" title="一、Kubernetes 证书分类"></a>一、Kubernetes 证书分类</h2><p>这里的证书分类只是我自己定义的一种 “并不 ok” 的概念；从整体的作用上 Kubernetes 证书大致上应当分为两类:</p><ul><li>API Server 用于校验请求合法性证书</li><li>对其他敏感信息进行签名的证书(如 Service Account)</li></ul><p>对于 API Server 用于检验请求合法性的证书配置一般会在 API Server 中配置好，而对其他敏感信息签名加密的证书一般会可能放在 Controller Manager 中配置，也可能还在 API Server，具体不同版本需要撸文档</p><p>另外需要明确的是: <strong>Kubernetes 中 CA 证书并不一定只有一个，很多证书配置实际上是不相干的，只是大家为了方便普遍选择了使用一个 CA 进行签发；同时有一些证书如果不设置也会自动默认一个，就目前我所知的大约有 5 个可以完全不同的证书签发体系(或者说由不同的 CA 签发)</strong></p><h2 id="二、API-Server-中的证书配置"><a href="#二、API-Server-中的证书配置" class="headerlink" title="二、API Server 中的证书配置"></a>二、API Server 中的证书配置</h2><h3 id="2-1、API-Server-证书"><a href="#2-1、API-Server-证书" class="headerlink" title="2.1、API Server 证书"></a>2.1、API Server 证书</h3><p>API Server 证书配置中最应当明确的两个选项应该是以下两个:</p><div class="hljs"><pre><code class="hljs sh">--tls-cert-file string    File containing the default x509 Certificate <span class="hljs-keyword">for</span> HTTPS. (CA cert, <span class="hljs-keyword">if</span> any, concatenated after server cert). If HTTPS serving is enabled, and --tls-cert-file and --tls-private-key-file are not provided, a self-signed certificate and key are generated <span class="hljs-keyword">for</span> the public address and saved to the directory specified by --cert-dir.--tls-private-key-file string    File containing the default x509 private key matching --tls-cert-file.</code></pre></div><p>从描述上就可以看出，这两个选项配置的就是 API Server HTTPS 端点应当使用的证书</p><h3 id="2-2、Client-CA-证书"><a href="#2-2、Client-CA-证书" class="headerlink" title="2.2、Client CA 证书"></a>2.2、Client CA 证书</h3><p>接下来就是我们常见的 CA 配置:</p><div class="hljs"><pre><code class="hljs sh">--client-ca-file string    If <span class="hljs-built_in">set</span>, any request presenting a client certificate signed by one of the authorities <span class="hljs-keyword">in</span> the client-ca-file is authenticated with an identity corresponding to the CommonName of the client certificate.</code></pre></div><p>该配置明确了 Clent 连接 API Server 时，API Server 应当确保其证书源自哪个 CA 签发；如果其证书不是由该 CA 签发，则拒绝请求；事实上，这个 CA 不必与 HTTPS 端点所使用的证书 CA 相同；同时这里的 Client 是一个泛指的，可以是 kubectl，也可能是你自己开发的应用</p><h3 id="2-3、请求头证书"><a href="#2-3、请求头证书" class="headerlink" title="2.3、请求头证书"></a>2.3、请求头证书</h3><p>由于 API Server 是支持多种认证方式的，其中一种就是使用 HTTP 头中的指定字段来进行认证，相关配置如下:</p><div class="hljs"><pre><code class="hljs sh">--requestheader-allowed-names stringSlice    List of client certificate common names to allow to provide usernames <span class="hljs-keyword">in</span> headers specified by --requestheader-username-headers. If empty, any client certificate validated by the authorities <span class="hljs-keyword">in</span> --requestheader-client-ca-file is allowed.--requestheader-client-ca-file string    Root certificate bundle to use to verify client certificates on incoming requests before trusting usernames <span class="hljs-keyword">in</span> headers specified by --requestheader-username-headers. WARNING: generally <span class="hljs-keyword">do</span> not depend on authorization being already <span class="hljs-keyword">done</span> <span class="hljs-keyword">for</span> incoming requests.</code></pre></div><p>当指定这个 CA 证书后，则 API Server 使用 HTTP 头进行认证时会检测其 HTTP 头中发送的证书是否由这个 CA 签发；同样它也可独立于其他 CA(可以是个独立的 CA)；具体可以参考 <a href="https://kubernetes.io/docs/reference/access-authn-authz/authentication/#authenticating-proxy" target="_blank" rel="noopener">Authenticating Proxy</a></p><h3 id="2-4、Kubelet-证书"><a href="#2-4、Kubelet-证书" class="headerlink" title="2.4、Kubelet 证书"></a>2.4、Kubelet 证书</h3><p>对于 Kubelet 组件，API Server 单独提供了证书配置选项，同时 Kubelet 组件也提供了反向设置的相关选项:</p><div class="hljs"><pre><code class="hljs sh"><span class="hljs-comment"># API Server</span>--kubelet-certificate-authority string    Path to a cert file <span class="hljs-keyword">for</span> the certificate authority.--kubelet-client-certificate string    Path to a client cert file <span class="hljs-keyword">for</span> TLS.--kubelet-client-key string    Path to a client key file <span class="hljs-keyword">for</span> TLS.<span class="hljs-comment"># Kubelet</span>--client-ca-file string    If <span class="hljs-built_in">set</span>, any request presenting a client certificate signed by one of the authorities <span class="hljs-keyword">in</span> the client-ca-file is authenticated with an identity corresponding to the CommonName of the client certificate.--tls-cert-file string    File containing x509 Certificate used <span class="hljs-keyword">for</span> serving HTTPS (with intermediate certs, <span class="hljs-keyword">if</span> any, concatenated after server cert). If --tls-cert-file and --tls-private-key-file are not provided, a self-signed certificate and key are generated <span class="hljs-keyword">for</span> the public address and saved to the directory passed to --cert-dir.--tls-private-key-file string    File containing x509 private key matching --tls-cert-file.</code></pre></div><p>相信这个配置不用多说就能猜到，这个就是用于指定 API Server 与 Kubelet 通讯所使用的证书以及其签署的 CA；同样这个 CA 可以完全独立与上述其他CA</p><h2 id="三、Service-Account-证书"><a href="#三、Service-Account-证书" class="headerlink" title="三、Service Account 证书"></a>三、Service Account 证书</h2><p>在 API Server 配置中，对于 Service Account 同样有两个证书配置:</p><div class="hljs"><pre><code class="hljs sh">--service-account-key-file stringArray    File containing PEM-encoded x509 RSA or ECDSA private or public keys, used to verify ServiceAccount tokens. The specified file can contain multiple keys, and the flag can be specified multiple <span class="hljs-built_in">times</span> with different files. If unspecified, --tls-private-key-file is used. Must be specified when --service-account-signing-key is provided--service-account-signing-key-file string    Path to the file that contains the current private key of the service account token issuer. The issuer will sign issued ID tokens with this private key. (Requires the <span class="hljs-string">'TokenRequest'</span> feature gate.)</code></pre></div><p>这两个配置描述了对 Service Account 进行签名验证时所使用的证书；不过需要注意的是这里并没有明确要求证书 CA，所以这两个证书的 CA 理论上也是可以完全独立的；至于未要求 CA 问题，可能是由于 jwt 库并不支持 CA 验证</p><h2 id="四、总结"><a href="#四、总结" class="headerlink" title="四、总结"></a>四、总结</h2><p>Kubernetes 中大部分证书都是用于 API Server 各种鉴权使用的；在不同鉴权方案或者对象上实际证书体系可以完全不同；具体是使用多个 CA 好还是都用一个，取决于集群规模、安全性要求等等因素，至少目前来说没有明确的那个好与不好</p><p>最后，嗯…吹牛逼就吹到这，有点晚了，得睡觉了…</p>]]></content:encoded>
      
      
      <category domain="https://mritd.com/categories/kubernetes/">Kubernetes</category>
      
      
      <category domain="https://mritd.com/tags/kubernetes/">Kubernetes</category>
      
      
      <comments>https://mritd.com/2018/08/26/kubernetes-certificate-configuration/#disqus_thread</comments>
      
    </item>
    
    <item>
      <title>编写 kubectl 插件</title>
      <link>https://mritd.com/2018/08/09/create-a-plugin-for-kubectl/</link>
      <guid>https://mritd.com/2018/08/09/create-a-plugin-for-kubectl/</guid>
      <pubDate>Thu, 09 Aug 2018 14:19:35 GMT</pubDate>
      
      <description>最近忙的晕头转向，博客停更了 1 个月，感觉对不起党、对不起人民、对不起 ~~CCAV~~...不过在忙的时候操作 Kubernetes 集群要频繁的使用 `kubectl` 命令，而在多个 NameSpace 下来回切换每次都得加个 `-n` 简直让我想打人；索性翻了下 `kubectl` 的插件机制，顺便写了一个快速切换 NameSpace 的小插件，以下记录一下插件编写过程</description>
      
      
      
      <content:encoded><![CDATA[<blockquote><p>最近忙的晕头转向，博客停更了 1 个月，感觉对不起党、对不起人民、对不起 <del>CCAV</del>…不过在忙的时候操作 Kubernetes 集群要频繁的使用 <code>kubectl</code> 命令，而在多个 NameSpace 下来回切换每次都得加个 <code>-n</code> 简直让我想打人；索性翻了下 <code>kubectl</code> 的插件机制，顺便写了一个快速切换 NameSpace 的小插件，以下记录一下插件编写过程</p></blockquote><h2 id="一、插件介绍"><a href="#一、插件介绍" class="headerlink" title="一、插件介绍"></a>一、插件介绍</h2><p><code>kubectl</code> 命令从 <code>v1.8.0</code> 版本开始引入了 alpha feature 的插件机制；在此机制下我们可以对 <code>kubectl</code> 命令进行扩展，从而编写一些自己的插件集成进 <code>kubectl</code> 命令中；<strong><code>kubectl</code> 插件机制是与语言无关的，也就是说你可以用任何语言编写插件，可以是 <code>bash</code>、<code>python</code> 脚本，也可以是 <code>go</code>、<code>java</code> 等编译型语言；所以选择你熟悉的语言即可</strong>，以下是一个用 <code>go</code> 编写的用于快速切换 NameSpace 的小插件，运行截图如下:</p><p><strong>所谓: 开局一张图，功能全靠编 😂</strong><br><img src="https://cdn.oss.link/markdown/6t89g.gif" srcset="/img/loading.gif" alt="swns.gif"></p><p>当前插件代码放在 <a href="https://github.com/mritd/swns" target="_blank" rel="noopener">mritd/swns</a> 这个项目下面</p><h2 id="二、插件加载"><a href="#二、插件加载" class="headerlink" title="二、插件加载"></a>二、插件加载</h2><p><code>kubectl</code> 插件机制目前并不提供包管理器一样的功能，比如你想执行 <code>kuebctl plugin install xxx</code> 这种操作目前还没有实现(个人感觉差个规范)；所以一旦我们编写或者下载一个插件后，我们只有正确放在特定目录才会生效；</p><p><strong>目前插件根据文档描述只有两部分内容: <code>plugin.yaml</code> 和其依赖的二进制/脚本等可执行文件</strong>；根据文档说明，<code>kubectl</code> 会尝试在如下位置查找并加载插件，所以我们只需要将 <code>plugin.yaml</code> 和相关二进制放在在对应位置即可:</p><ul><li><code>${KUBECTL_PLUGINS_PATH}</code>: 如果这个环境变量定义了，那么 <code>kubectl</code> <strong>只会</strong>从这里查找；<strong>注意: 这个变量可以是多个目录，类似 PATH 变量一样，做好分割即可</strong></li><li><code>${XDG_DATA_DIRS}/kubectl/plugins</code>: 关于这个变量具体请看 <a href="https://specifications.freedesktop.org/basedir-spec/basedir-spec-latest.html" target="_blank" rel="noopener">XDG System Directory Structure</a>，我了解也不多；<strong>如果这个变量没定义则默认为 <code>/usr/local/share:/usr/share</code></strong></li><li><code>~/.kube/plugins</code>: 这个没啥可说的，我推荐还是将插件放在这个位置比较友好一点</li></ul><p>所以最终插件目录结构类似这样:</p><div class="hljs"><pre><code class="hljs sh">➜  ~ tree .kube.kube├── config└── plugins    └── swns        ├── plugin.yaml        └── swns</code></pre></div><h2 id="三、Plugin-yaml"><a href="#三、Plugin-yaml" class="headerlink" title="三、Plugin.yaml"></a>三、Plugin.yaml</h2><p><code>plugin.yaml</code> 这个文件实际上才是插件的核心，在这个文件里声明了插件如何使用、调用的二进制/脚本等重要配置；<strong>一个插件可以没有任何脚本/二进制可执行文件，但至少应当有一个 <code>plugin.yaml</code> 描述文件</strong>；目前 <code>plugin.yaml</code> 的结构如下:</p><div class="hljs"><pre><code class="hljs yaml"><span class="hljs-attr">name:</span> <span class="hljs-string">"targaryen"</span>                 <span class="hljs-comment"># 必填项: 用于 kuebctl 调用的插件名称</span><span class="hljs-attr">shortDesc:</span> <span class="hljs-string">"Dragonized plugin"</span>    <span class="hljs-comment"># 必填项: 用于 help 该插件时的简短描述</span><span class="hljs-attr">longDesc:</span> <span class="hljs-string">""</span>                      <span class="hljs-comment"># 非必填: 插件的长描述</span><span class="hljs-attr">example:</span> <span class="hljs-string">""</span>                       <span class="hljs-comment"># 非必填: 插件的使用样例</span><span class="hljs-attr">command:</span> <span class="hljs-string">"./dracarys"</span>             <span class="hljs-comment"># 必填项: 插件实际执行的文件位置，可以相对路径 or 绝对路径，或者在 PATH 里也行</span><span class="hljs-attr">flags:</span>                            <span class="hljs-comment"># 非必填: 插件支持的 flag</span>  <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">"heat"</span>                  <span class="hljs-comment"># 必填项: 如果你写了支持的 flag，那么此项必填</span>    <span class="hljs-attr">shorthand:</span> <span class="hljs-string">"h"</span>                <span class="hljs-comment"># 非必填: 该选项的缩短形式</span>    <span class="hljs-attr">desc:</span> <span class="hljs-string">"Fire heat"</span>             <span class="hljs-comment"># 必填项: 同样每个 flag 都必须书写描述</span>    <span class="hljs-attr">defValue:</span> <span class="hljs-string">"extreme"</span>           <span class="hljs-comment"># 非必填: 默认值</span><span class="hljs-attr">tree:</span>                             <span class="hljs-comment"># 允许定义一些子命令</span>  <span class="hljs-bullet">-</span> <span class="hljs-string">...</span>                           <span class="hljs-comment"># 子命令支持同样的设置属性(我想知道子命令的子命令的子命令支不支持...我还没去试过)</span></code></pre></div><h2 id="四、插件环境变量"><a href="#四、插件环境变量" class="headerlink" title="四、插件环境变量"></a>四、插件环境变量</h2><p>在编写插件时，<strong>有时插件运行时需要获取到一些参数，比如 <code>kubectl</code> 执行时的全局 flag 等，</strong>为了方便插件开发者，<code>kuebctl</code> 的插件机制提供一些预置的环境变量方便我们读取；即如果你用 <code>bash</code> 写插件，那么这些变量你只需要 <code>${xxxx}</code> 即可拿到，然后做一些你想做的事情；这些变量目前支持如下:</p><ul><li><code>KUBECTL_PLUGINS_CALLER</code>: <code>kubectl</code> 二进制文件所在位置；<strong>作为插件编写者，我们无需关系 api server 是否能联通，因为配置是否正确应当由使用者决定；在需要时我们只需要直接调用 <code>kubectl</code> 即可；</strong>比如在 <code>bash</code> 脚本中执行 <code>get pod</code> 等</li><li><code>KUBECTL_PLUGINS_CURRENT_NAMESPACE</code>: 当前 <code>kuebctl</code> 命令所对应的 NameSpace，<strong>插件机制确保了该值一定正确；即这是经过解析了 <code>--namespace</code> 选项或者 <code>kubeconfig</code> 配置后的最终结果；作为插件编写者，我们无需关心处理过程</strong>；想详细了解的的可以去看源码，以及 <code>Cobra</code> 库(Kubernetes 用这个库解析命令行参数和配置)</li><li><code>KUBECTL_PLUGINS_DESCRIPTOR_*</code>: 插件自己本身位于 <code>plugin.yaml</code> 中的描述信息，比如 <code>KUBECTL_PLUGINS_DESCRIPTOR_NAME</code> 输出 <code>plugin.yaml</code> 下的 <code>name</code> 属性；一般可以用作插件输出自己的帮助文档等</li><li><code>KUBECTL_PLUGINS_GLOBAL_FLAG_*</code>: 获取 <code>kubectl</code> 所有全局 flag 值的变量，比如 <code>KUBECTL_PLUGINS_GLOBAL_FLAG_NAMESPACE</code> 能拿到 <code>--namespace</code> 选项的值</li><li><code>KUBECTL_PLUGINS_LOCAL_FLAG_*</code>: 同上面类似，只不过这个是获取插件自己本身 flag 的值，个人认为在脚本语言中，比如 <code>bash</code> 等处理选项不怎么好用时，可以考虑直接从变量拿</li></ul><p>以上变量我并未都测试，具体以测试为准，<strong>删库跑路等情况本人概不负责</strong></p><h2 id="五、写一个切换-NameSpace-的插件"><a href="#五、写一个切换-NameSpace-的插件" class="headerlink" title="五、写一个切换 NameSpace 的插件"></a>五、写一个切换 NameSpace 的插件</h2><p>前面墨迹一大堆只是为了描述清楚 <strong>要写一个插件应该怎么干</strong> 的问题，下面开始 <strong>这么干</strong></p><h3 id="5-1、编写配置"><a href="#5-1、编写配置" class="headerlink" title="5.1、编写配置"></a>5.1、编写配置</h3><p>上面已经介绍好了 <code>plugin.yaml</code> 怎么写，那么根据我自己的需求，我写的这个切换 NameSpace 插件的名字暂且叫做 <code>swns</code>；我希望 <code>swns</code> 执行后接受一个 NameSpace 的字符串，然后调用 <code>kuebctl config</code> 去设置当前默认的 NameSpace，这样在后续命令中我就不用再一直加个 <code>-n xxx</code> 参数了；同时我希望使用更方便点，当执行 <code>swns</code> 命令时，如果不提供 NameSpace 的字符串，那我就弹出下拉列表供用户选择；综上需求自己想明白后，就写一个 <code>plugin.yaml</code>，如下:</p><div class="hljs"><pre><code class="hljs yaml"><span class="hljs-attr">name:</span> <span class="hljs-string">"swns"</span><span class="hljs-attr">shortDesc:</span> <span class="hljs-string">"Switch NameSpace"</span><span class="hljs-attr">longDesc:</span> <span class="hljs-string">"Switch Kubernetes current context namespace."</span><span class="hljs-attr">example:</span> <span class="hljs-string">"kubectl plugin swns [NAMESPACE]"</span><span class="hljs-attr">command:</span> <span class="hljs-string">"./swns"</span></code></pre></div><h3 id="5-2、编写插件"><a href="#5-2、编写插件" class="headerlink" title="5.2、编写插件"></a>5.2、编写插件</h3><p>上面 <code>plugin.yaml</code> 已经定义好了，那么接下来就简单了，撸代码实现了就好；代码如下:</p><div class="hljs"><pre><code class="hljs go"><span class="hljs-comment">// 注意: 下面的模板语法大括号中间没有空格，此处空格是为了防止博客渲染出错</span><span class="hljs-keyword">package</span> main<span class="hljs-keyword">import</span> (<span class="hljs-string">"fmt"</span><span class="hljs-string">"os"</span><span class="hljs-string">"os/exec"</span><span class="hljs-string">"strings"</span><span class="hljs-string">"github.com/mritd/promptx"</span>)<span class="hljs-function"><span class="hljs-keyword">func</span> <span class="hljs-title">main</span><span class="hljs-params">()</span></span> &#123;<span class="hljs-comment">// 先拿到当前的 context</span>cmd := exec.Command(<span class="hljs-string">"kubectl"</span>, <span class="hljs-string">"config"</span>, <span class="hljs-string">"current-context"</span>)cmd.Stdin = os.Stdincmd.Stderr = os.Stderrb, err := cmd.Output()checkAndExit(err)currentContext := strings.TrimSpace(<span class="hljs-keyword">string</span>(b))<span class="hljs-comment">// 如果提供了 NameSpace 字符串，我直接改就行了</span><span class="hljs-keyword">if</span> <span class="hljs-built_in">len</span>(os.Args) &gt; <span class="hljs-number">1</span> &#123;cmd = exec.Command(<span class="hljs-string">"kubectl"</span>, <span class="hljs-string">"config"</span>, <span class="hljs-string">"set-context"</span>, currentContext, <span class="hljs-string">"--namespace="</span>+os.Args[<span class="hljs-number">1</span>])cmd.Stdout = os.StdoutcheckAndExit(cmd.Run())fmt.Printf(<span class="hljs-string">"Kubernetes namespace switch to %s.\n"</span>, os.Args[<span class="hljs-number">1</span>])&#125; <span class="hljs-keyword">else</span> &#123;<span class="hljs-comment">// 没提供我就得先把所有的 NameSpace 弄出来</span>cmd = exec.Command(<span class="hljs-string">"kubectl"</span>, <span class="hljs-string">"get"</span>, <span class="hljs-string">"ns"</span>, <span class="hljs-string">"-o"</span>, <span class="hljs-string">"template"</span>, <span class="hljs-string">"--template"</span>, <span class="hljs-string">"&#123; &#123; range .items &#125; &#125;&#123; &#123; .metadata.name &#125; &#125; &#123; &#123; end &#125; &#125;"</span>)b, err = cmd.Output()checkAndExit(err)allNameSpace := strings.Fields(<span class="hljs-keyword">string</span>(b))<span class="hljs-comment">// 弄到所有的 NameSpace 后，我在弄一个下拉列表(这是我自己造的一个下拉列表库)</span>cfg := &amp;promptx.SelectConfig&#123;ActiveTpl:    <span class="hljs-string">"»  &#123; &#123; . | cyan &#125; &#125;"</span>,InactiveTpl:  <span class="hljs-string">"  &#123; &#123; . | white &#125; &#125;"</span>,SelectPrompt: <span class="hljs-string">"NameSpace"</span>,SelectedTpl:  <span class="hljs-string">"&#123; &#123; \"» \" | green &#125; &#125;&#123; &#123;\"NameSpace:\" | cyan &#125; &#125; &#123; &#123; . &#125; &#125;"</span>,DisPlaySize:  <span class="hljs-number">9</span>,DetailsTpl:   <span class="hljs-string">` `</span>,&#125;s := &amp;promptx.Select&#123;Items:  allNameSpace,Config: cfg,&#125;<span class="hljs-comment">// 用户选中一个 NameSpace 后我就拿到了想要设置的 NameSpace 字符串</span>selectNameSpace := allNameSpace[s.Run()]<span class="hljs-comment">// 跟上面套路一样，写进去就行了</span>cmd = exec.Command(<span class="hljs-string">"kubectl"</span>, <span class="hljs-string">"config"</span>, <span class="hljs-string">"set-context"</span>, currentContext, <span class="hljs-string">"--namespace="</span>+selectNameSpace)cmd.Stdout = os.StdoutcheckAndExit(cmd.Run())fmt.Printf(<span class="hljs-string">"Kubernetes namespace switch to %s.\n"</span>, selectNameSpace)&#125;&#125;<span class="hljs-function"><span class="hljs-keyword">func</span> <span class="hljs-title">checkErr</span><span class="hljs-params">(err error)</span> <span class="hljs-title">bool</span></span> &#123;<span class="hljs-keyword">if</span> err != <span class="hljs-literal">nil</span> &#123;fmt.Println(err)<span class="hljs-keyword">return</span> <span class="hljs-literal">false</span>&#125;<span class="hljs-keyword">return</span> <span class="hljs-literal">true</span>&#125;<span class="hljs-function"><span class="hljs-keyword">func</span> <span class="hljs-title">checkAndExit</span><span class="hljs-params">(err error)</span></span> &#123;<span class="hljs-keyword">if</span> !checkErr(err) &#123;os.Exit(<span class="hljs-number">1</span>)&#125;&#125;</code></pre></div><p>最后编译后放到上面所说的插件加载目录即可</p><p>到此，<strong>“全局一张图，功能全靠编”</strong> 图上面也有了，编的的也差不多 😂</p>]]></content:encoded>
      
      
      <category domain="https://mritd.com/categories/kubernetes/">Kubernetes</category>
      
      
      <category domain="https://mritd.com/tags/kubernetes/">Kubernetes</category>
      
      
      <comments>https://mritd.com/2018/08/09/create-a-plugin-for-kubectl/#disqus_thread</comments>
      
    </item>
    
    <item>
      <title>Traefik 另类的服务暴露方式</title>
      <link>https://mritd.com/2018/05/24/kubernetes-traefik-service-exposure/</link>
      <guid>https://mritd.com/2018/05/24/kubernetes-traefik-service-exposure/</guid>
      <pubDate>Thu, 24 May 2018 15:53:09 GMT</pubDate>
      
      <description>最近准备重新折腾一下 Kubernetes 的服务暴露方式，以前的方式是彻底剥离 Kubenretes 本身的服务发现，然后改动应用实现 应用+Consul+Fabio 的服务暴露方式；总感觉这种方式不算优雅，所以折腾了一下 Traefik，试了下效果还不错，以下记录了使用 Traefik 的新的服务暴露方式(本文仅针对 HTTP 协议)</description>
      
      
      
      <content:encoded><![CDATA[<blockquote><p>最近准备重新折腾一下 Kubernetes 的服务暴露方式，以前的方式是彻底剥离 Kubenretes 本身的服务发现，然后改动应用实现 应用+Consul+Fabio 的服务暴露方式；总感觉这种方式不算优雅，所以折腾了一下 Traefik，试了下效果还不错，以下记录了使用 Traefik 的新的服务暴露方式(本文仅针对 HTTP 协议)；</p></blockquote><h2 id="一、Traefik-服务暴露方案"><a href="#一、Traefik-服务暴露方案" class="headerlink" title="一、Traefik 服务暴露方案"></a>一、Traefik 服务暴露方案</h2><h3 id="1-1、以前的-Consul-Fabio-方案"><a href="#1-1、以前的-Consul-Fabio-方案" class="headerlink" title="1.1、以前的 Consul+Fabio 方案"></a>1.1、以前的 Consul+Fabio 方案</h3><p>以前的服务暴露方案是修改应用代码，使其能对接 Consul，然后 Consul 负责健康监测，检测通过后 Fabio 负责读取，最终上层 Nginx 将流量打到 Fabio 上，Fabio 再将流量路由到健康的 Pod 上；总体架构如下</p><p><img src="https://cdn.oss.link/markdown/hkwp3.png" srcset="/img/loading.gif" alt="consul+fabio"></p><p>这种架构目前有几点不太好的地方，首先是必须应用能成功集成 Consul，需要动应用代码不通用；其次组件过多增加维护成本，尤其是调用链日志不好追踪；这里面需要吐槽下 Consul 和 Fabio，Consul 的集群设计模式要想做到完全的 HA 那么需要在每个 pod 中启动一个 agent，因为只要这个 agent 挂了那么集群认为其上所有注册服务都挂了，这点很恶心人；而 Fabio 的日志目前好像还是不支持合理的输出，好像只能 stdout；目前来看不论是组件复杂度还是维护成本都不怎么友好</p><h3 id="1-2、新的-Traefik-方案"><a href="#1-2、新的-Traefik-方案" class="headerlink" title="1.2、新的 Traefik 方案"></a>1.2、新的 Traefik 方案</h3><p>使用 Traefik 首先想到就是直接怼 Ingress，这个确实方便也简单；但是在集群 kube-proxy 不走 ipvs 的情况下 iptables 性能确实堪忧；虽说 Traefik 会直连 Pod，但是你 Ingress 暴露 80、443 端口在本机没有对应 Ingress Controller 的情况下还是会走一下 iptables；<strong>不论是换 kube-router、kube-proxy 走 ipvs 都不是我想要的，我们需要一种完全远离 Kubernetes Service 的新路子</strong>；在看 Traefik 文档的时候，其中说明了 Traefik 只利用 Kubernetes 的 API 来读取相关后端数据，那么我们就可以以此来使用如下的套路</p><p><img src="https://cdn.oss.link/markdown/tfo2f.png" srcset="/img/loading.gif" alt="traefik"></p><p>这个套路的方案很简单，<strong>将 Traefik 部署在物理机上，让其直连 Kubernets api 以读取 Ingress 配置和 Pod IP 等信息，然后在这几台物理机上部署好 Kubernetes 的网络组件使其能直连 Pod IP</strong>；这种方案能够让流量经过 Traefik 直接路由到后端 Pod，健康检测还是由集群来做；<strong>由于 Traefik 连接 Kubernetes api 需要获取一些数据；所以在集群内还是像往常一样创建 Ingress，只不过此时我们并没有 Ingress Controller；这样避免了经过 iptables 转发，不占用全部集群机器的 80、443 端口，同时还能做到高可控</strong></p><h2 id="二、Traefik-部署"><a href="#二、Traefik-部署" class="headerlink" title="二、Traefik 部署"></a>二、Traefik 部署</h2><p>部署之前首先需要有一个正常访问的集群，然后在另外几台机器上部署 Kubernetes 的网络组件；最终要保证另外几台机器能够直接连通 Pod 的 IP，我这里偷懒直接在 Kubernetes 的其中几个 Node 上部署 Traefik</p><h3 id="2-1、Docker-Compose"><a href="#2-1、Docker-Compose" class="headerlink" title="2.1、Docker Compose"></a>2.1、Docker Compose</h3><p>Traefik 的 Docker Compose 如下</p><div class="hljs"><pre><code class="hljs yml"><span class="hljs-attr">version:</span> <span class="hljs-string">'3.5'</span><span class="hljs-attr">services:</span>  <span class="hljs-attr">traefik:</span>    <span class="hljs-attr">image:</span> <span class="hljs-string">traefik:v1.6.1-alpine</span>    <span class="hljs-attr">container_name:</span> <span class="hljs-string">traefik</span>    <span class="hljs-attr">command:</span> <span class="hljs-string">--configFile=/etc/traefik.toml</span>    <span class="hljs-attr">ports:</span>      <span class="hljs-bullet">-</span> <span class="hljs-string">"2080:2080"</span>      <span class="hljs-bullet">-</span> <span class="hljs-string">"2180:2180"</span>    <span class="hljs-attr">volumes:</span>      <span class="hljs-bullet">-</span> <span class="hljs-string">./traefik.toml:/etc/traefik.toml</span>      <span class="hljs-bullet">-</span> <span class="hljs-string">./k8s-root-ca.pem:/etc/kubernetes/ssl/k8s-root-ca.pem</span>      <span class="hljs-bullet">-</span> <span class="hljs-string">./log:/var/log/traefik</span></code></pre></div><p>由于 Kubernetes 集群开启了 RBAC 认证同时采用 TLS 通讯，所以需要挂载 Kubernetes CA 证书，还需要为 Traefik 创建对应的 RBAC 账户以使其能够访问 Kubernetes API</p><h3 id="2-2、创建-RBAC-账户"><a href="#2-2、创建-RBAC-账户" class="headerlink" title="2.2、创建 RBAC 账户"></a>2.2、创建 RBAC 账户</h3><p>Traefik 连接 Kubernetes API 时需要使用 Service Account 的 Token，Service Account 以及 ClusterRole 等配置具体见 <a href="https://docs.traefik.io/user-guide/kubernetes/" target="_blank" rel="noopener">官方文档</a>，下面是我从当前版本的文档中 Copy 出来的</p><div class="hljs"><pre><code class="hljs yml"><span class="hljs-meta">---</span><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">v1</span><span class="hljs-attr">kind:</span> <span class="hljs-string">ServiceAccount</span><span class="hljs-attr">metadata:</span>  <span class="hljs-attr">name:</span> <span class="hljs-string">traefik-ingress-controller</span>  <span class="hljs-attr">namespace:</span> <span class="hljs-string">kube-system</span><span class="hljs-meta">---</span><span class="hljs-attr">kind:</span> <span class="hljs-string">ClusterRole</span><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">rbac.authorization.k8s.io/v1beta1</span><span class="hljs-attr">metadata:</span>  <span class="hljs-attr">name:</span> <span class="hljs-string">traefik-ingress-controller</span><span class="hljs-attr">rules:</span>  <span class="hljs-bullet">-</span> <span class="hljs-attr">apiGroups:</span>      <span class="hljs-bullet">-</span> <span class="hljs-string">""</span>    <span class="hljs-attr">resources:</span>      <span class="hljs-bullet">-</span> <span class="hljs-string">services</span>      <span class="hljs-bullet">-</span> <span class="hljs-string">endpoints</span>      <span class="hljs-bullet">-</span> <span class="hljs-string">secrets</span>    <span class="hljs-attr">verbs:</span>      <span class="hljs-bullet">-</span> <span class="hljs-string">get</span>      <span class="hljs-bullet">-</span> <span class="hljs-string">list</span>      <span class="hljs-bullet">-</span> <span class="hljs-string">watch</span>  <span class="hljs-bullet">-</span> <span class="hljs-attr">apiGroups:</span>      <span class="hljs-bullet">-</span> <span class="hljs-string">extensions</span>    <span class="hljs-attr">resources:</span>      <span class="hljs-bullet">-</span> <span class="hljs-string">ingresses</span>    <span class="hljs-attr">verbs:</span>      <span class="hljs-bullet">-</span> <span class="hljs-string">get</span>      <span class="hljs-bullet">-</span> <span class="hljs-string">list</span>      <span class="hljs-bullet">-</span> <span class="hljs-string">watch</span><span class="hljs-meta">---</span><span class="hljs-attr">kind:</span> <span class="hljs-string">ClusterRoleBinding</span><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">rbac.authorization.k8s.io/v1beta1</span><span class="hljs-attr">metadata:</span>  <span class="hljs-attr">name:</span> <span class="hljs-string">traefik-ingress-controller</span><span class="hljs-attr">roleRef:</span>  <span class="hljs-attr">apiGroup:</span> <span class="hljs-string">rbac.authorization.k8s.io</span>  <span class="hljs-attr">kind:</span> <span class="hljs-string">ClusterRole</span>  <span class="hljs-attr">name:</span> <span class="hljs-string">traefik-ingress-controller</span><span class="hljs-attr">subjects:</span><span class="hljs-bullet">-</span> <span class="hljs-attr">kind:</span> <span class="hljs-string">ServiceAccount</span>  <span class="hljs-attr">name:</span> <span class="hljs-string">traefik-ingress-controller</span>  <span class="hljs-attr">namespace:</span> <span class="hljs-string">kube-system</span></code></pre></div><p>创建好以后需要提取 Service Account 的 Token 方便下面使用，提取命令如下</p><div class="hljs"><pre><code class="hljs sh">kubectl describe secret -n kube-system $(kubectl get secrets -n kube-system | grep traefik-ingress-controller | cut -f1 -d <span class="hljs-string">' '</span>) | grep -E <span class="hljs-string">'^token'</span></code></pre></div><h3 id="2-3、创建-Traefik-配置"><a href="#2-3、创建-Traefik-配置" class="headerlink" title="2.3、创建 Traefik 配置"></a>2.3、创建 Traefik 配置</h3><p>Traefik 的具体配置细节请参考 <a href="https://docs.traefik.io/configuration/commons/" target="_blank" rel="noopener">官方文档</a>，以下仅给出一个样例配置</p><div class="hljs"><pre><code class="hljs toml"><span class="hljs-comment"># DEPRECATED - for general usage instruction see [lifeCycle.graceTimeOut].</span><span class="hljs-comment">#</span><span class="hljs-comment"># If both the deprecated option and the new one are given, the deprecated one</span><span class="hljs-comment"># takes precedence.</span><span class="hljs-comment"># A value of zero is equivalent to omitting the parameter, causing</span><span class="hljs-comment"># [lifeCycle.graceTimeOut] to be effective. Pass zero to the new option in</span><span class="hljs-comment"># order to disable the grace period.</span><span class="hljs-comment">#</span><span class="hljs-comment"># Optional</span><span class="hljs-comment"># Default: "0s"</span><span class="hljs-comment">#</span><span class="hljs-comment"># graceTimeOut = "10s"</span><span class="hljs-comment"># Enable debug mode.</span><span class="hljs-comment"># This will install HTTP handlers to expose Go expvars under /debug/vars and</span><span class="hljs-comment"># pprof profiling data under /debug/pprof.</span><span class="hljs-comment"># The log level will be set to DEBUG unless `logLevel` is specified.</span><span class="hljs-comment">#</span><span class="hljs-comment"># Optional</span><span class="hljs-comment"># Default: false</span><span class="hljs-comment">#</span><span class="hljs-comment"># debug = true</span><span class="hljs-comment"># Periodically check if a new version has been released.</span><span class="hljs-comment">#</span><span class="hljs-comment"># Optional</span><span class="hljs-comment"># Default: true</span><span class="hljs-comment">#</span><span class="hljs-attr">checkNewVersion</span> = <span class="hljs-literal">false</span><span class="hljs-comment"># Backends throttle duration.</span><span class="hljs-comment">#</span><span class="hljs-comment"># Optional</span><span class="hljs-comment"># Default: "2s"</span><span class="hljs-comment">#</span><span class="hljs-comment"># providersThrottleDuration = "2s"</span><span class="hljs-comment"># Controls the maximum idle (keep-alive) connections to keep per-host.</span><span class="hljs-comment">#</span><span class="hljs-comment"># Optional</span><span class="hljs-comment"># Default: 200</span><span class="hljs-comment">#</span><span class="hljs-comment"># maxIdleConnsPerHost = 200</span><span class="hljs-comment"># If set to true invalid SSL certificates are accepted for backends.</span><span class="hljs-comment"># This disables detection of man-in-the-middle attacks so should only be used on secure backend networks.</span><span class="hljs-comment">#</span><span class="hljs-comment"># Optional</span><span class="hljs-comment"># Default: false</span><span class="hljs-comment">#</span><span class="hljs-comment"># insecureSkipVerify = true</span><span class="hljs-comment"># Register Certificates in the rootCA.</span><span class="hljs-comment">#</span><span class="hljs-comment"># Optional</span><span class="hljs-comment"># Default: []</span><span class="hljs-comment">#</span><span class="hljs-comment"># rootCAs = [ "/mycert.cert" ]</span><span class="hljs-comment"># Entrypoints to be used by frontends that do not specify any entrypoint.</span><span class="hljs-comment"># Each frontend can specify its own entrypoints.</span><span class="hljs-comment">#</span><span class="hljs-comment"># Optional</span><span class="hljs-comment"># Default: ["http"]</span><span class="hljs-comment">#</span><span class="hljs-comment"># defaultEntryPoints = ["http", "https"]</span><span class="hljs-comment"># Allow the use of 0 as server weight.</span><span class="hljs-comment"># - false: a weight 0 means internally a weight of 1.</span><span class="hljs-comment"># - true: a weight 0 means internally a weight of 0 (a server with a weight of 0 is removed from the available servers).</span><span class="hljs-comment">#</span><span class="hljs-comment"># Optional</span><span class="hljs-comment"># Default: false</span><span class="hljs-comment">#</span><span class="hljs-comment"># AllowMinWeightZero = true</span><span class="hljs-attr">logLevel</span> = <span class="hljs-string">"INFO"</span><span class="hljs-section">[traefikLog]</span>  filePath = "/var/log/traefik/traefik.log"  format   = "json"<span class="hljs-section">[accessLog]</span>  filePath = "/var/log/traefik/access.log"  format = "json"  <span class="hljs-section">[accessLog.filters]</span>    statusCodes = ["200-511"]    retryAttempts = true<span class="hljs-comment">#  [accessLog.fields]</span><span class="hljs-comment">#    defaultMode = "keep"</span><span class="hljs-comment">#    [accessLog.fields.names]</span><span class="hljs-comment">#      "ClientUsername" = "drop"</span><span class="hljs-comment">#      # ...</span><span class="hljs-comment">#</span><span class="hljs-comment">#    [accessLog.fields.headers]</span><span class="hljs-comment">#      defaultMode = "keep"</span><span class="hljs-comment">#      [accessLog.fields.headers.names]</span><span class="hljs-comment">#        "User-Agent" = "redact"</span><span class="hljs-comment">#        "Authorization" = "drop"</span><span class="hljs-comment">#        "Content-Type" = "keep"</span><span class="hljs-comment">#        # ...</span><span class="hljs-section">[entryPoints]</span>  <span class="hljs-section">[entryPoints.http]</span>    address = ":2080"    compress = true  <span class="hljs-section">[entryPoints.traefik]</span>    address = ":2180"    compress = true<span class="hljs-comment">#    [entryPoints.http.whitelist]</span><span class="hljs-comment">#      sourceRange = ["192.168.1.0/24"]</span><span class="hljs-comment">#      useXForwardedFor = true</span><span class="hljs-comment">#    [entryPoints.http.tls]</span><span class="hljs-comment">#      minVersion = "VersionTLS12"</span><span class="hljs-comment">#      cipherSuites = [</span><span class="hljs-comment">#        "TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256",</span><span class="hljs-comment">#        "TLS_RSA_WITH_AES_256_GCM_SHA384"</span><span class="hljs-comment">#       ]</span><span class="hljs-comment">#      [[entryPoints.http.tls.certificates]]</span><span class="hljs-comment">#        certFile = "path/to/my.cert"</span><span class="hljs-comment">#        keyFile = "path/to/my.key"</span><span class="hljs-comment">#      [[entryPoints.http.tls.certificates]]</span><span class="hljs-comment">#        certFile = "path/to/other.cert"</span><span class="hljs-comment">#        keyFile = "path/to/other.key"</span><span class="hljs-comment">#      # ...</span><span class="hljs-comment">#      [entryPoints.http.tls.clientCA]</span><span class="hljs-comment">#        files = ["path/to/ca1.crt", "path/to/ca2.crt"]</span><span class="hljs-comment">#        optional = false</span><span class="hljs-comment">#</span><span class="hljs-comment">#    [entryPoints.http.redirect]</span><span class="hljs-comment">#      entryPoint = "https"</span><span class="hljs-comment">#      regex = "^http://localhost/(.*)"</span><span class="hljs-comment">#      replacement = "http://mydomain/$1"</span><span class="hljs-comment">#      permanent = true</span><span class="hljs-comment">#</span><span class="hljs-comment">#    [entryPoints.http.auth]</span><span class="hljs-comment">#      headerField = "X-WebAuth-User"</span><span class="hljs-comment">#      [entryPoints.http.auth.basic]</span><span class="hljs-comment">#        users = [</span><span class="hljs-comment">#          "test:$apr1$H6uskkkW$IgXLP6ewTrSuBkTrqE8wj/",</span><span class="hljs-comment">#          "test2:$apr1$d9hr9HBB$4HxwgUir3HP4EsggP/QNo0",</span><span class="hljs-comment">#        ]</span><span class="hljs-comment">#        usersFile = "/path/to/.htpasswd"</span><span class="hljs-comment">#      [entryPoints.http.auth.digest]</span><span class="hljs-comment">#        users = [</span><span class="hljs-comment">#          "test:traefik:a2688e031edb4be6a3797f3882655c05",</span><span class="hljs-comment">#          "test2:traefik:518845800f9e2bfb1f1f740ec24f074e",</span><span class="hljs-comment">#        ]</span><span class="hljs-comment">#        usersFile = "/path/to/.htdigest"</span><span class="hljs-comment">#      [entryPoints.http.auth.forward]</span><span class="hljs-comment">#        address = "https://authserver.com/auth"</span><span class="hljs-comment">#        trustForwardHeader = true</span><span class="hljs-comment">#        [entryPoints.http.auth.forward.tls]</span><span class="hljs-comment">#          ca =  [ "path/to/local.crt"]</span><span class="hljs-comment">#          caOptional = true</span><span class="hljs-comment">#          cert = "path/to/foo.cert"</span><span class="hljs-comment">#          key = "path/to/foo.key"</span><span class="hljs-comment">#          insecureSkipVerify = true</span><span class="hljs-comment">#</span><span class="hljs-comment">#    [entryPoints.http.proxyProtocol]</span><span class="hljs-comment">#      insecure = true</span><span class="hljs-comment">#      trustedIPs = ["10.10.10.1", "10.10.10.2"]</span><span class="hljs-comment">#</span><span class="hljs-comment">#    [entryPoints.http.forwardedHeaders]</span><span class="hljs-comment">#      trustedIPs = ["10.10.10.1", "10.10.10.2"]</span><span class="hljs-comment">#</span><span class="hljs-comment">#  [entryPoints.https]</span><span class="hljs-comment">#    # ...</span><span class="hljs-comment">################################################################</span><span class="hljs-comment"># Kubernetes Ingress configuration backend</span><span class="hljs-comment">################################################################</span><span class="hljs-comment"># Enable Kubernetes Ingress configuration backend.</span><span class="hljs-section">[kubernetes]</span><span class="hljs-comment"># Kubernetes server endpoint.</span><span class="hljs-comment">#</span><span class="hljs-comment"># Optional for in-cluster configuration, required otherwise.</span><span class="hljs-comment"># Default: empty</span><span class="hljs-comment">#</span><span class="hljs-attr">endpoint</span> = <span class="hljs-string">"https://172.16.0.36:6443"</span><span class="hljs-comment"># Bearer token used for the Kubernetes client configuration.</span><span class="hljs-comment">#</span><span class="hljs-comment"># Optional</span><span class="hljs-comment"># Default: empty</span><span class="hljs-comment">#</span><span class="hljs-attr">token</span> = <span class="hljs-string">"eyJhbGciOiJSUzI1NiIsImtpZCI6IiJ9.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJrdWJlLXN5c3RlbSIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VjcmV0Lm5hbWUiOiJ0cmFlZmlrLWluZ3Jlc3MtY29udHJvbGxlci10b2tlbi1zbm5iNSIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VydmljZS1hY2NvdW50Lm5hbWUiOiJ0cmFlZmlrLWlyZ3Jlc3MtY29udHJvbGxlciIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VydmljZS1hY2NvdW50LnVpZCI6ImE4NmI3YWEzLTVmNjQtMTFlOC1hZjYxLWM4MWY2NmUwMzRhNyIsInN1YiI6InN5c3RlbTpxZXJ2aWNlYWNjb3VudDPrdWJlLXN5c3RlbTp0cmFlZmlrLWluZ3Jlc3MtY29udHJvbGxlciJ9.vOFEITuANWGnkER8gukWkTs54BmHXqNpzM55bOb5qXPmI3pZsbei3gtE6tZoqME9P5Lb85cav-8mGZJcoQqqxNBkZJ1YRqy_1O9Apkxa4jA68ipe_NB3L5-exH5cEIrU8iql_r7ycDaKwzsMnAWGPolp1dRkF31u5u8g68oLwF3GR8Z5g4_tLJlTvA53doX7k6Wd6vUygTS3EaQ_qvfXwbcIeaSdWWo2Mym6O0CvIap4jH2w21MbredGURqkRlXEPezKAgRVkr75CdvuvwORnT8YxFLVwuAJs70V-13Ib9v6HAK64GmzcqkAuJtZT8NZKl8Y4TfRGl2_RMq2tk86gD4ShDMedcrto44ZUYHQccsSlpaW5PsN2KBBNPN0-6ca3jIpOmnJojAFUYGM42Wymnx9_4XwHUeeA18-RrercmOaRMdlNq8BzBomAxQB99TqUzRIqpe6m5OotXvouCUnE7qjMwRWmQ5LHjqUGEw_A1pHcalFXQZK0sOCaJOJZIJbc_8rVX-4uxkCBxoIXmzjq8x5a_xPsN4L0aWifkP6co--agw3kOT0O6my8T_CbcZGO9e3OqYPdT4FSl92XlXW8EXHdDpCUJ10aoqJGG2vZSud7IoDxkcScpkj3n6TvyvSRVtk3CtYiIYBpgi7-X2JKkun1a7yFpLogyazz9VlUE4"</span><span class="hljs-comment"># Path to the certificate authority file.</span><span class="hljs-comment"># Used for the Kubernetes client configuration.</span><span class="hljs-comment">#</span><span class="hljs-comment"># Optional</span><span class="hljs-comment"># Default: empty</span><span class="hljs-comment">#</span><span class="hljs-attr">certAuthFilePath</span> = <span class="hljs-string">"/etc/kubernetes/ssl/k8s-root-ca.pem"</span><span class="hljs-comment"># Array of namespaces to watch.</span><span class="hljs-comment">#</span><span class="hljs-comment"># Optional</span><span class="hljs-comment"># Default: all namespaces (empty array).</span><span class="hljs-comment">#</span><span class="hljs-attr">namespaces</span> = [<span class="hljs-string">"default"</span>]<span class="hljs-comment"># Ingress label selector to filter Ingress objects that should be processed.</span><span class="hljs-comment">#</span><span class="hljs-comment"># Optional</span><span class="hljs-comment"># Default: empty (process all Ingresses)</span><span class="hljs-comment">#</span><span class="hljs-comment"># labelselector = "A and not B"</span><span class="hljs-comment"># Value of `kubernetes.io/ingress.class` annotation that identifies Ingress objects to be processed.</span><span class="hljs-comment"># If the parameter is non-empty, only Ingresses containing an annotation with the same value are processed.</span><span class="hljs-comment"># Otherwise, Ingresses missing the annotation, having an empty value, or the value `traefik` are processed.</span><span class="hljs-comment">#</span><span class="hljs-comment"># Note : `ingressClass` option must begin with the "traefik" prefix.</span><span class="hljs-comment">#</span><span class="hljs-comment"># Optional</span><span class="hljs-comment"># Default: empty</span><span class="hljs-comment">#</span><span class="hljs-comment"># ingressClass = "traefik-internal"</span><span class="hljs-comment"># Disable PassHost Headers.</span><span class="hljs-comment">#</span><span class="hljs-comment"># Optional</span><span class="hljs-comment"># Default: false</span><span class="hljs-comment">#</span><span class="hljs-comment"># disablePassHostHeaders = true</span><span class="hljs-comment"># Enable PassTLSCert Headers.</span><span class="hljs-comment">#</span><span class="hljs-comment"># Optional</span><span class="hljs-comment"># Default: false</span><span class="hljs-comment">#</span><span class="hljs-comment"># enablePassTLSCert = true</span><span class="hljs-comment"># Override default configuration template.</span><span class="hljs-comment">#</span><span class="hljs-comment"># Optional</span><span class="hljs-comment"># Default: &lt;built-in template&gt;</span><span class="hljs-comment">#</span><span class="hljs-comment"># filename = "kubernetes.tmpl"</span><span class="hljs-comment"># API definition</span><span class="hljs-section">[api]</span>  <span class="hljs-comment"># Name of the related entry point</span>  <span class="hljs-comment">#</span>  <span class="hljs-comment"># Optional</span>  <span class="hljs-comment"># Default: "traefik"</span>  <span class="hljs-comment">#</span>  entryPoint = "traefik"  <span class="hljs-comment"># Enabled Dashboard</span>  <span class="hljs-comment">#</span>  <span class="hljs-comment"># Optional</span>  <span class="hljs-comment"># Default: true</span>  <span class="hljs-comment">#</span>  dashboard = true  <span class="hljs-comment"># Enable debug mode.</span>  <span class="hljs-comment"># This will install HTTP handlers to expose Go expvars under /debug/vars and</span>  <span class="hljs-comment"># pprof profiling data under /debug/pprof.</span>  <span class="hljs-comment"># Additionally, the log level will be set to DEBUG.</span>  <span class="hljs-comment">#</span>  <span class="hljs-comment"># Optional</span>  <span class="hljs-comment"># Default: false</span>  <span class="hljs-comment">#</span>  debug = false<span class="hljs-comment"># Ping definition</span><span class="hljs-comment">#[ping]</span><span class="hljs-comment">#  # Name of the related entry point</span><span class="hljs-comment">#  #</span><span class="hljs-comment">#  # Optional</span><span class="hljs-comment">#  # Default: "traefik"</span><span class="hljs-comment">#  #</span><span class="hljs-comment">#  entryPoint = "traefik"</span></code></pre></div><h3 id="2-4、启动-Traefik"><a href="#2-4、启动-Traefik" class="headerlink" title="2.4、启动 Traefik"></a>2.4、启动 Traefik</h3><p>所有文件准备好以后直接执行 <code>docker-compose up -d</code> 启动即可，所有文件目录结构如下</p><div class="hljs"><pre><code class="hljs sh">traefik├── docker-compose.yaml├── k8s-root-ca.pem├── <span class="hljs-built_in">log</span>│   ├── access.log│   └── traefik.log├── rbac.yaml└── traefik.toml</code></pre></div><p>启动成功后可以访问 <code>http://IP:2180</code> 查看 Traefik 的控制面板</p><p><img src="https://cdn.oss.link/markdown/phrps.png" srcset="/img/loading.gif" alt="dashboard"></p><h2 id="三、增加-Ingress-配置并测试"><a href="#三、增加-Ingress-配置并测试" class="headerlink" title="三、增加 Ingress 配置并测试"></a>三、增加 Ingress 配置并测试</h2><h3 id="3-1、增加-Ingress-配置"><a href="#3-1、增加-Ingress-配置" class="headerlink" title="3.1、增加 Ingress 配置"></a>3.1、增加 Ingress 配置</h3><p>虽然这种部署方式脱离了 Kubernetes 的 Service 与 Ingress 负载，但是 Traefik 还是需要通过 Kubernetes 的 Ingress 配置来确定后端负载规则，所以 Ingress 对象我们仍需照常创建；以下为一个 Demo 项目的 deployment、service、ingress 配置示例</p><ul><li>demo.deploy.yaml</li></ul><div class="hljs"><pre><code class="hljs yaml"><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">apps/v1</span><span class="hljs-attr">kind:</span> <span class="hljs-string">Deployment</span><span class="hljs-attr">metadata:</span>  <span class="hljs-attr">name:</span> <span class="hljs-string">demo</span><span class="hljs-attr">spec:</span>  <span class="hljs-attr">replicas:</span> <span class="hljs-number">5</span>  <span class="hljs-attr">selector:</span>    <span class="hljs-attr">matchLabels:</span>      <span class="hljs-attr">app:</span> <span class="hljs-string">demo</span>  <span class="hljs-attr">template:</span>    <span class="hljs-attr">metadata:</span>      <span class="hljs-attr">labels:</span>        <span class="hljs-attr">app:</span> <span class="hljs-string">demo</span>    <span class="hljs-attr">spec:</span>      <span class="hljs-attr">containers:</span>      <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">demo</span>        <span class="hljs-attr">image:</span> <span class="hljs-string">mritd/demo</span>        <span class="hljs-attr">imagePullPolicy:</span> <span class="hljs-string">IfNotPresent</span>        <span class="hljs-attr">ports:</span>        <span class="hljs-bullet">-</span> <span class="hljs-attr">containerPort:</span> <span class="hljs-number">80</span></code></pre></div><ul><li>demo.svc.yaml</li></ul><div class="hljs"><pre><code class="hljs yaml"><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">v1</span><span class="hljs-attr">kind:</span> <span class="hljs-string">Service</span><span class="hljs-attr">metadata:</span>  <span class="hljs-attr">name:</span> <span class="hljs-string">demo</span>  <span class="hljs-attr">labels:</span>    <span class="hljs-attr">svc:</span> <span class="hljs-string">demo</span><span class="hljs-attr">spec:</span>  <span class="hljs-attr">ports:</span>  <span class="hljs-bullet">-</span> <span class="hljs-attr">port:</span> <span class="hljs-number">8080</span>    <span class="hljs-attr">name:</span> <span class="hljs-string">http</span>    <span class="hljs-attr">targetPort:</span> <span class="hljs-number">80</span>  <span class="hljs-attr">selector:</span>    <span class="hljs-attr">app:</span> <span class="hljs-string">demo</span></code></pre></div><ul><li>demo.ing.yaml</li></ul><div class="hljs"><pre><code class="hljs yaml"><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">extensions/v1beta1</span><span class="hljs-attr">kind:</span> <span class="hljs-string">Ingress</span><span class="hljs-attr">metadata:</span>  <span class="hljs-attr">name:</span> <span class="hljs-string">demo</span>  <span class="hljs-attr">annotations:</span>    <span class="hljs-attr">traefik.ingress.kubernetes.io/preserve-host:</span> <span class="hljs-string">"true"</span><span class="hljs-attr">spec:</span>  <span class="hljs-attr">rules:</span>  <span class="hljs-bullet">-</span> <span class="hljs-attr">host:</span> <span class="hljs-string">demo.mritd.me</span>    <span class="hljs-attr">http:</span>      <span class="hljs-attr">paths:</span>      <span class="hljs-bullet">-</span> <span class="hljs-attr">backend:</span>          <span class="hljs-attr">serviceName:</span> <span class="hljs-string">demo</span>          <span class="hljs-attr">servicePort:</span> <span class="hljs-number">8080</span></code></pre></div><h3 id="3-2、测试访问"><a href="#3-2、测试访问" class="headerlink" title="3.2、测试访问"></a>3.2、测试访问</h3><p>部署好后应当能从 Traefik 的 Dashboard 中看到新增的 demo ingress，如下所示</p><p><img src="https://cdn.oss.link/markdown/zociy.png" srcset="/img/loading.gif" alt="dashboard-demo"></p><p>最后我们使用 curl 测试即可</p><div class="hljs"><pre><code class="hljs sh"><span class="hljs-comment"># 在不使用 Host 头的情况下 Traefik 会返 404(Traefik 根据 Host 路由后端，具体配置参考官方文档)</span>test36.node ➜  ~ curl http://172.16.0.36:2080404 page not found<span class="hljs-comment"># 指定 Host 头来路由到 demo 的相关 Pod</span>test36.node ➜  ~ curl -H <span class="hljs-string">"Host: demo.mritd.me"</span> http://172.16.0.36:2080&lt;!DOCTYPE html&gt;&lt;html lang=<span class="hljs-string">"zh"</span>&gt;&lt;head&gt;    &lt;meta http-equiv=<span class="hljs-string">"Content-Type"</span> content=<span class="hljs-string">"text/html; charset=UTF-8"</span>&gt;    &lt;title&gt;Running!&lt;/title&gt;    &lt;style <span class="hljs-built_in">type</span>=<span class="hljs-string">"text/css"</span>&gt;        body &#123;            width: 100%;            min-height: 100%;            background: linear-gradient(to bottom, <span class="hljs-comment">#fff 0, #b8edff 50%, #83dfff 100%);</span>            background-attachment: fixed;        &#125;    &lt;/style&gt;&lt;/head&gt;&lt;body class=<span class="hljs-string">" hasGoogleVoiceExt"</span>&gt;&lt;div align=<span class="hljs-string">"center"</span>&gt;    &lt;h1&gt;Your container is running!&lt;/h1&gt;    &lt;img src=<span class="hljs-string">"./docker.png"</span> alt=<span class="hljs-string">"docker"</span>&gt;&lt;/div&gt;&lt;/body&gt;&lt;/html&gt;<span class="hljs-comment">#</span></code></pre></div><h2 id="四、其他说明"><a href="#四、其他说明" class="headerlink" title="四、其他说明"></a>四、其他说明</h2><p>写这篇文章的目的是给予一种新的服务暴露思路，这篇文章的某些配置并不适合生产使用；<strong>生产环境尽量使用独立的机器部署 Traefik，同时最好宿主机二进制方式部署；应用的 Deployment 也应当加入健康检测以防止错误的流量路由</strong>；至于 Traefik 的具体细节配置，比如访问日志、Entrypoints 配置、如何连接 Kubernets HA api 等不在本文范畴内，请自行查阅文档；</p><p>最后说一下，关于 Traefik 的 HA 只需要部署多个实例即可，还有 Traefik 本身不做日志滚动等，需要自行处理一下日志。</p>]]></content:encoded>
      
      
      <category domain="https://mritd.com/categories/kubernetes/">Kubernetes</category>
      
      
      <category domain="https://mritd.com/tags/kubernetes/">Kubernetes</category>
      
      
      <comments>https://mritd.com/2018/05/24/kubernetes-traefik-service-exposure/#disqus_thread</comments>
      
    </item>
    
    <item>
      <title>为你的 GitLab 增加提交信息检测</title>
      <link>https://mritd.com/2018/05/11/add-commit-message-style-check-to-your-gitlab/</link>
      <guid>https://mritd.com/2018/05/11/add-commit-message-style-check-to-your-gitlab/</guid>
      <pubDate>Fri, 11 May 2018 09:44:40 GMT</pubDate>
      
      <description>最近准备对项目生成 Change Log，然而发现提交格式不统一根本没法处理；so 后来大家约定式遵循 GitFlow，并使用 Angular 社区规范的提交格式，同时扩展了一些前缀如 hotfix 等；但是时间长了发现还是有些提交为了 &quot;方便&quot; 不遵循 Angular 社区规范的提交格式，这时候我唯一能做的就是想办法在服务端增加一个提交检测；以下记录了 GitLab 增加自定义 Commit 提交格式检测的方案</description>
      
      
      
      <content:encoded><![CDATA[<blockquote><p>最近准备对项目生成 Change Log，然而发现提交格式不统一根本没法处理；so 后来大家约定式遵循 GitFlow，并使用 Angular 社区规范的提交格式，同时扩展了一些前缀如 hotfix 等；但是时间长了发现还是有些提交为了 “方便” 不遵循 Angular 社区规范的提交格式，这时候我唯一能做的就是想办法在服务端增加一个提交检测；以下记录了 GitLab 增加自定义 Commit 提交格式检测的方案</p></blockquote><h2 id="一、相关文章资料"><a href="#一、相关文章资料" class="headerlink" title="一、相关文章资料"></a>一、相关文章资料</h2><p>最开始用 Google 搜索到的方案是使用 GitLab 的 Push Rules 功能，具体文档见 <a href="https://docs.gitlab.com/ee/push_rules/push_rules.html" target="_blank" rel="noopener">这里</a>，看完了我才发现这是企业版独有的，作为比较有逼格(qiong)的我们是不可能接受这种 “没技术含量” 的方式的；后来找了好多资料，发现还得借助 Git Hook 功能，文档见 <a href="https://docs.gitlab.com/ee/administration/custom_hooks.html" target="_blank" rel="noopener">Custom Git Hooks</a>；简单地说 Git Hook 就是在 git 操作的不同阶段执行的预定义脚本，<strong>GitLab 目前仅支持 <code>pre-receive</code> 这个钩子，当然他可以链式调用</strong>；所以一切操作就得从这里入手</p><h2 id="二、pre-receive-实现"><a href="#二、pre-receive-实现" class="headerlink" title="二、pre-receive 实现"></a>二、pre-receive 实现</h2><p>查阅了相关资料得出，在进行 push 时，GitLab 会调用这个钩子文件，这个钩子文件必须放在 <code>/var/opt/gitlab/git-data/repositories/&lt;group&gt;/&lt;project&gt;.git/custom_hooks</code> 目录中，当然具体路径也可能是 <code>/home/git/repositories/&lt;group&gt;/&lt;project&gt;.git/custom_hooks</code>；<code>custom_hooks</code> 目录需要自己创建，具体可以参阅文档的 <a href="https://docs.gitlab.com/ee/administration/custom_hooks.html#setup" target="_blank" rel="noopener">Setup</a>；</p><p><strong>在进行 push 操作时，GitLab 会调用这个钩子文件，并且从 stdin 输入三个参数，分别为 之前的版本 commit ID、push 的版本 commit ID 和 push 的分支；根据 commit ID 我们就可以很轻松的获取到提交信息，从而实现进一步检测动作；根据 GitLab 的文档说明，当这个 hook 执行后以非 0 状态退出则认为执行失败，从而拒绝 push；同时会将 stderr 信息返回给 client 端；</strong>说了这么多，下面就可以直接上代码了，为了方便我就直接用 go 造了一个 <a href="https://github.com/mritd/pre-receive" target="_blank" rel="noopener">pre-receive</a>，官方文档说明了不限制语言</p><div class="hljs"><pre><code class="hljs golang"><span class="hljs-keyword">package</span> main<span class="hljs-keyword">import</span> (    <span class="hljs-string">"fmt"</span>    <span class="hljs-string">"io/ioutil"</span>    <span class="hljs-string">"os"</span>    <span class="hljs-string">"os/exec"</span>    <span class="hljs-string">"regexp"</span>    <span class="hljs-string">"strings"</span>)<span class="hljs-keyword">type</span> CommitType <span class="hljs-keyword">string</span><span class="hljs-keyword">const</span> (    FEAT     CommitType = <span class="hljs-string">"feat"</span>    FIX      CommitType = <span class="hljs-string">"fix"</span>    DOCS     CommitType = <span class="hljs-string">"docs"</span>    STYLE    CommitType = <span class="hljs-string">"style"</span>    REFACTOR CommitType = <span class="hljs-string">"refactor"</span>    TEST     CommitType = <span class="hljs-string">"test"</span>    CHORE    CommitType = <span class="hljs-string">"chore"</span>    PERF     CommitType = <span class="hljs-string">"perf"</span>    HOTFIX   CommitType = <span class="hljs-string">"hotfix"</span>)<span class="hljs-keyword">const</span> CommitMessagePattern = <span class="hljs-string">`^(?:fixup!\s*)?(\w*)(\(([\w\$\.\*/-].*)\))?\: (.*)|^Merge\ branch(.*)`</span><span class="hljs-keyword">const</span> checkFailedMeassge = <span class="hljs-string">`##############################################################################</span><span class="hljs-string">##                                                                          ##</span><span class="hljs-string">## Commit message style check failed!                                       ##</span><span class="hljs-string">##                                                                          ##</span><span class="hljs-string">## Commit message style must satisfy this regular:                          ##</span><span class="hljs-string">##   ^(?:fixup!\s*)?(\w*)(\(([\w\$\.\*/-].*)\))?\: (. *)|^Merge\ branch(.*) ##</span><span class="hljs-string">##                                                                          ##</span><span class="hljs-string">## Example:                                                                 ##</span><span class="hljs-string">##   feat(test): test commit style check.                                   ##</span><span class="hljs-string">##                                                                          ##</span><span class="hljs-string">##############################################################################`</span><span class="hljs-comment">// 是否开启严格模式，严格模式下将校验所有的提交信息格式(多 commit 下)</span><span class="hljs-keyword">const</span> strictMode = <span class="hljs-literal">false</span><span class="hljs-keyword">var</span> commitMsgReg = regexp.MustCompile(CommitMessagePattern)<span class="hljs-function"><span class="hljs-keyword">func</span> <span class="hljs-title">main</span><span class="hljs-params">()</span></span> &#123;    input, _ := ioutil.ReadAll(os.Stdin)    param := strings.Fields(<span class="hljs-keyword">string</span>(input))    <span class="hljs-comment">// allow branch/tag delete</span>    <span class="hljs-keyword">if</span> param[<span class="hljs-number">1</span>] == <span class="hljs-string">"0000000000000000000000000000000000000000"</span> &#123;        os.Exit(<span class="hljs-number">0</span>)    &#125;    commitMsg := getCommitMsg(param[<span class="hljs-number">0</span>], param[<span class="hljs-number">1</span>])    <span class="hljs-keyword">for</span> _, tmpStr := <span class="hljs-keyword">range</span> commitMsg &#123;        commitTypes := commitMsgReg.FindAllStringSubmatch(tmpStr, <span class="hljs-number">-1</span>)        <span class="hljs-keyword">if</span> <span class="hljs-built_in">len</span>(commitTypes) != <span class="hljs-number">1</span> &#123;            checkFailed()        &#125; <span class="hljs-keyword">else</span> &#123;            <span class="hljs-keyword">switch</span> commitTypes[<span class="hljs-number">0</span>][<span class="hljs-number">1</span>] &#123;            <span class="hljs-keyword">case</span> <span class="hljs-keyword">string</span>(FEAT):            <span class="hljs-keyword">case</span> <span class="hljs-keyword">string</span>(FIX):            <span class="hljs-keyword">case</span> <span class="hljs-keyword">string</span>(DOCS):            <span class="hljs-keyword">case</span> <span class="hljs-keyword">string</span>(STYLE):            <span class="hljs-keyword">case</span> <span class="hljs-keyword">string</span>(REFACTOR):            <span class="hljs-keyword">case</span> <span class="hljs-keyword">string</span>(TEST):            <span class="hljs-keyword">case</span> <span class="hljs-keyword">string</span>(CHORE):            <span class="hljs-keyword">case</span> <span class="hljs-keyword">string</span>(PERF):            <span class="hljs-keyword">case</span> <span class="hljs-keyword">string</span>(HOTFIX):            <span class="hljs-keyword">default</span>:                <span class="hljs-keyword">if</span> !strings.HasPrefix(tmpStr, <span class="hljs-string">"Merge branch"</span>) &#123;                    checkFailed()                &#125;            &#125;        &#125;        <span class="hljs-keyword">if</span> !strictMode &#123;            os.Exit(<span class="hljs-number">0</span>)        &#125;    &#125;&#125;<span class="hljs-function"><span class="hljs-keyword">func</span> <span class="hljs-title">getCommitMsg</span><span class="hljs-params">(odlCommitID, commitID <span class="hljs-keyword">string</span>)</span> []<span class="hljs-title">string</span></span> &#123;    getCommitMsgCmd := exec.Command(<span class="hljs-string">"git"</span>, <span class="hljs-string">"log"</span>, odlCommitID+<span class="hljs-string">".."</span>+commitID, <span class="hljs-string">"--pretty=format:%s"</span>)    getCommitMsgCmd.Stdin = os.Stdin    getCommitMsgCmd.Stderr = os.Stderr    b, err := getCommitMsgCmd.Output()    <span class="hljs-keyword">if</span> err != <span class="hljs-literal">nil</span> &#123;        fmt.Print(err)        os.Exit(<span class="hljs-number">1</span>)    &#125;    commitMsg := strings.Split(<span class="hljs-keyword">string</span>(b), <span class="hljs-string">"\n"</span>)    <span class="hljs-keyword">return</span> commitMsg&#125;<span class="hljs-function"><span class="hljs-keyword">func</span> <span class="hljs-title">checkFailed</span><span class="hljs-params">()</span></span> &#123;    fmt.Fprintln(os.Stderr, checkFailedMeassge)    os.Exit(<span class="hljs-number">1</span>)&#125;</code></pre></div><h2 id="三、安装-pre-receive"><a href="#三、安装-pre-receive" class="headerlink" title="三、安装 pre-receive"></a>三、安装 pre-receive</h2><p>把以上代码编译后生成的 <code>pre-receive</code> 文件复制到对应项目的钩子目录即可；<strong>要注意的是文件名必须为 <code>pre-receive</code>，同时 <code>custom_hooks</code> 目录需要自建；<code>custom_hooks</code> 目录以及 <code>pre-receive</code> 文件用户组必须为 <code>git:git</code>；在删除分支时 commit ID 为 <code>0000000000000000000000000000000000000000</code>，此时不需要检测提交信息，否则可能导致无法删除分支/tag</strong>；最后效果如下所示</p><p><img src="https://cdn.oss.link/markdown/hs9c2.png" srcset="/img/loading.gif" alt="commit msg check"></p>]]></content:encoded>
      
      
      <category domain="https://mritd.com/categories/ci-cd/">CI/CD</category>
      
      
      <category domain="https://mritd.com/tags/ci-cd/">CI/CD</category>
      
      
      <comments>https://mritd.com/2018/05/11/add-commit-message-style-check-to-your-gitlab/#disqus_thread</comments>
      
    </item>
    
    <item>
      <title>Kubernetes 1.10.1 集群搭建</title>
      <link>https://mritd.com/2018/04/19/set-up-kubernetes-1.10.1-cluster-by-hyperkube/</link>
      <guid>https://mritd.com/2018/04/19/set-up-kubernetes-1.10.1-cluster-by-hyperkube/</guid>
      <pubDate>Thu, 19 Apr 2018 08:19:08 GMT</pubDate>
      
      <description>年后比较忙，所以 1.9 也没去折腾(其实就是懒)，最近刚有点时间凑巧 1.10 发布；所以就折腾一下 1.10，感觉搭建配置没有太大变化，折腾了 2 天基本算是搞定了，这里记录一下搭建过程；本文用到的被 block 镜像已经上传至 [百度云](https://pan.baidu.com/s/14W86QQ4qi8qn8JqaDMcC3g) 密码: dy5p</description>
      
      
      
      <content:encoded><![CDATA[<blockquote><p>年后比较忙，所以 1.9 也没去折腾(其实就是懒)，最近刚有点时间凑巧 1.10 发布；所以就折腾一下 1.10，感觉搭建配置没有太大变化，折腾了 2 天基本算是搞定了，这里记录一下搭建过程；本文用到的被 block 镜像已经上传至 <a href="https://pan.baidu.com/s/14W86QQ4qi8qn8JqaDMcC3g" target="_blank" rel="noopener">百度云</a> 密码: dy5p</p></blockquote><h3 id="一、环境准备"><a href="#一、环境准备" class="headerlink" title="一、环境准备"></a>一、环境准备</h3><p>目前搭建仍然采用 5 台虚拟机测试，基本环境如下</p><table><thead><tr><th>IP</th><th>Type</th><th>Docker</th><th>OS</th></tr></thead><tbody><tr><td>192.168.1.61</td><td>master、node、etcd</td><td>18.03.0-ce</td><td>ubuntu 16.04</td></tr><tr><td>192.168.1.62</td><td>master、node、etcd</td><td>18.03.0-ce</td><td>ubuntu 16.04</td></tr><tr><td>192.168.1.63</td><td>master、node、etcd</td><td>18.03.0-ce</td><td>ubuntu 16.04</td></tr><tr><td>192.168.1.64</td><td>node</td><td>18.03.0-ce</td><td>ubuntu 16.04</td></tr><tr><td>192.168.1.65</td><td>node</td><td>18.03.0-ce</td><td>ubuntu 16.04</td></tr></tbody></table><p><strong>搭建前请看完整篇文章后再操作，一些变更说明我放到后面了；还有为了尽可能的懒，也不用什么 rpm、deb 了，直接 <code>hyperkube</code> + <code>service</code> 配置，布吉岛 <code>hyperkube</code> 的请看 <a href="https://github.com/kubernetes/kubernetes/blob/master/cluster/images/hyperkube/README.md" target="_blank" rel="noopener">GitHub</a>；本篇文章基于一些小脚本搭建(懒)，所以不会写太详细的步骤，具体请参考 <a href="https://github.com/mritd/ktool" target="_blank" rel="noopener">仓库脚本</a>，如果想看更详细的每一步的作用可以参考以前的 1.7、1.8 的搭建文档</strong></p><h3 id="二、搭建-Etcd-集群"><a href="#二、搭建-Etcd-集群" class="headerlink" title="二、搭建 Etcd 集群"></a>二、搭建 Etcd 集群</h3><h4 id="2-1、安装-cfssl"><a href="#2-1、安装-cfssl" class="headerlink" title="2.1、安装 cfssl"></a>2.1、安装 cfssl</h4><p>说实话这个章节我不想写，但是考虑可能有人真的需要，所以还是写了一下；<strong>这个安装脚本使用的是我私人的 cdn，文件可能随时删除，想使用最新版本请自行从 <a href="https://github.com/cloudflare/cfssl" target="_blank" rel="noopener">Github</a> clone 并编译</strong></p><div class="hljs"><pre><code class="hljs sh">wget https://mritdftp.b0.upaiyun.com/cfssl/cfssl.tar.gztar -zxvf cfssl.tar.gzmv cfssl cfssljson /usr/<span class="hljs-built_in">local</span>/binchmod +x /usr/<span class="hljs-built_in">local</span>/bin/cfssl /usr/<span class="hljs-built_in">local</span>/bin/cfssljsonrm -f cfssl.tar.gz</code></pre></div><h4 id="2-2、生成-Etcd-证书"><a href="#2-2、生成-Etcd-证书" class="headerlink" title="2.2、生成 Etcd 证书"></a>2.2、生成 Etcd 证书</h4><h5 id="etcd-csr-json"><a href="#etcd-csr-json" class="headerlink" title="etcd-csr.json"></a>etcd-csr.json</h5><div class="hljs"><pre><code class="hljs json">&#123;  <span class="hljs-attr">"key"</span>: &#123;    <span class="hljs-attr">"algo"</span>: <span class="hljs-string">"rsa"</span>,    <span class="hljs-attr">"size"</span>: <span class="hljs-number">2048</span>  &#125;,  <span class="hljs-attr">"names"</span>: [    &#123;      <span class="hljs-attr">"O"</span>: <span class="hljs-string">"etcd"</span>,      <span class="hljs-attr">"OU"</span>: <span class="hljs-string">"etcd Security"</span>,      <span class="hljs-attr">"L"</span>: <span class="hljs-string">"Beijing"</span>,      <span class="hljs-attr">"ST"</span>: <span class="hljs-string">"Beijing"</span>,      <span class="hljs-attr">"C"</span>: <span class="hljs-string">"CN"</span>    &#125;  ],  <span class="hljs-attr">"CN"</span>: <span class="hljs-string">"etcd"</span>,  <span class="hljs-attr">"hosts"</span>: [    <span class="hljs-string">"127.0.0.1"</span>,    <span class="hljs-string">"localhost"</span>,    <span class="hljs-string">"192.168.1.61"</span>,    <span class="hljs-string">"192.168.1.62"</span>,    <span class="hljs-string">"192.168.1.63"</span>  ]&#125;</code></pre></div><h5 id="etcd-gencert-json"><a href="#etcd-gencert-json" class="headerlink" title="etcd-gencert.json"></a>etcd-gencert.json</h5><div class="hljs"><pre><code class="hljs json">&#123;  <span class="hljs-attr">"signing"</span>: &#123;    <span class="hljs-attr">"default"</span>: &#123;        <span class="hljs-attr">"usages"</span>: [          <span class="hljs-string">"signing"</span>,          <span class="hljs-string">"key encipherment"</span>,          <span class="hljs-string">"server auth"</span>,          <span class="hljs-string">"client auth"</span>        ],        <span class="hljs-attr">"expiry"</span>: <span class="hljs-string">"87600h"</span>    &#125;  &#125;&#125;</code></pre></div><h5 id="etcd-root-ca-csr-json"><a href="#etcd-root-ca-csr-json" class="headerlink" title="etcd-root-ca-csr.json"></a>etcd-root-ca-csr.json</h5><div class="hljs"><pre><code class="hljs json">&#123;  <span class="hljs-attr">"key"</span>: &#123;    <span class="hljs-attr">"algo"</span>: <span class="hljs-string">"rsa"</span>,    <span class="hljs-attr">"size"</span>: <span class="hljs-number">4096</span>  &#125;,  <span class="hljs-attr">"names"</span>: [    &#123;      <span class="hljs-attr">"O"</span>: <span class="hljs-string">"etcd"</span>,      <span class="hljs-attr">"OU"</span>: <span class="hljs-string">"etcd Security"</span>,      <span class="hljs-attr">"L"</span>: <span class="hljs-string">"Beijing"</span>,      <span class="hljs-attr">"ST"</span>: <span class="hljs-string">"Beijing"</span>,      <span class="hljs-attr">"C"</span>: <span class="hljs-string">"CN"</span>    &#125;  ],  <span class="hljs-attr">"CN"</span>: <span class="hljs-string">"etcd-root-ca"</span>&#125;</code></pre></div><h5 id="生成证书"><a href="#生成证书" class="headerlink" title="生成证书"></a>生成证书</h5><div class="hljs"><pre><code class="hljs sh">cfssl gencert --initca=<span class="hljs-literal">true</span> etcd-root-ca-csr.json | cfssljson --bare etcd-root-cacfssl gencert --ca etcd-root-ca.pem --ca-key etcd-root-ca-key.pem --config etcd-gencert.json etcd-csr.json | cfssljson --bare etcd</code></pre></div><p>生成后如下</p><p><img src="https://cdn.oss.link/markdown/81203.png" srcset="/img/loading.gif" alt="gen etcd certs"></p><h4 id="2-3、安装-Etcd"><a href="#2-3、安装-Etcd" class="headerlink" title="2.3、安装 Etcd"></a>2.3、安装 Etcd</h4><p>Etcd 这里采用最新的 3.2.18 版本，安装方式直接复制二进制文件、systemd service 配置即可，不过需要注意相关用户权限问题，以下脚本配置等参考了 etcd rpm 安装包</p><h5 id="etcd-service"><a href="#etcd-service" class="headerlink" title="etcd.service"></a>etcd.service</h5><div class="hljs"><pre><code class="hljs sh">[Unit]Description=Etcd ServerAfter=network.targetAfter=network-online.targetWants=network-online.target[Service]Type=notifyWorkingDirectory=/var/lib/etcd/EnvironmentFile=-/etc/etcd/etcd.confUser=etcd<span class="hljs-comment"># set GOMAXPROCS to number of processors</span>ExecStart=/bin/bash -c <span class="hljs-string">"GOMAXPROCS=<span class="hljs-variable">$(nproc)</span> /usr/local/bin/etcd --name=\"<span class="hljs-variable">$&#123;ETCD_NAME&#125;</span>\" --data-dir=\"<span class="hljs-variable">$&#123;ETCD_DATA_DIR&#125;</span>\" --listen-client-urls=\"<span class="hljs-variable">$&#123;ETCD_LISTEN_CLIENT_URLS&#125;</span>\""</span>Restart=on-failureLimitNOFILE=65536[Install]WantedBy=multi-user.target</code></pre></div><h5 id="etcd-conf"><a href="#etcd-conf" class="headerlink" title="etcd.conf"></a>etcd.conf</h5><div class="hljs"><pre><code class="hljs sh"><span class="hljs-comment"># [member]</span>ETCD_NAME=etcd1ETCD_DATA_DIR=<span class="hljs-string">"/var/lib/etcd/etcd1.etcd"</span>ETCD_WAL_DIR=<span class="hljs-string">"/var/lib/etcd/wal"</span>ETCD_SNAPSHOT_COUNT=<span class="hljs-string">"100"</span>ETCD_HEARTBEAT_INTERVAL=<span class="hljs-string">"100"</span>ETCD_ELECTION_TIMEOUT=<span class="hljs-string">"1000"</span>ETCD_LISTEN_PEER_URLS=<span class="hljs-string">"https://192.168.1.61:2380"</span>ETCD_LISTEN_CLIENT_URLS=<span class="hljs-string">"https://192.168.1.61:2379,http://127.0.0.1:2379"</span>ETCD_MAX_SNAPSHOTS=<span class="hljs-string">"5"</span>ETCD_MAX_WALS=<span class="hljs-string">"5"</span><span class="hljs-comment">#ETCD_CORS=""</span><span class="hljs-comment"># [cluster]</span>ETCD_INITIAL_ADVERTISE_PEER_URLS=<span class="hljs-string">"https://192.168.1.61:2380"</span><span class="hljs-comment"># if you use different ETCD_NAME (e.g. test), set ETCD_INITIAL_CLUSTER value for this name, i.e. "test=http://..."</span>ETCD_INITIAL_CLUSTER=<span class="hljs-string">"etcd1=https://192.168.1.61:2380,etcd2=https://192.168.1.62:2380,etcd3=https://192.168.1.63:2380"</span>ETCD_INITIAL_CLUSTER_STATE=<span class="hljs-string">"new"</span>ETCD_INITIAL_CLUSTER_TOKEN=<span class="hljs-string">"etcd-cluster"</span>ETCD_ADVERTISE_CLIENT_URLS=<span class="hljs-string">"https://192.168.1.61:2379"</span><span class="hljs-comment">#ETCD_DISCOVERY=""</span><span class="hljs-comment">#ETCD_DISCOVERY_SRV=""</span><span class="hljs-comment">#ETCD_DISCOVERY_FALLBACK="proxy"</span><span class="hljs-comment">#ETCD_DISCOVERY_PROXY=""</span><span class="hljs-comment">#ETCD_STRICT_RECONFIG_CHECK="false"</span><span class="hljs-comment">#ETCD_AUTO_COMPACTION_RETENTION="0"</span><span class="hljs-comment"># [proxy]</span><span class="hljs-comment">#ETCD_PROXY="off"</span><span class="hljs-comment">#ETCD_PROXY_FAILURE_WAIT="5000"</span><span class="hljs-comment">#ETCD_PROXY_REFRESH_INTERVAL="30000"</span><span class="hljs-comment">#ETCD_PROXY_DIAL_TIMEOUT="1000"</span><span class="hljs-comment">#ETCD_PROXY_WRITE_TIMEOUT="5000"</span><span class="hljs-comment">#ETCD_PROXY_READ_TIMEOUT="0"</span><span class="hljs-comment"># [security]</span>ETCD_CERT_FILE=<span class="hljs-string">"/etc/etcd/ssl/etcd.pem"</span>ETCD_KEY_FILE=<span class="hljs-string">"/etc/etcd/ssl/etcd-key.pem"</span>ETCD_CLIENT_CERT_AUTH=<span class="hljs-string">"true"</span>ETCD_TRUSTED_CA_FILE=<span class="hljs-string">"/etc/etcd/ssl/etcd-root-ca.pem"</span>ETCD_AUTO_TLS=<span class="hljs-string">"true"</span>ETCD_PEER_CERT_FILE=<span class="hljs-string">"/etc/etcd/ssl/etcd.pem"</span>ETCD_PEER_KEY_FILE=<span class="hljs-string">"/etc/etcd/ssl/etcd-key.pem"</span>ETCD_PEER_CLIENT_CERT_AUTH=<span class="hljs-string">"true"</span>ETCD_PEER_TRUSTED_CA_FILE=<span class="hljs-string">"/etc/etcd/ssl/etcd-root-ca.pem"</span>ETCD_PEER_AUTO_TLS=<span class="hljs-string">"true"</span><span class="hljs-comment"># [logging]</span><span class="hljs-comment">#ETCD_DEBUG="false"</span><span class="hljs-comment"># examples for -log-package-levels etcdserver=WARNING,security=DEBUG</span><span class="hljs-comment">#ETCD_LOG_PACKAGE_LEVELS=""</span></code></pre></div><h5 id="install-sh"><a href="#install-sh" class="headerlink" title="install.sh"></a>install.sh</h5><div class="hljs"><pre><code class="hljs sh"><span class="hljs-meta">#!/bin/bash</span><span class="hljs-built_in">set</span> -eETCD_VERSION=<span class="hljs-string">"3.2.18"</span><span class="hljs-keyword">function</span> <span class="hljs-function"><span class="hljs-title">download</span></span>()&#123;    <span class="hljs-keyword">if</span> [ ! -f <span class="hljs-string">"etcd-v<span class="hljs-variable">$&#123;ETCD_VERSION&#125;</span>-linux-amd64.tar.gz"</span> ]; <span class="hljs-keyword">then</span>        wget https://github.com/coreos/etcd/releases/download/v<span class="hljs-variable">$&#123;ETCD_VERSION&#125;</span>/etcd-v<span class="hljs-variable">$&#123;ETCD_VERSION&#125;</span>-linux-amd64.tar.gz        tar -zxvf etcd-v<span class="hljs-variable">$&#123;ETCD_VERSION&#125;</span>-linux-amd64.tar.gz    <span class="hljs-keyword">fi</span>&#125;<span class="hljs-keyword">function</span> <span class="hljs-function"><span class="hljs-title">preinstall</span></span>()&#123;    getent group etcd &gt;/dev/null || groupadd -r etcd    getent passwd etcd &gt;/dev/null || useradd -r -g etcd -d /var/lib/etcd -s /sbin/nologin -c <span class="hljs-string">"etcd user"</span> etcd&#125;<span class="hljs-keyword">function</span> <span class="hljs-function"><span class="hljs-title">install</span></span>()&#123;    <span class="hljs-built_in">echo</span> -e <span class="hljs-string">"\033[32mINFO: Copy etcd...\033[0m"</span>    tar -zxvf etcd-v<span class="hljs-variable">$&#123;ETCD_VERSION&#125;</span>-linux-amd64.tar.gz    cp etcd-v<span class="hljs-variable">$&#123;ETCD_VERSION&#125;</span>-linux-amd64/etcd* /usr/<span class="hljs-built_in">local</span>/bin    rm -rf etcd-v<span class="hljs-variable">$&#123;ETCD_VERSION&#125;</span>-linux-amd64    <span class="hljs-built_in">echo</span> -e <span class="hljs-string">"\033[32mINFO: Copy etcd config...\033[0m"</span>    cp -r conf /etc/etcd    chown -R etcd:etcd /etc/etcd    chmod -R 755 /etc/etcd/ssl    <span class="hljs-built_in">echo</span> -e <span class="hljs-string">"\033[32mINFO: Copy etcd systemd config...\033[0m"</span>    cp systemd/*.service /lib/systemd/system    systemctl daemon-reload&#125;<span class="hljs-keyword">function</span> <span class="hljs-function"><span class="hljs-title">postinstall</span></span>()&#123;    <span class="hljs-keyword">if</span> [ ! -d <span class="hljs-string">"/var/lib/etcd"</span> ]; <span class="hljs-keyword">then</span>        mkdir /var/lib/etcd        chown -R etcd:etcd /var/lib/etcd    <span class="hljs-keyword">fi</span>&#125;downloadpreinstallinstallpostinstall</code></pre></div><p><strong>脚本解释如下:</strong></p><ul><li>download: 从 Github 下载二进制文件并解压</li><li>preinstall: 为 Etcd 安装做准备，创建 etcd 用户，并指定家目录登录 shell 等</li><li>install: 将 etcd 二进制文件复制到安装目录(<code>/usr/local/bin</code>)，复制 conf 目录到 <code>/etc/etcd</code></li><li>postinstall: 安装后收尾工作，比如检测 <code>/var/lib/etcd</code> 是否存在，纠正权限等</li></ul><p>整体目录结构如下</p><div class="hljs"><pre><code class="hljs sh">etcd├── conf│   ├── etcd.conf│   └── ssl│       ├── etcd.csr│       ├── etcd-csr.json│       ├── etcd-gencert.json│       ├── etcd-key.pem│       ├── etcd.pem│       ├── etcd-root-ca.csr│       ├── etcd-root-ca-csr.json│       ├── etcd-root-ca-key.pem│       └── etcd-root-ca.pem├── etcd.service└── install.sh</code></pre></div><p><strong>请自行创建 conf 目录等，并放置好相关文件，保存上面脚本为 <code>install.sh</code>，直接执行即可；在每台机器上更改好对应的配置，如 etcd 名称等，etcd 估计都是轻车熟路了，这里不做过多阐述；安装后启动即可</strong></p><div class="hljs"><pre><code class="hljs sh">systemctl start etcdsystemctl <span class="hljs-built_in">enable</span> etcd</code></pre></div><p><strong>注意: 集群 etcd 要 3 个一起启动，集群模式下单个启动会卡半天最后失败，不要傻等；启动成功后测试如下</strong></p><div class="hljs"><pre><code class="hljs sh"><span class="hljs-built_in">export</span> ETCDCTL_API=3etcdctl --cacert=/etc/etcd/ssl/etcd-root-ca.pem --cert=/etc/etcd/ssl/etcd.pem --key=/etc/etcd/ssl/etcd-key.pem --endpoints=https://192.168.1.61:2379,https://192.168.1.62:2379,https://192.168.1.63:2379 endpoint health</code></pre></div><p><img src="https://cdn.oss.link/markdown/ji94m.png" srcset="/img/loading.gif" alt="check etcd"></p><h3 id="三、安装-Kubernets-集群组件"><a href="#三、安装-Kubernets-集群组件" class="headerlink" title="三、安装 Kubernets 集群组件"></a>三、安装 Kubernets 集群组件</h3><blockquote><p><strong>注意：与以前文档不同的是，这次不依赖 rpm 等特定安装包，而是基于 hyperkube 二进制手动安装，每个节点都会同时安装 Master 与 Node 配置文件，具体作为 Master 还是 Node 取决于服务开启情况</strong></p></blockquote><h4 id="3-1、生成-Kubernetes-证书"><a href="#3-1、生成-Kubernetes-证书" class="headerlink" title="3.1、生成 Kubernetes 证书"></a>3.1、生成 Kubernetes 证书</h4><p>由于 kubelet 和 kube-proxy 用到的 kubeconfig 配置文件需要借助 kubectl 来生成，所以需要先安装一下 kubectl</p><div class="hljs"><pre><code class="hljs sh">wget https://storage.googleapis.com/kubernetes-release/release/v1.10.1/bin/linux/amd64/hyperkube -O hyperkube_1.10.1chmod +x hyperkube_1.10.1cp hyperkube_1.10.1 /usr/<span class="hljs-built_in">local</span>/bin/hyperkubeln -s /usr/<span class="hljs-built_in">local</span>/bin/hyperkube /usr/<span class="hljs-built_in">local</span>/bin/kubectl</code></pre></div><h5 id="admin-csr-json"><a href="#admin-csr-json" class="headerlink" title="admin-csr.json"></a>admin-csr.json</h5><div class="hljs"><pre><code class="hljs json">&#123;  <span class="hljs-attr">"CN"</span>: <span class="hljs-string">"admin"</span>,  <span class="hljs-attr">"hosts"</span>: [],  <span class="hljs-attr">"key"</span>: &#123;    <span class="hljs-attr">"algo"</span>: <span class="hljs-string">"rsa"</span>,    <span class="hljs-attr">"size"</span>: <span class="hljs-number">2048</span>  &#125;,  <span class="hljs-attr">"names"</span>: [    &#123;      <span class="hljs-attr">"C"</span>: <span class="hljs-string">"CN"</span>,      <span class="hljs-attr">"ST"</span>: <span class="hljs-string">"BeiJing"</span>,      <span class="hljs-attr">"L"</span>: <span class="hljs-string">"BeiJing"</span>,      <span class="hljs-attr">"O"</span>: <span class="hljs-string">"system:masters"</span>,      <span class="hljs-attr">"OU"</span>: <span class="hljs-string">"System"</span>    &#125;  ]&#125;</code></pre></div><h5 id="k8s-gencert-json"><a href="#k8s-gencert-json" class="headerlink" title="k8s-gencert.json"></a>k8s-gencert.json</h5><div class="hljs"><pre><code class="hljs json">&#123;  <span class="hljs-attr">"signing"</span>: &#123;    <span class="hljs-attr">"default"</span>: &#123;      <span class="hljs-attr">"expiry"</span>: <span class="hljs-string">"87600h"</span>    &#125;,    <span class="hljs-attr">"profiles"</span>: &#123;      <span class="hljs-attr">"kubernetes"</span>: &#123;        <span class="hljs-attr">"usages"</span>: [            <span class="hljs-string">"signing"</span>,            <span class="hljs-string">"key encipherment"</span>,            <span class="hljs-string">"server auth"</span>,            <span class="hljs-string">"client auth"</span>        ],        <span class="hljs-attr">"expiry"</span>: <span class="hljs-string">"87600h"</span>      &#125;    &#125;  &#125;&#125;</code></pre></div><h5 id="k8s-root-ca-csr-json"><a href="#k8s-root-ca-csr-json" class="headerlink" title="k8s-root-ca-csr.json"></a>k8s-root-ca-csr.json</h5><div class="hljs"><pre><code class="hljs json">&#123;  <span class="hljs-attr">"CN"</span>: <span class="hljs-string">"kubernetes"</span>,  <span class="hljs-attr">"key"</span>: &#123;    <span class="hljs-attr">"algo"</span>: <span class="hljs-string">"rsa"</span>,    <span class="hljs-attr">"size"</span>: <span class="hljs-number">4096</span>  &#125;,  <span class="hljs-attr">"names"</span>: [    &#123;      <span class="hljs-attr">"C"</span>: <span class="hljs-string">"CN"</span>,      <span class="hljs-attr">"ST"</span>: <span class="hljs-string">"BeiJing"</span>,      <span class="hljs-attr">"L"</span>: <span class="hljs-string">"BeiJing"</span>,      <span class="hljs-attr">"O"</span>: <span class="hljs-string">"k8s"</span>,      <span class="hljs-attr">"OU"</span>: <span class="hljs-string">"System"</span>    &#125;  ]&#125;</code></pre></div><h5 id="kube-apiserver-csr-json"><a href="#kube-apiserver-csr-json" class="headerlink" title="kube-apiserver-csr.json"></a>kube-apiserver-csr.json</h5><p><strong>注意: 在以前的文档中这个配置叫 <code>kubernetes-csr.json</code>，为了明确划分职责，这个证书目前被重命名以表示其专属于 <code>apiserver</code> 使用；加了一个 <code>*.kubernetes.master</code> 域名以便内部私有 DNS 解析使用(可删除)；至于很多人问过 <code>kubernetes</code> 这几个能不能删掉，答案是不可以的；因为当集群创建好后，default namespace 下会创建一个叫 <code>kubenretes</code> 的 svc，有一些组件会直接连接这个 svc 来跟 api 通讯的，证书如果不包含可能会出现无法连接的情况；其他几个 <code>kubernetes</code> 开头的域名作用相同</strong></p><div class="hljs"><pre><code class="hljs json">&#123;    <span class="hljs-attr">"CN"</span>: <span class="hljs-string">"kubernetes"</span>,    <span class="hljs-attr">"hosts"</span>: [        <span class="hljs-string">"127.0.0.1"</span>,        <span class="hljs-string">"10.254.0.1"</span>,        <span class="hljs-string">"192.168.1.61"</span>,        <span class="hljs-string">"192.168.1.62"</span>,        <span class="hljs-string">"192.168.1.63"</span>,        <span class="hljs-string">"192.168.1.64"</span>,        <span class="hljs-string">"192.168.1.65"</span>,        <span class="hljs-string">"*.kubernetes.master"</span>,        <span class="hljs-string">"localhost"</span>,        <span class="hljs-string">"kubernetes"</span>,        <span class="hljs-string">"kubernetes.default"</span>,        <span class="hljs-string">"kubernetes.default.svc"</span>,        <span class="hljs-string">"kubernetes.default.svc.cluster"</span>,        <span class="hljs-string">"kubernetes.default.svc.cluster.local"</span>    ],    <span class="hljs-attr">"key"</span>: &#123;        <span class="hljs-attr">"algo"</span>: <span class="hljs-string">"rsa"</span>,        <span class="hljs-attr">"size"</span>: <span class="hljs-number">2048</span>    &#125;,    <span class="hljs-attr">"names"</span>: [        &#123;            <span class="hljs-attr">"C"</span>: <span class="hljs-string">"CN"</span>,            <span class="hljs-attr">"ST"</span>: <span class="hljs-string">"BeiJing"</span>,            <span class="hljs-attr">"L"</span>: <span class="hljs-string">"BeiJing"</span>,            <span class="hljs-attr">"O"</span>: <span class="hljs-string">"k8s"</span>,            <span class="hljs-attr">"OU"</span>: <span class="hljs-string">"System"</span>        &#125;    ]&#125;</code></pre></div><h5 id="kube-proxy-csr-json"><a href="#kube-proxy-csr-json" class="headerlink" title="kube-proxy-csr.json"></a>kube-proxy-csr.json</h5><div class="hljs"><pre><code class="hljs json">&#123;  <span class="hljs-attr">"CN"</span>: <span class="hljs-string">"system:kube-proxy"</span>,  <span class="hljs-attr">"hosts"</span>: [],  <span class="hljs-attr">"key"</span>: &#123;    <span class="hljs-attr">"algo"</span>: <span class="hljs-string">"rsa"</span>,    <span class="hljs-attr">"size"</span>: <span class="hljs-number">2048</span>  &#125;,  <span class="hljs-attr">"names"</span>: [    &#123;      <span class="hljs-attr">"C"</span>: <span class="hljs-string">"CN"</span>,      <span class="hljs-attr">"ST"</span>: <span class="hljs-string">"BeiJing"</span>,      <span class="hljs-attr">"L"</span>: <span class="hljs-string">"BeiJing"</span>,      <span class="hljs-attr">"O"</span>: <span class="hljs-string">"k8s"</span>,      <span class="hljs-attr">"OU"</span>: <span class="hljs-string">"System"</span>    &#125;  ]&#125;</code></pre></div><h5 id="生成证书及配置"><a href="#生成证书及配置" class="headerlink" title="生成证书及配置"></a>生成证书及配置</h5><div class="hljs"><pre><code class="hljs sh"><span class="hljs-comment"># 生成 CA</span>cfssl gencert --initca=<span class="hljs-literal">true</span> k8s-root-ca-csr.json | cfssljson --bare k8s-root-ca<span class="hljs-comment"># 依次生成其他组件证书</span><span class="hljs-keyword">for</span> targetName <span class="hljs-keyword">in</span> kube-apiserver admin kube-proxy; <span class="hljs-keyword">do</span>    cfssl gencert --ca k8s-root-ca.pem --ca-key k8s-root-ca-key.pem --config k8s-gencert.json --profile kubernetes <span class="hljs-variable">$targetName</span>-csr.json | cfssljson --bare <span class="hljs-variable">$targetName</span><span class="hljs-keyword">done</span><span class="hljs-comment"># 地址默认为 127.0.0.1:6443</span><span class="hljs-comment"># 如果在 master 上启用 kubelet 请在生成后的 kubeconfig 中</span><span class="hljs-comment"># 修改该地址为 当前MASTER_IP:6443</span>KUBE_APISERVER=<span class="hljs-string">"https://127.0.0.1:6443"</span>BOOTSTRAP_TOKEN=$(head -c 16 /dev/urandom | od -An -t x | tr -d <span class="hljs-string">' '</span>)<span class="hljs-built_in">echo</span> <span class="hljs-string">"Tokne: <span class="hljs-variable">$&#123;BOOTSTRAP_TOKEN&#125;</span>"</span><span class="hljs-comment"># 不要质疑 system:bootstrappers 用户组是否写错了，有疑问请参考官方文档</span><span class="hljs-comment"># https://kubernetes.io/docs/admin/kubelet-tls-bootstrapping/</span>cat &gt; token.csv &lt;&lt;EOF<span class="hljs-variable">$&#123;BOOTSTRAP_TOKEN&#125;</span>,kubelet-bootstrap,10001,<span class="hljs-string">"system:bootstrappers"</span>EOF<span class="hljs-built_in">echo</span> <span class="hljs-string">"Create kubelet bootstrapping kubeconfig..."</span><span class="hljs-comment"># 设置集群参数</span>kubectl config <span class="hljs-built_in">set</span>-cluster kubernetes \  --certificate-authority=k8s-root-ca.pem \  --embed-certs=<span class="hljs-literal">true</span> \  --server=<span class="hljs-variable">$&#123;KUBE_APISERVER&#125;</span> \  --kubeconfig=bootstrap.kubeconfig<span class="hljs-comment"># 设置客户端认证参数</span>kubectl config <span class="hljs-built_in">set</span>-credentials kubelet-bootstrap \  --token=<span class="hljs-variable">$&#123;BOOTSTRAP_TOKEN&#125;</span> \  --kubeconfig=bootstrap.kubeconfig<span class="hljs-comment"># 设置上下文参数</span>kubectl config <span class="hljs-built_in">set</span>-context default \  --cluster=kubernetes \  --user=kubelet-bootstrap \  --kubeconfig=bootstrap.kubeconfig<span class="hljs-comment"># 设置默认上下文</span>kubectl config use-context default --kubeconfig=bootstrap.kubeconfig<span class="hljs-built_in">echo</span> <span class="hljs-string">"Create kube-proxy kubeconfig..."</span><span class="hljs-comment"># 设置集群参数</span>kubectl config <span class="hljs-built_in">set</span>-cluster kubernetes \  --certificate-authority=k8s-root-ca.pem \  --embed-certs=<span class="hljs-literal">true</span> \  --server=<span class="hljs-variable">$&#123;KUBE_APISERVER&#125;</span> \  --kubeconfig=kube-proxy.kubeconfig<span class="hljs-comment"># 设置客户端认证参数</span>kubectl config <span class="hljs-built_in">set</span>-credentials kube-proxy \  --client-certificate=kube-proxy.pem \  --client-key=kube-proxy-key.pem \  --embed-certs=<span class="hljs-literal">true</span> \  --kubeconfig=kube-proxy.kubeconfig<span class="hljs-comment"># 设置上下文参数</span>kubectl config <span class="hljs-built_in">set</span>-context default \  --cluster=kubernetes \  --user=kube-proxy \  --kubeconfig=kube-proxy.kubeconfig<span class="hljs-comment"># 设置默认上下文</span>kubectl config use-context default --kubeconfig=kube-proxy.kubeconfig<span class="hljs-comment"># 创建高级审计配置</span>cat &gt;&gt; audit-policy.yaml &lt;&lt;EOF<span class="hljs-comment"># Log all requests at the Metadata level.</span>apiVersion: audit.k8s.io/v1beta1kind: Policyrules:- level: MetadataEOF</code></pre></div><p>生成后文件如下</p><p><img src="https://cdn.oss.link/markdown/xk8uj.png" srcset="/img/loading.gif" alt="k8s certs"></p><h4 id="3-2、准备-systemd-配置"><a href="#3-2、准备-systemd-配置" class="headerlink" title="3.2、准备 systemd 配置"></a>3.2、准备 systemd 配置</h4><p>所有组件的 <code>systemd</code> 配置如下</p><h5 id="kube-apiserver-service"><a href="#kube-apiserver-service" class="headerlink" title="kube-apiserver.service"></a>kube-apiserver.service</h5><div class="hljs"><pre><code class="hljs sh">[Unit]Description=Kubernetes API ServerDocumentation=https://github.com/GoogleCloudPlatform/kubernetesAfter=network.targetAfter=etcd.service[Service]EnvironmentFile=-/etc/kubernetes/configEnvironmentFile=-/etc/kubernetes/apiserverUser=kubeExecStart=/usr/<span class="hljs-built_in">local</span>/bin/hyperkube apiserver \            <span class="hljs-variable">$KUBE_LOGTOSTDERR</span> \            <span class="hljs-variable">$KUBE_LOG_LEVEL</span> \            <span class="hljs-variable">$KUBE_ETCD_SERVERS</span> \            <span class="hljs-variable">$KUBE_API_ADDRESS</span> \            <span class="hljs-variable">$KUBE_API_PORT</span> \            <span class="hljs-variable">$KUBELET_PORT</span> \            <span class="hljs-variable">$KUBE_ALLOW_PRIV</span> \            <span class="hljs-variable">$KUBE_SERVICE_ADDRESSES</span> \            <span class="hljs-variable">$KUBE_ADMISSION_CONTROL</span> \            <span class="hljs-variable">$KUBE_API_ARGS</span>Restart=on-failureType=notifyLimitNOFILE=65536[Install]WantedBy=multi-user.target</code></pre></div><h5 id="kube-controller-manager-service"><a href="#kube-controller-manager-service" class="headerlink" title="kube-controller-manager.service"></a>kube-controller-manager.service</h5><div class="hljs"><pre><code class="hljs sh">[Unit]Description=Kubernetes Controller ManagerDocumentation=https://github.com/GoogleCloudPlatform/kubernetes[Service]EnvironmentFile=-/etc/kubernetes/configEnvironmentFile=-/etc/kubernetes/controller-managerUser=kubeExecStart=/usr/<span class="hljs-built_in">local</span>/bin/hyperkube controller-manager \            <span class="hljs-variable">$KUBE_LOGTOSTDERR</span> \            <span class="hljs-variable">$KUBE_LOG_LEVEL</span> \            <span class="hljs-variable">$KUBE_MASTER</span> \            <span class="hljs-variable">$KUBE_CONTROLLER_MANAGER_ARGS</span>Restart=on-failureLimitNOFILE=65536[Install]WantedBy=multi-user.target</code></pre></div><h5 id="kubelet-service"><a href="#kubelet-service" class="headerlink" title="kubelet.service"></a>kubelet.service</h5><div class="hljs"><pre><code class="hljs sh">[Unit]Description=Kubernetes Kubelet ServerDocumentation=https://github.com/GoogleCloudPlatform/kubernetesAfter=docker.serviceRequires=docker.service[Service]WorkingDirectory=/var/lib/kubeletEnvironmentFile=-/etc/kubernetes/configEnvironmentFile=-/etc/kubernetes/kubeletExecStart=/usr/<span class="hljs-built_in">local</span>/bin/hyperkube kubelet \            <span class="hljs-variable">$KUBE_LOGTOSTDERR</span> \            <span class="hljs-variable">$KUBE_LOG_LEVEL</span> \            <span class="hljs-variable">$KUBELET_API_SERVER</span> \            <span class="hljs-variable">$KUBELET_ADDRESS</span> \            <span class="hljs-variable">$KUBELET_PORT</span> \            <span class="hljs-variable">$KUBELET_HOSTNAME</span> \            <span class="hljs-variable">$KUBE_ALLOW_PRIV</span> \            <span class="hljs-variable">$KUBELET_ARGS</span>Restart=on-failureKillMode=process[Install]WantedBy=multi-user.target</code></pre></div><h5 id="kube-proxy-service"><a href="#kube-proxy-service" class="headerlink" title="kube-proxy.service"></a>kube-proxy.service</h5><div class="hljs"><pre><code class="hljs sh">[Unit]Description=Kubernetes Kube-Proxy ServerDocumentation=https://github.com/GoogleCloudPlatform/kubernetesAfter=network.target[Service]EnvironmentFile=-/etc/kubernetes/configEnvironmentFile=-/etc/kubernetes/proxyExecStart=/usr/<span class="hljs-built_in">local</span>/bin/hyperkube proxy \            <span class="hljs-variable">$KUBE_LOGTOSTDERR</span> \            <span class="hljs-variable">$KUBE_LOG_LEVEL</span> \            <span class="hljs-variable">$KUBE_MASTER</span> \            <span class="hljs-variable">$KUBE_PROXY_ARGS</span>Restart=on-failureLimitNOFILE=65536[Install]WantedBy=multi-user.target</code></pre></div><h5 id="kube-scheduler-service"><a href="#kube-scheduler-service" class="headerlink" title="kube-scheduler.service"></a>kube-scheduler.service</h5><div class="hljs"><pre><code class="hljs sh">[Unit]Description=Kubernetes Scheduler PluginDocumentation=https://github.com/GoogleCloudPlatform/kubernetes[Service]EnvironmentFile=-/etc/kubernetes/configEnvironmentFile=-/etc/kubernetes/schedulerUser=kubeExecStart=/usr/<span class="hljs-built_in">local</span>/bin/hyperkube scheduler \            <span class="hljs-variable">$KUBE_LOGTOSTDERR</span> \            <span class="hljs-variable">$KUBE_LOG_LEVEL</span> \            <span class="hljs-variable">$KUBE_MASTER</span> \            <span class="hljs-variable">$KUBE_SCHEDULER_ARGS</span>Restart=on-failureLimitNOFILE=65536[Install]WantedBy=multi-user.target</code></pre></div><h4 id="3-3、Master-节点配置"><a href="#3-3、Master-节点配置" class="headerlink" title="3.3、Master 节点配置"></a>3.3、Master 节点配置</h4><p>Master 节点主要会运行 3 各组件: <code>kube-apiserver</code>、<code>kube-controller-manager</code>、<code>kube-scheduler</code>，其中用到的配置文件如下</p><h5 id="config"><a href="#config" class="headerlink" title="config"></a>config</h5><p><strong>config 是一个通用配置文件，值得注意的是由于安装时对于 Node、Master 节点都会包含该文件，在 Node 节点上请注释掉 <code>KUBE_MASTER</code> 变量，因为 Node 节点需要做 HA，要连接本地的 6443 加密端口；而这个变量将会覆盖 <code>kubeconfig</code> 中指定的 <code>127.0.0.1:6443</code> 地址</strong></p><div class="hljs"><pre><code class="hljs sh"><span class="hljs-comment">###</span><span class="hljs-comment"># kubernetes system config</span><span class="hljs-comment">#</span><span class="hljs-comment"># The following values are used to configure various aspects of all</span><span class="hljs-comment"># kubernetes services, including</span><span class="hljs-comment">#</span><span class="hljs-comment">#   kube-apiserver.service</span><span class="hljs-comment">#   kube-controller-manager.service</span><span class="hljs-comment">#   kube-scheduler.service</span><span class="hljs-comment">#   kubelet.service</span><span class="hljs-comment">#   kube-proxy.service</span><span class="hljs-comment"># logging to stderr means we get it in the systemd journal</span>KUBE_LOGTOSTDERR=<span class="hljs-string">"--logtostderr=true"</span><span class="hljs-comment"># journal message level, 0 is debug</span>KUBE_LOG_LEVEL=<span class="hljs-string">"--v=2"</span><span class="hljs-comment"># Should this cluster be allowed to run privileged docker containers</span>KUBE_ALLOW_PRIV=<span class="hljs-string">"--allow-privileged=true"</span><span class="hljs-comment"># How the controller-manager, scheduler, and proxy find the apiserver</span>KUBE_MASTER=<span class="hljs-string">"--master=http://127.0.0.1:8080"</span></code></pre></div><h5 id="apiserver"><a href="#apiserver" class="headerlink" title="apiserver"></a>apiserver</h5><p>apiserver 配置相对于 1.8 略有变动，其中准入控制器(<code>admission control</code>)选项名称变为了 <code>--enable-admission-plugins</code>，控制器列表也有相应变化，这里采用官方推荐配置，具体请参考 <a href="https://kubernetes.io/docs/admin/admission-controllers/#is-there-a-recommended-set-of-admission-controllers-to-use" target="_blank" rel="noopener">官方文档</a></p><div class="hljs"><pre><code class="hljs sh"><span class="hljs-comment">###</span><span class="hljs-comment"># kubernetes system config</span><span class="hljs-comment">#</span><span class="hljs-comment"># The following values are used to configure the kube-apiserver</span><span class="hljs-comment">#</span><span class="hljs-comment"># The address on the local server to listen to.</span>KUBE_API_ADDRESS=<span class="hljs-string">"--advertise-address=192.168.1.61 --bind-address=192.168.1.61"</span><span class="hljs-comment"># The port on the local server to listen on.</span>KUBE_API_PORT=<span class="hljs-string">"--secure-port=6443"</span><span class="hljs-comment"># Port minions listen on</span><span class="hljs-comment"># KUBELET_PORT="--kubelet-port=10250"</span><span class="hljs-comment"># Comma separated list of nodes in the etcd cluster</span>KUBE_ETCD_SERVERS=<span class="hljs-string">"--etcd-servers=https://192.168.1.61:2379,https://192.168.1.62:2379,https://192.168.1.63:2379"</span><span class="hljs-comment"># Address range to use for services</span>KUBE_SERVICE_ADDRESSES=<span class="hljs-string">"--service-cluster-ip-range=10.254.0.0/16"</span><span class="hljs-comment"># default admission control policies</span>KUBE_ADMISSION_CONTROL=<span class="hljs-string">"--enable-admission-plugins=NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota,NodeRestriction"</span><span class="hljs-comment"># Add your own!</span>KUBE_API_ARGS=<span class="hljs-string">" --anonymous-auth=false \</span><span class="hljs-string">                --apiserver-count=3 \</span><span class="hljs-string">                --audit-log-maxage=30 \</span><span class="hljs-string">                --audit-log-maxbackup=3 \</span><span class="hljs-string">                --audit-log-maxsize=100 \</span><span class="hljs-string">                --audit-log-path=/var/log/kube-audit/audit.log \</span><span class="hljs-string">                --audit-policy-file=/etc/kubernetes/audit-policy.yaml \</span><span class="hljs-string">                --authorization-mode=Node,RBAC \</span><span class="hljs-string">                --client-ca-file=/etc/kubernetes/ssl/k8s-root-ca.pem \</span><span class="hljs-string">                --enable-bootstrap-token-auth \</span><span class="hljs-string">                --enable-garbage-collector \</span><span class="hljs-string">                --enable-logs-handler \</span><span class="hljs-string">                --enable-swagger-ui \</span><span class="hljs-string">                --etcd-cafile=/etc/etcd/ssl/etcd-root-ca.pem \</span><span class="hljs-string">                --etcd-certfile=/etc/etcd/ssl/etcd.pem \</span><span class="hljs-string">                --etcd-keyfile=/etc/etcd/ssl/etcd-key.pem \</span><span class="hljs-string">                --etcd-compaction-interval=5m0s \</span><span class="hljs-string">                --etcd-count-metric-poll-period=1m0s \</span><span class="hljs-string">                --event-ttl=48h0m0s \</span><span class="hljs-string">                --kubelet-https=true \</span><span class="hljs-string">                --kubelet-timeout=3s \</span><span class="hljs-string">                --log-flush-frequency=5s \</span><span class="hljs-string">                --token-auth-file=/etc/kubernetes/token.csv \</span><span class="hljs-string">                --tls-cert-file=/etc/kubernetes/ssl/kube-apiserver.pem \</span><span class="hljs-string">                --tls-private-key-file=/etc/kubernetes/ssl/kube-apiserver-key.pem \</span><span class="hljs-string">                --service-node-port-range=30000-50000 \</span><span class="hljs-string">                --service-account-key-file=/etc/kubernetes/ssl/k8s-root-ca.pem \</span><span class="hljs-string">                --storage-backend=etcd3 \</span><span class="hljs-string">                --enable-swagger-ui=true"</span></code></pre></div><h5 id="controller-manager"><a href="#controller-manager" class="headerlink" title="controller-manager"></a>controller-manager</h5><p>controller manager 配置默认开启了证书轮换能力用于自动签署 kueblet 证书，并且证书时间也设置了 10 年，可自行调整；增加了 <code>--controllers</code> 选项以指定开启全部控制器</p><div class="hljs"><pre><code class="hljs sh"><span class="hljs-comment">###</span><span class="hljs-comment"># The following values are used to configure the kubernetes controller-manager</span><span class="hljs-comment"># defaults from config and apiserver should be adequate</span><span class="hljs-comment"># Add your own!</span>KUBE_CONTROLLER_MANAGER_ARGS=<span class="hljs-string">"  --bind-address=0.0.0.0 \</span><span class="hljs-string">                                --cluster-name=kubernetes \</span><span class="hljs-string">                                --cluster-signing-cert-file=/etc/kubernetes/ssl/k8s-root-ca.pem \</span><span class="hljs-string">                                --cluster-signing-key-file=/etc/kubernetes/ssl/k8s-root-ca-key.pem \</span><span class="hljs-string">                                --controllers=*,bootstrapsigner,tokencleaner \</span><span class="hljs-string">                                --deployment-controller-sync-period=10s \</span><span class="hljs-string">                                --experimental-cluster-signing-duration=86700h0m0s \</span><span class="hljs-string">                                --leader-elect=true \</span><span class="hljs-string">                                --node-monitor-grace-period=40s \</span><span class="hljs-string">                                --node-monitor-period=5s \</span><span class="hljs-string">                                --pod-eviction-timeout=5m0s \</span><span class="hljs-string">                                --terminated-pod-gc-threshold=50 \</span><span class="hljs-string">                                --root-ca-file=/etc/kubernetes/ssl/k8s-root-ca.pem \</span><span class="hljs-string">                                --service-account-private-key-file=/etc/kubernetes/ssl/k8s-root-ca-key.pem \</span><span class="hljs-string">                                --feature-gates=RotateKubeletServerCertificate=true"</span></code></pre></div><h5 id="scheduler"><a href="#scheduler" class="headerlink" title="scheduler"></a>scheduler</h5><div class="hljs"><pre><code class="hljs sh"><span class="hljs-comment">###</span><span class="hljs-comment"># kubernetes scheduler config</span><span class="hljs-comment"># default config should be adequate</span><span class="hljs-comment"># Add your own!</span>KUBE_SCHEDULER_ARGS=<span class="hljs-string">"   --address=0.0.0.0 \</span><span class="hljs-string">                        --leader-elect=true \</span><span class="hljs-string">                        --algorithm-provider=DefaultProvider"</span></code></pre></div><h4 id="3-4、Node-节点配置"><a href="#3-4、Node-节点配置" class="headerlink" title="3.4、Node 节点配置"></a>3.4、Node 节点配置</h4><p>Node 节点上主要有 <code>kubelet</code>、<code>kube-proxy</code> 组件，用到的配置如下</p><h5 id="kubelet"><a href="#kubelet" class="headerlink" title="kubelet"></a>kubelet</h5><p>kubeket 默认也开启了证书轮换能力以保证自动续签相关证书，同时增加了 <code>--node-labels</code> 选项为 node 打一个标签，关于这个标签最后部分会有讨论，<strong>如果在 master 上启动 kubelet，请将 <code>node-role.kubernetes.io/k8s-node=true</code> 修改为 <code>node-role.kubernetes.io/k8s-master=true</code></strong></p><div class="hljs"><pre><code class="hljs sh"><span class="hljs-comment">###</span><span class="hljs-comment"># kubernetes kubelet (minion) config</span><span class="hljs-comment"># The address for the info server to serve on (set to 0.0.0.0 or "" for all interfaces)</span>KUBELET_ADDRESS=<span class="hljs-string">"--node-ip=192.168.1.61"</span><span class="hljs-comment"># The port for the info server to serve on</span><span class="hljs-comment"># KUBELET_PORT="--port=10250"</span><span class="hljs-comment"># You may leave this blank to use the actual hostname</span>KUBELET_HOSTNAME=<span class="hljs-string">"--hostname-override=k1.node"</span><span class="hljs-comment"># location of the api-server</span><span class="hljs-comment"># KUBELET_API_SERVER=""</span><span class="hljs-comment"># Add your own!</span>KUBELET_ARGS=<span class="hljs-string">"  --bootstrap-kubeconfig=/etc/kubernetes/bootstrap.kubeconfig \</span><span class="hljs-string">                --cert-dir=/etc/kubernetes/ssl \</span><span class="hljs-string">                --cgroup-driver=cgroupfs \</span><span class="hljs-string">                --cluster-dns=10.254.0.2 \</span><span class="hljs-string">                --cluster-domain=cluster.local. \</span><span class="hljs-string">                --fail-swap-on=false \</span><span class="hljs-string">                --feature-gates=RotateKubeletClientCertificate=true,RotateKubeletServerCertificate=true \</span><span class="hljs-string">                --node-labels=node-role.kubernetes.io/k8s-node=true \</span><span class="hljs-string">                --image-gc-high-threshold=70 \</span><span class="hljs-string">                --image-gc-low-threshold=50 \</span><span class="hljs-string">                --kube-reserved=cpu=500m,memory=512Mi,ephemeral-storage=1Gi \</span><span class="hljs-string">                --kubeconfig=/etc/kubernetes/kubelet.kubeconfig \</span><span class="hljs-string">                --system-reserved=cpu=1000m,memory=1024Mi,ephemeral-storage=1Gi \</span><span class="hljs-string">                --serialize-image-pulls=false \</span><span class="hljs-string">                --sync-frequency=30s \</span><span class="hljs-string">                --pod-infra-container-image=k8s.gcr.io/pause-amd64:3.0 \</span><span class="hljs-string">                --resolv-conf=/etc/resolv.conf \</span><span class="hljs-string">                --rotate-certificates"</span></code></pre></div><h5 id="proxy"><a href="#proxy" class="headerlink" title="proxy"></a>proxy</h5><div class="hljs"><pre><code class="hljs sh"><span class="hljs-comment">###</span><span class="hljs-comment"># kubernetes proxy config</span><span class="hljs-comment"># default config should be adequate</span><span class="hljs-comment"># Add your own!</span>KUBE_PROXY_ARGS=<span class="hljs-string">"--bind-address=0.0.0.0 \</span><span class="hljs-string">                 --hostname-override=k1.node \</span><span class="hljs-string">                 --kubeconfig=/etc/kubernetes/kube-proxy.kubeconfig \</span><span class="hljs-string">                 --cluster-cidr=10.254.0.0/16"</span></code></pre></div><h4 id="3-5、安装集群组件"><a href="#3-5、安装集群组件" class="headerlink" title="3.5、安装集群组件"></a>3.5、安装集群组件</h4><p>上面已经准备好了相关配置文件，接下来将这些配置文件组织成如下目录结构以便后续脚本安装</p><div class="hljs"><pre><code class="hljs sh">k8s├── conf│   ├── apiserver│   ├── audit-policy.yaml│   ├── bootstrap.kubeconfig│   ├── config│   ├── controller-manager│   ├── kubelet│   ├── kube-proxy.kubeconfig│   ├── proxy│   ├── scheduler│   ├── ssl│   │   ├── admin.csr│   │   ├── admin-csr.json│   │   ├── admin-key.pem│   │   ├── admin.pem│   │   ├── k8s-gencert.json│   │   ├── k8s-root-ca.csr│   │   ├── k8s-root-ca-csr.json│   │   ├── k8s-root-ca-key.pem│   │   ├── k8s-root-ca.pem│   │   ├── kube-apiserver.csr│   │   ├── kube-apiserver-csr.json│   │   ├── kube-apiserver-key.pem│   │   ├── kube-apiserver.pem│   │   ├── kube-proxy.csr│   │   ├── kube-proxy-csr.json│   │   ├── kube-proxy-key.pem│   │   └── kube-proxy.pem│   └── token.csv├── hyperkube_1.10.1├── install.sh└── systemd    ├── kube-apiserver.service    ├── kube-controller-manager.service    ├── kubelet.service    ├── kube-proxy.service    └── kube-scheduler.service</code></pre></div><p>其中 <code>install.sh</code> 内容如下</p><div class="hljs"><pre><code class="hljs sh"><span class="hljs-meta">#!/bin/bash</span><span class="hljs-built_in">set</span> -eKUBE_VERSION=<span class="hljs-string">"1.10.1"</span><span class="hljs-keyword">function</span> <span class="hljs-function"><span class="hljs-title">download_k8s</span></span>()&#123;    <span class="hljs-keyword">if</span> [ ! -f <span class="hljs-string">"hyperkube_<span class="hljs-variable">$&#123;KUBE_VERSION&#125;</span>"</span> ]; <span class="hljs-keyword">then</span>        wget https://storage.googleapis.com/kubernetes-release/release/v<span class="hljs-variable">$&#123;KUBE_VERSION&#125;</span>/bin/linux/amd64/hyperkube -O hyperkube_<span class="hljs-variable">$&#123;KUBE_VERSION&#125;</span>        chmod +x hyperkube_<span class="hljs-variable">$&#123;KUBE_VERSION&#125;</span>    <span class="hljs-keyword">fi</span>&#125;<span class="hljs-keyword">function</span> <span class="hljs-function"><span class="hljs-title">preinstall</span></span>()&#123;    getent group kube &gt;/dev/null || groupadd -r kube    getent passwd kube &gt;/dev/null || useradd -r -g kube -d / -s /sbin/nologin -c <span class="hljs-string">"Kubernetes user"</span> kube&#125;<span class="hljs-keyword">function</span> <span class="hljs-function"><span class="hljs-title">install_k8s</span></span>()&#123;    <span class="hljs-built_in">echo</span> -e <span class="hljs-string">"\033[32mINFO: Copy hyperkube...\033[0m"</span>    cp hyperkube_<span class="hljs-variable">$&#123;KUBE_VERSION&#125;</span> /usr/<span class="hljs-built_in">local</span>/bin/hyperkube    <span class="hljs-built_in">echo</span> -e <span class="hljs-string">"\033[32mINFO: Create symbolic link...\033[0m"</span>    ln -sf /usr/<span class="hljs-built_in">local</span>/bin/hyperkube /usr/<span class="hljs-built_in">local</span>/bin/kubectl    <span class="hljs-built_in">echo</span> -e <span class="hljs-string">"\033[32mINFO: Copy kubernetes config...\033[0m"</span>    cp -r conf /etc/kubernetes    <span class="hljs-keyword">if</span> [ -d <span class="hljs-string">"/etc/kubernetes/ssl"</span> ]; <span class="hljs-keyword">then</span>        chown -R kube:kube /etc/kubernetes/ssl    <span class="hljs-keyword">fi</span>    <span class="hljs-built_in">echo</span> -e <span class="hljs-string">"\033[32mINFO: Copy kubernetes systemd config...\033[0m"</span>    cp systemd/*.service /lib/systemd/system    systemctl daemon-reload&#125;<span class="hljs-keyword">function</span> <span class="hljs-function"><span class="hljs-title">postinstall</span></span>()&#123;    <span class="hljs-keyword">if</span> [ ! -d <span class="hljs-string">"/var/log/kube-audit"</span> ]; <span class="hljs-keyword">then</span>        mkdir /var/<span class="hljs-built_in">log</span>/kube-audit    <span class="hljs-keyword">fi</span>    <span class="hljs-keyword">if</span> [ ! -d <span class="hljs-string">"/var/lib/kubelet"</span> ]; <span class="hljs-keyword">then</span>        mkdir /var/lib/kubelet    <span class="hljs-keyword">fi</span>    <span class="hljs-keyword">if</span> [ ! -d <span class="hljs-string">"/usr/libexec"</span> ]; <span class="hljs-keyword">then</span>        mkdir /usr/libexec    <span class="hljs-keyword">fi</span>    chown -R kube:kube /var/<span class="hljs-built_in">log</span>/kube-audit /var/lib/kubelet /usr/libexec&#125;download_k8spreinstallinstall_k8spostinstall</code></pre></div><p><strong>脚本解释如下:</strong></p><ul><li>download_k8s: 下载 hyperkube 二进制文件</li><li>preinstall: 安装前处理，同 etcd 一样创建 kube 普通用户指定家目录、shell 等</li><li>install_k8s: 复制 hyperkube 到安装目录，为 kubectl 创建软连接(为啥创建软连接就能执行请自行阅读 <a href="https://github.com/kubernetes/kubernetes/blob/cce67ed8e7d461657d350a1cdd55791d1637fc43/cmd/hyperkube/main.go#L69" target="_blank" rel="noopener">源码</a>)，复制相关配置到对应目录，并处理权限</li><li>postinstall: 收尾工作，创建日志目录等，并处理权限</li></ul><p>最后执行此脚本安装即可，<strong>此外，应确保每个节点安装了 <code>ipset</code>、<code>conntrack</code> 两个包，因为 kube-proxy 组件会使用其处理 iptables 规则等</strong></p><h3 id="四、启动-Kubernetes-Master-节点"><a href="#四、启动-Kubernetes-Master-节点" class="headerlink" title="四、启动 Kubernetes Master 节点"></a>四、启动 Kubernetes Master 节点</h3><p>对于 <code>master</code> 节点启动无需做过多处理，多个 <code>master</code> 只要保证 <code>apiserver</code> 等配置中的 ip 地址监听没问题后直接启动即可</p><div class="hljs"><pre><code class="hljs sh">systemctl daemon-reloadsystemctl start kube-apiserversystemctl start kube-controller-managersystemctl start kube-schedulersystemctl <span class="hljs-built_in">enable</span> kube-apiserversystemctl <span class="hljs-built_in">enable</span> kube-controller-managersystemctl <span class="hljs-built_in">enable</span> kube-scheduler</code></pre></div><p>成功后截图如下</p><p><img src="https://cdn.oss.link/markdown/lqur1.png" srcset="/img/loading.gif" alt="Master success"></p><h3 id="五、启动-Kubernetes-Node-节点"><a href="#五、启动-Kubernetes-Node-节点" class="headerlink" title="五、启动 Kubernetes Node 节点"></a>五、启动 Kubernetes Node 节点</h3><p>由于 HA 等功能需要，对于 Node 需要做一些处理才能启动，主要有以下两个地方需要处理</p><h4 id="5-1、nginx-proxy"><a href="#5-1、nginx-proxy" class="headerlink" title="5.1、nginx-proxy"></a>5.1、nginx-proxy</h4><p>在启动 <code>kubelet</code>、<code>kube-proxy</code> 服务之前，需要在本地启动 <code>nginx</code> 来 tcp 负载均衡 <code>apiserver</code> 6443 端口，<code>nginx-proxy</code> 使用 <code>docker</code> + <code>systemd</code> 启动，配置如下</p><p><strong>注意: 对于在 master 节点启动 kubelet 来说，不需要 nginx 做负载均衡；可以跳过此步骤，并修改 <code>kubelet.kubeconfig</code>、<code>kube-proxy.kubeconfig</code> 中的 apiserver 地址为当前 master ip 6443 端口即可</strong></p><ul><li>nginx-proxy.service</li></ul><div class="hljs"><pre><code class="hljs sh">[Unit]Description=kubernetes apiserver docker wrapperWants=docker.socketAfter=docker.service[Service]User=rootPermissionsStartOnly=<span class="hljs-literal">true</span>ExecStart=/usr/bin/docker run -p 127.0.0.1:6443:6443 \                              -v /etc/nginx:/etc/nginx \                              --name nginx-proxy \                              --net=host \                              --restart=on-failure:5 \                              --memory=512M \                              nginx:1.13.12-alpineExecStartPre=-/usr/bin/docker rm -f nginx-proxyExecStop=/usr/bin/docker stop nginx-proxyRestart=alwaysRestartSec=15sTimeoutStartSec=30s[Install]WantedBy=multi-user.target</code></pre></div><ul><li>nginx.conf</li></ul><div class="hljs"><pre><code class="hljs sh">error_log stderr notice;worker_processes auto;events &#123;        multi_accept on;        use epoll;        worker_connections 1024;&#125;stream &#123;    upstream kube_apiserver &#123;        least_conn;        server 192.168.1.61:6443;        server 192.168.1.62:6443;        server 192.168.1.63:6443;    &#125;    server &#123;        listen        0.0.0.0:6443;        proxy_pass    kube_apiserver;        proxy_timeout 10m;        proxy_connect_timeout 1s;    &#125;&#125;</code></pre></div><p><strong>启动 apiserver 的本地负载均衡</strong></p><div class="hljs"><pre><code class="hljs sh">mkdir /etc/nginxcp nginx.conf /etc/nginxcp nginx-proxy.service /lib/systemd/systemsystemctl daemon-reloadsystemctl start nginx-proxysystemctl <span class="hljs-built_in">enable</span> nginx-proxy</code></pre></div><h4 id="5-2、TLS-bootstrapping"><a href="#5-2、TLS-bootstrapping" class="headerlink" title="5.2、TLS bootstrapping"></a>5.2、TLS bootstrapping</h4><p>创建好 <code>nginx-proxy</code> 后不要忘记为 <code>TLS Bootstrap</code> 创建相应的 <code>RBAC</code> 规则，这些规则能实现证自动签署 <code>TLS Bootstrap</code> 发出的 <code>CSR</code> 请求，从而实现证书轮换(创建一次即可)；详情请参考 <a href="https://mritd.me/2018/01/07/kubernetes-tls-bootstrapping-note/" target="_blank" rel="noopener">Kubernetes TLS bootstrapping 那点事</a></p><ul><li>tls-bootstrapping-clusterrole.yaml(与 1.8 一样)</li></ul><div class="hljs"><pre><code class="hljs yaml"><span class="hljs-comment"># A ClusterRole which instructs the CSR approver to approve a node requesting a</span><span class="hljs-comment"># serving cert matching its client cert.</span><span class="hljs-attr">kind:</span> <span class="hljs-string">ClusterRole</span><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">rbac.authorization.k8s.io/v1</span><span class="hljs-attr">metadata:</span>  <span class="hljs-attr">name:</span> <span class="hljs-string">system:certificates.k8s.io:certificatesigningrequests:selfnodeserver</span><span class="hljs-attr">rules:</span><span class="hljs-bullet">-</span> <span class="hljs-attr">apiGroups:</span> <span class="hljs-string">["certificates.k8s.io"]</span>  <span class="hljs-attr">resources:</span> <span class="hljs-string">["certificatesigningrequests/selfnodeserver"]</span>  <span class="hljs-attr">verbs:</span> <span class="hljs-string">["create"]</span></code></pre></div><p><strong>在 master 执行创建</strong></p><div class="hljs"><pre><code class="hljs sh"><span class="hljs-comment"># 给与 kubelet-bootstrap 用户进行 node-bootstrapper 的权限</span>kubectl create clusterrolebinding kubelet-bootstrap \    --clusterrole=system:node-bootstrapper \    --user=kubelet-bootstrapkubectl create -f tls-bootstrapping-clusterrole.yaml<span class="hljs-comment"># 自动批准 system:bootstrappers 组用户 TLS bootstrapping 首次申请证书的 CSR 请求</span>kubectl create clusterrolebinding node-client-auto-approve-csr \        --clusterrole=system:certificates.k8s.io:certificatesigningrequests:nodeclient \        --group=system:bootstrappers<span class="hljs-comment"># 自动批准 system:nodes 组用户更新 kubelet 自身与 apiserver 通讯证书的 CSR 请求</span>kubectl create clusterrolebinding node-client-auto-renew-crt \        --clusterrole=system:certificates.k8s.io:certificatesigningrequests:selfnodeclient \        --group=system:nodes<span class="hljs-comment"># 自动批准 system:nodes 组用户更新 kubelet 10250 api 端口证书的 CSR 请求</span>kubectl create clusterrolebinding node-server-auto-renew-crt \        --clusterrole=system:certificates.k8s.io:certificatesigningrequests:selfnodeserver \        --group=system:nodes</code></pre></div><h4 id="5-3、执行启动"><a href="#5-3、执行启动" class="headerlink" title="5.3、执行启动"></a>5.3、执行启动</h4><p>多节点部署时先启动好 <code>nginx-proxy</code>，然后修改好相应配置的 ip 地址等配置，最终直接启动即可(master 上启动 kubelet 不要忘了修改 kubeconfig 中的 apiserver 地址，还有对应的 kubelet 的 node label)</p><div class="hljs"><pre><code class="hljs sh">systemctl daemon-reloadsystemctl start kubeletsystemctl start kube-proxysystemctl <span class="hljs-built_in">enable</span> kubeletsystemctl <span class="hljs-built_in">enable</span> kube-proxy</code></pre></div><p>最后启动成功后如下</p><p><img src="https://cdn.oss.link/markdown/r4s34.png" srcset="/img/loading.gif" alt="cluster started"></p><h3 id="五、安装-Calico"><a href="#五、安装-Calico" class="headerlink" title="五、安装 Calico"></a>五、安装 Calico</h3><p>Calico 安装仍然延续以前的方案，使用 Daemonset 安装 cni 组件，使用 systemd 控制 calico-node 以确保 calico-node 能正确的拿到主机名等</p><h4 id="5-1、修改-Calico-配置"><a href="#5-1、修改-Calico-配置" class="headerlink" title="5.1、修改 Calico 配置"></a>5.1、修改 Calico 配置</h4><div class="hljs"><pre><code class="hljs sh">wget https://docs.projectcalico.org/v3.1/getting-started/kubernetes/installation/hosted/calico.yaml -O calico.example.yamlETCD_CERT=`cat /etc/etcd/ssl/etcd.pem | base64 | tr -d <span class="hljs-string">'\n'</span>`ETCD_KEY=`cat /etc/etcd/ssl/etcd-key.pem | base64 | tr -d <span class="hljs-string">'\n'</span>`ETCD_CA=`cat /etc/etcd/ssl/etcd-root-ca.pem | base64 | tr -d <span class="hljs-string">'\n'</span>`ETCD_ENDPOINTS=<span class="hljs-string">"https://192.168.1.61:2379,https://192.168.1.62:2379,https://192.168.1.63:2379"</span>cp calico.example.yaml calico.yamlsed -i <span class="hljs-string">"s@.*etcd_endpoints:.*@\ \ etcd_endpoints:\ \"<span class="hljs-variable">$&#123;ETCD_ENDPOINTS&#125;</span>\"@gi"</span> calico.yamlsed -i <span class="hljs-string">"s@.*etcd-cert:.*@\ \ etcd-cert:\ <span class="hljs-variable">$&#123;ETCD_CERT&#125;</span>@gi"</span> calico.yamlsed -i <span class="hljs-string">"s@.*etcd-key:.*@\ \ etcd-key:\ <span class="hljs-variable">$&#123;ETCD_KEY&#125;</span>@gi"</span> calico.yamlsed -i <span class="hljs-string">"s@.*etcd-ca:.*@\ \ etcd-ca:\ <span class="hljs-variable">$&#123;ETCD_CA&#125;</span>@gi"</span> calico.yamlsed -i <span class="hljs-string">'s@.*etcd_ca:.*@\ \ etcd_ca:\ "/calico-secrets/etcd-ca"@gi'</span> calico.yamlsed -i <span class="hljs-string">'s@.*etcd_cert:.*@\ \ etcd_cert:\ "/calico-secrets/etcd-cert"@gi'</span> calico.yamlsed -i <span class="hljs-string">'s@.*etcd_key:.*@\ \ etcd_key:\ "/calico-secrets/etcd-key"@gi'</span> calico.yaml<span class="hljs-comment"># 注释掉 calico-node 部分(由 Systemd 接管)</span>sed -i <span class="hljs-string">'123,219s@.*@#&amp;@gi'</span> calico.yaml</code></pre></div><h4 id="5-2、创建-Systemd-文件"><a href="#5-2、创建-Systemd-文件" class="headerlink" title="5.2、创建 Systemd 文件"></a>5.2、创建 Systemd 文件</h4><p><strong>注意: 创建 systemd service 配置文件要在每个节点上都执行</strong></p><div class="hljs"><pre><code class="hljs sh">K8S_MASTER_IP=<span class="hljs-string">"192.168.1.61"</span>HOSTNAME=`cat /etc/hostname`ETCD_ENDPOINTS=<span class="hljs-string">"https://192.168.1.61:2379,https://192.168.1.62:2379,https://192.168.1.63:2379"</span>cat &gt; /lib/systemd/system/calico-node.service &lt;&lt;EOF[Unit]Description=calico nodeAfter=docker.serviceRequires=docker.service[Service]User=rootEnvironment=ETCD_ENDPOINTS=<span class="hljs-variable">$&#123;ETCD_ENDPOINTS&#125;</span>PermissionsStartOnly=<span class="hljs-literal">true</span>ExecStart=/usr/bin/docker run   --net=host --privileged --name=calico-node \\                                -e ETCD_ENDPOINTS=\<span class="hljs-variable">$&#123;ETCD_ENDPOINTS&#125;</span> \\                                -e ETCD_CA_CERT_FILE=/etc/etcd/ssl/etcd-root-ca.pem \\                                -e ETCD_CERT_FILE=/etc/etcd/ssl/etcd.pem \\                                -e ETCD_KEY_FILE=/etc/etcd/ssl/etcd-key.pem \\                                -e NODENAME=<span class="hljs-variable">$&#123;HOSTNAME&#125;</span> \\                                -e IP= \\                                -e IP_AUTODETECTION_METHOD=can-reach=<span class="hljs-variable">$&#123;K8S_MASTER_IP&#125;</span> \\                                -e AS=64512 \\                                -e CLUSTER_TYPE=k8s,bgp \\                                -e CALICO_IPV4POOL_CIDR=10.20.0.0/16 \\                                -e CALICO_IPV4POOL_IPIP=always \\                                -e CALICO_LIBNETWORK_ENABLED=<span class="hljs-literal">true</span> \\                                -e CALICO_NETWORKING_BACKEND=bird \\                                -e CALICO_DISABLE_FILE_LOGGING=<span class="hljs-literal">true</span> \\                                -e FELIX_IPV6SUPPORT=<span class="hljs-literal">false</span> \\                                -e FELIX_DEFAULTENDPOINTTOHOSTACTION=ACCEPT \\                                -e FELIX_LOGSEVERITYSCREEN=info \\                                -e FELIX_IPINIPMTU=1440 \\                                -e FELIX_HEALTHENABLED=<span class="hljs-literal">true</span> \\                                -e CALICO_K8S_NODE_REF=<span class="hljs-variable">$&#123;HOSTNAME&#125;</span> \\                                -v /etc/calico/etcd-root-ca.pem:/etc/etcd/ssl/etcd-root-ca.pem \\                                -v /etc/calico/etcd.pem:/etc/etcd/ssl/etcd.pem \\                                -v /etc/calico/etcd-key.pem:/etc/etcd/ssl/etcd-key.pem \\                                -v /lib/modules:/lib/modules \\                                -v /var/lib/calico:/var/lib/calico \\                                -v /var/run/calico:/var/run/calico \\                                quay.io/calico/node:v3.1.0ExecStop=/usr/bin/docker rm -f calico-nodeRestart=alwaysRestartSec=10[Install]WantedBy=multi-user.targetEOF</code></pre></div><p><strong>对于以上脚本中的 <code>K8S_MASTER_IP</code> 变量，只需要填写一个 master ip 即可，这个变量用于 calico 自动选择 IP 使用；在宿主机有多张网卡的情况下，calcio node 会自动获取一个 IP，获取原则就是尝试是否能够联通这个 master ip</strong></p><p>由于 calico 需要使用 etcd 存储数据，所以需要复制 etcd 证书到相关目录，<strong><code>/etc/calico</code> 需要在每个节点都有</strong></p><div class="hljs"><pre><code class="hljs sh">cp -r /etc/etcd/ssl /etc/calico</code></pre></div><h4 id="5-3、修改-kubelet-配置"><a href="#5-3、修改-kubelet-配置" class="headerlink" title="5.3、修改 kubelet 配置"></a>5.3、修改 kubelet 配置</h4><p>使用 Calico 后需要修改 kubelet 配置增加 CNI 设置(<code>--network-plugin=cni</code>)，修改后配置如下</p><div class="hljs"><pre><code class="hljs sh"><span class="hljs-comment">###</span><span class="hljs-comment"># kubernetes kubelet (minion) config</span><span class="hljs-comment"># The address for the info server to serve on (set to 0.0.0.0 or "" for all interfaces)</span>KUBELET_ADDRESS=<span class="hljs-string">"--node-ip=192.168.1.61"</span><span class="hljs-comment"># The port for the info server to serve on</span><span class="hljs-comment"># KUBELET_PORT="--port=10250"</span><span class="hljs-comment"># You may leave this blank to use the actual hostname</span>KUBELET_HOSTNAME=<span class="hljs-string">"--hostname-override=k1.node"</span><span class="hljs-comment"># location of the api-server</span><span class="hljs-comment"># KUBELET_API_SERVER=""</span><span class="hljs-comment"># Add your own!</span>KUBELET_ARGS=<span class="hljs-string">"  --bootstrap-kubeconfig=/etc/kubernetes/bootstrap.kubeconfig \</span><span class="hljs-string">                --cert-dir=/etc/kubernetes/ssl \</span><span class="hljs-string">                --cgroup-driver=cgroupfs \</span><span class="hljs-string">                --network-plugin=cni \</span><span class="hljs-string">                --cluster-dns=10.254.0.2 \</span><span class="hljs-string">                --cluster-domain=cluster.local. \</span><span class="hljs-string">                --fail-swap-on=false \</span><span class="hljs-string">                --feature-gates=RotateKubeletClientCertificate=true,RotateKubeletServerCertificate=true \</span><span class="hljs-string">                --node-labels=node-role.kubernetes.io/k8s-master=true \</span><span class="hljs-string">                --image-gc-high-threshold=70 \</span><span class="hljs-string">                --image-gc-low-threshold=50 \</span><span class="hljs-string">                --kube-reserved=cpu=500m,memory=512Mi,ephemeral-storage=1Gi \</span><span class="hljs-string">                --kubeconfig=/etc/kubernetes/kubelet.kubeconfig \</span><span class="hljs-string">                --system-reserved=cpu=1000m,memory=1024Mi,ephemeral-storage=1Gi \</span><span class="hljs-string">                --serialize-image-pulls=false \</span><span class="hljs-string">                --sync-frequency=30s \</span><span class="hljs-string">                --pod-infra-container-image=k8s.gcr.io/pause-amd64:3.0 \</span><span class="hljs-string">                --resolv-conf=/etc/resolv.conf \</span><span class="hljs-string">                --rotate-certificates"</span></code></pre></div><h4 id="5-4、创建-Calico-Daemonset"><a href="#5-4、创建-Calico-Daemonset" class="headerlink" title="5.4、创建 Calico Daemonset"></a>5.4、创建 Calico Daemonset</h4><div class="hljs"><pre><code class="hljs sh"><span class="hljs-comment"># 先创建 RBAC</span>kubectl apply -f \https://docs.projectcalico.org/v3.1/getting-started/kubernetes/installation/rbac.yaml<span class="hljs-comment"># 再创建 Calico Daemonset</span>kubectl create -f calico.yaml</code></pre></div><h4 id="5-5、启动-Calico-Node"><a href="#5-5、启动-Calico-Node" class="headerlink" title="5.5、启动 Calico Node"></a>5.5、启动 Calico Node</h4><div class="hljs"><pre><code class="hljs sh">systemctl daemon-reloadsystemctl restart calico-nodesystemctl <span class="hljs-built_in">enable</span> calico-node<span class="hljs-comment"># 等待 20s 拉取镜像</span>sleep 20systemctl restart kubelet</code></pre></div><h4 id="5-6、测试网络"><a href="#5-6、测试网络" class="headerlink" title="5.6、测试网络"></a>5.6、测试网络</h4><p>网络测试与其他几篇文章一样，创建几个 pod 测试即可</p><div class="hljs"><pre><code class="hljs sh"><span class="hljs-comment"># 创建 deployment</span>cat &lt;&lt; EOF &gt;&gt; demo.deploy.ymlapiVersion: apps/v1kind: Deploymentmetadata:  name: demo-deploymentspec:  replicas: 5  selector:    matchLabels:      app: demo  template:    metadata:      labels:        app: demo    spec:      containers:      - name: demo        image: mritd/demo        imagePullPolicy: IfNotPresent        ports:        - containerPort: 80EOFkubectl create -f demo.deploy.yml</code></pre></div><p>测试结果如图所示</p><p><img src="https://cdn.oss.link/markdown/u9j3v.png" srcset="/img/loading.gif" alt="test calico"></p><h3 id="六、部署集群-DNS"><a href="#六、部署集群-DNS" class="headerlink" title="六、部署集群 DNS"></a>六、部署集群 DNS</h3><h4 id="6-1、部署-CoreDNS"><a href="#6-1、部署-CoreDNS" class="headerlink" title="6.1、部署 CoreDNS"></a>6.1、部署 CoreDNS</h4><p>CoreDNS 给出了标准的 deployment 配置，如下</p><ul><li>coredns.yaml.sed</li></ul><div class="hljs"><pre><code class="hljs sh">apiVersion: v1kind: ServiceAccountmetadata:  name: coredns  namespace: kube-system---apiVersion: rbac.authorization.k8s.io/v1beta1kind: ClusterRolemetadata:  labels:    kubernetes.io/bootstrapping: rbac-defaults  name: system:corednsrules:- apiGroups:  - <span class="hljs-string">""</span>  resources:  - endpoints  - services  - pods  - namespaces  verbs:  - list  - watch---apiVersion: rbac.authorization.k8s.io/v1beta1kind: ClusterRoleBindingmetadata:  annotations:    rbac.authorization.kubernetes.io/autoupdate: <span class="hljs-string">"true"</span>  labels:    kubernetes.io/bootstrapping: rbac-defaults  name: system:corednsroleRef:  apiGroup: rbac.authorization.k8s.io  kind: ClusterRole  name: system:corednssubjects:- kind: ServiceAccount  name: coredns  namespace: kube-system---apiVersion: v1kind: ConfigMapmetadata:  name: coredns  namespace: kube-systemdata:  Corefile: |    .:53 &#123;        errors        health        kubernetes CLUSTER_DOMAIN REVERSE_CIDRS &#123;          pods insecure          upstream          fallthrough <span class="hljs-keyword">in</span>-addr.arpa ip6.arpa        &#125;        prometheus :9153        proxy . /etc/resolv.conf        cache 30    &#125;---apiVersion: extensions/v1beta1kind: Deploymentmetadata:  name: coredns  namespace: kube-system  labels:    k8s-app: kube-dns    kubernetes.io/name: <span class="hljs-string">"CoreDNS"</span>spec:  replicas: 2  strategy:    <span class="hljs-built_in">type</span>: RollingUpdate    rollingUpdate:      maxUnavailable: 1  selector:    matchLabels:      k8s-app: kube-dns  template:    metadata:      labels:        k8s-app: kube-dns    spec:      serviceAccountName: coredns      tolerations:        - key: <span class="hljs-string">"CriticalAddonsOnly"</span>          operator: <span class="hljs-string">"Exists"</span>      containers:      - name: coredns        image: coredns/coredns:1.1.1        imagePullPolicy: IfNotPresent        args: [ <span class="hljs-string">"-conf"</span>, <span class="hljs-string">"/etc/coredns/Corefile"</span> ]        volumeMounts:        - name: config-volume          mountPath: /etc/coredns        ports:        - containerPort: 53          name: dns          protocol: UDP        - containerPort: 53          name: dns-tcp          protocol: TCP        - containerPort: 9153          name: metrics          protocol: TCP        livenessProbe:          httpGet:            path: /health            port: 8080            scheme: HTTP          initialDelaySeconds: 60          timeoutSeconds: 5          successThreshold: 1          failureThreshold: 5      dnsPolicy: Default      volumes:        - name: config-volume          configMap:            name: coredns            items:            - key: Corefile              path: Corefile---apiVersion: v1kind: Servicemetadata:  name: kube-dns  namespace: kube-system  annotations:    prometheus.io/scrape: <span class="hljs-string">"true"</span>  labels:    k8s-app: kube-dns    kubernetes.io/cluster-service: <span class="hljs-string">"true"</span>    kubernetes.io/name: <span class="hljs-string">"CoreDNS"</span>spec:  selector:    k8s-app: kube-dns  clusterIP: CLUSTER_DNS_IP  ports:  - name: dns    port: 53    protocol: UDP  - name: dns-tcp    port: 53    protocol: TCP</code></pre></div><p>然后直接使用脚本替换即可(脚本变量我已经修改了)</p><div class="hljs"><pre><code class="hljs sh"><span class="hljs-meta">#!/bin/bash</span><span class="hljs-comment"># Deploys CoreDNS to a cluster currently running Kube-DNS.</span>SERVICE_CIDR=<span class="hljs-variable">$&#123;1:-10.254.0.0/16&#125;</span>POD_CIDR=<span class="hljs-variable">$&#123;2:-10.20.0.0/16&#125;</span>CLUSTER_DNS_IP=<span class="hljs-variable">$&#123;3:-10.254.0.2&#125;</span>CLUSTER_DOMAIN=<span class="hljs-variable">$&#123;4:-cluster.local&#125;</span>YAML_TEMPLATE=<span class="hljs-variable">$&#123;5:-`pwd`/coredns.yaml.sed&#125;</span>sed -e s/CLUSTER_DNS_IP/<span class="hljs-variable">$CLUSTER_DNS_IP</span>/g -e s/CLUSTER_DOMAIN/<span class="hljs-variable">$CLUSTER_DOMAIN</span>/g -e s?SERVICE_CIDR?<span class="hljs-variable">$SERVICE_CIDR</span>?g -e s?POD_CIDR?<span class="hljs-variable">$POD_CIDR</span>?g <span class="hljs-variable">$YAML_TEMPLATE</span> &gt; coredns.yaml</code></pre></div><p>最后使用 <code>kubectl</code> 创建一下</p><div class="hljs"><pre><code class="hljs sh"><span class="hljs-comment"># 执行上面的替换脚本</span>./deploy.sh<span class="hljs-comment"># 创建 CoreDNS</span>kubectl create -f coredns.yaml</code></pre></div><p>测试截图如下</p><p><img src="https://cdn.oss.link/markdown/v1jdc.png" srcset="/img/loading.gif" alt="test dns"></p><h4 id="6-2、部署-DNS-自动扩容"><a href="#6-2、部署-DNS-自动扩容" class="headerlink" title="6.2、部署 DNS 自动扩容"></a>6.2、部署 DNS 自动扩容</h4><p>自动扩容跟以往一样，yaml 创建一下就行</p><ul><li>dns-horizontal-autoscaler.yaml</li></ul><div class="hljs"><pre><code class="hljs sh"><span class="hljs-comment"># Copyright 2016 The Kubernetes Authors.</span><span class="hljs-comment">#</span><span class="hljs-comment"># Licensed under the Apache License, Version 2.0 (the "License");</span><span class="hljs-comment"># you may not use this file except in compliance with the License.</span><span class="hljs-comment"># You may obtain a copy of the License at</span><span class="hljs-comment">#</span><span class="hljs-comment">#     http://www.apache.org/licenses/LICENSE-2.0</span><span class="hljs-comment">#</span><span class="hljs-comment"># Unless required by applicable law or agreed to in writing, software</span><span class="hljs-comment"># distributed under the License is distributed on an "AS IS" BASIS,</span><span class="hljs-comment"># WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.</span><span class="hljs-comment"># See the License for the specific language governing permissions and</span><span class="hljs-comment"># limitations under the License.</span>kind: ServiceAccountapiVersion: v1metadata:  name: kube-dns-autoscaler  namespace: kube-system  labels:    addonmanager.kubernetes.io/mode: Reconcile---kind: ClusterRoleapiVersion: rbac.authorization.k8s.io/v1metadata:  name: system:kube-dns-autoscaler  labels:    addonmanager.kubernetes.io/mode: Reconcilerules:  - apiGroups: [<span class="hljs-string">""</span>]    resources: [<span class="hljs-string">"nodes"</span>]    verbs: [<span class="hljs-string">"list"</span>]  - apiGroups: [<span class="hljs-string">""</span>]    resources: [<span class="hljs-string">"replicationcontrollers/scale"</span>]    verbs: [<span class="hljs-string">"get"</span>, <span class="hljs-string">"update"</span>]  - apiGroups: [<span class="hljs-string">"extensions"</span>]    resources: [<span class="hljs-string">"deployments/scale"</span>, <span class="hljs-string">"replicasets/scale"</span>]    verbs: [<span class="hljs-string">"get"</span>, <span class="hljs-string">"update"</span>]<span class="hljs-comment"># Remove the configmaps rule once below issue is fixed:</span><span class="hljs-comment"># kubernetes-incubator/cluster-proportional-autoscaler#16</span>  - apiGroups: [<span class="hljs-string">""</span>]    resources: [<span class="hljs-string">"configmaps"</span>]    verbs: [<span class="hljs-string">"get"</span>, <span class="hljs-string">"create"</span>]---kind: ClusterRoleBindingapiVersion: rbac.authorization.k8s.io/v1metadata:  name: system:kube-dns-autoscaler  labels:    addonmanager.kubernetes.io/mode: Reconcilesubjects:  - kind: ServiceAccount    name: kube-dns-autoscaler    namespace: kube-systemroleRef:  kind: ClusterRole  name: system:kube-dns-autoscaler  apiGroup: rbac.authorization.k8s.io---apiVersion: apps/v1kind: Deploymentmetadata:  name: kube-dns-autoscaler  namespace: kube-system  labels:    k8s-app: kube-dns-autoscaler    kubernetes.io/cluster-service: <span class="hljs-string">"true"</span>    addonmanager.kubernetes.io/mode: Reconcilespec:  selector:    matchLabels:      k8s-app: kube-dns-autoscaler  template:    metadata:      labels:        k8s-app: kube-dns-autoscaler      annotations:        scheduler.alpha.kubernetes.io/critical-pod: <span class="hljs-string">''</span>    spec:      priorityClassName: system-cluster-critical      containers:      - name: autoscaler        image: k8s.gcr.io/cluster-proportional-autoscaler-amd64:1.1.2-r2        resources:            requests:                cpu: <span class="hljs-string">"20m"</span>                memory: <span class="hljs-string">"10Mi"</span>        <span class="hljs-built_in">command</span>:          - /cluster-proportional-autoscaler          - --namespace=kube-system          - --configmap=kube-dns-autoscaler          <span class="hljs-comment"># Should keep target in sync with cluster/addons/dns/kube-dns.yaml.base</span>          - --target=Deployment/kube-dns          <span class="hljs-comment"># When cluster is using large nodes(with more cores), "coresPerReplica" should dominate.</span>          <span class="hljs-comment"># If using small nodes, "nodesPerReplica" should dominate.</span>          - --default-params=&#123;<span class="hljs-string">"linear"</span>:&#123;<span class="hljs-string">"coresPerReplica"</span>:256,<span class="hljs-string">"nodesPerReplica"</span>:16,<span class="hljs-string">"preventSinglePointFailure"</span>:<span class="hljs-literal">true</span>&#125;&#125;          - --logtostderr=<span class="hljs-literal">true</span>          - --v=2      tolerations:      - key: <span class="hljs-string">"CriticalAddonsOnly"</span>        operator: <span class="hljs-string">"Exists"</span>      serviceAccountName: kube-dns-autoscaler</code></pre></div><h3 id="七、部署-heapster"><a href="#七、部署-heapster" class="headerlink" title="七、部署 heapster"></a>七、部署 heapster</h3><p>heapster 部署相对简单的多，yaml 创建一下就可以了</p><div class="hljs"><pre><code class="hljs sh">kubectl create -f https://raw.githubusercontent.com/kubernetes/heapster/master/deploy/kube-config/influxdb/grafana.yamlkubectl create -f https://raw.githubusercontent.com/kubernetes/heapster/master/deploy/kube-config/influxdb/heapster.yamlkubectl create -f https://raw.githubusercontent.com/kubernetes/heapster/master/deploy/kube-config/influxdb/influxdb.yamlkubectl create -f https://raw.githubusercontent.com/kubernetes/heapster/master/deploy/kube-config/rbac/heapster-rbac.yaml</code></pre></div><h3 id="八、部署-Dashboard"><a href="#八、部署-Dashboard" class="headerlink" title="八、部署 Dashboard"></a>八、部署 Dashboard</h3><h4 id="8-1、部署-Dashboard"><a href="#8-1、部署-Dashboard" class="headerlink" title="8.1、部署 Dashboard"></a>8.1、部署 Dashboard</h4><p>Dashboard 部署同 heapster 一样，不过为了方便访问，我设置了 NodePort，还注意到一点是 yaml 拉取策略已经没有比较傻的 <code>Always</code> 了</p><div class="hljs"><pre><code class="hljs sh">wget https://raw.githubusercontent.com/kubernetes/dashboard/master/src/deploy/recommended/kubernetes-dashboard.yaml -O kubernetes-dashboard.yaml</code></pre></div><p>将最后部分的端口暴露修改如下</p><div class="hljs"><pre><code class="hljs yaml"><span class="hljs-comment"># ------------------- Dashboard Service ------------------- #</span><span class="hljs-attr">kind:</span> <span class="hljs-string">Service</span><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">v1</span><span class="hljs-attr">metadata:</span>  <span class="hljs-attr">labels:</span>    <span class="hljs-attr">k8s-app:</span> <span class="hljs-string">kubernetes-dashboard</span>  <span class="hljs-attr">name:</span> <span class="hljs-string">kubernetes-dashboard</span>  <span class="hljs-attr">namespace:</span> <span class="hljs-string">kube-system</span><span class="hljs-attr">spec:</span>  <span class="hljs-attr">type:</span> <span class="hljs-string">NodePort</span>  <span class="hljs-attr">ports:</span>    <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">dashboard-tls</span>      <span class="hljs-attr">port:</span> <span class="hljs-number">443</span>      <span class="hljs-attr">targetPort:</span> <span class="hljs-number">8443</span>      <span class="hljs-attr">nodePort:</span> <span class="hljs-number">30000</span>      <span class="hljs-attr">protocol:</span> <span class="hljs-string">TCP</span>  <span class="hljs-attr">selector:</span>    <span class="hljs-attr">k8s-app:</span> <span class="hljs-string">kubernetes-dashboard</span></code></pre></div><p>然后执行 <code>kubectl create -f kubernetes-dashboard.yaml</code> 即可</p><h4 id="8-2、创建-admin-账户"><a href="#8-2、创建-admin-账户" class="headerlink" title="8.2、创建 admin 账户"></a>8.2、创建 admin 账户</h4><p>默认情况下部署成功后可以直接访问 <code>https://NODE_IP:30000</code> 访问，但是想要登录进去查看的话需要使用 kubeconfig 或者 access token 的方式；实际上这个就是 RBAC 授权控制，以下提供一个创建 admin access token 的脚本，更细节的权限控制比如只读用户可以参考 <a href="https://mritd.me/2018/03/20/use-rbac-to-control-kubectl-permissions/" target="_blank" rel="noopener">使用 RBAC 控制 kubectl 权限</a>，RBAC 权限控制原理是一样的</p><div class="hljs"><pre><code class="hljs sh"><span class="hljs-meta">#!/bin/bash</span><span class="hljs-keyword">if</span> kubectl get sa dashboard-admin -n kube-system &amp;&gt; /dev/null;<span class="hljs-keyword">then</span>    <span class="hljs-built_in">echo</span> -e <span class="hljs-string">"\033[33mWARNING: ServiceAccount dashboard-admin exist!\033[0m"</span><span class="hljs-keyword">else</span>    kubectl create sa dashboard-admin -n kube-system    kubectl create clusterrolebinding dashboard-admin --clusterrole=cluster-admin --serviceaccount=kube-system:dashboard-admin<span class="hljs-keyword">fi</span>kubectl describe secret -n kube-system $(kubectl get secrets -n kube-system | grep dashboard-admin | cut -f1 -d <span class="hljs-string">' '</span>) | grep -E <span class="hljs-string">'^token'</span></code></pre></div><p>将以上脚本保存为 <code>create_dashboard_sa.sh</code> 执行即可，成功后访问截图如下(<strong>如果访问不了的话请检查下 iptable FORWARD 默认规则是否为 DROP，如果是将其改为 ACCEPT 即可</strong>)</p><p><img src="https://cdn.oss.link/markdown/oxmms.png" srcset="/img/loading.gif" alt="create_dashboard_sa"></p><p><img src="https://cdn.oss.link/markdown/pyplb.png" srcset="/img/loading.gif" alt="dashboard"></p><h3 id="九、其他说明"><a href="#九、其他说明" class="headerlink" title="九、其他说明"></a>九、其他说明</h3><h4 id="9-1、选项-label-等说明"><a href="#9-1、选项-label-等说明" class="headerlink" title="9.1、选项 label 等说明"></a>9.1、选项 label 等说明</h4><p>部署过程中注意到一些选项已经做了名称更改，比如 <code>--network-plugin-dir</code> 变更为 <code>--cni-bin-dir</code> 等，具体的那些选项做了变更请自行对比配置，以及查看官方文档；</p><p>对于 Node label <code>--node-labels=node-role.kubernetes.io/k8s-node=true</code> 这个选项，它的作用只是在 <code>kubectl get node</code> 时 ROLES 栏显示是什么节点；不过需要注意 <strong>master 上的 kubelet 不要将 <code>node-role.kubernetes.io/k8s-master=true</code> 更改成 <code>node-role.kubernetes.io/master=xxxx</code>；后面这个 <code>node-role.kubernetes.io/master</code> 是 kubeadm 用的，这个 label 会告诉 k8s 调度器当前节点为 master，从而执行一些特定动作，比如 <code>node-role.kubernetes.io/master:NoSchedule</code> 此节点将不会被分配 pod；具体参见 <a href="https://github.com/kubernetes-incubator/kubespray/issues/2108" target="_blank" rel="noopener">kubespray issue</a> 以及 <a href="https://github.com/kubernetes/kubeadm/blob/master/docs/design/design_v1.9.md#mark-master" target="_blank" rel="noopener">官方设计文档</a></strong></p><p>很多人可能会发现大约 1 小时候 <code>kubectl get csr</code> 看不到任何 csr 了，这是因为最新版本增加了 csr 清理功能，<strong>默认对于 <code>approved</code> 和 <code>denied</code> 状态的 csr 一小时后会被清理，对于 <code>pending</code> 状态的 csr 24 小时后会被清理，想问时间从哪来的请看 <a href="https://github.com/kubernetes/kubernetes/blob/fa85bf7094a8a503fede964b7038eed51360ffc7/pkg/controller/certificates/cleaner/cleaner.go#L47" target="_blank" rel="noopener">代码</a>；PR issue 我忘记了，增加这个功能的起因大致就是因为当开启了证书轮换后，csr 会不断增加，所以需要增加一个清理功能</strong></p><h4 id="9-2、异常及警告说明"><a href="#9-2、异常及警告说明" class="headerlink" title="9.2、异常及警告说明"></a>9.2、异常及警告说明</h4><p>在部署过程中我记录了一些异常警告等，以下做一下统一说明</p><div class="hljs"><pre><code class="hljs sh"><span class="hljs-comment"># https://github.com/kubernetes/kubernetes/issues/42158</span><span class="hljs-comment"># 这个问题还没解决，PR 没有合并被关闭了，可以关注一下上面这个 issue，被关闭的 PR 在下面</span><span class="hljs-comment"># https://github.com/kubernetes/kubernetes/pull/49567</span>Failed to update statusUpdateNeeded field <span class="hljs-keyword">in</span> actual state of world: Failed to <span class="hljs-built_in">set</span> statusUpdateNeeded to needed <span class="hljs-literal">true</span>, because nodeName=...<span class="hljs-comment"># https://github.com/kubernetes/kubernetes/issues/59993</span><span class="hljs-comment"># 这个似乎已经解决了，没时间测试，PR 地址在下面，我大致 debug 一下 好像是 cAdvisor 的问题</span><span class="hljs-comment"># https://github.com/opencontainers/runc/pull/1722</span>Failed to get system container stats <span class="hljs-keyword">for</span> <span class="hljs-string">"/kubepods"</span>: failed to get cgroup stats <span class="hljs-keyword">for</span> <span class="hljs-string">"/kubepods"</span>: failed to get container info <span class="hljs-keyword">for</span> <span class="hljs-string">"/kubepods"</span>: unknown containe <span class="hljs-string">"/kubepods"</span><span class="hljs-comment"># https://github.com/kubernetes/kubernetes/issues/58217</span><span class="hljs-comment"># 注意: 这个问题现在仍未解决，可关注上面的 issue，这个问题可能影响 node image gc</span><span class="hljs-comment"># 强烈依赖于 kubelet 做 宿主机 image gc 的需要注意一下</span>Image garbage collection failed once. Stats initialization may not have completed yet: failed to get imageFs info: unable to find data <span class="hljs-keyword">for</span> container /<span class="hljs-comment"># 没找到太多资料，不过感觉跟上面问题类似</span>failed to construct signal: <span class="hljs-string">"allocatableMemory.available"</span> error: system container <span class="hljs-string">"pods"</span> not found <span class="hljs-keyword">in</span> metrics</code></pre></div>]]></content:encoded>
      
      
      <category domain="https://mritd.com/categories/kubernetes/">Kubernetes</category>
      
      
      <category domain="https://mritd.com/tags/kubernetes/">Kubernetes</category>
      
      
      <comments>https://mritd.com/2018/04/19/set-up-kubernetes-1.10.1-cluster-by-hyperkube/#disqus_thread</comments>
      
    </item>
    
    <item>
      <title>Drone CI 搭建</title>
      <link>https://mritd.com/2018/03/30/set-up-drone-ci/</link>
      <guid>https://mritd.com/2018/03/30/set-up-drone-ci/</guid>
      <pubDate>Fri, 30 Mar 2018 13:38:29 GMT</pubDate>
      
      <description>最近感觉 GitLab CI 稍有繁琐，所以尝试了一下 Drone CI，这里记录一下搭建过程；虽然 Drone CI 看似简单，但是坑还是有不少的</description>
      
      
      
      <content:encoded><![CDATA[<blockquote><p>最近感觉 GitLab CI 稍有繁琐，所以尝试了一下 Drone CI，这里记录一下搭建过程；虽然 Drone CI 看似简单，但是坑还是有不少的</p></blockquote><h2 id="一、环境准备"><a href="#一、环境准备" class="headerlink" title="一、环境准备"></a>一、环境准备</h2><p>基本环境如下:</p><ul><li>Docker: 17.09.0-ce</li><li>GitLab: 10.4.3-ce.0</li><li>Drone: 0.8.5</li></ul><p>其中 GitLab 采用 TLS 链接，为了方便使用 git 协议 clone 代码，所以 docker compose 部署时采用了 macvlan 网络获取独立 IP</p><h2 id="二、GitLab-配置"><a href="#二、GitLab-配置" class="headerlink" title="二、GitLab 配置"></a>二、GitLab 配置</h2><h3 id="2-1、GitLab-搭建"><a href="#2-1、GitLab-搭建" class="headerlink" title="2.1、GitLab 搭建"></a>2.1、GitLab 搭建</h3><p>为了测试 CI build 需要一个 GitLab 服务器以及测试项目，GitLab 这里直接采用 docker compose 启动，同时为了方便 git clone，网络使用了 macvlan 方式，macvlan 网络接口、IP 等参数请自行修改</p><div class="hljs"><pre><code class="hljs sh"><span class="hljs-comment"># config refs ==&gt; https://gitlab.com/gitlab-org/omnibus-gitlab/blob/master/files/gitlab-config-template/gitlab.rb.template</span>version: <span class="hljs-string">'3'</span>services:  gitlab:    image: <span class="hljs-string">'gitlab/gitlab-ce:10.4.3-ce.0'</span>    container_name: gitlab    restart: always    hostname: <span class="hljs-string">'gitlab.mritd.me'</span>    environment:      GITLAB_OMNIBUS_CONFIG: |        external_url <span class="hljs-string">'https://gitlab.mritd.me'</span>        nginx[<span class="hljs-string">'redirect_http_to_https'</span>] = <span class="hljs-literal">true</span>        nginx[<span class="hljs-string">'ssl_certificate'</span>] = <span class="hljs-string">"/etc/gitlab/ssl/mritd.me.cer"</span>        nginx[<span class="hljs-string">'ssl_certificate_key'</span>] = <span class="hljs-string">"/etc/gitlab/ssl/mritd.me.key"</span>        nginx[<span class="hljs-string">'real_ip_header'</span>] = <span class="hljs-string">'X-Real-IP'</span>        nginx[<span class="hljs-string">'real_ip_recursive'</span>] = <span class="hljs-string">'on'</span>        <span class="hljs-comment">#gitlab_rails['ldap_enabled'] = true</span>        <span class="hljs-comment">#gitlab_rails['ldap_servers'] = YAML.load &lt;&lt;-EOS # remember to close this block with 'EOS' below</span>        <span class="hljs-comment">#main: # 'main' is the GitLab 'provider ID' of this LDAP server</span>        <span class="hljs-comment">#  ## label</span>        <span class="hljs-comment">#  #</span>        <span class="hljs-comment">#  # A human-friendly name for your LDAP server. It is OK to change the label later,</span>        <span class="hljs-comment">#  # for instance if you find out it is too large to fit on the web page.</span>        <span class="hljs-comment">#  #</span>        <span class="hljs-comment">#  # Example: 'Paris' or 'Acme, Ltd.'</span>        <span class="hljs-comment">#  label: 'LDAP'</span>        <span class="hljs-comment">#  host: 'mail.mritd.me'</span>        <span class="hljs-comment">#  port: 389 # or 636</span>        <span class="hljs-comment">#  uid: 'uid'</span>        <span class="hljs-comment">#  method: 'plain' # "tls" or "ssl" or "plain"</span>        <span class="hljs-comment">#  bind_dn: 'uid=zimbra,cn=admins,cn=zimbra'</span>        <span class="hljs-comment">#  password: 'PASSWORD'</span>        <span class="hljs-comment">#  # This setting specifies if LDAP server is Active Directory LDAP server.</span>        <span class="hljs-comment">#  # For non AD servers it skips the AD specific queries.</span>        <span class="hljs-comment">#  # If your LDAP server is not AD, set this to false.</span>        <span class="hljs-comment">#  active_directory: true</span>        <span class="hljs-comment">#  # If allow_username_or_email_login is enabled, GitLab will ignore everything</span>        <span class="hljs-comment">#  # after the first '@' in the LDAP username submitted by the user on login.</span>        <span class="hljs-comment">#  #</span>        <span class="hljs-comment">#  # Example:</span>        <span class="hljs-comment">#  # - the user enters 'jane.doe@example.com' and 'p@ssw0rd' as LDAP credentials;</span>        <span class="hljs-comment">#  # - GitLab queries the LDAP server with 'jane.doe' and 'p@ssw0rd'.</span>        <span class="hljs-comment">#  #</span>        <span class="hljs-comment">#  # If you are using "uid: 'userPrincipalName'" on ActiveDirectory you need to</span>        <span class="hljs-comment">#  # disable this setting, because the userPrincipalName contains an '@'.</span>        <span class="hljs-comment">#  allow_username_or_email_login: true</span>        <span class="hljs-comment">#  # Base where we can search for users</span>        <span class="hljs-comment">#  #</span>        <span class="hljs-comment">#  #   Ex. ou=People,dc=gitlab,dc=example</span>        <span class="hljs-comment">#  #</span>        <span class="hljs-comment">#  base: ''</span>        <span class="hljs-comment">#  # Filter LDAP users</span>        <span class="hljs-comment">#  #</span>        <span class="hljs-comment">#  #   Format: RFC 4515 http://tools.ietf.org/search/rfc4515</span>        <span class="hljs-comment">#  #   Ex. (employeeType=developer)</span>        <span class="hljs-comment">#  #</span>        <span class="hljs-comment">#  #   Note: GitLab does not support omniauth-ldap's custom filter syntax.</span>        <span class="hljs-comment">#  #</span>        <span class="hljs-comment">#  user_filter: ''</span>        <span class="hljs-comment">#EOS</span>        gitlab_rails[<span class="hljs-string">'log_directory'</span>] = <span class="hljs-string">"/var/log/gitlab/gitlab-rails"</span>        unicorn[<span class="hljs-string">'log_directory'</span>] = <span class="hljs-string">"/var/log/gitlab/unicorn"</span>        registry[<span class="hljs-string">'log_directory'</span>] = <span class="hljs-string">"/var/log/gitlab/registry"</span>        <span class="hljs-comment"># Below are some of the default settings</span>        logging[<span class="hljs-string">'logrotate_frequency'</span>] = <span class="hljs-string">"daily"</span> <span class="hljs-comment"># rotate logs daily</span>        logging[<span class="hljs-string">'logrotate_size'</span>] = nil <span class="hljs-comment"># do not rotate by size by default</span>        logging[<span class="hljs-string">'logrotate_rotate'</span>] = 30 <span class="hljs-comment"># keep 30 rotated logs</span>        logging[<span class="hljs-string">'logrotate_compress'</span>] = <span class="hljs-string">"compress"</span> <span class="hljs-comment"># see 'man logrotate'</span>        logging[<span class="hljs-string">'logrotate_method'</span>] = <span class="hljs-string">"copytruncate"</span> <span class="hljs-comment"># see 'man logrotate'</span>        logging[<span class="hljs-string">'logrotate_postrotate'</span>] = nil <span class="hljs-comment"># no postrotate command by default</span>        logging[<span class="hljs-string">'logrotate_dateformat'</span>] = nil <span class="hljs-comment"># use date extensions for rotated files rather than numbers e.g. a value of "-%Y-%m-%d" would give rotated files like p</span>        <span class="hljs-comment"># You can add overrides per service</span>        nginx[<span class="hljs-string">'logrotate_frequency'</span>] = nil        nginx[<span class="hljs-string">'logrotate_size'</span>] = <span class="hljs-string">"200M"</span>        <span class="hljs-comment"># You can also disable the built-in logrotate service if you want</span>        logrotate[<span class="hljs-string">'enable'</span>] = <span class="hljs-literal">false</span>        gitlab_rails[<span class="hljs-string">'smtp_enable'</span>] = <span class="hljs-literal">true</span>        gitlab_rails[<span class="hljs-string">'smtp_address'</span>] = <span class="hljs-string">"mail.mritd.me"</span>        gitlab_rails[<span class="hljs-string">'smtp_port'</span>] = 25        gitlab_rails[<span class="hljs-string">'smtp_user_name'</span>] = <span class="hljs-string">"no-reply@mritd.me"</span>        gitlab_rails[<span class="hljs-string">'smtp_password'</span>] = <span class="hljs-string">"PASSWORD"</span>        gitlab_rails[<span class="hljs-string">'smtp_domain'</span>] = <span class="hljs-string">"mritd.me"</span>        gitlab_rails[<span class="hljs-string">'smtp_authentication'</span>] = <span class="hljs-string">"login"</span>        gitlab_rails[<span class="hljs-string">'smtp_enable_starttls_auto'</span>] = <span class="hljs-literal">true</span>        gitlab_rails[<span class="hljs-string">'smtp_openssl_verify_mode'</span>] = <span class="hljs-string">'peer'</span>        <span class="hljs-comment"># If your SMTP server does not like the default 'From: gitlab@localhost' you</span>        <span class="hljs-comment"># can change the 'From' with this setting.</span>        gitlab_rails[<span class="hljs-string">'gitlab_email_from'</span>] = <span class="hljs-string">'gitlab@mritd.me'</span>        gitlab_rails[<span class="hljs-string">'gitlab_email_reply_to'</span>] = <span class="hljs-string">'no-reply@mritd.me'</span>        gitlab_rails[<span class="hljs-string">'initial_root_password'</span>] = <span class="hljs-string">'PASSWORD'</span>        gitlab_rails[<span class="hljs-string">'initial_shared_runners_registration_token'</span>] = <span class="hljs-string">"iuLaUhGZYyFgTxAyZ6HbdFUZ"</span>    networks:      macvlan:        ipv4_address: 172.16.0.70    ports:      - <span class="hljs-string">'80:80'</span>      - <span class="hljs-string">'443:443'</span>      - <span class="hljs-string">'22:22'</span>    volumes:      - config:/etc/gitlab      - logs:/var/<span class="hljs-built_in">log</span>/gitlab      - data:/var/opt/gitlabnetworks:  macvlan:    driver: macvlan    driver_opts:      parent: ens18    ipam:      config:      - subnet: 172.16.0.0/19volumes:  config:  logs:  data:</code></pre></div><h3 id="2-2、创建-Drone-App"><a href="#2-2、创建-Drone-App" class="headerlink" title="2.2、创建 Drone App"></a>2.2、创建 Drone App</h3><p>Drone CI 工作时需要接入 GitLab 以完成项目同步等功能，所以在搭建好 GitLab 后需要为其创建 Application，创建方式如下所示</p><p><img src="https://cdn.oss.link/markdown/lzm4j.png" srcset="/img/loading.gif" alt="create drone app"></p><p>创建 Application 时请自行更换回调地址域名，创建好后如下所示(后续 Drone CI 需要使用这两个 key)</p><p><img src="https://cdn.oss.link/markdown/sl4yl.png" srcset="/img/loading.gif" alt="drone app create success"></p><h2 id="三、Drone-服务端配置"><a href="#三、Drone-服务端配置" class="headerlink" title="三、Drone 服务端配置"></a>三、Drone 服务端配置</h2><h3 id="3-1、Drone-CI-搭建"><a href="#3-1、Drone-CI-搭建" class="headerlink" title="3.1、Drone CI 搭建"></a>3.1、Drone CI 搭建</h3><p>Drone CI 服务器与 GitLab 等传统 CI 相似，都是 CS 模式，为了方便测试这里将 Agent 与 Server 端都放在一个 docker compose 中启动；docker compose 配置如下</p><div class="hljs"><pre><code class="hljs sh">version: <span class="hljs-string">'3'</span>services:  drone-server:    image: drone/drone:0.8-alpine    container_name: drone-server    ports:      - 8000:8000      - 9000:9000    volumes:      - data:/var/lib/drone/    restart: always    environment:      - DRONE_OPEN=<span class="hljs-literal">true</span>      - DRONE_ADMIN=drone,mritd      - DRONE_HOST=https://drone.mritd.me      - DRONE_GITLAB=<span class="hljs-literal">true</span>      - DRONE_GITLAB_PRIVATE_MODE=<span class="hljs-literal">true</span>      - DRONE_GITLAB_URL=https://gitlab.mritd.me      - DRONE_GITLAB_CLIENT=76155ab75bafd73d4ebfe0a02d9d6284a032f7d8667d558e3f929a64805d1fa1      - DRONE_GITLAB_SECRET=6957b06f53b80d4dd17051ceb36f9139ae83b9077e345a404f476e317b0c8f3d      - DRONE_SECRET=XsJnj4DmzuXBKkcgHeUAJQxq  drone-agent:    image: drone/agent:0.8    container_name: drone-agent    <span class="hljs-built_in">command</span>: agent    restart: always    volumes:      - /var/run/docker.sock:/var/run/docker.sock    environment:      - DRONE_SERVER=172.16.0.36:9000      - DRONE_SECRET=XsJnj4DmzuXBKkcgHeUAJQxqvolumes:  data:</code></pre></div><p>docker compose 中 <code>DRONE_GITLAB_CLIENT</code> 为 GitLab 创建 Application 时的 <code>Application Id</code>，<code>DRONE_GITLAB_SECRET</code> 为 <code>Secret</code>；其他环境变量解释如下:</p><ul><li>DRONE_OPEN: 是否允许开放注册</li><li>DRONE_ADMIN: 注册后的管理员用户</li><li>DRONE_HOST: Server 地址</li><li>DRONE_GITLAB: 声明 Drone CI 对接为 GitLab</li><li>DRONE_GITLAB_PRIVATE_MODE: GitLab 私有化部署</li><li>DRONE_GITLAB_URL: GitLab 地址</li><li>DRONE_SECRET: Server 端认证秘钥，Agent 连接时需要</li></ul><p>实际上 Agent 可以与 Server 分离部署，不过需要注意 Server 端 9000 端口走的是 grpc 协议基于 HTTP2，nginx 等反向代理时需要做好对应处理</p><p>搭建成功这里外面套了一层 nginx 用来反向代理 Drone Server 的 8000 端口，Nginx 配置如下:</p><div class="hljs"><pre><code class="hljs sh">upstream drone&#123;    server 172.16.0.36:8000;&#125;server &#123;    listen 80;    listen [::]:80;    server_name drone.mritd.me;    <span class="hljs-comment"># Redirect all HTTP requests to HTTPS with a 301 Moved Permanently response.</span>    <span class="hljs-built_in">return</span> 301 https://<span class="hljs-variable">$host</span><span class="hljs-variable">$request_uri</span>;&#125;server &#123;    listen 443 ssl http2;    listen [::]:443 ssl http2;    server_name drone.mritd.me;    <span class="hljs-comment"># certs sent to the client in SERVER HELLO are concatenated in ssl_certificate</span>    ssl_certificate /etc/nginx/ssl/mritd.me.cer;    ssl_certificate_key /etc/nginx/ssl/mritd.me.key;    ssl_session_timeout 1d;    ssl_session_cache shared:SSL:50m;    ssl_session_tickets off;        <span class="hljs-comment"># Diffie-Hellman parameter for DHE ciphersuites, recommended 2048 bits</span>    ssl_dhparam /etc/nginx/ssl/dhparam.pem;    <span class="hljs-comment"># intermediate configuration. tweak to your needs.</span>    ssl_protocols TLSv1 TLSv1.1 TLSv1.2;    ssl_ciphers <span class="hljs-string">'ECDHE-ECDSA-CHACHA20-POLY1305:ECDHE-RSA-CHACHA20-POLY1305:ECDHE-ECDSA-AES128-GCM-SHA256:ECDHE-RSA-AES128-GCM-SHA256:ECDHE-ECDSA-AES256-GCM-SHA384:EC</span><span class="hljs-string">DHE-RSA-AES256-GCM-SHA384:DHE-RSA-AES128-GCM-SHA256:DHE-RSA-AES256-GCM-SHA384:ECDHE-ECDSA-AES128-SHA256:ECDHE-RSA-AES128-SHA256:ECDHE-ECDSA-AES128-SHA:ECDHE-RSA-AES2</span><span class="hljs-string">56-SHA384:ECDHE-RSA-AES128-SHA:ECDHE-ECDSA-AES256-SHA384:ECDHE-ECDSA-AES256-SHA:ECDHE-RSA-AES256-SHA:DHE-RSA-AES128-SHA256:DHE-RSA-AES128-SHA:DHE-RSA-AES256-SHA256:D</span><span class="hljs-string">HE-RSA-AES256-SHA:ECDHE-ECDSA-DES-CBC3-SHA:ECDHE-RSA-DES-CBC3-SHA:EDH-RSA-DES-CBC3-SHA:AES128-GCM-SHA256:AES256-GCM-SHA384:AES128-SHA256:AES256-SHA256:AES128-SHA:AES</span><span class="hljs-string">256-SHA:DES-CBC3-SHA:!DSS'</span>;    ssl_prefer_server_ciphers on;    <span class="hljs-comment"># HSTS (ngx_http_headers_module is required) (15768000 seconds = 6 months)</span>    add_header Strict-Transport-Security max-age=15768000;    <span class="hljs-comment"># OCSP Stapling ---</span>    <span class="hljs-comment"># fetch OCSP records from URL in ssl_certificate and cache them</span>    ssl_stapling on;    ssl_stapling_verify on;    <span class="hljs-comment">## verify chain of trust of OCSP response using Root CA and Intermediate certs</span>    ssl_trusted_certificate /etc/nginx/ssl/mritd-ca.cer;    <span class="hljs-comment">#resolver &lt;IP DNS resolver&gt;;</span>    location / &#123;        log_not_found on;        proxy_set_header X-Forwarded-For <span class="hljs-variable">$remote_addr</span>;        proxy_set_header X-Forwarded-Proto <span class="hljs-variable">$scheme</span>;        proxy_set_header Host <span class="hljs-variable">$http_host</span>;        proxy_pass http://drone;        proxy_redirect off;        proxy_http_version 1.1;        proxy_buffering off;        chunked_transfer_encoding off;    &#125;&#125;</code></pre></div><p>然后访问 <code>https://YOUR_DRONE_SERVER</code> 将会自动跳转到 GitLab Auth2 授权界面，授权登录即可；随后将会返回 Drone CI 界面，界面上会列出相应的项目列表，点击后面的开关按钮来开启对应项目的 Drone CI 服务</p><p><img src="https://cdn.oss.link/markdown/6u4fk.png" srcset="/img/loading.gif" alt="drone ci project list"></p><h3 id="3-2、创建示例项目"><a href="#3-2、创建示例项目" class="headerlink" title="3.2、创建示例项目"></a>3.2、创建示例项目</h3><p>这里的示例项目为 Java 项目，采用 Gradle 构建，项目整体结构如下所示，源码可以从 <a href="">GitHub</a> 下载</p><p><img src="https://cdn.oss.link/markdown/ybrjc.png" srcset="/img/loading.gif" alt="drone test project"></p><p>将此项目推送到 GitLab 就会触发 Drone CI 自动构建(第一次肯定构建失败，具体看下面配置)</p><h3 id="3-3、Drone-CLI"><a href="#3-3、Drone-CLI" class="headerlink" title="3.3、Drone CLI"></a>3.3、Drone CLI</h3><p>这里不得不说一下官方文档真的很烂，有些东西只能自己摸索，而且各种错误提示也是烂的不能再烂，经常遇到 <code>Client Error 404:</code> 这种错误，后面任何提示信息也没有；官方文档中介绍了有些操作只能通过 cli 执行，CLI 下载需要到 GitHub 下载页下载，地址 <a href="https://github.com/drone/drone-cli/releases" target="_blank" rel="noopener">点这里</a></p><p>cli 工具下载后需要进行配置，目前只支持读取环境变量，使用前需要 <code>export</code> 以下两个变量</p><ul><li>DRONE_SERVER: Drone CI 地址</li><li>DRONE_TOKEN: cli 控制 Server 端使用的用户 Token</li></ul><p>其中 Token 可以在用户设置页面找到，如下</p><p><img src="https://cdn.oss.link/markdown/5fkvi.png" srcset="/img/loading.gif" alt="drone user token"></p><p>配置好以后就可以使用 cli 操作 CI Server 了</p><h3 id="3-4、Drone-CI-配置文件"><a href="#3-4、Drone-CI-配置文件" class="headerlink" title="3.4、Drone CI 配置文件"></a>3.4、Drone CI 配置文件</h3><p>Drone CI 对一个项目进行 CI 构建取决于两个因素，第一必须保证该项目在 Drone 控制面板中开启了构建(构建按钮开启)，第二保证项目根目录下存在 <code>.drone.yml</code>；满足这两点后每次提交 Drone 就会根据 <code>.drone.yml</code> 中配置进行按步骤构建；本示例中 <code>.drone.yml</code> 配置如下</p><div class="hljs"><pre><code class="hljs sh"><span class="hljs-built_in">clone</span>:  git:    image: plugins/gitpipeline:  backend:    image: reg.mritd.me/base/build:2.1.5    commands:      - gradle --no-daemon clean assemble    when:      branch:        event: [ push, pull_request ]        include: [ master ]        exclude: [ develop ]<span class="hljs-comment">#  rebuild-cache:</span><span class="hljs-comment">#    image: drillster/drone-volume-cache</span><span class="hljs-comment">#    rebuild: true</span><span class="hljs-comment">#    mount:</span><span class="hljs-comment">#      - ./build</span><span class="hljs-comment">#    volumes:</span><span class="hljs-comment">#      - /data/drone/$DRONE_COMMIT_SHA:/cache</span>  docker:    image: mritd/docker-kubectl:v1.8.8    commands:      - bash build_image.sh    volumes:      - /var/run/docker.sock:/var/run/docker.sock<span class="hljs-comment"># Pipeline Conditions</span>branches:  include: [ master, feature/* ]  exclude: [ develop, <span class="hljs-built_in">test</span>/* ]</code></pre></div><p>Drone CI 配置文件为 docker compose 的超集，<strong>Drone CI 构建思想是使用不同的阶段定义完成对 CI 流程的整体划分，然后每个阶段内定义不同的任务(task)，这些任务所有操作无论是 build、package 等全部由单独的 Docker 镜像完成，同时以 <code>plugins</code> 开头的 image 被解释为内部插件；其他的插件实际上可以看做为标准的 Docker image</strong></p><p>第一段 <code>clone</code> 配置声明了源码版本控制系统拉取方式，具体参见 <a href="http://docs.drone.io/cloning" target="_blank" rel="noopener">cloning</a>部分，定义后 Drone CI 将自动拉取源码</p><p>此后的 <code>pipeline</code> 配置段为定义整个 CI 流程段，该段中可以自定义具体 task，比如后端构建可以取名字为 <code>backend</code>，前端构建可以叫做 <code>frontend</code>；中间可以穿插辅助的如打包 docker 镜像等 task；同 GitLab CI 一样，Agent 在使用 Docker 进行构建时必然涉及到拉取私有镜像，Drone CI 想要拉取私有镜像目前仅能通过 cli 命令行进行设置，而且仅针对项目级设置(全局需要企业版…这也行)</p><div class="hljs"><pre><code class="hljs sh">drone registry add --repository drone/DroneCI-TestProject --hostname reg.mritd.me --username gitlab --password 123456</code></pre></div><p>在构建时需要注意一点，Drone CI 不同的 task 之间共享源码文件，<strong>也就是说如果你在第一个 task 中对源码或者编译后的发布物做了什么更改，在下一个 task 中同样可见，Drone CI 并没有 GitLab CI 在每个 task 中都进行还原的机制</strong></p><p>除此之外，某些特殊性的挂载行为默认也是不被允许的，需要在 Drone CI 中对项目做 <code>Trusted</code> 设置</p><p><img src="https://cdn.oss.link/markdown/gd60v.png" srcset="/img/loading.gif" alt="Drone Project Trusted Setting"></p><h2 id="四、与-GitLab-CI-对比"><a href="#四、与-GitLab-CI-对比" class="headerlink" title="四、与 GitLab CI 对比"></a>四、与 GitLab CI 对比</h2><p>写到这里基本接近尾声了，可能常看我博客的人现在想喷我，这篇文章确实有点水…因为我真不推荐用这玩意，未来发展倒是不确定；下面对比一下与 GitLab CI 的区别</p><p>先说一下 Drone CI 的优点，Drone CI 更加轻量级，而且也支持 HA 等设置，配置文件使用 docker compose 的方式对于玩容器多的人确实很爽，启动速度等感觉也比 GitLab CI 要快；而且我个人用 GitLab CI Docker build 的方式时也是尽量将不同功能交给不同的镜像，通过切换镜像实现不同的功能；这个思想在 Drone CI 中表现的非常明显</p><p>至于 Drone CI 的缺点，目前我最大的吐槽就是文档烂，报错烂；很多时候搞得莫名其妙，比如上来安装讲的那个管理员账户配置，我现在也没明白怎么能关闭注册启动然后添加用户(可能是我笨)；还有就是报错问题，感觉就像写代码不打 log 一样，比如 CI Server 在没有 agent 链接时，如果触发了 build 任务，Drone CI 不会报错，只会在任务上显示一个小闹钟，也没有超时…我傻傻的等了 1 小时；其他的比如全局变量、全局加密参数等都需要企业版才能支持，同时一些细节东西也缺失，比如查看当前 Server 连接的 Agent，对 Agent 打标签实现不同 task 分配等等</p><p>总结: Drone CI 目前还是个小玩具阶段，与传统 CI 基本没有抗衡之力，文档功能呢也是缺失比较严重，出问题很难排查</p>]]></content:encoded>
      
      
      <category domain="https://mritd.com/categories/docker/">Docker</category>
      
      <category domain="https://mritd.com/categories/ci-cd/">CI/CD</category>
      
      
      <category domain="https://mritd.com/tags/linux/">Linux</category>
      
      <category domain="https://mritd.com/tags/ci-cd/">CI/CD</category>
      
      <category domain="https://mritd.com/tags/docker/">Docker</category>
      
      <category domain="https://mritd.com/tags/drone/">Drone</category>
      
      
      <comments>https://mritd.com/2018/03/30/set-up-drone-ci/#disqus_thread</comments>
      
    </item>
    
    <item>
      <title>Unix 平台下各种加速配置</title>
      <link>https://mritd.com/2018/03/28/unix-proxy-setting/</link>
      <guid>https://mritd.com/2018/03/28/unix-proxy-setting/</guid>
      <pubDate>Wed, 28 Mar 2018 14:09:57 GMT</pubDate>
      
      <description>本文主要阐述在 *Uinx 平台下，各种常用开发工具的加速配置，**加速前提是你需要有一个能够加速的 socks5 端口，常用工具请自行搭建**；本文档包括 docker、terminal、git、chrome 常用加速配置，其他工具可能后续补充</description>
      
      
      
      <content:encoded><![CDATA[<blockquote><p>本文主要阐述在 *Uinx 平台下，各种常用开发工具的加速配置，<strong>加速前提是你需要有一个能够加速的 socks5 端口，常用工具请自行搭建</strong>；本文档包括 docker、terminal、git、chrome 常用加速配置，其他工具可能后续补充</p></blockquote><h3 id="一、加速类型"><a href="#一、加速类型" class="headerlink" title="一、加速类型"></a>一、加速类型</h3><p>目前大部分工具在原始版本都是只提供 socks5 加速，常用平台一些工具已经支持手动设置加速端口，如 telegram、mega 同步客户端等等；但是某些工具并不支持 socks5，通用的加速目前各个平台只支持 http、https 设置(包括 terminal 下)；<strong>综上所述，在设置之前你至少需要保证有一个 socks5 端口能够进行加速，然后根据以下教程将 socks5 转换成 http，最后配置各个软件或系统的加速方式为 http，这也是我们常用的某些带有图形化客户端实际的背后实现</strong></p><h3 id="二、socks5-to-http"><a href="#二、socks5-to-http" class="headerlink" title="二、socks5 to http"></a>二、socks5 to http</h3><p>sock5 转 http 这里采用 privoxy 进行转换，根据各个平台不同，安装方式可能不同，主要就是包管理器的区别，以下只列举 Ubuntu、Mac 下的命令，其他平台自行 Google</p><ul><li>Mac: <code>brew install privoxy</code></li><li>Ubuntu: <code>apt-get -y install privoxy</code></li></ul><p>安装成功后，需要修改配置以指定 socks5 端口以及不代理的白名单，配置文件位置如下:</p><ul><li>Mac: <code>/usr/local/etc/privoxy/config</code></li><li>Ubuntu: <code>/etc/privoxy/config</code></li></ul><p>在修改之前请备份默认配置文件，这是个好习惯，备份后修改内容如下:</p><div class="hljs"><pre><code class="hljs sh"><span class="hljs-comment"># 转发地址</span>forward-socks5   /               127.0.0.1:1080 .<span class="hljs-comment"># 监听地址</span>listen-address  localhost:8118<span class="hljs-comment"># local network do not use proxy</span>forward         192.168.*.*/     .forward            10.*.*.*/     .forward           127.*.*.*/     .</code></pre></div><p><strong>其中 <code>127.0.0.1:1080</code> 为你的 socks5 ip 及 端口，<code>localhost:8118</code> 为你转换后的 http 监听地址和端口</strong>；配置完成后启动 privoxy 即可，启动命令如下:</p><ul><li>Mac: <code>brew services start privoxy</code></li><li>Ubuntu: <code>systemctl start privoxy</code></li></ul><h3 id="三、Docker-加速拉取-gcr-io-镜像"><a href="#三、Docker-加速拉取-gcr-io-镜像" class="headerlink" title="三、Docker 加速拉取 gcr.io 镜像"></a>三、Docker 加速拉取 gcr.io 镜像</h3><p>对于 docker 来说，terminal 下执行 <code>docker pull</code> 等命令实质上都是通过调用 docker daemon 操作的；而 docker daemon 是由 systemd 启动的(就目前来讲，别跟我掰什么 service start…)；对于 docker daemon 来说，一旦它启动以后就不会再接受加速设置，所以我们需要在 systemd 的 service 配置中配置它的加速。</p><p>目前 docker daemon 接受标准的终端加速设置(读取 <code>http_proxy</code>、<code>https_proxy</code>)，同时也支持 socks5 加速；为了保证配置清晰方便修改，这里采用创建单独配置文件的方式来配置 daemon 的 socks5 加速，配置脚本如下(Ubuntu、CentOS):</p><div class="hljs"><pre><code class="hljs sh"><span class="hljs-meta">#!/bin/bash</span><span class="hljs-built_in">set</span> -eOS_TYPE=<span class="hljs-variable">$1</span>PROXY_ADDRESS=<span class="hljs-variable">$2</span><span class="hljs-keyword">if</span> [ <span class="hljs-string">"<span class="hljs-variable">$&#123;PROXY_ADDRESS&#125;</span>"</span> == <span class="hljs-string">""</span> ]; <span class="hljs-keyword">then</span>    <span class="hljs-built_in">echo</span> -e <span class="hljs-string">"\033[31mError: PROXY_ADDRESS is blank!\033[0m"</span>    <span class="hljs-built_in">echo</span> -e <span class="hljs-string">"\033[32mUse: sudo <span class="hljs-variable">$0</span> centos|ubuntu 1.2.3.4:1080\033[0m"</span>    <span class="hljs-built_in">exit</span> 1<span class="hljs-keyword">fi</span><span class="hljs-keyword">if</span> [ <span class="hljs-string">"<span class="hljs-variable">$&#123;OS_TYPE&#125;</span>"</span> == <span class="hljs-string">""</span> ];<span class="hljs-keyword">then</span>    <span class="hljs-built_in">echo</span> -e <span class="hljs-string">"\033[31mError: OS_TYPE is blank!\033[0m"</span>    <span class="hljs-built_in">echo</span> -e <span class="hljs-string">"\033[32mUse: sudo <span class="hljs-variable">$0</span> centos|ubuntu\033[0m"</span>    <span class="hljs-built_in">exit</span> 1<span class="hljs-keyword">elif</span> [ <span class="hljs-string">"<span class="hljs-variable">$&#123;OS_TYPE&#125;</span>"</span> == <span class="hljs-string">"centos"</span> ];<span class="hljs-keyword">then</span>    mkdir /etc/systemd/system/docker.service.d || <span class="hljs-literal">true</span>    tee /etc/systemd/system/docker.service.d/socks5-proxy.conf &lt;&lt;-EOF[Service]Environment=<span class="hljs-string">"ALL_PROXY=socks5://<span class="hljs-variable">$&#123;PROXY_ADDRESS&#125;</span>"</span>EOF<span class="hljs-keyword">elif</span> [ <span class="hljs-string">"<span class="hljs-variable">$&#123;OS_TYPE&#125;</span>"</span> == <span class="hljs-string">"ubuntu"</span> ];<span class="hljs-keyword">then</span>    mkdir /lib/systemd/system/docker.service.d || <span class="hljs-literal">true</span>    tee /lib/systemd/system/docker.service.d/socks5-proxy.conf &lt;&lt;-EOF[Service]Environment=<span class="hljs-string">"ALL_PROXY=socks5://<span class="hljs-variable">$&#123;PROXY_ADDRESS&#125;</span>"</span>EOF<span class="hljs-keyword">fi</span>systemctl daemon-reloadsystemctl restart dockersystemctl show docker --property Environment</code></pre></div><p>将该脚本内容保存为 <code>docker_proxy.sh</code>，终端执行 <code>bash docker_proxy.sh ubuntu 1.2.3.4:1080</code> 即可(自行替换 socks5 地址)；脚本实际上很简单，就是创建一个与 <code>docker.service</code> 文件同级的 <code>docker.service.d</code> 目录，然后在里面写入一个 <code>socks5-proxy.conf</code>，配置内容只有两行:</p><div class="hljs"><pre><code class="hljs sh">[Service]Environment=<span class="hljs-string">"ALL_PROXY=socks5://1.2.3.4:1080</span></code></pre></div><p>这样 systemd 会自动读取，只需要 reload 一下，然后 restart docker daemon 即可，此后  docker 就可以通过加速端口直接 pull <code>gcr.io</code> 的镜像；<strong>注意: 配置加速后，docker 将无法 pull 私服镜像(一般私服都是内网 DNS 解析)，但是不会影响容器启动以及启动后的容器中的网络</strong></p><h3 id="四、Chrome-加速访问"><a href="#四、Chrome-加速访问" class="headerlink" title="四、Chrome 加速访问"></a>四、Chrome 加速访问</h3><p>对于 Chrome 浏览器来说，目前有比较好的插件实现用来配置根据策略的加速访问；这里使用的插件为 <code>SwitchyOmega</code></p><h4 id="4-1、SwitchyOmega-下载"><a href="#4-1、SwitchyOmega-下载" class="headerlink" title="4.1、SwitchyOmega 下载"></a>4.1、SwitchyOmega 下载</h4><p>默认情况下 <code>SwitchyOmega</code> 可以通过 Chrome 进行在线安装，但是众所周知的原因这是不可能的，不过国内有一些网站提供代理下载 Chrome 扩展的服务，如 <code>https://chrome-extension-downloader.com</code>、<code>http://yurl.sinaapp.com/crx.php</code>，这些网站只需要提供插件 ID 即可帮你下载下来；<strong><code>SwitchyOmega</code> 插件的 ID 为 <code>padekgcemlokbadohgkifijomclgjgif</code>，注意下载时不要使用 chrome 下载，因为他自身的防护机制会阻止你下载扩展程序</strong>；下载后打开 chrome 的扩展设置页，将 crx 文件拖入安装即可，如下所示:</p><p><img src="https://cdn.oss.link/markdown/zruoq.png" srcset="/img/loading.gif" alt="install chrome plugin"></p><h4 id="4-2、SwitchyOmega-配置"><a href="#4-2、SwitchyOmega-配置" class="headerlink" title="4.2、SwitchyOmega 配置"></a>4.2、SwitchyOmega 配置</h4><p>SwitchyOmega 安装成功后在 Chrome 右上角有显示，右键点击该图标，进入选项设置后如下所示:</p><p><img src="https://cdn.oss.link/markdown/ouh48.png" srcset="/img/loading.gif" alt="SwitchyOmega detail"></p><p>默认情况下左侧只有两个加速模式，一个叫做 <code>proxy</code> 另一个叫做 <code>autoproxy</code>；根据加速模式不同 SwitchyOmega 在浏览网页时选择的加速通道也不同，不同的加速方式可以通过点击 <strong>新建情景模式</strong> 按钮创建，下面介绍一下常用的两种情景模式:</p><p><strong>代理服务器:</strong> 这种情景模式创建后需要填写一个代理地址，该地址可以是 http(s)/socks5(4) 类型；创建成功后，浏览器右上角切换到该情景模式，<strong>浏览器访问所有网页的流量全部通过该代理地址发出</strong>，不论你是访问百度还是 Google</p><p> <img src="https://cdn.oss.link/markdown/idbi4.png" srcset="/img/loading.gif" alt="create test proxy1"></p><p> <img src="https://cdn.oss.link/markdown/52m7b.png" srcset="/img/loading.gif" alt="create test proxy2"></p><p><strong>自动切换模式:</strong> 这种情景模式并不需要填写实际的代理地址，而是需要填写一些规则；创建完成后插件中选择此种情景模式时，浏览器访问所有网页流量会根据填写的规则自动路由，然后选择合适的代理情景模式；可以实现智能切换代理</p><p> <img src="https://cdn.oss.link/markdown/7u6mv.png" srcset="/img/loading.gif" alt="create test auto proxy1"></p><p> <img src="https://cdn.oss.link/markdown/m5x36.png" srcset="/img/loading.gif" alt="create test auto proxy2"></p><p>综上所述，首先应该创建(或者修改默认的 proxy 情景模式)一个代理服务器的情景模式，然后填写好你的加速 IP 和对应的协议端口；接下来在浏览器中切换到该情景模式尝试访问 kubenretes.io 等网站测试加速效果；成功后再次新建一个自动切换情景模式，<strong>保证 <code>规则列表规则</code> 一栏后面的下拉列表对应到你刚刚创建的代理服务器情景模式，<code>默认情景模式</code> 后面的下拉列表对应到直接连接情景模式，然后点击下面的 <code>添加规则列表</code> 按钮，选择 <code>AutoProxy</code> 单选框，<code>规则列表网址</code> 填写 <code>https://raw.githubusercontent.com/gfwlist/gfwlist/master/gfwlist.txt</code>(这是一个开源项目收集的需要加速的网址列表)</strong>；最后在浏览器中切换到自动切换情景模式，然后访问 kubernetes.io、baidu.com 等网站测试是否能自动切换情景模式</p><h3 id="五、Terminal-加速"><a href="#五、Terminal-加速" class="headerlink" title="五、Terminal 加速"></a>五、Terminal 加速</h3><h4 id="5-1、脚本方式"><a href="#5-1、脚本方式" class="headerlink" title="5.1、脚本方式"></a>5.1、脚本方式</h4><p>对于终端下的应用程序，百分之九十的程序都会识别 <code>http_proxy</code> 和 <code>https_proxy</code> 两个变量；所以终端加速最简单的方式就是在执行命令前声明这两个变量即可，为了方便起见也可以写个小脚本，示例如下:</p><div class="hljs"><pre><code class="hljs sh">sudo tee /usr/<span class="hljs-built_in">local</span>/bin/proxy &lt;&lt;-EOF<span class="hljs-meta">#!/bin/bash</span>http_proxy=http://1.2.3.4:8118 https_proxy=http://1.2.3.4:8118 \$*EOFsudo chmod +x /usr/<span class="hljs-built_in">local</span>/bin/proxy</code></pre></div><p>将上面的地址自行更换成你的 http 加速地址后，终端运行 <code>proxy curl ip.cn</code> 即可测试加速效果</p><h4 id="5-2、proxychains-ng"><a href="#5-2、proxychains-ng" class="headerlink" title="5.2、proxychains-ng"></a>5.2、proxychains-ng</h4><p>proxychains-ng 是一个终端下的工具，它可以 hook libc 下的网络相关方法实现加速效果；目前支持后端为 http(s)/socks5(4a)，前段协议仅支持对 TCP 加速；</p><p>Mac 下安装方式:</p><div class="hljs"><pre><code class="hljs sh">brew install proxychains-ng</code></pre></div><p>Ubuntu 等平台下需要手动编译安装:</p><div class="hljs"><pre><code class="hljs sh"><span class="hljs-comment"># 安装编译依赖</span>apt-get -y install gcc make git<span class="hljs-comment"># 下载源码</span>git <span class="hljs-built_in">clone</span> https://github.com/rofl0r/proxychains-ng.git<span class="hljs-comment"># 编译安装</span><span class="hljs-built_in">cd</span> /proxychains-ng./configure --prefix=/usr --sysconfdir=/etcsudo make installsudo make install-config</code></pre></div><p>安装完成后编辑配置使用即可，Mac 下配置位于 <code>/usr/local/etc/proxychains.conf</code>，Ubuntu 下配置位于 <code>/etc/proxychains.conf</code>；配置修改如下:</p><div class="hljs"><pre><code class="hljs sh"><span class="hljs-comment"># 主要修改 [ProxyList] 下的加速地址</span>[ProxyList]socks5 1.2.3.4 1080</code></pre></div><p>然后命令行使用 <code>proxychains4 curl ip.cn</code> 测试即可</p><h3 id="六、Git-加速"><a href="#六、Git-加速" class="headerlink" title="六、Git 加速"></a>六、Git 加速</h3><p>目前 Git 的协议大致上只有三种 <code>https</code>、<code>ssh</code> 和 <code>git</code>，对于使用 <code>https</code> 方式进行 clone 和 push 操作时，可以使用第五部分 Terminal 加速方案即可实现对 Git 的加速；对于 <code>ssh</code>、<code>git</code> 协议，实际上都在调用 ssh 协议相关进行通讯(具体细节请 Google，这里的描述可能不精准)，此时同样可以使用 <code>proxychains-ng</code> 进行加速，<strong>不过需要注意 <code>proxychains-ng</code> 要自行编译安装，同时 <code>./configure</code> 增加 <code>--fat-binary</code> 选项，具体参考 <a href="https://github.com/rofl0r/proxychains-ng/issues/109" target="_blank" rel="noopener">GitHub Issue</a></strong>；<code>ssh</code>、<code>git</code> 由于都在调用 ssh 协议进行通讯，所以实际上还可以通过设置 ssh 的 <code>ProxyCommand</code> 来实现，具体操作如下:</p><div class="hljs"><pre><code class="hljs sh">sudo tee /usr/<span class="hljs-built_in">local</span>/bin/proxy-wrapper &lt;&lt;-EOF<span class="hljs-meta">#!/bin/bash</span>nc -x1.2.3.4:1080 -X5 \$*<span class="hljs-comment">#connect-proxy -S 1.2.3.4:1080 \$*</span>EOFsudo chmod +x /usr/<span class="hljs-built_in">local</span>/bin/proxy-wrappersudo tee ~/.ssh/config &lt;&lt;-EOFHost github.com    ProxyCommand /usr/<span class="hljs-built_in">local</span>/bin/proxy-wrapper <span class="hljs-string">'%h %p'</span>EOF</code></pre></div><p>需要注意: <strong>nc 命令是 netcat-openbsd 版本，Mac 下默认提供，Ubuntu 下需要使用 <code>apt-get install -y netcat-openbsd</code> 安装；CentOS 没有 netcat-openbsd，需要安装 EPEL 源，然后安装 connect-proxy 包，使用 connect-proxy 命令替代</strong></p>]]></content:encoded>
      
      
      <category domain="https://mritd.com/categories/linux/">Linux</category>
      
      
      <category domain="https://mritd.com/tags/linux/">Linux</category>
      
      
      <comments>https://mritd.com/2018/03/28/unix-proxy-setting/#disqus_thread</comments>
      
    </item>
    
    <item>
      <title>使用 RBAC 控制 kubectl 权限</title>
      <link>https://mritd.com/2018/03/20/use-rbac-to-control-kubectl-permissions/</link>
      <guid>https://mritd.com/2018/03/20/use-rbac-to-control-kubectl-permissions/</guid>
      <pubDate>Tue, 20 Mar 2018 15:58:37 GMT</pubDate>
      
      <description>好久没写文章了，过年以后就有点懒... 最近也在学习 golang，再加上不断造轮子所以没太多时间；凑巧最近想控制一下 kubectl 权限，这里便记录一下。</description>
      
      
      
      <content:encoded><![CDATA[<blockquote><p>好久没写文章了，过年以后就有点懒… 最近也在学习 golang，再加上不断造轮子所以没太多时间；凑巧最近想控制一下 kubectl 权限，这里便记录一下。</p></blockquote><h3 id="一、RBAC-相关"><a href="#一、RBAC-相关" class="headerlink" title="一、RBAC 相关"></a>一、RBAC 相关</h3><p>相信现在大部分人用的集群已经都是 1.6 版本以上，而且在安装各种组件的时候也已经或多或少的处理过 RBAC 的东西，所以这里不做太细节性的讲述，RBAC 文档我以前胡乱翻译过一篇，请看 <a href="https://mritd.me/2017/07/17/kubernetes-rbac-chinese-translation/" target="_blank" rel="noopener">这里</a>，以下内容仅说主要的</p><h4 id="1-1、RBAC-用户角色相关"><a href="#1-1、RBAC-用户角色相关" class="headerlink" title="1.1、RBAC 用户角色相关"></a>1.1、RBAC 用户角色相关</h4><p>我在第一次接触 Kubernetes RBAC 的时候，对于基于角色控制权限这种做法是有了解的，基本结构主要就是三个:</p><ul><li>权限: 即对系统中指定资源的增删改查权限</li><li>角色: 将一定的权限组合在一起产生权限组，如管理员角色</li><li>用户: 具体的使用者，具有唯一身份标识(ID)，其后与角色绑定便拥有角色的对应权限</li></ul><p>但是翻了一会文档，最晕的就是 <strong>这个用户标识(ID)存在哪</strong>，因为传统的授权模型都是下面这样</p><p><img src="https://cdn.oss.link/markdown/sn1qp.png" srcset="/img/loading.gif" alt="ctrole"></p><p>不论怎样，在进行授权时总要有个地方存放用户信息(DB/文件)，但是在 Kubernetes 里却没找到；后来翻阅文档，找到<a href="https://kubernetes.io/docs/admin/authentication/" target="_blank" rel="noopener">这么一段</a></p><div class="hljs"><pre><code class="hljs routeros">Normal<span class="hljs-built_in"> users </span>are assumed <span class="hljs-keyword">to</span> be managed by an outside, independent service. An admin distributing private keys, a<span class="hljs-built_in"> user </span>store like Keystone <span class="hljs-keyword">or</span> Google Accounts, even a file with a list of usernames <span class="hljs-keyword">and</span> passwords.</code></pre></div><p><strong>也就是说，Kubernetes 是不负责维护存储用户数据的；对于 Kubernetes 来说，它识别或者说认识一个用户主要就几种方式</strong></p><ul><li>X509 Client Certs: 使用由 k8s 根 CA 签发的证书，提取 O 字段</li><li>Static Token File: 预先在 API Server 放置 Token 文件(bootstrap 阶段使用过)</li><li>Bootstrap Tokens: 一种在集群内创建的 Bootstrap 专用 Token(新的 Bootstarp 推荐)</li><li>Static Password File: 跟静态 Token 类似</li><li>Service Account Tokens: 使用 Service Account 的 Token</li></ul><p>其他不再一一列举，具体请看文档 <a href="https://kubernetes.io/docs/admin/authentication/" target="_blank" rel="noopener">Authenticating</a>；了解了这些，后面我们使用 RBAC 控制 kubectl 权限的时候就要使用如上几种方法创建对应用户</p><h4 id="1-2、RBAC-权限相关"><a href="#1-2、RBAC-权限相关" class="headerlink" title="1.2、RBAC 权限相关"></a>1.2、RBAC 权限相关</h4><p>RBAC 权限定义部分主要有三个层级</p><ul><li>apiGroups: 指定那个 API 组下的权限</li><li>resources: 该组下具体资源，如 pod 等</li><li>verbs: 指对该资源具体执行哪些动作</li></ul><p>定义一组权限(角色)时要根据其所需的真正需求做最细粒度的划分</p><h3 id="二、创建一个只读的用户"><a href="#二、创建一个只读的用户" class="headerlink" title="二、创建一个只读的用户"></a>二、创建一个只读的用户</h3><h4 id="2-1、创建用户"><a href="#2-1、创建用户" class="headerlink" title="2.1、创建用户"></a>2.1、创建用户</h4><p>首先根据上文可以得知，Kubernetes 不存储用户具体细节信息，也就是说只要通过它的那几种方式能进来的用户，Kubernetes 就认为它是合法的；那么为了让 kubectl 只读，所以我们需要先给它创建一个用来承载只读权限的用户；这里用户创建我们选择使用证书方式</p><div class="hljs"><pre><code class="hljs sh"><span class="hljs-comment"># 首先先创建一个用于签发证书的 json(证书创建使用 cfssl)</span>&#123;  <span class="hljs-string">"CN"</span>: <span class="hljs-string">"readonly"</span>,  <span class="hljs-string">"hosts"</span>: [],  <span class="hljs-string">"key"</span>: &#123;    <span class="hljs-string">"algo"</span>: <span class="hljs-string">"rsa"</span>,    <span class="hljs-string">"size"</span>: 2048  &#125;,  <span class="hljs-string">"names"</span>: [    &#123;      <span class="hljs-string">"C"</span>: <span class="hljs-string">"CN"</span>,      <span class="hljs-string">"ST"</span>: <span class="hljs-string">"BeiJing"</span>,      <span class="hljs-string">"L"</span>: <span class="hljs-string">"BeiJing"</span>,      <span class="hljs-string">"O"</span>: <span class="hljs-string">"develop:readonly"</span>,      <span class="hljs-string">"OU"</span>: <span class="hljs-string">"develop"</span>    &#125;  ]&#125;</code></pre></div><p>然后基于以 Kubernetes CA 证书创建只读用户的证书</p><div class="hljs"><pre><code class="hljs sh">cfssl gencert --ca /etc/kubernetes/ssl/k8s-root-ca.pem \              --ca-key /etc/kubernetes/ssl/k8s-root-ca-key.pem \              --config k8s-gencert.json \              --profile kubernetes readonly.json | \              cfssljson --bare <span class="hljs-built_in">readonly</span></code></pre></div><p>以上命令会生成 <code>readonly-key.pem</code>、<code>readonly.pem</code> 两个证书文件以及一个 csr 请求文件</p><h4 id="2-2、创建-kubeconfig"><a href="#2-2、创建-kubeconfig" class="headerlink" title="2.2、创建 kubeconfig"></a>2.2、创建 kubeconfig</h4><p>有了用于证明身份的证书以后，接下来创建一个 kubeconfig 文件方便 kubectl 使用</p><div class="hljs"><pre><code class="hljs sh"><span class="hljs-meta">#!/bin/bash</span>KUBE_API_SERVER=<span class="hljs-string">"https://172.16.0.18:6443"</span>CERT_DIR=<span class="hljs-variable">$&#123;2:-"/etc/kubernetes/ssl"&#125;</span>kubectl config <span class="hljs-built_in">set</span>-cluster default-cluster --server=<span class="hljs-variable">$&#123;KUBE_API_SERVER&#125;</span> \    --certificate-authority=<span class="hljs-variable">$&#123;CERT_DIR&#125;</span>/k8s-root-ca.pem \    --embed-certs=<span class="hljs-literal">true</span> \    --kubeconfig=readonly.kubeconfigkubectl config <span class="hljs-built_in">set</span>-credentials develop-readonly \    --certificate-authority=<span class="hljs-variable">$&#123;CERT_DIR&#125;</span>/k8s-root-ca.pem \    --embed-certs=<span class="hljs-literal">true</span> \    --client-key=<span class="hljs-built_in">readonly</span>-key.pem \    --client-certificate=readonly.pem \    --kubeconfig=readonly.kubeconfigkubectl config <span class="hljs-built_in">set</span>-context default-system --cluster=default-cluster \    --user=develop-readonly \    --kubeconfig=readonly.kubeconfigkubectl config use-context default-system --kubeconfig=readonly.kubeconfig</code></pre></div><p>这条命令会将证书也写入到 readonly.kubeconfig 配置文件中，将该文件放在 <code>~/.kube/config</code> 位置，kubectl 会自动读取</p><h4 id="2-3、创建-ClusterRole"><a href="#2-3、创建-ClusterRole" class="headerlink" title="2.3、创建 ClusterRole"></a>2.3、创建 ClusterRole</h4><p>本示例创建的只读用户权限范围为 Cluster 集群范围，所以先创建一个只读权限的 ClusterRole；创建 ClusterRole 不知道都有哪些权限的话，最简单的办法是将集群的 admin ClusterRole 保存出来，然后做修改</p><div class="hljs"><pre><code class="hljs sh"><span class="hljs-comment"># 导出 admin ClusterRole</span>kubectl get clusterrole admin -o yaml &gt; readonly.yaml</code></pre></div><p>这个 admin ClusterRole 是默认存在的，导出后我们根据自己需求修改就行；最基本的原则就是像 update、delete 这种权限必须删掉(我们要创建只读用户)，修改后如下</p><div class="hljs"><pre><code class="hljs yaml"><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">rbac.authorization.k8s.io/v1beta1</span><span class="hljs-attr">kind:</span> <span class="hljs-string">ClusterRole</span><span class="hljs-attr">metadata:</span>  <span class="hljs-attr">name:</span> <span class="hljs-string">cluster-readonly</span><span class="hljs-attr">rules:</span><span class="hljs-bullet">-</span> <span class="hljs-attr">apiGroups:</span>  <span class="hljs-bullet">-</span> <span class="hljs-string">""</span>  <span class="hljs-attr">resources:</span>  <span class="hljs-bullet">-</span> <span class="hljs-string">pods</span>  <span class="hljs-bullet">-</span> <span class="hljs-string">pods/attach</span>  <span class="hljs-bullet">-</span> <span class="hljs-string">pods/exec</span>  <span class="hljs-bullet">-</span> <span class="hljs-string">pods/portforward</span>  <span class="hljs-bullet">-</span> <span class="hljs-string">pods/proxy</span>  <span class="hljs-attr">verbs:</span>  <span class="hljs-bullet">-</span> <span class="hljs-string">get</span>  <span class="hljs-bullet">-</span> <span class="hljs-string">list</span>  <span class="hljs-bullet">-</span> <span class="hljs-string">watch</span><span class="hljs-bullet">-</span> <span class="hljs-attr">apiGroups:</span>  <span class="hljs-bullet">-</span> <span class="hljs-string">""</span>  <span class="hljs-attr">resources:</span>  <span class="hljs-bullet">-</span> <span class="hljs-string">configmaps</span>  <span class="hljs-bullet">-</span> <span class="hljs-string">endpoints</span>  <span class="hljs-bullet">-</span> <span class="hljs-string">persistentvolumeclaims</span>  <span class="hljs-bullet">-</span> <span class="hljs-string">replicationcontrollers</span>  <span class="hljs-bullet">-</span> <span class="hljs-string">replicationcontrollers/scale</span>  <span class="hljs-bullet">-</span> <span class="hljs-string">secrets</span>  <span class="hljs-bullet">-</span> <span class="hljs-string">serviceaccounts</span>  <span class="hljs-bullet">-</span> <span class="hljs-string">services</span>  <span class="hljs-bullet">-</span> <span class="hljs-string">services/proxy</span>  <span class="hljs-attr">verbs:</span>  <span class="hljs-bullet">-</span> <span class="hljs-string">get</span>  <span class="hljs-bullet">-</span> <span class="hljs-string">list</span>  <span class="hljs-bullet">-</span> <span class="hljs-string">watch</span><span class="hljs-bullet">-</span> <span class="hljs-attr">apiGroups:</span>  <span class="hljs-bullet">-</span> <span class="hljs-string">""</span>  <span class="hljs-attr">resources:</span>  <span class="hljs-bullet">-</span> <span class="hljs-string">bindings</span>  <span class="hljs-bullet">-</span> <span class="hljs-string">events</span>  <span class="hljs-bullet">-</span> <span class="hljs-string">limitranges</span>  <span class="hljs-bullet">-</span> <span class="hljs-string">namespaces/status</span>  <span class="hljs-bullet">-</span> <span class="hljs-string">pods/log</span>  <span class="hljs-bullet">-</span> <span class="hljs-string">pods/status</span>  <span class="hljs-bullet">-</span> <span class="hljs-string">replicationcontrollers/status</span>  <span class="hljs-bullet">-</span> <span class="hljs-string">resourcequotas</span>  <span class="hljs-bullet">-</span> <span class="hljs-string">resourcequotas/status</span>  <span class="hljs-attr">verbs:</span>  <span class="hljs-bullet">-</span> <span class="hljs-string">get</span>  <span class="hljs-bullet">-</span> <span class="hljs-string">list</span>  <span class="hljs-bullet">-</span> <span class="hljs-string">watch</span><span class="hljs-bullet">-</span> <span class="hljs-attr">apiGroups:</span>  <span class="hljs-bullet">-</span> <span class="hljs-string">""</span>  <span class="hljs-attr">resources:</span>  <span class="hljs-bullet">-</span> <span class="hljs-string">namespaces</span>  <span class="hljs-attr">verbs:</span>  <span class="hljs-bullet">-</span> <span class="hljs-string">get</span>  <span class="hljs-bullet">-</span> <span class="hljs-string">list</span>  <span class="hljs-bullet">-</span> <span class="hljs-string">watch</span><span class="hljs-bullet">-</span> <span class="hljs-attr">apiGroups:</span>  <span class="hljs-bullet">-</span> <span class="hljs-string">apps</span>  <span class="hljs-attr">resources:</span>  <span class="hljs-bullet">-</span> <span class="hljs-string">deployments</span>  <span class="hljs-bullet">-</span> <span class="hljs-string">deployments/rollback</span>  <span class="hljs-bullet">-</span> <span class="hljs-string">deployments/scale</span>  <span class="hljs-bullet">-</span> <span class="hljs-string">statefulsets</span>  <span class="hljs-attr">verbs:</span>  <span class="hljs-bullet">-</span> <span class="hljs-string">get</span>  <span class="hljs-bullet">-</span> <span class="hljs-string">list</span>  <span class="hljs-bullet">-</span> <span class="hljs-string">watch</span><span class="hljs-bullet">-</span> <span class="hljs-attr">apiGroups:</span>  <span class="hljs-bullet">-</span> <span class="hljs-string">autoscaling</span>  <span class="hljs-attr">resources:</span>  <span class="hljs-bullet">-</span> <span class="hljs-string">horizontalpodautoscalers</span>  <span class="hljs-attr">verbs:</span>  <span class="hljs-bullet">-</span> <span class="hljs-string">get</span>  <span class="hljs-bullet">-</span> <span class="hljs-string">list</span>  <span class="hljs-bullet">-</span> <span class="hljs-string">watch</span><span class="hljs-bullet">-</span> <span class="hljs-attr">apiGroups:</span>  <span class="hljs-bullet">-</span> <span class="hljs-string">batch</span>  <span class="hljs-attr">resources:</span>  <span class="hljs-bullet">-</span> <span class="hljs-string">cronjobs</span>  <span class="hljs-bullet">-</span> <span class="hljs-string">jobs</span>  <span class="hljs-bullet">-</span> <span class="hljs-string">scheduledjobs</span>  <span class="hljs-attr">verbs:</span>  <span class="hljs-bullet">-</span> <span class="hljs-string">get</span>  <span class="hljs-bullet">-</span> <span class="hljs-string">list</span>  <span class="hljs-bullet">-</span> <span class="hljs-string">watch</span><span class="hljs-bullet">-</span> <span class="hljs-attr">apiGroups:</span>  <span class="hljs-bullet">-</span> <span class="hljs-string">extensions</span>  <span class="hljs-attr">resources:</span>  <span class="hljs-bullet">-</span> <span class="hljs-string">daemonsets</span>  <span class="hljs-bullet">-</span> <span class="hljs-string">deployments</span>  <span class="hljs-bullet">-</span> <span class="hljs-string">ingresses</span>  <span class="hljs-bullet">-</span> <span class="hljs-string">replicasets</span>  <span class="hljs-attr">verbs:</span>  <span class="hljs-bullet">-</span> <span class="hljs-string">get</span>  <span class="hljs-bullet">-</span> <span class="hljs-string">list</span>  <span class="hljs-bullet">-</span> <span class="hljs-string">watch</span></code></pre></div><p>最后执行 <code>kubectl create -f readonly.yaml</code> 创建即可</p><h4 id="2-4、创建-ClusterRoleBinding"><a href="#2-4、创建-ClusterRoleBinding" class="headerlink" title="2.4、创建 ClusterRoleBinding"></a>2.4、创建 ClusterRoleBinding</h4><p>用户已经创建完成，集群权限也有了，接下来使用 ClusterRoleBinding 绑定到一起即可</p><div class="hljs"><pre><code class="hljs yaml"><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">rbac.authorization.k8s.io/v1beta1</span><span class="hljs-attr">kind:</span> <span class="hljs-string">ClusterRoleBinding</span><span class="hljs-attr">metadata:</span>  <span class="hljs-attr">name:</span> <span class="hljs-string">cluster-readonly</span><span class="hljs-attr">roleRef:</span>  <span class="hljs-attr">apiGroup:</span> <span class="hljs-string">rbac.authorization.k8s.io</span>  <span class="hljs-attr">kind:</span> <span class="hljs-string">ClusterRole</span>  <span class="hljs-attr">name:</span> <span class="hljs-string">cluster-readonly</span><span class="hljs-attr">subjects:</span><span class="hljs-bullet">-</span> <span class="hljs-attr">apiGroup:</span> <span class="hljs-string">rbac.authorization.k8s.io</span>  <span class="hljs-attr">kind:</span> <span class="hljs-string">Group</span>  <span class="hljs-attr">name:</span> <span class="hljs-string">develop:readonly</span></code></pre></div><p>将以上保存为 <code>readonly-bind.yaml</code> 执行 <code>kubectl create -f readonly-bind.yaml</code> 即可</p><h4 id="2-5、测试权限"><a href="#2-5、测试权限" class="headerlink" title="2.5、测试权限"></a>2.5、测试权限</h4><p>将最初创建的 kubeconfig 放到 <code>~/.kube/config</code> 或者直接使用 <code>--kubeconfig</code> 选项测试读取、删除 pod 等权限即可，测试后如下所示</p><p><img src="https://cdn.oss.link/markdown/68ukm.png" srcset="/img/loading.gif" alt="test readonly"></p>]]></content:encoded>
      
      
      <category domain="https://mritd.com/categories/kubernetes/">Kubernetes</category>
      
      
      <category domain="https://mritd.com/tags/linux/">Linux</category>
      
      <category domain="https://mritd.com/tags/docker/">Docker</category>
      
      <category domain="https://mritd.com/tags/kubernetes/">Kubernetes</category>
      
      
      <comments>https://mritd.com/2018/03/20/use-rbac-to-control-kubectl-permissions/#disqus_thread</comments>
      
    </item>
    
    <item>
      <title>Kubernetes TLS bootstrapping 那点事</title>
      <link>https://mritd.com/2018/01/07/kubernetes-tls-bootstrapping-note/</link>
      <guid>https://mritd.com/2018/01/07/kubernetes-tls-bootstrapping-note/</guid>
      <pubDate>Sun, 07 Jan 2018 10:06:06 GMT</pubDate>
      
      <description>前段时间撸了一会 Kubernetes 官方文档，在查看 TLS bootstrapping 这块是发现已经跟 1.4 的时候完全不一样了；目前所有搭建文档也都保留着 1.4 时代的配置，在看完文档后发现目前配置有很多问题，同时也埋下了 **隐藏炸弹**，这个问题可能会在一年后爆发.....后果就是集群 node 全部掉线；所以仔细的撸了一下这个文档，从元旦到写此文章的时间都在测试这个 TLS bootstrapping，以下记录一下这次的成果</description>
      
      
      
      <content:encoded><![CDATA[<blockquote><p>前段时间撸了一会 Kubernetes 官方文档，在查看 TLS bootstrapping 这块是发现已经跟 1.4 的时候完全不一样了；目前所有搭建文档也都保留着 1.4 时代的配置，在看完文档后发现目前配置有很多问题，同时也埋下了 <strong>隐藏炸弹</strong>，这个问题可能会在一年后爆发…..后果就是集群 node 全部掉线；所以仔细的撸了一下这个文档，从元旦到写此文章的时间都在测试这个 TLS bootstrapping，以下记录一下这次的成果</p></blockquote><p>阅读本文章前，请先阅读一下本文参考的相关文档:</p><ul><li><a href="https://kubernetes.io/docs/admin/kubelet-tls-bootstrapping/" target="_blank" rel="noopener">TLS bootstrapping</a></li><li><a href="https://github.com/jcbsmpsn/community/blob/a843295a4f7594d41e66a8342e174f48d06b4f9f/contributors/design-proposals/kubelet-server-certificate-bootstrap-rotation.md" target="_blank" rel="noopener">Kubelet Server Certificate Bootstrap &amp; Rotation</a></li><li><a href="https://kubernetes.io/docs/admin/authorization/rbac/" target="_blank" rel="noopener">Using RBAC Authorization</a></li></ul><h3 id="一、TLS-bootstrapping-简介"><a href="#一、TLS-bootstrapping-简介" class="headerlink" title="一、TLS bootstrapping 简介"></a>一、TLS bootstrapping 简介</h3><p>Kubernetes 在 1.4 版本(我记着是)推出了 TLS bootstrapping 功能；这个功能主要解决了以下问题:</p><p>当集群开启了 TLS 认证后，每个节点的 kubelet 组件都要使用由 apiserver 使用的 CA 签发的有效证书才能与 apiserver 通讯；此时如果节点多起来，为每个节点单独签署证书将是一件非常繁琐的事情；TLS bootstrapping 功能就是让 kubelet 先使用一个预定的低权限用户连接到 apiserver，然后向 apiserver 申请证书，kubelet 的证书由 apiserver 动态签署；在配合 RBAC 授权模型下的工作流程大致如下所示(不完整，下面细说)</p><p><img src="https://cdn.oss.link/markdown/ixtwd.png" srcset="/img/loading.gif" alt="tls_bootstrapping"></p><h3 id="二、TLS-bootstrapping-相关术语"><a href="#二、TLS-bootstrapping-相关术语" class="headerlink" title="二、TLS bootstrapping 相关术语"></a>二、TLS bootstrapping 相关术语</h3><h4 id="2-1、kubelet-server"><a href="#2-1、kubelet-server" class="headerlink" title="2.1、kubelet server"></a>2.1、kubelet server</h4><p>在官方 TLS bootstrapping 文档中多次提到过 <code>kubelet server</code> 这个东西；在经过翻阅大量文档以及 TLS bootstrapping 设计文档后得出，<strong><code>kubelet server</code> 指的应该是 kubelet 的 10250 端口；</strong></p><p><strong>kubelet 组件在工作时，采用主动的查询机制，即定期请求 apiserver 获取自己所应当处理的任务，如哪些 pod 分配到了自己身上，从而去处理这些任务；同时 kubelet 自己还会暴露出两个本身 api 的端口，用于将自己本身的私有 api 暴露出去，这两个端口分别是 10250 与 10255；对于 10250 端口，kubelet 会在其上采用 TLS 加密以提供适当的鉴权功能；对于 10255 端口，kubelet 会以只读形式暴露组件本身的私有 api，并且不做鉴权处理</strong></p><p><strong>总结一下，就是说 kubelet 上实际上有两个地方用到证书，一个是用于与 API server 通讯所用到的证书，另一个是 kubelet 的 10250 私有 api 端口需要用到的证书</strong></p><h4 id="2-2、CSR-请求类型"><a href="#2-2、CSR-请求类型" class="headerlink" title="2.2、CSR 请求类型"></a>2.2、CSR 请求类型</h4><p>kubelet 发起的 CSR 请求都是由 controller manager 来做实际签署的，对于 controller manager 来说，TLS bootstrapping 下 kubelet 发起的 CSR 请求大致分为以下三种</p><ul><li>nodeclient: kubelet 以 <code>O=system:nodes</code> 和 <code>CN=system:node:(node name)</code> 形式发起的 CSR 请求</li><li>selfnodeclient: kubelet client renew 自己的证书发起的 CSR 请求(与上一个证书就有相同的 O 和 CN)</li><li>selfnodeserver: kubelet server renew 自己的证书发起的 CSR 请求</li></ul><p><strong>大白话加自己测试得出的结果: nodeclient 类型的 CSR 仅在第一次启动时会产生，selfnodeclient 类型的 CSR 请求实际上就是 kubelet renew 自己作为 client 跟 apiserver 通讯时使用的证书产生的，selfnodeserver 类型的 CSR 请求则是 kubelet 首次申请或后续 renew 自己的 10250 api 端口证书时产生的</strong></p><h3 id="三、TLS-bootstrapping-具体引导过程"><a href="#三、TLS-bootstrapping-具体引导过程" class="headerlink" title="三、TLS bootstrapping 具体引导过程"></a>三、TLS bootstrapping 具体引导过程</h3><h4 id="3-1、Kubernetes-TLS-与-RBAC-认证"><a href="#3-1、Kubernetes-TLS-与-RBAC-认证" class="headerlink" title="3.1、Kubernetes TLS 与 RBAC 认证"></a>3.1、Kubernetes TLS 与 RBAC 认证</h4><p>在说具体的引导过程之前先谈一下 TLS 和 RBAC，因为这两个事不整明白下面的都不用谈；</p><ul><li>TLS 作用</li></ul><p>众所周知 TLS 的作用就是对通讯加密，防止中间人窃听；同时如果证书不信任的话根本就无法与 apiserver 建立连接，更不用提有没有权限向 apiserver 请求指定内容</p><ul><li>RBAC 作用</li></ul><p>当 TLS 解决了通讯问题后，那么权限问题就应由 RBAC 解决(可以使用其他权限模型，如 ABAC)；RBAC 中规定了一个用户或者用户组(subject)具有请求哪些 api 的权限；<strong>在配合 TLS 加密的时候，实际上 apiserver 读取客户端证书的 CN 字段作为用户名，读取 O 字段作为用户组</strong></p><p>从以上两点上可以总结出两点: 第一，想要与 apiserver 通讯就必须采用由 apiserver CA 签发的证书，这样才能形成信任关系，建立 TLS 连接；第二，可以通过证书的 CN、O 字段来提供 RBAC 所需的用户与用户组</p><h4 id="3-2、kubelet-首次启动流程"><a href="#3-2、kubelet-首次启动流程" class="headerlink" title="3.2、kubelet 首次启动流程"></a>3.2、kubelet 首次启动流程</h4><p>看完上面的介绍，不知道有没有人想过，既然 TLS bootstrapping 功能是让 kubelet 组件去 apiserver 申请证书，然后用于连接 apiserver；<strong>那么第一次启动时没有证书如何连接 apiserver ?</strong></p><p>这个问题实际上可以去查看一下 <code>bootstrap.kubeconfig</code> 和 <code>token.csv</code> 得到答案: <strong>在 apiserver 配置中指定了一个 <code>token.csv</code> 文件，该文件中是一个预设的用户配置；同时该用户的 Token 和 apiserver 的 CA 证书被写入了 kubelet 所使用的 <code>bootstrap.kubeconfig</code> 配置文件中；这样在首次请求时，kubelet 使用 <code>bootstrap.kubeconfig</code> 中的 apiserver CA 证书来与 apiserver 建立 TLS 通讯，使用 <code>bootstrap.kubeconfig</code> 中的用户 Token 来向 apiserver 声明自己的 RBAC 授权身份</strong>，如下图所示</p><p><img src="https://cdn.oss.link/markdown/ji5ug.png" srcset="/img/loading.gif" alt="first_request"></p><p>在有些用户首次启动时，可能与遇到 kubelet 报 401 无权访问 apiserver 的错误；<strong>这是因为在默认情况下，kubelet 通过 <code>bootstrap.kubeconfig</code> 中的预设用户 Token 声明了自己的身份，然后创建 CSR 请求；但是不要忘记这个用户在我们不处理的情况下他没任何权限的，包括创建 CSR 请求；所以需要如下命令创建一个 ClusterRoleBinding，将预设用户 <code>kubelet-bootstrap</code> 与内置的 ClusterRole <code>system:node-bootstrapper</code> 绑定到一起，使其能够发起 CSR 请求</strong></p><div class="hljs"><pre><code class="hljs sh">kubectl create clusterrolebinding kubelet-bootstrap \  --clusterrole=system:node-bootstrapper \  --user=kubelet-bootstrap</code></pre></div><h4 id="3-3、手动签发证书"><a href="#3-3、手动签发证书" class="headerlink" title="3.3、手动签发证书"></a>3.3、手动签发证书</h4><p>在 kubelet 首次启动后，如果用户 Token 没问题，并且 RBAC 也做了相应的设置，那么此时在集群内应该能看到 kubelet 发起的 CSR 请求</p><p><img src="https://cdn.oss.link/markdown/n9bbw.png" srcset="/img/loading.gif" alt="bootstrap_csr"></p><p>出现 CSR 请求后，可以使用 kubectl 手动签发(允许) kubelet 的证书</p><p><img src="https://cdn.oss.link/markdown/5ssf8.png" srcset="/img/loading.gif" alt="bootstrap_approve_crt"></p><p><strong>当成功签发证书后，目标节点的 kubelet 会将证书写入到 <code>--cert-dir=</code> 选项指定的目录中；注意此时如果不做其他设置应当生成四个文件</strong></p><p><img src="https://cdn.oss.link/markdown/a25ip.png" srcset="/img/loading.gif" alt="bootstrap_crt"></p><p><strong>而 kubelet 与 apiserver 通讯所使用的证书为 <code>kubelet-client.crt</code>，剩下的 <code>kubelet.crt</code> 将会被用于 <code>kubelet server</code>(10250) 做鉴权使用；注意，此时 <code>kubelet.crt</code> 这个证书是个独立于 apiserver CA 的自签 CA，并且删除后 kubelet 组件会重新生成它</strong></p><h3 id="四、TLS-bootstrapping-证书自动续期"><a href="#四、TLS-bootstrapping-证书自动续期" class="headerlink" title="四、TLS bootstrapping 证书自动续期"></a>四、TLS bootstrapping 证书自动续期</h3><blockquote><p>单独把这部分拿出来写，是因为个人觉得上面已经有点乱了；这部分实际上更复杂，只好单独写一下了，因为这部分涉及的东西比较多，所以也不想草率的几笔带过</p></blockquote><h4 id="4-1、RBAC-授权"><a href="#4-1、RBAC-授权" class="headerlink" title="4.1、RBAC 授权"></a>4.1、RBAC 授权</h4><p>首先…首先好几次了…嗯，就是说 kubelet 所发起的 CSR 请求是由 controller manager 签署的；如果想要是实现自动续期，就需要让 controller manager 能够在 kubelet 发起证书请求的时候自动帮助其签署证书；那么 controller manager 不可能对所有的 CSR 证书申请都自动签署，这时候就需要配置 RBAC 规则，<strong>保证 controller manager 只对 kubelet 发起的特定 CSR 请求自动批准即可</strong>；在 TLS bootstrapping 官方文档中，针对上面 2.2 章节提出的 3 种 CSR 请求分别给出了 3 种对应的 ClusterRole，如下所示</p><div class="hljs"><pre><code class="hljs yaml"><span class="hljs-comment"># A ClusterRole which instructs the CSR approver to approve a user requesting</span><span class="hljs-comment"># node client credentials.</span><span class="hljs-attr">kind:</span> <span class="hljs-string">ClusterRole</span><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">rbac.authorization.k8s.io/v1</span><span class="hljs-attr">metadata:</span>  <span class="hljs-attr">name:</span> <span class="hljs-string">approve-node-client-csr</span><span class="hljs-attr">rules:</span><span class="hljs-bullet">-</span> <span class="hljs-attr">apiGroups:</span> <span class="hljs-string">["certificates.k8s.io"]</span>  <span class="hljs-attr">resources:</span> <span class="hljs-string">["certificatesigningrequests/nodeclient"]</span>  <span class="hljs-attr">verbs:</span> <span class="hljs-string">["create"]</span><span class="hljs-meta">---</span><span class="hljs-comment"># A ClusterRole which instructs the CSR approver to approve a node renewing its</span><span class="hljs-comment"># own client credentials.</span><span class="hljs-attr">kind:</span> <span class="hljs-string">ClusterRole</span><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">rbac.authorization.k8s.io/v1</span><span class="hljs-attr">metadata:</span>  <span class="hljs-attr">name:</span> <span class="hljs-string">approve-node-client-renewal-csr</span><span class="hljs-attr">rules:</span><span class="hljs-bullet">-</span> <span class="hljs-attr">apiGroups:</span> <span class="hljs-string">["certificates.k8s.io"]</span>  <span class="hljs-attr">resources:</span> <span class="hljs-string">["certificatesigningrequests/selfnodeclient"]</span>  <span class="hljs-attr">verbs:</span> <span class="hljs-string">["create"]</span><span class="hljs-meta">---</span><span class="hljs-comment"># A ClusterRole which instructs the CSR approver to approve a node requesting a</span><span class="hljs-comment"># serving cert matching its client cert.</span><span class="hljs-attr">kind:</span> <span class="hljs-string">ClusterRole</span><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">rbac.authorization.k8s.io/v1</span><span class="hljs-attr">metadata:</span>  <span class="hljs-attr">name:</span> <span class="hljs-string">approve-node-server-renewal-csr</span><span class="hljs-attr">rules:</span><span class="hljs-bullet">-</span> <span class="hljs-attr">apiGroups:</span> <span class="hljs-string">["certificates.k8s.io"]</span>  <span class="hljs-attr">resources:</span> <span class="hljs-string">["certificatesigningrequests/selfnodeserver"]</span>  <span class="hljs-attr">verbs:</span> <span class="hljs-string">["create"]</span></code></pre></div><p>RBAC 中 ClusterRole 只是描述或者说定义一种集群范围内的能力，这三个 ClusterRole 在 1.7 之前需要自己手动创建，在 1.8 后 apiserver 会自动创建前两个(1.8 以后名称有改变，自己查看文档)；以上三个 ClusterRole 含义如下</p><ul><li>approve-node-client-csr: 具有自动批准 nodeclient 类型 CSR 请求的能力</li><li>approve-node-client-renewal-csr: 具有自动批准 selfnodeclient 类型 CSR 请求的能力</li><li>approve-node-server-renewal-csr: 具有自动批准 selfnodeserver 类型 CSR 请求的能力</li></ul><p><strong>所以，如果想要 kubelet 能够自动续期，那么就应当将适当的 ClusterRole 绑定到 kubelet 自动续期时所所采用的用户或者用户组身上</strong></p><h4 id="4-2、自动续期下的引导过程"><a href="#4-2、自动续期下的引导过程" class="headerlink" title="4.2、自动续期下的引导过程"></a>4.2、自动续期下的引导过程</h4><p>在自动续期下引导过程与单纯的手动批准 CSR 有点差异，具体的引导流程地址如下</p><ul><li>kubelet 读取 bootstrap.kubeconfig，使用其 CA 与 Token 向 apiserver 发起第一次 CSR 请求(nodeclient)</li><li>apiserver 根据 RBAC 规则自动批准首次 CSR 请求(approve-node-client-csr)，并下发证书(kubelet-client.crt)</li><li>kubelet <strong>使用刚刚签发的证书(O=system:nodes, CN=system:node:NODE_NAME)</strong>与 apiserver 通讯，并发起申请 10250 server 所使用证书的 CSR 请求</li><li>apiserver 根据 RBAC 规则自动批准 kubelet 为其 10250 端口申请的证书(kubelet-server-current.crt)</li><li>证书即将到期时，kubelet 自动向 apiserver 发起用于与 apiserver 通讯所用证书的 renew CSR 请求和 renew 本身 10250 端口所用证书的 CSR 请求</li><li>apiserver 根据 RBAC 规则自动批准两个证书</li><li>kubelet 拿到新证书后关闭所有连接，reload 新证书，以后便一直如此</li></ul><p><strong>从以上流程我们可以看出，我们如果要创建 RBAC 规则，则至少能满足四种情况:</strong></p><ul><li>自动批准 kubelet 首次用于与 apiserver 通讯证书的 CSR 请求(nodeclient)</li><li>自动批准 kubelet 首次用于 10250 端口鉴权的 CSR 请求(实际上这个请求走的也是 selfnodeserver 类型 CSR)</li><li>自动批准 kubelet 后续 renew 用于与 apiserver 通讯证书的 CSR 请求(selfnodeclient)</li><li>自动批准 kubelet 后续 renew 用于 10250 端口鉴权的 CSR 请求(selfnodeserver)</li></ul><p>基于以上四种情况，我们需要创建 3 个 ClusterRoleBinding，创建如下</p><div class="hljs"><pre><code class="hljs sh"><span class="hljs-comment"># 自动批准 kubelet 的首次 CSR 请求(用于与 apiserver 通讯的证书)</span>kubectl create clusterrolebinding node-client-auto-approve-csr --clusterrole=approve-node-client-csr --group=system:bootstrappers<span class="hljs-comment"># 自动批准 kubelet 后续 renew 用于与 apiserver 通讯证书的 CSR 请求</span>kubectl create clusterrolebinding node-client-auto-renew-crt --clusterrole=approve-node-client-renewal-csr --group=system:nodes<span class="hljs-comment"># 自动批准 kubelet 发起的用于 10250 端口鉴权证书的 CSR 请求(包括后续 renew)</span>kubectl create clusterrolebinding node-server-auto-renew-crt --clusterrole=approve-node-server-renewal-csr --group=system:nodes</code></pre></div><h4 id="4-3、开启自动续期"><a href="#4-3、开启自动续期" class="headerlink" title="4.3、开启自动续期"></a>4.3、开启自动续期</h4><p>在 1.7 后，kubelet 启动时增加 <code>--feature-gates=RotateKubeletClientCertificate=true,RotateKubeletServerCertificate=true</code> 选项，则 kubelet 在证书即将到期时会自动发起一个 renew 自己证书的 CSR 请求；同时 controller manager 需要在启动时增加 <code>--feature-gates=RotateKubeletServerCertificate=true</code> 参数，再配合上面创建好的 ClusterRoleBinding，kubelet client 和 kubelet server 证才书会被自动签署；</p><p><strong>注意，1.7 版本设置自动续期参数后，新的 renew 请求不会立即开始，而是在证书总有效期的 <code>70%~90%</code> 的时间时发起；而且经测试 1.7 版本即使自动签发了证书，kubelet 在不重启的情况下不会重新应用新证书；在 1.8 后 kubelet 组件在增加一个 <code>--rotate-certificates</code> 参数后，kubelet 才会自动重载新证书</strong></p><h4 id="4-3、证书过期问题"><a href="#4-3、证书过期问题" class="headerlink" title="4.3、证书过期问题"></a>4.3、证书过期问题</h4><p>需要重复强调一个问题是: <strong>TLS bootstrapping 时的证书实际是由 kube-controller-manager 组件来签署的，也就是说证书有效期是 kube-controller-manager 组件控制的</strong>；所以在 1.7 版本以后(我查文档发现的从1.7开始有) kube-controller-manager 组件提供了一个 <code>--experimental-cluster-signing-duration</code> 参数来设置签署的证书有效时间；默认为 <code>8760h0m0s</code>，将其改为 <code>87600h0m0s</code> 即 10 年后再进行 TLS bootstrapping 签署证书即可。</p><h3 id="五、TLS-bootstrapping-总结以及详细操作"><a href="#五、TLS-bootstrapping-总结以及详细操作" class="headerlink" title="五、TLS bootstrapping 总结以及详细操作"></a>五、TLS bootstrapping 总结以及详细操作</h3><h4 id="5-1、主要流程细节"><a href="#5-1、主要流程细节" class="headerlink" title="5.1、主要流程细节"></a>5.1、主要流程细节</h4><p>kubelet 首次启动通过加载 <code>bootstrap.kubeconfig</code> 中的用户 Token 和 apiserver CA 证书发起首次 CSR 请求，这个 Token 被预先内置在 apiserver 节点的 token.csv 中，其身份为 <code>kubelet-bootstrap</code> 用户和 <code>system:bootstrappers</code> 用户组；想要首次 CSR 请求能成功(成功指的是不会被 apiserver 401 拒绝)，则需要先将 <code>kubelet-bootstrap</code> 用户和 <code>system:node-bootstrapper</code> 内置 ClusterRole 绑定；</p><p>对于首次 CSR 请求可以手动批准，也可以将 <code>system:bootstrappers</code> 用户组与 <code>approve-node-client-csr</code> ClusterRole 绑定实现自动批准(1.8 之前这个 ClusterRole 需要手动创建，1.8 后 apiserver 自动创建，并更名为 <code>system:certificates.k8s.io:certificatesigningrequests:nodeclient</code>)</p><p>默认签署的的证书只有 1 年有效期，如果想要调整证书有效期可以通过设置 kube-controller-manager 的 <code>--experimental-cluster-signing-duration</code> 参数实现，该参数默认值为 <code>8760h0m0s</code></p><p>对于证书自动续签，需要通过协调两个方面实现；第一，想要 kubelet 在证书到期后自动发起续期请求，则需要在 kubelet 启动时增加 <code>--feature-gates=RotateKubeletClientCertificate=true,RotateKubeletServerCertificate=true</code> 来实现；第二，想要让 controller manager 自动批准续签的 CSR 请求需要在 controller manager 启动时增加 <code>--feature-gates=RotateKubeletServerCertificate=true</code> 参数，并绑定对应的 RBAC 规则；<strong>同时需要注意的是 1.7 版本的 kubelet 自动续签后需要手动重启 kubelet 以使其重新加载新证书，而 1.8 后只需要在 kublet 启动时附带 <code>--rotate-certificates</code> 选项就会自动重新加载新证书</strong></p><h4 id="5-2、证书及配置文件作用"><a href="#5-2、证书及配置文件作用" class="headerlink" title="5.2、证书及配置文件作用"></a>5.2、证书及配置文件作用</h4><ul><li>token.csv</li></ul><p>该文件为一个用户的描述文件，基本格式为 <code>Token,用户名,UID,用户组</code>；这个文件在 apiserver 启动时被 apiserver 加载，然后就相当于在集群内创建了一个这个用户；接下来就可以用 RBAC 给他授权；持有这个用户 Token 的组件访问 apiserver 的时候，apiserver 根据 RBAC 定义的该用户应当具有的权限来处理相应请求</p><ul><li>bootstarp.kubeconfig</li></ul><p>该文件中内置了 token.csv 中用户的 Token，以及 apiserver CA 证书；kubelet 首次启动会加载此文件，使用 apiserver CA 证书建立与 apiserver 的 TLS 通讯，使用其中的用户 Token 作为身份标识像 apiserver 发起 CSR 请求</p><ul><li>kubelet-client.crt</li></ul><p>该文件在 kubelet 完成 TLS bootstrapping 后生成，此证书是由 controller manager 签署的，此后 kubelet 将会加载该证书，用于与 apiserver 建立 TLS 通讯，同时使用该证书的 CN 字段作为用户名，O 字段作为用户组向 apiserver 发起其他请求</p><ul><li>kubelet.crt</li></ul><p>该文件在 kubelet 完成 TLS bootstrapping 后并且<strong>没有配置 <code>--feature-gates=RotateKubeletServerCertificate=true</code> 时才会生成</strong>；这种情况下该文件为一个独立于 apiserver CA 的自签 CA 证书，有效期为 1 年；被用作 kubelet 10250 api 端口</p><ul><li>kubelet-server.crt</li></ul><p>该文件在 kubelet 完成 TLS bootstrapping 后并且<strong>配置了 <code>--feature-gates=RotateKubeletServerCertificate=true</code> 时才会生成</strong>；这种情况下该证书由 apiserver CA 签署，默认有效期同样是 1 年，被用作 kubelet 10250 api 端口鉴权</p><ul><li>kubelet-client-current.pem</li></ul><p>这是一个软连接文件，当 kubelet 配置了 <code>--feature-gates=RotateKubeletClientCertificate=true</code> 选项后，会在证书总有效期的 <code>70%~90%</code> 的时间内发起续期请求，请求被批准后会生成一个 <code>kubelet-client-时间戳.pem</code>；<code>kubelet-client-current.pem</code> 文件则始终软连接到最新的真实证书文件，除首次启动外，kubelet 一直会使用这个证书同  apiserver 通讯</p><ul><li>kubelet-server-current.pem</li></ul><p>同样是一个软连接文件，当 kubelet 配置了 <code>--feature-gates=RotateKubeletServerCertificate=true</code> 选项后，会在证书总有效期的 <code>70%~90%</code> 的时间内发起续期请求，请求被批准后会生成一个 <code>kubelet-server-时间戳.pem</code>；<code>kubelet-server-current.pem</code> 文件则始终软连接到最新的真实证书文件，该文件将会一直被用于 kubelet 10250 api 端口鉴权</p><h4 id="5-3、1-7-TLS-bootstrapping-配置"><a href="#5-3、1-7-TLS-bootstrapping-配置" class="headerlink" title="5.3、1.7 TLS bootstrapping 配置"></a>5.3、1.7 TLS bootstrapping 配置</h4><p>apiserver 预先放置 token.csv，内容样例如下</p><div class="hljs"><pre><code class="hljs sh">6df3c701f979cee17732c30958745947,kubelet-bootstrap,10001,<span class="hljs-string">"system:bootstrappers"</span></code></pre></div><p>允许 kubelet-bootstrap 用户创建首次启动的 CSR 请求</p><div class="hljs"><pre><code class="hljs sh">kubectl create clusterrolebinding kubelet-bootstrap \  --clusterrole=system:node-bootstrapper \  --user=kubelet-bootstrap</code></pre></div><p>配置 kubelet 自动续期，<strong>RotateKubeletClientCertificate 用于自动续期 kubelet 连接 apiserver 所用的证书(kubelet-client-xxxx.pem)，RotateKubeletServerCertificate 用于自动续期 kubelet 10250 api 端口所使用的证书(kubelet-server-xxxx.pem)</strong></p><div class="hljs"><pre><code class="hljs sh">KUBELET_ARGS=<span class="hljs-string">"--cgroup-driver=cgroupfs \</span><span class="hljs-string">              --cluster-dns=10.254.0.2 \</span><span class="hljs-string">              --resolv-conf=/etc/resolv.conf \</span><span class="hljs-string">              --experimental-bootstrap-kubeconfig=/etc/kubernetes/bootstrap.kubeconfig \</span><span class="hljs-string">              --feature-gates=RotateKubeletClientCertificate=true,RotateKubeletServerCertificate=true \</span><span class="hljs-string">              --kubeconfig=/etc/kubernetes/kubelet.kubeconfig \</span><span class="hljs-string">              --fail-swap-on=false \</span><span class="hljs-string">              --cert-dir=/etc/kubernetes/ssl \</span><span class="hljs-string">              --cluster-domain=cluster.local. \</span><span class="hljs-string">              --hairpin-mode=promiscuous-bridge \</span><span class="hljs-string">              --serialize-image-pulls=false \</span><span class="hljs-string">              --pod-infra-container-image=gcr.io/google_containers/pause-amd64:3.0"</span></code></pre></div><p>配置 controller manager 自动批准相关 CSR 请求，<strong>如果不配置 <code>--feature-gates=RotateKubeletServerCertificate=true</code> 参数，则即使配置了相关的 RBAC 规则，也只会自动批准 kubelet client 的 renew 请求</strong> </p><div class="hljs"><pre><code class="hljs sh">KUBE_CONTROLLER_MANAGER_ARGS=<span class="hljs-string">"--address=0.0.0.0 \</span><span class="hljs-string">                              --service-cluster-ip-range=10.254.0.0/16 \</span><span class="hljs-string">                              --feature-gates=RotateKubeletServerCertificate=true \</span><span class="hljs-string">                              --cluster-name=kubernetes \</span><span class="hljs-string">                              --cluster-signing-cert-file=/etc/kubernetes/ssl/k8s-root-ca.pem \</span><span class="hljs-string">                              --cluster-signing-key-file=/etc/kubernetes/ssl/k8s-root-ca-key.pem \</span><span class="hljs-string">                              --service-account-private-key-file=/etc/kubernetes/ssl/k8s-root-ca-key.pem \</span><span class="hljs-string">                              --root-ca-file=/etc/kubernetes/ssl/k8s-root-ca.pem \</span><span class="hljs-string">                              --leader-elect=true \</span><span class="hljs-string">                              --node-monitor-grace-period=40s \</span><span class="hljs-string">                              --node-monitor-period=5s \</span><span class="hljs-string">                              --pod-eviction-timeout=5m0s"</span></code></pre></div><p>创建自动批准相关 CSR 请求的 ClusterRole</p><div class="hljs"><pre><code class="hljs yaml"><span class="hljs-comment"># A ClusterRole which instructs the CSR approver to approve a user requesting</span><span class="hljs-comment"># node client credentials.</span><span class="hljs-attr">kind:</span> <span class="hljs-string">ClusterRole</span><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">rbac.authorization.k8s.io/v1</span><span class="hljs-attr">metadata:</span>  <span class="hljs-attr">name:</span> <span class="hljs-string">approve-node-client-csr</span><span class="hljs-attr">rules:</span><span class="hljs-bullet">-</span> <span class="hljs-attr">apiGroups:</span> <span class="hljs-string">["certificates.k8s.io"]</span>  <span class="hljs-attr">resources:</span> <span class="hljs-string">["certificatesigningrequests/nodeclient"]</span>  <span class="hljs-attr">verbs:</span> <span class="hljs-string">["create"]</span><span class="hljs-meta">---</span><span class="hljs-comment"># A ClusterRole which instructs the CSR approver to approve a node renewing its</span><span class="hljs-comment"># own client credentials.</span><span class="hljs-attr">kind:</span> <span class="hljs-string">ClusterRole</span><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">rbac.authorization.k8s.io/v1</span><span class="hljs-attr">metadata:</span>  <span class="hljs-attr">name:</span> <span class="hljs-string">approve-node-client-renewal-csr</span><span class="hljs-attr">rules:</span><span class="hljs-bullet">-</span> <span class="hljs-attr">apiGroups:</span> <span class="hljs-string">["certificates.k8s.io"]</span>  <span class="hljs-attr">resources:</span> <span class="hljs-string">["certificatesigningrequests/selfnodeclient"]</span>  <span class="hljs-attr">verbs:</span> <span class="hljs-string">["create"]</span><span class="hljs-meta">---</span><span class="hljs-comment"># A ClusterRole which instructs the CSR approver to approve a node requesting a</span><span class="hljs-comment"># serving cert matching its client cert.</span><span class="hljs-attr">kind:</span> <span class="hljs-string">ClusterRole</span><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">rbac.authorization.k8s.io/v1</span><span class="hljs-attr">metadata:</span>  <span class="hljs-attr">name:</span> <span class="hljs-string">approve-node-server-renewal-csr</span><span class="hljs-attr">rules:</span><span class="hljs-bullet">-</span> <span class="hljs-attr">apiGroups:</span> <span class="hljs-string">["certificates.k8s.io"]</span>  <span class="hljs-attr">resources:</span> <span class="hljs-string">["certificatesigningrequests/selfnodeserver"]</span>  <span class="hljs-attr">verbs:</span> <span class="hljs-string">["create"]</span></code></pre></div><p>将 ClusterRole 绑定到适当的用户组，以完成自动批准相关 CSR 请求</p><div class="hljs"><pre><code class="hljs sh"><span class="hljs-comment"># 自动批准 system:bootstrappers 组用户 TLS bootstrapping 首次申请证书的 CSR 请求</span>kubectl create clusterrolebinding node-client-auto-approve-csr --clusterrole=approve-node-client-csr --group=system:bootstrappers<span class="hljs-comment"># 自动批准 system:nodes 组用户更新 kubelet 自身与 apiserver 通讯证书的 CSR 请求</span>kubectl create clusterrolebinding node-client-auto-renew-crt --clusterrole=approve-node-client-renewal-csr --group=system:nodes<span class="hljs-comment"># 自动批准 system:nodes 组用户更新 kubelet 10250 api 端口证书的 CSR 请求</span>kubectl create clusterrolebinding node-server-auto-renew-crt --clusterrole=approve-node-server-renewal-csr --group=system:nodes</code></pre></div><p><strong>一切就绪后启动 kubelet 组件即可，不过需要注意的是 1.7 版本 kubelet 不会自动重载 renew 的证书，需要自己手动重启</strong></p><h4 id="5-4、1-8-TLS-bootstrapping-配置"><a href="#5-4、1-8-TLS-bootstrapping-配置" class="headerlink" title="5.4、1.8 TLS bootstrapping 配置"></a>5.4、1.8 TLS bootstrapping 配置</h4><p>apiserver 预先放置 token.csv，内容样例如下</p><div class="hljs"><pre><code class="hljs sh">6df3c701f979cee17732c30958745947,kubelet-bootstrap,10001,<span class="hljs-string">"system:bootstrappers"</span></code></pre></div><p>允许 kubelet-bootstrap 用户创建首次启动的 CSR 请求</p><div class="hljs"><pre><code class="hljs sh">kubectl create clusterrolebinding kubelet-bootstrap \  --clusterrole=system:node-bootstrapper \  --user=kubelet-bootstrap</code></pre></div><p>配置 kubelet 自动续期，<strong>RotateKubeletClientCertificate 用于自动续期 kubelet 连接 apiserver 所用的证书(kubelet-client-xxxx.pem)，RotateKubeletServerCertificate 用于自动续期 kubelet 10250 api 端口所使用的证书(kubelet-server-xxxx.pem)，<code>--rotate-certificates</code> 选项使得 kubelet 能够自动重载新证书</strong></p><div class="hljs"><pre><code class="hljs sh">KUBELET_ARGS=<span class="hljs-string">"--cgroup-driver=cgroupfs \</span><span class="hljs-string">              --cluster-dns=10.254.0.2 \</span><span class="hljs-string">              --resolv-conf=/etc/resolv.conf \</span><span class="hljs-string">              --experimental-bootstrap-kubeconfig=/etc/kubernetes/bootstrap.kubeconfig \</span><span class="hljs-string">              --feature-gates=RotateKubeletClientCertificate=true,RotateKubeletServerCertificate=true \</span><span class="hljs-string">              --rotate-certificates \</span><span class="hljs-string">              --kubeconfig=/etc/kubernetes/kubelet.kubeconfig \</span><span class="hljs-string">              --fail-swap-on=false \</span><span class="hljs-string">              --cert-dir=/etc/kubernetes/ssl \</span><span class="hljs-string">              --cluster-domain=cluster.local. \</span><span class="hljs-string">              --hairpin-mode=promiscuous-bridge \</span><span class="hljs-string">              --serialize-image-pulls=false \</span><span class="hljs-string">              --pod-infra-container-image=gcr.io/google_containers/pause-amd64:3.0"</span></code></pre></div><p>配置 controller manager 自动批准相关 CSR 请求，<strong>如果不配置 <code>--feature-gates=RotateKubeletServerCertificate=true</code> 参数，则即使配置了相关的 RBAC 规则，也只会自动批准 kubelet client 的 renew 请求</strong> </p><div class="hljs"><pre><code class="hljs sh">KUBE_CONTROLLER_MANAGER_ARGS=<span class="hljs-string">"--address=0.0.0.0 \</span><span class="hljs-string">                              --service-cluster-ip-range=10.254.0.0/16 \</span><span class="hljs-string">                              --cluster-name=kubernetes \</span><span class="hljs-string">                              --cluster-signing-cert-file=/etc/kubernetes/ssl/k8s-root-ca.pem \</span><span class="hljs-string">                              --cluster-signing-key-file=/etc/kubernetes/ssl/k8s-root-ca-key.pem \</span><span class="hljs-string">                              --service-account-private-key-file=/etc/kubernetes/ssl/k8s-root-ca-key.pem \</span><span class="hljs-string">                              --feature-gates=RotateKubeletServerCertificate=true \</span><span class="hljs-string">                              --root-ca-file=/etc/kubernetes/ssl/k8s-root-ca.pem \</span><span class="hljs-string">                              --leader-elect=true \</span><span class="hljs-string">                              --experimental-cluster-signing-duration 10m0s \</span><span class="hljs-string">                              --node-monitor-grace-period=40s \</span><span class="hljs-string">                              --node-monitor-period=5s \</span><span class="hljs-string">                              --pod-eviction-timeout=5m0s"</span></code></pre></div><p>创建自动批准相关 CSR 请求的 ClusterRole，相对于 1.7 版本，1.8 的 apiserver 自动创建了前两条 ClusterRole，所以只需要创建一条就行了</p><div class="hljs"><pre><code class="hljs yaml"><span class="hljs-comment"># A ClusterRole which instructs the CSR approver to approve a node requesting a</span><span class="hljs-comment"># serving cert matching its client cert.</span><span class="hljs-attr">kind:</span> <span class="hljs-string">ClusterRole</span><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">rbac.authorization.k8s.io/v1</span><span class="hljs-attr">metadata:</span>  <span class="hljs-attr">name:</span> <span class="hljs-string">system:certificates.k8s.io:certificatesigningrequests:selfnodeserver</span><span class="hljs-attr">rules:</span><span class="hljs-bullet">-</span> <span class="hljs-attr">apiGroups:</span> <span class="hljs-string">["certificates.k8s.io"]</span>  <span class="hljs-attr">resources:</span> <span class="hljs-string">["certificatesigningrequests/selfnodeserver"]</span>  <span class="hljs-attr">verbs:</span> <span class="hljs-string">["create"]</span></code></pre></div><p>将 ClusterRole 绑定到适当的用户组，以完成自动批准相关 CSR 请求</p><div class="hljs"><pre><code class="hljs sh"><span class="hljs-comment"># 自动批准 system:bootstrappers 组用户 TLS bootstrapping 首次申请证书的 CSR 请求</span>kubectl create clusterrolebinding node-client-auto-approve-csr --clusterrole=system:certificates.k8s.io:certificatesigningrequests:nodeclient --group=system:bootstrappers<span class="hljs-comment"># 自动批准 system:nodes 组用户更新 kubelet 自身与 apiserver 通讯证书的 CSR 请求</span>kubectl create clusterrolebinding node-client-auto-renew-crt --clusterrole=system:certificates.k8s.io:certificatesigningrequests:selfnodeclient --group=system:nodes<span class="hljs-comment"># 自动批准 system:nodes 组用户更新 kubelet 10250 api 端口证书的 CSR 请求</span>kubectl create clusterrolebinding node-server-auto-renew-crt --clusterrole=system:certificates.k8s.io:certificatesigningrequests:selfnodeserver --group=system:nodes</code></pre></div><p><strong>一切就绪后启动 kubelet 组件即可，1.8 版本 kubelet 会自动重载证书，以下为 1.8 版本在运行一段时间后的相关证书截图</strong></p><p><img src="https://cdn.oss.link/markdown/570wk.png" srcset="/img/loading.gif" alt="tls_bootstrapping_crts"></p>]]></content:encoded>
      
      
      <category domain="https://mritd.com/categories/kubernetes/">Kubernetes</category>
      
      
      <category domain="https://mritd.com/tags/linux/">Linux</category>
      
      <category domain="https://mritd.com/tags/docker/">Docker</category>
      
      <category domain="https://mritd.com/tags/kubernetes/">Kubernetes</category>
      
      
      <comments>https://mritd.com/2018/01/07/kubernetes-tls-bootstrapping-note/#disqus_thread</comments>
      
    </item>
    
    <item>
      <title>CI/CD 之 GitLab CI</title>
      <link>https://mritd.com/2017/11/28/ci-cd-gitlab-ci/</link>
      <guid>https://mritd.com/2017/11/28/ci-cd-gitlab-ci/</guid>
      <pubDate>Tue, 28 Nov 2017 09:43:23 GMT</pubDate>
      
      <description>接着上篇文章整理，这篇文章主要介绍一下 GitLab CI 相关功能，并通过 GitLab CI 实现自动化构建项目；项目中所用的示例项目已经上传到了 [GitHub](https://github.com/mritd/GitLabCI-TestProject)</description>
      
      
      
      <content:encoded><![CDATA[<blockquote><p>接着上篇文章整理，这篇文章主要介绍一下 GitLab CI 相关功能，并通过 GitLab CI 实现自动化构建项目；项目中所用的示例项目已经上传到了 <a href="https://github.com/mritd/GitLabCI-TestProject" target="_blank" rel="noopener">GitHub</a></p></blockquote><h3 id="一、环境准备"><a href="#一、环境准备" class="headerlink" title="一、环境准备"></a>一、环境准备</h3><p>首先需要有一台 GitLab 服务器，然后需要有个项目；这里示例项目以 Spring Boot 项目为例，然后最好有一台专门用来 Build 的机器，实际生产中如果 Build 任务不频繁可适当用一些业务机器进行 Build；本文示例所有组件将采用 Docker 启动， GitLab HA 等不在本文阐述范围内</p><ul><li>Docker Version : 1.13.1</li><li>GitLab Version : 10.1.4-ce.0</li><li>GitLab Runner Version : 10.1.0</li><li>GitLab IP : 172.16.0.37</li><li>GitLab Runner IP : 172.16.0.36</li></ul><h3 id="二、GitLab-CI-简介"><a href="#二、GitLab-CI-简介" class="headerlink" title="二、GitLab CI 简介"></a>二、GitLab CI 简介</h3><p>GitLab CI 是 GitLab 默认集成的 CI 功能，GitLab CI 通过在项目内 <code>.gitlab-ci.yaml</code> 配置文件读取 CI 任务并进行相应处理；GitLab CI 通过其称为 GitLab Runner 的 Agent 端进行 build 操作；Runner 本身可以使用多种方式安装，比如使用 Docker 镜像启动等；Runner 在进行 build 操作时也可以选择多种 build 环境提供者；比如直接在 Runner 所在宿主机 build、通过新创建虚拟机(vmware、virtualbox)进行 build等；同时 Runner 支持 Docker 作为 build 提供者，即每次 build 新启动容器进行 build；GitLab CI 其大致架构如下</p><p><img src="https://cdn.oss.link/markdown/wejnz.png" srcset="/img/loading.gif" alt="GitLab"></p><h3 id="三、搭建-GitLab-服务器"><a href="#三、搭建-GitLab-服务器" class="headerlink" title="三、搭建 GitLab 服务器"></a>三、搭建 GitLab 服务器</h3><h4 id="3-1、GitLab-搭建"><a href="#3-1、GitLab-搭建" class="headerlink" title="3.1、GitLab 搭建"></a>3.1、GitLab 搭建</h4><p>GitLab 搭建这里直接使用 docker compose 启动，compose 配置如下</p><div class="hljs"><pre><code class="hljs sh">version: <span class="hljs-string">'2'</span>services:  gitlab:    image: <span class="hljs-string">'gitlab/gitlab-ce:10.1.4-ce.0'</span>    restart: always    container_name: gitlab    hostname: <span class="hljs-string">'git.mritd.me'</span>    environment:      GITLAB_OMNIBUS_CONFIG: |        external_url <span class="hljs-string">'http://git.mritd.me'</span>        <span class="hljs-comment"># Add any other gitlab.rb configuration here, each on its own line</span>    ports:      - <span class="hljs-string">'80:80'</span>      - <span class="hljs-string">'443:443'</span>      - <span class="hljs-string">'8022:22'</span>    volumes:      - <span class="hljs-string">'./data/gitlab/config:/etc/gitlab'</span>      - <span class="hljs-string">'./data/gitlab/logs:/var/log/gitlab'</span>      - <span class="hljs-string">'./data/gitlab/data:/var/opt/gitlab'</span></code></pre></div><p>直接启动后，首次登陆需要设置初始密码如下，默认用户为 <code>root</code></p><p><img src="https://cdn.oss.link/markdown/5go94.png" srcset="/img/loading.gif" alt="gitkab init"></p><p>登陆成功后创建一个用户(该用户最好给予 Admin 权限，以后操作以该用户为例)，并且创建一个测试 Group 和 Project，如下所示</p><p><img src="https://cdn.oss.link/markdown/vtyhi.png" srcset="/img/loading.gif" alt="Create User"></p><p><img src="https://cdn.oss.link/markdown/3b7gl.png" srcset="/img/loading.gif" alt="Test Project"></p><h4 id="3-2、增加示例项目"><a href="#3-2、增加示例项目" class="headerlink" title="3.2、增加示例项目"></a>3.2、增加示例项目</h4><p>这里示例项目采用 Java 的 SpringBoot 项目，并采用 Gradle 构建，其他语言原理一样；<strong>如果不熟悉 Java 的没必要死磕此步配置，任意语言(最好 Java)整一个能用的 Web 项目就行，并不强求一定 Java 并且使用 Gradle 构建，以下只是一个样例项目</strong>；SpringBoot 可以采用 <a href="https://start.spring.io/" target="_blank" rel="noopener">Spring Initializr</a> 直接生成(依赖要加入 WEB)，如下所示</p><p><img src="https://cdn.oss.link/markdown/0wx6d.png" srcset="/img/loading.gif" alt="Spring Initializr"></p><p>将项目导入 IDEA，然后创建一个 index 示例页面，主要修改如下</p><ul><li>build.gradle</li></ul><div class="hljs"><pre><code class="hljs groovy">buildscript &#123;    ext &#123;        springBootVersion = <span class="hljs-string">'1.5.8.RELEASE'</span>    &#125;    repositories &#123;        mavenCentral()    &#125;    dependencies &#123;        classpath(<span class="hljs-string">"org.springframework.boot:spring-boot-gradle-plugin:$&#123;springBootVersion&#125;"</span>)    &#125;&#125;apply <span class="hljs-string">plugin:</span> <span class="hljs-string">'java'</span>apply <span class="hljs-string">plugin:</span> <span class="hljs-string">'eclipse'</span>apply <span class="hljs-string">plugin:</span> <span class="hljs-string">'idea'</span>apply <span class="hljs-string">plugin:</span> <span class="hljs-string">'org.springframework.boot'</span>group = <span class="hljs-string">'me.mritd'</span>version = <span class="hljs-string">'0.0.1-SNAPSHOT'</span>sourceCompatibility = <span class="hljs-number">1.8</span>repositories &#123;    mavenCentral()&#125;dependencies &#123;    compile(<span class="hljs-string">'org.springframework.boot:spring-boot-starter'</span>)    compile(<span class="hljs-string">'org.springframework.boot:spring-boot-starter-web'</span>)    compile(<span class="hljs-string">'org.springframework.boot:spring-boot-starter-thymeleaf'</span>)    testCompile(<span class="hljs-string">'org.springframework.boot:spring-boot-starter-test'</span>)&#125;</code></pre></div><ul><li>新建一个 HomeController</li></ul><div class="hljs"><pre><code class="hljs java"><span class="hljs-keyword">package</span> me.mritd.TestProject;<span class="hljs-keyword">import</span> org.springframework.stereotype.Controller;<span class="hljs-keyword">import</span> org.springframework.web.bind.annotation.RequestMapping;<span class="hljs-comment">/*******************************************************************************</span><span class="hljs-comment"> * Copyright (c) 2005-2017 Mritd, Inc.</span><span class="hljs-comment"> * TestProject</span><span class="hljs-comment"> * me.mritd.TestProject</span><span class="hljs-comment"> * Created by mritd on 2017/11/24 下午12:23.</span><span class="hljs-comment"> * Description: </span><span class="hljs-comment"> *******************************************************************************/</span><span class="hljs-meta">@Controller</span><span class="hljs-keyword">public</span> <span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">HomeController</span> </span>&#123;    <span class="hljs-meta">@RequestMapping</span>(<span class="hljs-string">"/"</span>)    <span class="hljs-function"><span class="hljs-keyword">public</span> String <span class="hljs-title">home</span><span class="hljs-params">()</span></span>&#123;        <span class="hljs-keyword">return</span> <span class="hljs-string">"index"</span>;    &#125;&#125;</code></pre></div><ul><li>templates 下新建 index.html</li></ul><div class="hljs"><pre><code class="hljs html"><span class="hljs-meta">&lt;!DOCTYPE <span class="hljs-meta-keyword">html</span>&gt;</span><span class="hljs-tag">&lt;<span class="hljs-name">html</span> <span class="hljs-attr">lang</span>=<span class="hljs-string">"en"</span>&gt;</span><span class="hljs-tag">&lt;<span class="hljs-name">head</span>&gt;</span>    <span class="hljs-tag">&lt;<span class="hljs-name">meta</span> <span class="hljs-attr">charset</span>=<span class="hljs-string">"UTF-8"</span>/&gt;</span>    <span class="hljs-tag">&lt;<span class="hljs-name">title</span>&gt;</span>Title<span class="hljs-tag">&lt;/<span class="hljs-name">title</span>&gt;</span><span class="hljs-tag">&lt;/<span class="hljs-name">head</span>&gt;</span><span class="hljs-tag">&lt;<span class="hljs-name">body</span>&gt;</span><span class="hljs-tag">&lt;<span class="hljs-name">h1</span>&gt;</span>Test...<span class="hljs-tag">&lt;/<span class="hljs-name">h1</span>&gt;</span><span class="hljs-tag">&lt;/<span class="hljs-name">body</span>&gt;</span><span class="hljs-tag">&lt;/<span class="hljs-name">html</span>&gt;</span></code></pre></div><p>最后项目整体结构如下</p><p><img src="https://cdn.oss.link/markdown/5k12p.png" srcset="/img/loading.gif" alt="TestProject"></p><p>执行 <code>assemble</code> Task 打包出可执行 jar 包，并运行 <code>java -jar TestProject-0.0.1-SNAPSHOT.jar</code> 测试下能启动访问页面即可</p><p><img src="https://cdn.oss.link/markdown/xoj3d.png" srcset="/img/loading.gif" alt="TestProject assemble"></p><p>最后将项目提交到 GitLab 后如下</p><p><img src="https://cdn.oss.link/markdown/1fuex.png" srcset="/img/loading.gif" alt="init Project"></p><h3 id="四、GitLab-CI-配置"><a href="#四、GitLab-CI-配置" class="headerlink" title="四、GitLab CI 配置"></a>四、GitLab CI 配置</h3><blockquote><p>针对这一章节创建基础镜像以及项目镜像，这里仅以 Java 项目为例；其他语言原理相通，按照其他语言对应的运行环境修改即可</p></blockquote><h4 id="4-1、增加-Runner"><a href="#4-1、增加-Runner" class="headerlink" title="4.1、增加 Runner"></a>4.1、增加 Runner</h4><p>GitLab CI 在进行构建时会将任务下发给 Runner，让 Runner 去执行；所以先要添加一个 Runner，Runner 这里采用 Docker Compose 启动，build 方式也使用 Docker 方式 Build；compose 文件如下</p><div class="hljs"><pre><code class="hljs yaml"><span class="hljs-attr">version:</span> <span class="hljs-string">'2'</span><span class="hljs-attr">services:</span>  <span class="hljs-attr">gitlab-runner:</span>    <span class="hljs-attr">container_name:</span> <span class="hljs-string">gitlab-runner</span>    <span class="hljs-attr">image:</span> <span class="hljs-string">gitlab/gitlab-runner:alpine-v10.1.0</span>    <span class="hljs-attr">restart:</span> <span class="hljs-string">always</span>    <span class="hljs-attr">network_mode:</span> <span class="hljs-string">"host"</span>    <span class="hljs-attr">volumes:</span>      <span class="hljs-bullet">-</span> <span class="hljs-string">/var/run/docker.sock:/var/run/docker.sock</span>      <span class="hljs-bullet">-</span> <span class="hljs-string">./config.toml:/etc/gitlab-runner/config.toml</span>    <span class="hljs-attr">extra_hosts:</span>      <span class="hljs-bullet">-</span> <span class="hljs-string">"git.mritd.me:172.16.0.37"</span></code></pre></div><p><strong>在启动前，我们需要先 touch 一下这个 config.toml 配置文件</strong>；该文件是 Runner 的运行配置，此后 Runner 所有配置都会写入这个文件(不 touch 出来 docker-compose 发现不存在会挂载一个目录进去，导致 Runner 启动失败)；启动 docker-compose 后，<strong>需要进入容器执行注册，让 Runner 主动去连接 GitLab 服务器</strong></p><div class="hljs"><pre><code class="hljs sh"><span class="hljs-comment"># 生成 Runner 配置文件</span>touch config.toml<span class="hljs-comment"># 启动 Runner</span>docker-compose up -d<span class="hljs-comment"># 激活 Runner</span>docker <span class="hljs-built_in">exec</span> -it gitlab-runner gitlab-runner register</code></pre></div><p>在执行上一条激活命令后，会按照提示让你输入一些信息；<strong>首先输入 GitLab 地址，然后是 Runner Token，Runner Token 可以从 GitLab 设置中查看</strong>，如下所示</p><p><img src="https://cdn.oss.link/markdown/mfqg7.png" srcset="/img/loading.gif" alt="Runner Token"></p><p>整体注册流程如下</p><p><img src="https://cdn.oss.link/markdown/r7xay.png" srcset="/img/loading.gif" alt="Runner registry"></p><p>注册完成后，在 GitLab Runner 设置中就可以看到刚刚注册的 Runner，如下所示</p><p><img src="https://cdn.oss.link/markdown/xv03e.png" srcset="/img/loading.gif" alt="Runner List"></p><p><strong>Runner 注册成功后会将配置写入到 config.toml 配置文件；由于两个测试宿主机都没有配置内网 DNS，所以为了保证 runner 在使用 docker build 时能正确的找到 GitLab 仓库地址，还需要增加一个 docker 的 host 映射( <code>extra_hosts</code> )；同时为了能调用 宿主机 Docker 和持久化 build 的一些缓存还挂载了一些文件和目录；完整的 配置如下(配置文件可以做一些更高级的配置，具体参考 <a href="https://docs.gitlab.com/runner/configuration/advanced-configuration.html" target="_blank" rel="noopener">官方文档</a> )</strong></p><ul><li>config.toml</li></ul><div class="hljs"><pre><code class="hljs toml"><span class="hljs-attr">concurrent</span> = <span class="hljs-number">1</span><span class="hljs-attr">check_interval</span> = <span class="hljs-number">0</span><span class="hljs-section">[[runners]]</span>  name = "Test Runner"  url = "http://git.mritd.me"  token = "c279ec1ac08aec98c7141c7cf2d474"  executor = "docker"  builds_dir = "/gitlab/runner-builds"  cache_dir = "/gitlab/runner-cache"  <span class="hljs-section">[runners.docker]</span>    tls_verify = false    image = "debian"    privileged = false    disable_cache = false    shm_size = 0    volumes = ["/data/gitlab-runner:/gitlab","/var/run/docker.sock:/var/run/docker.sock","/data/maven_repo:/data/repo","/data/maven_repo:/data/maven","/data/gradle:/data/gradle","/data/sonar_cache:/root/.sonar","/data/androidsdk:/usr/local/android","/data/node_modules:/data/node_modules"]    extra_hosts = ["git.mritd.me:172.16.0.37"]  <span class="hljs-section">[runners.cache]</span></code></pre></div><p><strong>注意，这里声明的 Volumes 会在每个运行的容器中都生效；也就是说 build 时新开启的每个容器都会被挂载这些目录</strong>；修改完成后重启 runner 容器即可，由于 runner 中没啥可保存的东西，所以可以直接 <code>docker-compose down &amp;&amp; docker-compose up -d</code> 重启</p><h4 id="4-2、创建基础镜像"><a href="#4-2、创建基础镜像" class="headerlink" title="4.2、创建基础镜像"></a>4.2、创建基础镜像</h4><p>由于示例项目是一个 Java 项目，而且是采用 Spring Boot 的，所以该项目想要运行起来只需要一个 java 环境即可，中间件已经被打包到了 jar 包中；以下是一个作为基础运行环境的 openjdk 镜像的 Dockerfile</p><div class="hljs"><pre><code class="hljs sh">FROM alpine:edge LABEL maintainer=<span class="hljs-string">"mritd &lt;mritd1234@gmail.com&gt;"</span>ENV JAVA_HOME /usr/lib/jvm/java-1.8-openjdkENV PATH <span class="hljs-variable">$PATH</span>:/usr/lib/jvm/java-1.8-openjdk/jre/bin:/usr/lib/jvm/java-1.8-openjdk/binRUN apk add --update bash curl tar wget ca-certificates unzip \        openjdk8 font-adobe-100dpi ttf-dejavu fontconfig \    &amp;&amp; rm -rf /var/cache/apk/* \CMD [<span class="hljs-string">"bash"</span>]</code></pre></div><p><strong>这个 openjdk Dockerfile 升级到了 8.151 版本，并且集成了一些字体相关的软件，以解决在 Java 中某些验证码库无法运行问题，详见 <a href="https://mritd.me/2017/09/27/alpine-3.6-openjdk-8-bug/" target="_blank" rel="noopener">Alpine 3.6 OpenJDK 8 Bug</a></strong>；使用这个 Dockerfile，在当前目录执行 <code>docker build -t mritd/openjdk:8 .</code> build 一个 openjdk8 的基础镜像，然后将其推送到私服，或者 Docker Hub 即可</p><h4 id="4-3、创建项目镜像"><a href="#4-3、创建项目镜像" class="headerlink" title="4.3、创建项目镜像"></a>4.3、创建项目镜像</h4><p>有了基本的 openjdk 的 docker 镜像后，针对于项目每次 build 都应该生成一个包含发布物的 docker 镜像，所以对于项目来说还需要一个项目本身的 Dockerfile；<strong>项目的 Dockerfile 有两种使用方式；一种是动态生成 Dockerfile，然后每次使用新生成的 Dockerfile 去 build；还有一种是写一个通用的 Dockerfile，build 时利用 ARG 参数传入变量</strong>；这里采用第二种方式，以下为一个可以反复使用的 Dockerfile</p><div class="hljs"><pre><code class="hljs sh">FROM mritd/openjdk:8-144-01MAINTAINER mritd &lt;mritd1234@gmail.com&gt;ARG PROJECT_BUILD_FINALNAMEENV TZ <span class="hljs-string">'Asia/Shanghai'</span>ENV PROJECT_BUILD_FINALNAME <span class="hljs-variable">$&#123;PROJECT_BUILD_FINALNAME&#125;</span>COPY build/libs/<span class="hljs-variable">$&#123;PROJECT_BUILD_FINALNAME&#125;</span>.jar /<span class="hljs-variable">$&#123;PROJECT_BUILD_FINALNAME&#125;</span>.jarCMD [<span class="hljs-string">"bash"</span>,<span class="hljs-string">"-c"</span>,<span class="hljs-string">"java -jar /<span class="hljs-variable">$&#123;PROJECT_BUILD_FINALNAME&#125;</span>.jar"</span>]</code></pre></div><p><strong>该 Dockerfile 通过声明一个 <code>PROJECT_BUILD_FINALNAME</code> 变量来表示项目的发布物名称；然后将其复制到根目录下，最终利用 java 执行这个 jar 包；所以每次 build 之前只要能拿到项目发布物的名称即可</strong></p><h4 id="4-4、Gradle-修改"><a href="#4-4、Gradle-修改" class="headerlink" title="4.4、Gradle 修改"></a>4.4、Gradle 修改</h4><p>上面已经创建了一个标准的通用型 Dockerfile，每次 build 镜像只要传入 <code>PROJECT_BUILD_FINALNAME</code> 这个最终发布物名称即可；对于发布物名称来说，最好不要固定死；当然不论是 Java 还是其他语言的项目我们都能将最终发布物变成一个固定名字，最不济可以写脚本重命名一下；但是不建议那么干，最好保留版本号信息，以便于异常情况下进入容器能够分辨；对于当前 Java 项目来说，想要拿到 <code>PROJECT_BUILD_FINALNAME</code> 很简单，我们只需要略微修改一下 Gradle 的 build 脚本，让其每次打包 jar 包时将项目的名称及版本号导出到文件中即可；同时这里也加入了镜像版本号的处理，Gradle 脚本修改如下</p><ul><li>build.gradle 最后面增加如下</li></ul><div class="hljs"><pre><code class="hljs groovy">bootRepackage &#123;    mainClass = <span class="hljs-string">'me.mritd.TestProject.TestProjectApplication'</span>    executable = <span class="hljs-literal">true</span>    doLast &#123;        File envFile = <span class="hljs-keyword">new</span> File(<span class="hljs-string">"build/tmp/PROJECT_ENV"</span>)        println(<span class="hljs-string">"Create $&#123;archivesBaseName&#125; ENV File ===&gt; "</span> + envFile.createNewFile())        println(<span class="hljs-string">"Export $&#123;archivesBaseName&#125; Build Version ===&gt; $&#123;version&#125;"</span>)        envFile.write(<span class="hljs-string">"export PROJECT_BUILD_FINALNAME=$&#123;archivesBaseName&#125;-$&#123;version&#125;\n"</span>)        println(<span class="hljs-string">"Generate Docker image tag..."</span>)        envFile.append(<span class="hljs-string">"export BUILD_DATE=`date +%Y%m%d%H%M%S`\n"</span>)        envFile.append(<span class="hljs-string">"export IMAGE_NAME=mritd/test:`echo \$&#123;CI_BUILD_REF_NAME&#125; | tr '/' '-'`-`echo \$&#123;CI_COMMIT_SHA&#125; | cut -c1-8`-\$&#123;BUILD_DATE&#125;\n"</span>)        envFile.append(<span class="hljs-string">"export LATEST_IMAGE_NAME=mritd/test:latest\n"</span>)    &#125;&#125;</code></pre></div><p><strong>这一步操作实际上是修改了 <code>bootRepackage</code> 这个 Task(不了解 Gradle 或者不是 Java 项目的请忽略)，在其结束后创建了一个叫 <code>PROJECT_ENV</code> 的文件，里面实际上就是写入了一些 bash 环境变量声明，以方便后面 source 一下这个文件拿到一些变量，然后用户 build 镜像使用</strong>，<code>PROJECT_ENV</code> 最终生成如下</p><div class="hljs"><pre><code class="hljs sh"><span class="hljs-built_in">export</span> PROJECT_BUILD_FINALNAME=TestProject-0.0.1-SNAPSHOT<span class="hljs-built_in">export</span> BUILD_DATE=`date +%Y%m%d%H%M%S`<span class="hljs-built_in">export</span> IMAGE_NAME=mritd/<span class="hljs-built_in">test</span>:`<span class="hljs-built_in">echo</span> <span class="hljs-variable">$&#123;CI_BUILD_REF_NAME&#125;</span> | tr <span class="hljs-string">'/'</span> <span class="hljs-string">'-'</span>`-`<span class="hljs-built_in">echo</span> <span class="hljs-variable">$&#123;CI_COMMIT_SHA&#125;</span> | cut -c1-8`-<span class="hljs-variable">$&#123;BUILD_DATE&#125;</span><span class="hljs-built_in">export</span> LATEST_IMAGE_NAME=mritd/<span class="hljs-built_in">test</span>:latest</code></pre></div><p><img src="https://cdn.oss.link/markdown/gr6kc.png" srcset="/img/loading.gif" alt="PROJECT_ENV"></p><h4 id="4-5、创建-CI-配置文件"><a href="#4-5、创建-CI-配置文件" class="headerlink" title="4.5、创建 CI 配置文件"></a>4.5、创建 CI 配置文件</h4><p>一切准备就绪以后，就可以编写 CI 脚本了；GitLab 依靠读取项目根目录下的 <code>.gitlab-ci.yml</code> 文件来执行相应的 CI 操作；以下为测试项目的 <code>.gitlab-ci.yml</code> 配置</p><div class="hljs"><pre><code class="hljs yaml"><span class="hljs-comment"># 调试开启</span><span class="hljs-comment">#before_script:</span><span class="hljs-comment">#  - pwd</span><span class="hljs-comment">#  - env</span><span class="hljs-attr">cache:</span>  <span class="hljs-attr">key:</span> <span class="hljs-string">$CI_PROJECT_NAME/$CI_COMMIT_REF_NAME-$CI_COMMIT_SHA</span>  <span class="hljs-attr">paths:</span>    <span class="hljs-bullet">-</span> <span class="hljs-string">build</span><span class="hljs-attr">stages:</span>  <span class="hljs-bullet">-</span> <span class="hljs-string">build</span>  <span class="hljs-bullet">-</span> <span class="hljs-string">deploy</span><span class="hljs-attr">auto-build:</span>  <span class="hljs-attr">image:</span> <span class="hljs-string">mritd/build:2.1.1</span>  <span class="hljs-attr">stage:</span> <span class="hljs-string">build</span>  <span class="hljs-attr">script:</span>    <span class="hljs-bullet">-</span> <span class="hljs-string">gradle</span> <span class="hljs-string">--no-daemon</span> <span class="hljs-string">clean</span> <span class="hljs-string">assemble</span>  <span class="hljs-attr">tags:</span>    <span class="hljs-bullet">-</span> <span class="hljs-string">test</span><span class="hljs-attr">deploy:</span>  <span class="hljs-attr">image:</span> <span class="hljs-string">mritd/docker-kubectl:v1.7.4</span>  <span class="hljs-attr">stage:</span> <span class="hljs-string">deploy</span>  <span class="hljs-attr">script:</span>    <span class="hljs-bullet">-</span> <span class="hljs-string">source</span> <span class="hljs-string">build/tmp/PROJECT_ENV</span>    <span class="hljs-bullet">-</span> <span class="hljs-string">echo</span> <span class="hljs-string">"Build Docker Image ==&gt; $&#123;IMAGE_NAME&#125;"</span>    <span class="hljs-bullet">-</span> <span class="hljs-string">docker</span> <span class="hljs-string">build</span> <span class="hljs-string">-t</span> <span class="hljs-string">$&#123;IMAGE_NAME&#125;</span> <span class="hljs-string">--build-arg</span> <span class="hljs-string">PROJECT_BUILD_FINALNAME=$&#123;PROJECT_BUILD_FINALNAME&#125;</span> <span class="hljs-string">.</span><span class="hljs-comment">#    - docker push $&#123;IMAGE_NAME&#125;</span>    <span class="hljs-bullet">-</span> <span class="hljs-string">docker</span> <span class="hljs-string">tag</span> <span class="hljs-string">$&#123;IMAGE_NAME&#125;</span> <span class="hljs-string">$&#123;LATEST_IMAGE_NAME&#125;</span><span class="hljs-comment">#    - docker push $&#123;LATEST_IMAGE_NAME&#125;</span><span class="hljs-comment">#    - docker rmi $&#123;IMAGE_NAME&#125; $&#123;LATEST_IMAGE_NAME&#125;</span><span class="hljs-comment">#    - kubectl --kubeconfig $&#123;KUBE_CONFIG&#125; set image deployment/test test=$IMAGE_NAME</span>  <span class="hljs-attr">tags:</span>    <span class="hljs-bullet">-</span> <span class="hljs-string">test</span>  <span class="hljs-attr">only:</span>    <span class="hljs-bullet">-</span> <span class="hljs-string">master</span>    <span class="hljs-bullet">-</span> <span class="hljs-string">develop</span>    <span class="hljs-bullet">-</span> <span class="hljs-string">/^chore.*$/</span></code></pre></div><p><strong>关于 CI 配置的一些简要说明如下</strong></p><h5 id="stages"><a href="#stages" class="headerlink" title="stages"></a>stages</h5><p>stages 字段定义了整个 CI 一共有哪些阶段流程，以上的 CI 配置中，定义了该项目的 CI 总共分为 <code>build</code>、<code>deploy</code> 两个阶段；GitLab CI 会根据其顺序执行对应阶段下的所有任务；<strong>在正常生产环境流程可以定义很多个，比如可以有 <code>test</code>、<code>publish</code>，甚至可能有代码扫描的 <code>sonar</code> 阶段等；这些阶段没有任何限制，完全是自定义的</strong>，上面的阶段定义好后在 CI 中表现如下图</p><p><img src="https://cdn.oss.link/markdown/8c7gs.png" srcset="/img/loading.gif" alt="stages"></p><h5 id="task"><a href="#task" class="headerlink" title="task"></a>task</h5><p>task 隶属于 stages 之下；也就是说一个阶段可以有多个任务，任务执行顺序默认不指定会并发执行；对于上面的 CI 配置来说 <code>auto-build</code> 和 <code>deploy</code> 都是 task，他们通过 <code>stage: xxxx</code> 这个标签来指定他们隶属于哪个 stage；当 Runner 使用 Docker 作为 build 提供者时，我们可以在 task 的 <code>image</code> 标签下声明该 task 要使用哪个镜像运行，不指定则默认为 Runner 注册时的镜像(这里是 debian)；<strong>同时 task 还有一个 <code>tags</code> 的标签，该标签指明了这个任务将可以在哪些 Runner 上运行；这个标签可以从 Runner 页面看到，实际上就是 Runner 注册时输入的哪个 tag；对于某些特殊的项目，比如 IOS 项目，则必须在特定机器上执行，所以此时指定 tags 标签很有用</strong>，当 task 运行后如下图所示</p><p><img src="https://cdn.oss.link/markdown/qzvlh.png" srcset="/img/loading.gif" alt="Task"></p><p>除此之外 task 还能指定 <code>only</code> 标签用于限定那些分支才能触发这个 task，如果分支名字不满足则不会触发；<strong>默认情况下，这些 task 都是自动执行的，如果感觉某些任务太过危险，则可以通过增加 <code>when: manual</code> 改为手动执行；注意: 手动执行被 GitLab 认为是高权限的写操作，所以只有项目管理员才能手动运行一个 task，直白的说就是管理员才能点击</strong>；手动执行如下图所示</p><p><img src="https://cdn.oss.link/markdown/vcjci.png" srcset="/img/loading.gif" alt="manual task"></p><h5 id="cache"><a href="#cache" class="headerlink" title="cache"></a>cache</h5><p>cache 这个参数用于定义全局那些文件将被 cache；<strong>在 GitLab CI 中，跨 stage 是不能保存东西的；也就是说在第一步 build 的操作生成的 jar 包，到第二部打包 docker image 时就会被删除；GitLab 会保证每个 stage 中任务在执行时都将工作目录(Docker 容器 中)还原到跟 GitLab 代码仓库中一模一样，多余文件及变更都会被删除</strong>；正常情况下，第一步 build 生成 jar 包应当立即推送到 nexus 私服；但是这里测试没有搭建，所以只能放到本地；但是放到本地下一个 task 就会删除它，所以利用 <code>cache</code> 这个参数将 <code>build</code> 目录 cache 住，保证其跨 stage 也能存在</p><p><strong>关于 <code>.gitlab-ci.yml</code> 具体配置更完整的请参考 <a href="https://docs.gitlab.com/ee/ci/yaml/" target="_blank" rel="noopener">官方文档</a></strong></p><h3 id="五、其他相关"><a href="#五、其他相关" class="headerlink" title="五、其他相关"></a>五、其他相关</h3><h4 id="5-1、GitLab-内置环境变量"><a href="#5-1、GitLab-内置环境变量" class="headerlink" title="5.1、GitLab 内置环境变量"></a>5.1、GitLab 内置环境变量</h4><p>上面已经基本搞定了一个项目的 CI，但是有些变量可能并未说清楚；比如在创建的 <code>PROJECT_ENV</code> 文件中引用了 <code>${CI_COMMIT_SHA}</code> 变量；这种变量其实是 GitLab CI 的内置隐藏变量，这些变量在每次 CI 调用 Runner 运行某个任务时都会传递到对应的 Runner 的执行环境中；<strong>也就是说这些变量在每次的任务容器 SHELL 环境中都会存在，可以直接引用</strong>，具体的完整环境变量列表可以从 <a href="https://docs.gitlab.com/ee/ci/variables/" target="_blank" rel="noopener">官方文档</a> 中获取；如果想知道环境变量具体的值，实际上可以通过在任务执行前用 <code>env</code> 指令打印出来，如下所示</p><p><img src="https://cdn.oss.link/markdown/la9kn.png" srcset="/img/loading.gif" alt="env"></p><p><img src="https://cdn.oss.link/markdown/0175j.png" srcset="/img/loading.gif" alt="env task"></p><h4 id="5-2、GitLab-自定义环境变量"><a href="#5-2、GitLab-自定义环境变量" class="headerlink" title="5.2、GitLab 自定义环境变量"></a>5.2、GitLab 自定义环境变量</h4><p>在某些情况下，我们希望 CI 能自动的发布或者修改一些东西；比如将 jar 包上传到 nexus、将 docker 镜像 push 到私服；这些动作往往需要一个高权限或者说有可写入对应仓库权限的账户来支持，但是这些账户又不想写到项目的 CI 配置里；因为这样很不安全，谁都能看到；此时我们可以将这些敏感变量写入到 GitLab 自定义环境变量中，GitLab 会像对待内置变量一样将其传送到 Runner 端，以供我们使用；GitLab 中自定义的环境变量可以有两种，一种是项目级别的，只能够在当前项目使用，如下</p><p><img src="https://cdn.oss.link/markdown/ennug.png" srcset="/img/loading.gif" alt="project env"></p><p>另一种是组级别的，可以在整个组内的所有项目中使用，如下</p><p><img src="https://cdn.oss.link/markdown/si8ig.png" srcset="/img/loading.gif" alt="group env"></p><p>这两种变量添加后都可以在 CI 的脚本中直接引用</p><h4 id="5-3、Kubernetes-集成"><a href="#5-3、Kubernetes-集成" class="headerlink" title="5.3、Kubernetes 集成"></a>5.3、Kubernetes 集成</h4><p>对于 Kubernetes 集成实际上有两种方案，一种是对接 Kubernetes 的 api，纯代码实现；另一种取巧的方案是调用 kubectl 工具，用 kubectl 工具来实现滚动升级；这里采用后一种取巧的方式，将 kubectl 二进制文件封装到镜像中，然后在 deploy 阶段使用这个镜像直接部署就可以</p><p><img src="https://cdn.oss.link/markdown/bu17r.png" srcset="/img/loading.gif" alt="kubectl"></p><p>其中 <code>mritd/docker-kubectl:v1.7.4</code> 这个镜像的 Dockerfile 如下</p><div class="hljs"><pre><code class="hljs sh">FROM docker:dind LABEL maintainer=<span class="hljs-string">"mritd &lt;mritd1234@gmail.com&gt;"</span>ARG TZ=<span class="hljs-string">"Asia/Shanghai"</span>ENV TZ <span class="hljs-variable">$&#123;TZ&#125;</span>ENV KUBE_VERSION v1.8.0RUN apk upgrade --update \    &amp;&amp; apk add bash tzdata wget ca-certificates \    &amp;&amp; wget https://storage.googleapis.com/kubernetes-release/release/<span class="hljs-variable">$&#123;KUBE_VERSION&#125;</span>/bin/linux/amd64/kubectl -O /usr/<span class="hljs-built_in">local</span>/bin/kubectl \    &amp;&amp; chmod +x /usr/<span class="hljs-built_in">local</span>/bin/kubectl \    &amp;&amp; ln -sf /usr/share/zoneinfo/<span class="hljs-variable">$&#123;TZ&#125;</span> /etc/localtime \    &amp;&amp; <span class="hljs-built_in">echo</span> <span class="hljs-variable">$&#123;TZ&#125;</span> &gt; /etc/timezone \    &amp;&amp; rm -rf /var/cache/apk/*CMD [<span class="hljs-string">"/bin/bash"</span>]</code></pre></div><p>这里面的 <code>${KUBE_CONFIG}</code> 是一个自定义的环境变量，对于测试环境我将配置文件直接挂载入了容器中，然后 <code>${KUBE_CONFIG}</code> 只是指定了一个配置文件位置，实际生产环境中可以选择将配置文件变成自定义环境变量使用</p><h4 id="5-4、GitLab-CI-总结"><a href="#5-4、GitLab-CI-总结" class="headerlink" title="5.4、GitLab CI 总结"></a>5.4、GitLab CI 总结</h4><p>关于 GitLab CI 上面已经讲了很多，但是并不全面，也不算太细致；因为这东西说起来实际太多了，现在目测已经 1W 多字了；以下总结一下 GitLab CI 的总体思想，当思路清晰了以后，我想后面的只是查查文档自己试一试就行了</p><p><strong>CS 架构</strong></p><p>GitLab 作为 Server 端，控制 Runner 端执行一系列的 CI 任务；代码 clone 等无需关心，GitLab 会自动处理好一切；Runner 每次都会启动新的容器执行 CI 任务</p><p><strong>容器即环境</strong></p><p>在 Runner 使用 Docker build 的前提下；<strong>所有依赖切换、环境切换应当由切换不同镜像实现，即 build 那就使用 build 的镜像，deploy 就用带有 deploy 功能的镜像；通过不同镜像容器实现完整的环境隔离</strong></p><p><strong>CI即脚本</strong></p><p>不同的 CI 任务实际上就是在使用不同镜像的容器中执行 SHELL 命令，自动化 CI 就是执行预先写好的一些小脚本</p><p><strong>敏感信息走环境变量</strong></p><p>一切重要的敏感信息，如账户密码等，不要写到 CI 配置中，直接放到 GitLab 的环境变量中；GitLab 会保证将其推送到远端 Runner 的 SHELL 变量中</p>]]></content:encoded>
      
      
      <category domain="https://mritd.com/categories/ci-cd/">CI/CD</category>
      
      
      <category domain="https://mritd.com/tags/linux/">Linux</category>
      
      <category domain="https://mritd.com/tags/ci-cd/">CI/CD</category>
      
      <category domain="https://mritd.com/tags/docker/">Docker</category>
      
      
      <comments>https://mritd.com/2017/11/28/ci-cd-gitlab-ci/#disqus_thread</comments>
      
    </item>
    
    <item>
      <title>CI/CD 之 Dockerfile</title>
      <link>https://mritd.com/2017/11/12/ci-cd-dockerfile/</link>
      <guid>https://mritd.com/2017/11/12/ci-cd-dockerfile/</guid>
      <pubDate>Sun, 12 Nov 2017 14:46:53 GMT</pubDate>
      
      <description>最近准备整理一下关于 CI/CD 的相关文档，写一个关于 CI/CD 的系列文章，这篇先从最基本的 Dockerfile 书写开始，本系列文章默认读者已经熟悉 Docker、Kubernetes 相关工具</description>
      
      
      
      <content:encoded><![CDATA[<blockquote><p>最近准备整理一下关于 CI/CD 的相关文档，写一个关于 CI/CD 的系列文章，这篇先从最基本的 Dockerfile 书写开始，本系列文章默认读者已经熟悉 Docker、Kubernetes 相关工具</p></blockquote><h3 id="一、基础镜像选择"><a href="#一、基础镜像选择" class="headerlink" title="一、基础镜像选择"></a>一、基础镜像选择</h3><p>这里的基础镜像指的是实际项目运行时的基础环境镜像，比如 Java 的 JDK 基础镜像、Nodejs 的基础镜像等；在制作项目的基础镜像时，我个人认为应当考虑一下几点因素:</p><h4 id="1-1、可维护性"><a href="#1-1、可维护性" class="headerlink" title="1.1、可维护性"></a>1.1、可维护性</h4><p>可维护性应当放在首要位置，如果在制作基础镜像时，选择了一个你根本不熟悉的基础镜像，或者说你完全不知道这个基础镜像里有哪些环境变量、Entrypoint 脚本做了什么时，请果断放弃这个基础镜像，选择一个你自己更加熟悉的基础镜像，不要为以后挖坑；还有就是如果对应的应用已经有官方镜像，那么尽量采用官方的，因为你可以省去维护 <strong>自己造的轮子</strong> 的精力，<strong>除非你对基础镜像制作已经得心应手，否则请不要造轮子</strong></p><h4 id="1-2、稳定性"><a href="#1-2、稳定性" class="headerlink" title="1.2、稳定性"></a>1.2、稳定性</h4><p>基础镜像稳定性实际上是个很微妙的话题，因为普遍来说成熟的 Linux 发行版都很稳定；但是对于不同发行版镜像之间还是存在差异的，比如 alpine 的镜像用的是 musl libc，而 debian 用的是 glibc，某些依赖 glibc 的程序可能无法在 alpine 上工作；alpine 版本的 nginx 能使用 http2，debian 版本 nginx 则不行，因为 openssl 版本不同；甚至在相同发行版不同版本之间也会有差异，譬如 openjdk alpine 3.6 版本 java 某些图形库无法工作，在 alpine edge 上安装最新的 openjdk 却没问题等；所以稳定性这个话题对于基础镜像自己来说，他永远稳定，但是对于你的应用来说，则不同基础镜像会产生不同的稳定性；<strong>最后，如果你完全熟悉你的应用，甚至应用层代码也是你写的，那么你可以根据你的习惯和喜好去选择基础镜像，因为你能把控应用运行时依赖；否则的话，请尽量选择 debian 这种比较成熟的发行版作为基础镜像，因为它在普遍上兼容性更好一点；还有尽量不要使用 CentOS 作为基础镜像，因为他的体积将会成为大规模网络分发瓶颈</strong></p><h4 id="1-3、易用性"><a href="#1-3、易用性" class="headerlink" title="1.3、易用性"></a>1.3、易用性</h4><p>易用性简单地说就是是否可调试，因为有些极端情况下，并不是应用只要运行起来就没事了；可能出现一些很棘手的问题需要你进入容器进行调试，此时你的镜像易用性就会体现出来；譬如一个 Java 项目你的基础镜像是 JRE，那么 JDK 的调试工具将完全不可用，还有就是如果你的基础镜像选择了 alpine，那么它默认没有 bash，可能你的脚本无法在里面工作；<strong>所有在选择基础镜像的时候最好也考虑一下未来极端情况的可调试性</strong></p><h3 id="二、格式化及注意事项"><a href="#二、格式化及注意事项" class="headerlink" title="二、格式化及注意事项"></a>二、格式化及注意事项</h3><h4 id="2-1、书写格式"><a href="#2-1、书写格式" class="headerlink" title="2.1、书写格式"></a>2.1、书写格式</h4><p>Dockerfile 类似一堆 shell 命令的堆砌，实际上在构建阶段也可以简单的看做是一个 shell 脚本；但是为了更高效的利用缓存层，通常都会在一个 RUN 命令中连续书写大量的脚本命令，这时候一个良好的书写格式可以使 Dockerfile 看起来更加清晰易懂，也方便以后维护；我个人比较推崇的格式是按照 <a href="https://github.com/nginxinc/docker-nginx/blob/master/mainline/alpine/Dockerfile" target="_blank" rel="noopener">nginx-alpine官方 Dockerfile</a> 的样式来书写，这个 Dockerfile 大致包括了以下规则:</p><ul><li>换行以 <code>&amp;&amp;</code> 开头保持每行对齐，看起来干净又舒服</li><li>安装大量软件包时，每个包一行并添加换行符，虽然会造成很多行，但是看起来很清晰；也可根据实际需要增加每行软件包个数，但是建议不要超过 5 个</li><li>configure 的配置尽量放在统一的变量里，并做好合理换行，方便以后集中化修改</li><li>注释同样和对应命令对齐，并保持单行长度不超出视野，即不能造成拉动滚动条才能看完你的注释</li><li>alpine 作为基础镜像的话，必要时可以使用 scanelf 来减少安装依赖</li></ul><p>除了以上规则，说下我个人的一些小习惯，仅供参考:</p><ul><li>当需要编译时，尽量避免多次 <code>cd</code> 目录，必须进入目录编译时可以开启子 shell 使其完成后还停留但在当前目录，避免 <code>cd</code> 进去再 <code>cd</code> 回来，如</li></ul><div class="hljs"><pre><code class="hljs sh"><span class="hljs-built_in">cd</span> xxxx \&amp;&amp; ./configure \&amp;&amp; make \&amp;&amp; make install \&amp;&amp; <span class="hljs-built_in">cd</span> ../</code></pre></div><p>可以变为</p><div class="hljs"><pre><code class="hljs sh">(<span class="hljs-built_in">cd</span> xxx \&amp;&amp; ./configure \&amp;&amp; make \&amp;&amp; make install)</code></pre></div><ul><li>同样意义的操作统一放在相邻行处理，比如镜像需要安装两个软件，做两次 <code>wget</code>，那么没必要安装完一个删除一个安装包，可以在最后统一的进行清理动作，简而言之是 <strong>合并具有相同目的的命令</strong></li><li>尽量使用网络资源，也就是说尽量不要在当前目录下放置那种二进制文件，然后进行 <code>ADD</code>/<code>COPY</code> 操作，因为一般 Dockerfile 都是存放到 git 仓库的，同目录下的二进制变动会给 git 仓库带来很大负担</li><li>调整好镜像时区，最好内置一下 bash，可能以后临时进入容器会处理一些东西</li><li><code>FROM</code> 时指定具体的版本号，防止后续升级或者更换主机 build 造成不可预知的结果</li></ul><h4 id="2-2、合理利用缓存"><a href="#2-2、合理利用缓存" class="headerlink" title="2.2、合理利用缓存"></a>2.2、合理利用缓存</h4><p>Docker 在 build 或者说是拉取镜像时是以层为单位作为缓存的；通俗的讲，一个 Dockerfile 命令就会形成一个镜像层(不绝对)，尤其是 <code>RUN</code> 命令形成的镜像层可能会很大；此时应当合理组织 Dockerfile，以便每次拉取或者 build 时高效的利用缓存层</p><ul><li>重复 build 的缓存利用</li></ul><p>Docker 在进行 build 操作时，对于同一个 Dockerfile 来说，<strong>只要执行过一次 build，那么下次 build 将从命令更改处开始</strong>；简单的例子如下</p><div class="hljs"><pre><code class="hljs sh">FROM alpine:3.6COPY test.jar /test.jarRUN apk add openjdk8 --no-cacheCMD [<span class="hljs-string">"java"</span>,<span class="hljs-string">"-jar"</span>,<span class="hljs-string">"/test.tar"</span>]</code></pre></div><p>假设我们的项目发布物为 <code>test.jar</code>，那么以上 Dockerfile 放到 CI 里每次 build 都会相当慢，原因就是 <strong>每次更改的发布物为 <code>test.jar</code>，那么也就是相当于每次 build 失效位置从 <code>COPY</code> 命令开始，这将导致下面的 <code>RUN</code> 命令每次都会不走缓存重复执行，当 <code>RUN</code> 命令涉及网络下载等复杂动作时这会极大拖慢 build 进度</strong>，解决方案很简单，移动一下 <code>COPY</code> 命令即可</p><div class="hljs"><pre><code class="hljs sh">FROM alpine:3.6RUN apk add openjdk8 --no-cacheCOPY test.jar /test.jarCMD [<span class="hljs-string">"java"</span>,<span class="hljs-string">"-jar"</span>,<span class="hljs-string">"/test.tar"</span>]</code></pre></div><p>此时每次 build 失效位置仍然是 <code>COPY</code> 命令，但是上面的 <code>RUN</code> 命令层已经被 build 过，而且无任何改变，那么每次 build 时 <code>RUN</code> 命令都会命中缓存层从而秒过</p><ul><li>多次拉取的缓存利用</li></ul><p>同上面的 build 一个原理，在 Docker 进行 pull 操作时，也是按照镜像层来进行缓存；当项目进行更新版本，那么只要当前主机 pull 过一次上一个版本的项目，那么下一次将会直接 pull 变更的层，也就是说上面安装 openjdk 的层将会复用；这种情况为了看起来清晰一点也可以将 Dockerfile 拆分成两个</p><p><strong>OpenJDK8 base</strong></p><div class="hljs"><pre><code class="hljs sh">FROM alpine:3.6RUN RUN apk add openjdk8 --no-cache</code></pre></div><p><strong>Java Web image</strong></p><div class="hljs"><pre><code class="hljs sh">FROM xxx.com/base/openjdk8COPY test.jar /test.jarCMD [<span class="hljs-string">"java"</span>,<span class="hljs-string">"-jar"</span>,<span class="hljs-string">"/test.tar"</span>]</code></pre></div><h3 id="三、镜像安全"><a href="#三、镜像安全" class="headerlink" title="三、镜像安全"></a>三、镜像安全</h3><h4 id="3-1、用户切换"><a href="#3-1、用户切换" class="headerlink" title="3.1、用户切换"></a>3.1、用户切换</h4><p>当我们不在 Dockerfile 中指定内部用户时，那么默认以 root 用户运行；由于 Linux 系统权限判定是根据 UID、GID 来进行的，也就是说 <strong>容器里面的 root 用户有权限访问宿主机 root 用户的东西；所以一旦挂载错误(比如将 <code>/root/.ssh</code> 目录挂载进去)，并且里面的用户具有高权限那么就很危险</strong>；通常习惯是遵从最小权限原则，也就是说尽量保证容器里的程序以低权限运行，此时可以在 Dockerfile 中通过 <code>USER</code> 命令指定后续运行命令所使用的账户，通过 <code>WORKDIR</code> 指定后续命令在那个目录下执行</p><div class="hljs"><pre><code class="hljs sh">FROM alpine:3.6RUN apk add openjdk8 --no-cacheCOPY test.jar /test.jarUSER testuser:testuserWORKDIR /tmpCMD [<span class="hljs-string">"java"</span>,<span class="hljs-string">"-jar"</span>,<span class="hljs-string">"/test.tar"</span>]</code></pre></div><p>有时直接使用 <code>USER</code> 指令来切换用户并不算方便，比如你的镜像需要挂载外部存储，如果外部存储中文件权限被意外修改，你的程序接下来可能就会启动失败；此时可以使用一下两个小工具来动态切换用户，巧妙的做法是 <strong>在正式运行程序之前先使用 root 用户进行权限修复，然后使用以下工具切换到具体用户运行</strong></p><ul><li><a href="https://github.com/tianon/gosu" target="_blank" rel="noopener">gosu</a> Golang 实现的一个切换用户身份执行其他程序的小工具</li><li><a href="https://github.com/hlovdal/su-exec" target="_blank" rel="noopener">su-exec</a> C 实现的一个更轻量级的用户切换工具</li></ul><p>具体的 Dockerfile 可以参见我写的 elasticsearch 的 <a href="https://github.com/mritd/dockerfile/blob/master/elasticsearch/docker-entrypoint.sh" target="_blank" rel="noopener">entrypoint 脚本</a></p><h4 id="3-2、容器运行时"><a href="#3-2、容器运行时" class="headerlink" title="3.2、容器运行时"></a>3.2、容器运行时</h4><p>并不是每个容器都一定能切换到低权限用户来运行的，可能某些程序就希望在 root 下运行，此时一定要确认好容器是否需要 <strong>特权模式</strong> 运行；因为一旦开启了特权模式运行的容器将有能力修改宿主机内核参数等重要设置；具体的 Docker 容器运行设置前请参考 <a href="https://docs.docker.com/engine/reference/run/#runtime-privilege-and-linux-capabilities" target="_blank" rel="noopener">官方文档</a></p><p>关于 Dockerfile 方面暂时总结出这些，可能也会有遗漏，待后续补充吧；同时欢迎各位提出相关修改意见 😊</p>]]></content:encoded>
      
      
      <category domain="https://mritd.com/categories/ci-cd/">CI/CD</category>
      
      
      <category domain="https://mritd.com/tags/linux/">Linux</category>
      
      <category domain="https://mritd.com/tags/docker/">Docker</category>
      
      <category domain="https://mritd.com/tags/kubernetes/">Kubernetes</category>
      
      
      <comments>https://mritd.com/2017/11/12/ci-cd-dockerfile/#disqus_thread</comments>
      
    </item>
    
  </channel>
</rss>
