<!DOCTYPE html><html lang="zh-CN" data-default-color-scheme="&#34;auto&#34;"><head><meta charset="UTF-8"><link rel="apple-touch-icon" sizes="76x76" href="/img/apple-touch-icon.png"><link rel="icon" type="image/png" href="/img/favicon.ico"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=1,user-scalable=no,shrink-to-fit=no"><meta http-equiv="x-ua-compatible" content="ie=edge"><meta name="theme-color" content="#2f4154"><meta name="description" content="Upward, not Northward."><meta name="author" content="bleem"><meta name="keywords" content="漠然,bleem,mritd"><title>kubeadm 搭建 HA kubernetes 集群 - bleem</title><link rel="stylesheet" href="https://cdn.staticfile.org/twitter-bootstrap/4.4.1/css/bootstrap.min.css"><link rel="stylesheet" href="https://cdn.staticfile.org/github-markdown-css/4.0.0/github-markdown.min.css"><link rel="stylesheet" href="/lib/hint/hint.min.css"><link rel="stylesheet" href="https://cdn.staticfile.org/highlight.js/10.0.0/styles/atom-one-dark.min.css"><link rel="stylesheet" href="//at.alicdn.com/t/font_1749284_pf9vaxs7x7b.css"><link rel="stylesheet" href="//at.alicdn.com/t/font_1736178_kmeydafke9r.css"><link rel="stylesheet" href="/css/main.css"><script src="/js/utils.js"></script><script src="/js/color-schema.js"></script><meta name="generator" content="Hexo 4.2.0"><link rel="alternate" href="/atom.xml" title="bleem" type="application/atom+xml"><link rel="alternate" href="/rss.xml" title="bleem" type="application/rss+xml"></head><body><header style="height:70vh"><nav id="navbar" class="navbar fixed-top navbar-expand-lg navbar-dark scrolling-navbar"><div class="container"> <a class="navbar-brand" href="/">&nbsp;<strong>bleem</strong>&nbsp;</a> <button id="navbar-toggler-btn" class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarSupportedContent" aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation"><div class="animated-icon"><span></span><span></span><span></span></div></button><div class="collapse navbar-collapse" id="navbarSupportedContent"><ul class="navbar-nav ml-auto text-center"><li class="nav-item"><a class="nav-link" href="/"><i class="iconfont icon-home-fill"></i> 首页</a></li><li class="nav-item"><a class="nav-link" href="/archives/"><i class="iconfont icon-archive-fill"></i> 归档</a></li><li class="nav-item"><a class="nav-link" href="/categories/"><i class="iconfont icon-category-fill"></i> 分类</a></li><li class="nav-item"><a class="nav-link" href="/tags/"><i class="iconfont icon-tags-fill"></i> 标签</a></li><li class="nav-item"><a class="nav-link" href="/about/"><i class="iconfont icon-user-fill"></i> 关于</a></li><li class="nav-item"><a class="nav-link" href="/friends/"><i class="iconfont icon-link-fill"></i> 友链</a></li><li class="nav-item" id="search-btn"> <a class="nav-link" data-toggle="modal" data-target="#modalSearch">&nbsp;<i class="iconfont icon-search"></i>&nbsp;</a></li><li class="nav-item" id="color-toggle-btn"> <a class="nav-link" href="javascript:">&nbsp;<i class="iconfont icon-dark" id="color-toggle-icon"></i>&nbsp;</a></li></ul></div></div></nav><div class="banner intro-2" id="background" parallax="true" style="background:url(/img/default.jpg) no-repeat center center;background-size:cover"><div class="full-bg-img"><div class="mask flex-center" style="background-color:rgba(0,0,0,.3)"><div class="container page-header text-center fade-in-up"><span class="h2" id="subtitle"></span><div class="mt-3"><span class="post-meta"><i class="iconfont icon-date-fill" aria-hidden="true"></i> <time datetime="2020-01-21 12:40" pubdate>2020年1月21日 下午</time></span></div><div class="mt-1"><span class="post-meta mr-2"><i class="iconfont icon-chart"></i> 5.8k 字</span><span class="post-meta mr-2"><i class="iconfont icon-clock-fill"></i> 86 分钟</span></div></div></div></div></div></header><main><div class="container-fluid"><div class="row"><div class="d-none d-lg-block col-lg-2"></div><div class="col-lg-8 nopadding-md"><div class="container nopadding-md" id="board-ctn"><div class="py-5" id="board"><article class="post-content mx-auto" id="post"><h1 style="display:none">kubeadm 搭建 HA kubernetes 集群</h1><div class="markdown-body" id="post-body"><h2 id="一、环境准备"><a href="#一、环境准备" class="headerlink" title="一、环境准备"></a>一、环境准备</h2><p>搭建环境为 5 台虚拟机，每台虚拟机配置为 4 核心 8G 内存，虚拟机 IP 范围为 <code>172.16.10.21~25</code>，其他软件配置如下</p><ul><li>os version: ubuntu 18.04</li><li>kubeadm version: 1.17.0</li><li>kubernetes version: 1.17.0</li><li>etcd version: 3.3.18</li><li>docker version: 19.03.5</li></ul><h2 id="二、HA-方案"><a href="#二、HA-方案" class="headerlink" title="二、HA 方案"></a>二、HA 方案</h2><p>目前的 HA 方案与官方的不同，官方 HA 方案推荐使用类似 haproxy 等工具进行 4 层代理 apiserver，但是同样会有一个问题就是我们还需要对这个 haproxy 做 HA；由于目前我们实际生产环境都是多个独立的小集群，所以单独弄 2 台 haproxy + keeplived 去维持这个 apiserver LB 的 HA 有点不划算；所以还是准备延续老的 HA 方案，将外部 apiserver 的 4 层 LB 前置到每个 node 节点上；<strong>目前是采用在每个 node 节点上部署 nginx 4 层代理所有 apiserver，nginx 本身资源消耗低而且请求量不大，综合来说对宿主机影响很小；</strong>以下为 HA 的大致方案图</p><p><img src="https://cdn.oss.link/markdown/mktld.png" srcset="/img/loading.gif" alt="ha"></p><h2 id="三、环境初始化"><a href="#三、环境初始化" class="headerlink" title="三、环境初始化"></a>三、环境初始化</h2><h3 id="3-1、系统环境"><a href="#3-1、系统环境" class="headerlink" title="3.1、系统环境"></a>3.1、系统环境</h3><p>由于个人操作习惯原因，目前已经将常用的初始化环境整理到一个小脚本里了，脚本具体参见 <a href="https://github.com/mritd/shell_scripts/blob/master/init_ubuntu.sh" target="_blank" rel="noopener">mritd/shell_scripts</a> 仓库，基本上常用的初始化内容为:</p><ul><li>设置 locale(en_US.UTF-8)</li><li>设置时区(Asia/Shanghai)</li><li>更新所有系统软件包(system update)</li><li>配置 vim(vim8 + 常用插件、配色)</li><li>ohmyzsh(别跟我说不兼容 bash 脚本，我就是喜欢)</li><li>docker</li><li>ctop(一个 docker 的辅助工具)</li><li>docker-compose</li></ul><p><strong>在以上初始化中，实际对 kubernetes 安装产生影响的主要有三个地方:</strong></p><ul><li><strong>docker 的 cgroup driver 调整为 systemd，具体参考 <a href="https://github.com/mritd/config/blob/master/docker/docker.service" target="_blank" rel="noopener">docker.service</a></strong></li><li><strong>docker 一定要限制 conatiner 日志大小，防止 apiserver 等日志大量输出导致磁盘占用过大</strong></li><li><strong>安装 <code>conntrack</code> 和 <code>ipvsadm</code>，后面可能需要借助其排查问题</strong></li></ul><h3 id="3-2、配置-ipvs"><a href="#3-2、配置-ipvs" class="headerlink" title="3.2、配置 ipvs"></a>3.2、配置 ipvs</h3><p>由于后面 kube-proxy 需要使用 ipvs 模式，所以需要对内核参数、模块做一些调整，调整命令如下:</p><div class="hljs"><pre><code class="hljs sh">cat &gt;&gt; /etc/sysctl.conf &lt;&lt;EOF
net.ipv4.ip_forward=1
net.bridge.bridge-nf-call-iptables=1
net.bridge.bridge-nf-call-ip6tables=1
EOF

sysctl -p

cat &gt;&gt; /etc/modules &lt;&lt;EOF
ip_vs
ip_vs_lc
ip_vs_wlc
ip_vs_rr
ip_vs_wrr
ip_vs_lblc
ip_vs_lblcr
ip_vs_dh
ip_vs_sh
ip_vs_fo
ip_vs_nq
ip_vs_sed
ip_vs_ftp
EOF</code></pre></div><p><strong>配置完成后切记需要重启，重启完成后使用 <code>lsmod | grep ip_vs</code> 验证相关 ipvs 模块加载是否正常，本文将主要使用 <code>ip_vs_wrr</code>，所以目前只关注这个模块既可。</strong></p><p><img src="https://cdn.oss.link/markdown/4irz1.png" srcset="/img/loading.gif" alt="ipvs_mode"></p><h2 id="四、安装-Etcd"><a href="#四、安装-Etcd" class="headerlink" title="四、安装 Etcd"></a>四、安装 Etcd</h2><h3 id="4-1、方案选择"><a href="#4-1、方案选择" class="headerlink" title="4.1、方案选择"></a>4.1、方案选择</h3><p>官方对于集群 HA 给出了两种有关于 Etcd 的部署方案:</p><ul><li>一种是深度耦合到 <code>control plane</code> 上，即每个 <code>control plane</code> 一个 etcd</li><li>另一种是使用外部的 Etcd 集群，通过在配置中指定外部集群让 apiserver 等组件连接</li></ul><p>在测试深度耦合 <code>control plane</code> 方案后，发现一些比较恶心的问题；比如说开始创建第二个 <code>control plane</code> 时配置写错了需要重建，此时你一旦删除第二个 <code>control plane</code> 会导致第一个 <code>control plane</code> 也会失败，原因是<strong>创建第二个 <code>control plane</code> 时 kubeadm 已经自动完成了 etcd 的集群模式，当删除第二个 <code>control plane</code> 的时候由于集群可用原因会导致第一个 <code>control plane</code> 下的 etcd 发现节点失联从而也不提供服务；</strong>所以综合考虑到后续迁移、灾备等因素，这里选择了将 etcd 放置在外部集群中；同样也方便我以后各种折腾应对一些极端情况啥的。</p><h3 id="4-2、部署-Etcd"><a href="#4-2、部署-Etcd" class="headerlink" title="4.2、部署 Etcd"></a>4.2、部署 Etcd</h3><p>确定了需要在外部部署 etcd 集群后，只需要开干就完事了；查了一下 ubuntu 官方源已经有了 etcd 安装包，但是版本比较老，测试了一下 golang 的 build 版本是 1.10；所以我还是选择了从官方 release 下载最新的版本安装；当然最后还是因为懒，我自己打了一个 deb 包… deb 包可以从这个项目 <a href="https://github.com/mritd/etcd-deb/releases" target="_blank" rel="noopener">mritd/etcd-deb</a> 下载，担心安全性的可以利用项目脚本自己打包，以下是安装过程:</p><div class="hljs"><pre><code class="hljs sh"><span class="hljs-comment"># 下载软件包</span>
wget https://github.com/mritd/etcd-deb/releases/download/v3.3.18/etcd_3.3.18_amd64.deb
wget https://github.com/mritd/etcd-deb/releases/download/v3.3.18/cfssl_1.4.1_amd64.deb
<span class="hljs-comment"># 安装 etcd(至少在 3 台节点上执行)</span>
dpkg -i etcd_3.3.18_amd64.deb cfssl_1.4.1_amd64.deb</code></pre></div><p><strong>既然自己部署 etcd，那么证书签署啥的还得自己来了，证书签署这里借助 cfssl 工具，cfssl 目前提供了 deb 的 make target，但是没找到 deb 包，所以也自己 build 了(担心安全性的可自行去官方下载)；</strong>接着编辑一下 <code>/etc/etcd/cfssl/etcd-csr.json</code> 文件，用 <code>/etc/etcd/cfssl/create.sh</code> 脚本创建证书，并将证书复制到指定目录</p><div class="hljs"><pre><code class="hljs sh"><span class="hljs-comment"># 创建证书</span>
<span class="hljs-built_in">cd</span> /etc/etcd/cfssl &amp;&amp; ./create.sh
<span class="hljs-comment"># 复制证书</span>
mv /etc/etcd/cfssl/*.pem /etc/etcd/ssl</code></pre></div><p>最后在 3 台节点上修改配置，并将刚刚创建的证书同步到其他两台节点启动既可；下面是单台节点的配置样例</p><div class="hljs"><pre><code class="hljs sh"><span class="hljs-comment"># /etc/etcd/etcd.conf</span>
<span class="hljs-comment"># [member]</span>
ETCD_NAME=etcd1
ETCD_DATA_DIR=<span class="hljs-string">"/var/lib/etcd/data"</span>
ETCD_WAL_DIR=<span class="hljs-string">"/var/lib/etcd/wal"</span>
ETCD_SNAPSHOT_COUNT=<span class="hljs-string">"100"</span>
ETCD_HEARTBEAT_INTERVAL=<span class="hljs-string">"100"</span>
ETCD_ELECTION_TIMEOUT=<span class="hljs-string">"1000"</span>
ETCD_LISTEN_PEER_URLS=<span class="hljs-string">"https://172.16.10.21:2380"</span>
ETCD_LISTEN_CLIENT_URLS=<span class="hljs-string">"https://172.16.10.21:2379,http://127.0.0.1:2379"</span>
ETCD_MAX_SNAPSHOTS=<span class="hljs-string">"5"</span>
ETCD_MAX_WALS=<span class="hljs-string">"5"</span>
<span class="hljs-comment">#ETCD_CORS=""</span>

<span class="hljs-comment"># [cluster]</span>
ETCD_INITIAL_ADVERTISE_PEER_URLS=<span class="hljs-string">"https://172.16.10.21:2380"</span>
<span class="hljs-comment"># if you use different ETCD_NAME (e.g. test), set ETCD_INITIAL_CLUSTER value for this name, i.e. "test=http://..."</span>
ETCD_INITIAL_CLUSTER=<span class="hljs-string">"etcd1=https://172.16.10.21:2380,etcd2=https://172.16.10.22:2380,etcd3=https://172.16.10.23:2380"</span>
ETCD_INITIAL_CLUSTER_STATE=<span class="hljs-string">"new"</span>
ETCD_INITIAL_CLUSTER_TOKEN=<span class="hljs-string">"etcd-cluster"</span>
ETCD_ADVERTISE_CLIENT_URLS=<span class="hljs-string">"https://172.16.10.21:2379"</span>
<span class="hljs-comment">#ETCD_DISCOVERY=""</span>
<span class="hljs-comment">#ETCD_DISCOVERY_SRV=""</span>
<span class="hljs-comment">#ETCD_DISCOVERY_FALLBACK="proxy"</span>
<span class="hljs-comment">#ETCD_DISCOVERY_PROXY=""</span>
<span class="hljs-comment">#ETCD_STRICT_RECONFIG_CHECK="false"</span>
ETCD_AUTO_COMPACTION_RETENTION=<span class="hljs-string">"24"</span>

<span class="hljs-comment"># [proxy]</span>
<span class="hljs-comment">#ETCD_PROXY="off"</span>
<span class="hljs-comment">#ETCD_PROXY_FAILURE_WAIT="5000"</span>
<span class="hljs-comment">#ETCD_PROXY_REFRESH_INTERVAL="30000"</span>
<span class="hljs-comment">#ETCD_PROXY_DIAL_TIMEOUT="1000"</span>
<span class="hljs-comment">#ETCD_PROXY_WRITE_TIMEOUT="5000"</span>
<span class="hljs-comment">#ETCD_PROXY_READ_TIMEOUT="0"</span>

<span class="hljs-comment"># [security]</span>
ETCD_CERT_FILE=<span class="hljs-string">"/etc/etcd/ssl/etcd.pem"</span>
ETCD_KEY_FILE=<span class="hljs-string">"/etc/etcd/ssl/etcd-key.pem"</span>
ETCD_CLIENT_CERT_AUTH=<span class="hljs-string">"true"</span>
ETCD_TRUSTED_CA_FILE=<span class="hljs-string">"/etc/etcd/ssl/etcd-root-ca.pem"</span>
ETCD_AUTO_TLS=<span class="hljs-string">"true"</span>
ETCD_PEER_CERT_FILE=<span class="hljs-string">"/etc/etcd/ssl/etcd.pem"</span>
ETCD_PEER_KEY_FILE=<span class="hljs-string">"/etc/etcd/ssl/etcd-key.pem"</span>
ETCD_PEER_CLIENT_CERT_AUTH=<span class="hljs-string">"true"</span>
ETCD_PEER_TRUSTED_CA_FILE=<span class="hljs-string">"/etc/etcd/ssl/etcd-root-ca.pem"</span>
ETCD_PEER_AUTO_TLS=<span class="hljs-string">"true"</span>

<span class="hljs-comment"># [logging]</span>
<span class="hljs-comment">#ETCD_DEBUG="false"</span>
<span class="hljs-comment"># examples for -log-package-levels etcdserver=WARNING,security=DEBUG</span>
<span class="hljs-comment">#ETCD_LOG_PACKAGE_LEVELS=""</span>

<span class="hljs-comment"># [performance]</span>
ETCD_QUOTA_BACKEND_BYTES=<span class="hljs-string">"5368709120"</span>
ETCD_AUTO_COMPACTION_RETENTION=<span class="hljs-string">"3"</span></code></pre></div><p><strong>注意: 其他两台节点请调整 <code>ETCD_NAME</code> 为不重复的其他名称，调整 <code>ETCD_LISTEN_PEER_URLS</code>、<code>ETCD_LISTEN_CLIENT_URLS</code>、<code>ETCD_INITIAL_ADVERTISE_PEER_URLS</code>、<code>ETCD_ADVERTISE_CLIENT_URLS</code> 为其他节点对应的 IP；同时生产环境请将 <code>ETCD_INITIAL_CLUSTER_TOKEN</code> 替换为复杂的 token</strong></p><div class="hljs"><pre><code class="hljs sh"><span class="hljs-comment"># 同步证书</span>
scp -r /etc/etcd/ssl 172.16.10.22:/etc/etcd/ssl
scp -r /etc/etcd/ssl 172.16.10.23:/etc/etcd/ssl
<span class="hljs-comment"># 修复权限(3台节点都要执行)</span>
chown -R etcd:etcd /etc/etcd
<span class="hljs-comment"># 最后每个节点依次启动既可</span>
systemctl start etcd</code></pre></div><p>启动完成后可以通过以下命令测试是否正常</p><div class="hljs"><pre><code class="hljs sh"><span class="hljs-comment"># 查看集群成员</span>
k1.node ➜ etcdctl member list

3cbbaf77904c6153, started, etcd2, https://172.16.10.22:2380, https://172.16.10.22:2379
8eb7652b6bd99c30, started, etcd1, https://172.16.10.21:2380, https://172.16.10.21:2379
91f4e10726460d8c, started, etcd3, https://172.16.10.23:2380, https://172.16.10.23:2379

<span class="hljs-comment"># 检测集群健康状态</span>
k1.node ➜ etcdctl endpoint health --cacert /etc/etcd/ssl/etcd-root-ca.pem --cert /etc/etcd/ssl/etcd.pem --key /etc/etcd/ssl/etcd-key.pem --endpoints https://172.16.10.21:2379,https://172.16.10.22:2379,https://172.16.10.23:2379

https://172.16.10.21:2379 is healthy: successfully committed proposal: took = 16.632246ms
https://172.16.10.23:2379 is healthy: successfully committed proposal: took = 21.122603ms
https://172.16.10.22:2379 is healthy: successfully committed proposal: took = 22.592005ms</code></pre></div><h2 id="五、部署-Kubernetes"><a href="#五、部署-Kubernetes" class="headerlink" title="五、部署 Kubernetes"></a>五、部署 Kubernetes</h2><h3 id="5-1、安装-kueadm"><a href="#5-1、安装-kueadm" class="headerlink" title="5.1、安装 kueadm"></a>5.1、安装 kueadm</h3><p>安装 kubeadm 没什么好说的，国内被墙用阿里的源既可</p><div class="hljs"><pre><code class="hljs sh">apt-get install -y apt-transport-https
curl https://mirrors.aliyun.com/kubernetes/apt/doc/apt-key.gpg | apt-key add -
cat &lt;&lt;EOF &gt;/etc/apt/sources.list.d/kubernetes.list
deb https://mirrors.aliyun.com/kubernetes/apt/ kubernetes-xenial main
EOF
apt update

<span class="hljs-comment"># ebtables、ethtool kubelet 可能会用，具体忘了，反正从官方文档上看到的</span>
apt install kubelet kubeadm kubectl ebtables ethtool -y</code></pre></div><h3 id="5-2、部署-Nginx"><a href="#5-2、部署-Nginx" class="headerlink" title="5.2、部署 Nginx"></a>5.2、部署 Nginx</h3><p>从上面的 HA 架构图上可以看到，为了维持 apiserver 的 HA，需要在每个机器上部署一个 nginx 做 4 层的 LB；为保证后续的 node 节点正常加入，需要首先行部署 nginx；nginx 安装同样喜欢偷懒，直接 docker 跑了…毕竟都开始 kubeadm 了，那么也没必要去纠结 docker 是否稳定的问题了；以下为 nginx 相关配置</p><p><strong>apiserver-proxy.conf</strong></p><div class="hljs"><pre><code class="hljs sh">error_log stderr notice;

worker_processes auto;
events &#123;
	multi_accept on;
	use epoll;
	worker_connections 1024;
&#125;

stream &#123;
    upstream kube_apiserver &#123;
        least_conn;
        <span class="hljs-comment"># 后端为三台 master 节点的 apiserver 地址</span>
        server 172.16.10.21:5443;
        server 172.16.10.22:5443;
        server 172.16.10.23:5443;
    &#125;
    
    server &#123;
        listen        0.0.0.0:6443;
        proxy_pass    kube_apiserver;
        proxy_timeout 10m;
        proxy_connect_timeout 1s;
    &#125;
&#125;</code></pre></div><p><strong>kube-apiserver-proxy.service</strong></p><div class="hljs"><pre><code class="hljs sh">[Unit]
Description=kubernetes apiserver docker wrapper
Wants=docker.socket
After=docker.service

[Service]
User=root
PermissionsStartOnly=<span class="hljs-literal">true</span>
ExecStart=/usr/bin/docker run -p 6443:6443 \
                          -v /etc/kubernetes/apiserver-proxy.conf:/etc/nginx/nginx.conf \
                          --name kube-apiserver-proxy \
                          --net=host \
                          --restart=on-failure:5 \
                          --memory=512M \
                          nginx:1.17.6-alpine
ExecStartPre=-/usr/bin/docker rm -f kube-apiserver-proxy
ExecStop=/usr/bin/docker rm -rf kube-apiserver-proxy
Restart=always
RestartSec=15s
TimeoutStartSec=30s

[Install]
WantedBy=multi-user.target</code></pre></div><p>启动 nginx 代理(每台机器都要启动，包括 master 节点)</p><div class="hljs"><pre><code class="hljs sh">cp apiserver-proxy.conf /etc/kubernetes
cp kube-apiserver-proxy.service /lib/systemd/system
systemctl daemon-reload
systemctl <span class="hljs-built_in">enable</span> kube-apiserver-proxy.service &amp;&amp; systemctl start kube-apiserver-proxy.service</code></pre></div><h3 id="5-3、启动-control-plane"><a href="#5-3、启动-control-plane" class="headerlink" title="5.3、启动 control plane"></a>5.3、启动 control plane</h3><h4 id="5-3-1、关于-Swap"><a href="#5-3-1、关于-Swap" class="headerlink" title="5.3.1、关于 Swap"></a>5.3.1、关于 Swap</h4><p>目前 kubelet 为了保证内存 limit，需要在每个节点上关闭 swap；但是说实话我看了这篇文章 <a href="https://chrisdown.name/2018/01/02/in-defence-of-swap.html" target="_blank" rel="noopener">In defence of swap: common misconceptions</a> 以后还是不想关闭 swap；更确切的说其实我们生产环境比较 “富”，pod 都不 limit 内存，所以下面的部署我忽略了 swap 错误检测</p><h4 id="5-3-2、kubeadm-配置"><a href="#5-3-2、kubeadm-配置" class="headerlink" title="5.3.2、kubeadm 配置"></a>5.3.2、kubeadm 配置</h4><p>当前版本的 kubeadm 已经支持了完善的配置管理(当然细节部分还有待支持)，以下为我目前使用的配置，相关位置已经做了注释，更具体的配置自行查阅官方文档</p><p><strong>kubeadm.yaml</strong></p><div class="hljs"><pre><code class="hljs yaml"><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">kubeadm.k8s.io/v1beta2</span>
<span class="hljs-attr">kind:</span> <span class="hljs-string">InitConfiguration</span>
<span class="hljs-attr">localAPIEndpoint:</span>
  <span class="hljs-comment"># 第一个 master 节点 IP</span>
  <span class="hljs-attr">advertiseAddress:</span> <span class="hljs-string">"172.16.10.21"</span>
  <span class="hljs-comment"># 6443 留给了 nginx，apiserver 换到 5443</span>
  <span class="hljs-attr">bindPort:</span> <span class="hljs-number">5443</span>
<span class="hljs-comment"># 这个 token 使用以下命令生成</span>
<span class="hljs-comment"># kubeadm alpha certs certificate-key</span>
<span class="hljs-attr">certificateKey:</span> <span class="hljs-string">7373f829c733b46fb78f0069f90185e0f00254381641d8d5a7c5984b2cf17cd3</span> 
<span class="hljs-meta">---</span>
<span class="hljs-attr">apiVersion:</span> <span class="hljs-string">kubeadm.k8s.io/v1beta2</span>
<span class="hljs-attr">kind:</span> <span class="hljs-string">ClusterConfiguration</span>
<span class="hljs-comment"># 使用外部 etcd 配置</span>
<span class="hljs-attr">etcd:</span>
  <span class="hljs-attr">external:</span>
    <span class="hljs-attr">endpoints:</span>
    <span class="hljs-bullet">-</span> <span class="hljs-string">"https://172.16.10.21:2379"</span>
    <span class="hljs-bullet">-</span> <span class="hljs-string">"https://172.16.10.22:2379"</span>
    <span class="hljs-bullet">-</span> <span class="hljs-string">"https://172.16.10.23:2379"</span>
    <span class="hljs-attr">caFile:</span> <span class="hljs-string">"/etc/etcd/ssl/etcd-root-ca.pem"</span>
    <span class="hljs-attr">certFile:</span> <span class="hljs-string">"/etc/etcd/ssl/etcd.pem"</span>
    <span class="hljs-attr">keyFile:</span> <span class="hljs-string">"/etc/etcd/ssl/etcd-key.pem"</span>
<span class="hljs-comment"># 网络配置</span>
<span class="hljs-attr">networking:</span>
  <span class="hljs-attr">serviceSubnet:</span> <span class="hljs-string">"10.25.0.0/16"</span>
  <span class="hljs-attr">podSubnet:</span> <span class="hljs-string">"10.30.0.1/16"</span>
  <span class="hljs-attr">dnsDomain:</span> <span class="hljs-string">"cluster.local"</span>
<span class="hljs-attr">kubernetesVersion:</span> <span class="hljs-string">"v1.17.0"</span>
<span class="hljs-comment"># 全局 apiserver LB 地址，由于采用了 nginx 负载，所以直接指向本地既可</span>
<span class="hljs-attr">controlPlaneEndpoint:</span> <span class="hljs-string">"127.0.0.1:6443"</span>
<span class="hljs-attr">apiServer:</span>
  <span class="hljs-comment"># apiserver 的自定义扩展参数</span>
  <span class="hljs-attr">extraArgs:</span>
    <span class="hljs-attr">v:</span> <span class="hljs-string">"4"</span>
    <span class="hljs-attr">alsologtostderr:</span> <span class="hljs-string">"true"</span>
    <span class="hljs-comment"># 审计日志相关配置</span>
    <span class="hljs-attr">audit-log-maxage:</span> <span class="hljs-string">"20"</span>
    <span class="hljs-attr">audit-log-maxbackup:</span> <span class="hljs-string">"10"</span>
    <span class="hljs-attr">audit-log-maxsize:</span> <span class="hljs-string">"100"</span>
    <span class="hljs-attr">audit-log-path:</span> <span class="hljs-string">"/var/log/kube-audit/audit.log"</span>
    <span class="hljs-attr">audit-policy-file:</span> <span class="hljs-string">"/etc/kubernetes/audit-policy.yaml"</span>
    <span class="hljs-attr">authorization-mode:</span> <span class="hljs-string">"Node,RBAC"</span>
    <span class="hljs-attr">event-ttl:</span> <span class="hljs-string">"720h"</span>
    <span class="hljs-attr">runtime-config:</span> <span class="hljs-string">"api/all=true"</span>
    <span class="hljs-attr">service-node-port-range:</span> <span class="hljs-string">"30000-50000"</span>
    <span class="hljs-attr">service-cluster-ip-range:</span> <span class="hljs-string">"10.25.0.0/16"</span>
  <span class="hljs-comment"># 由于自行定义了审计日志配置，所以需要将宿主机上的审计配置</span>
  <span class="hljs-comment"># 挂载到 kube-apiserver 的 pod 容器中</span>
  <span class="hljs-attr">extraVolumes:</span>
  <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">"audit-config"</span>
    <span class="hljs-attr">hostPath:</span> <span class="hljs-string">"/etc/kubernetes/audit-policy.yaml"</span>
    <span class="hljs-attr">mountPath:</span> <span class="hljs-string">"/etc/kubernetes/audit-policy.yaml"</span>
    <span class="hljs-attr">readOnly:</span> <span class="hljs-literal">true</span>
    <span class="hljs-attr">pathType:</span> <span class="hljs-string">"File"</span>
  <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">"audit-log"</span>
    <span class="hljs-attr">hostPath:</span> <span class="hljs-string">"/var/log/kube-audit"</span>
    <span class="hljs-attr">mountPath:</span> <span class="hljs-string">"/var/log/kube-audit"</span>
    <span class="hljs-attr">pathType:</span> <span class="hljs-string">"DirectoryOrCreate"</span>
  <span class="hljs-comment"># 这里是 apiserver 的证书地址配置</span>
  <span class="hljs-comment"># 为了防止以后出特殊情况，我增加了一个泛域名</span>
  <span class="hljs-attr">certSANs:</span>
  <span class="hljs-bullet">-</span> <span class="hljs-string">"*.kubernetes.node"</span>
  <span class="hljs-bullet">-</span> <span class="hljs-string">"172.16.10.21"</span>
  <span class="hljs-bullet">-</span> <span class="hljs-string">"172.16.10.22"</span>
  <span class="hljs-bullet">-</span> <span class="hljs-string">"172.16.10.23"</span>
  <span class="hljs-attr">timeoutForControlPlane:</span> <span class="hljs-string">5m</span>
<span class="hljs-attr">controllerManager:</span>
  <span class="hljs-attr">extraArgs:</span>
    <span class="hljs-attr">v:</span> <span class="hljs-string">"4"</span>
    <span class="hljs-comment"># 宿主机 ip 掩码</span>
    <span class="hljs-attr">node-cidr-mask-size:</span> <span class="hljs-string">"19"</span>
    <span class="hljs-attr">deployment-controller-sync-period:</span> <span class="hljs-string">"10s"</span>
    <span class="hljs-attr">experimental-cluster-signing-duration:</span> <span class="hljs-string">"87600h"</span>
    <span class="hljs-attr">node-monitor-grace-period:</span> <span class="hljs-string">"20s"</span>
    <span class="hljs-attr">pod-eviction-timeout:</span> <span class="hljs-string">"2m"</span>
    <span class="hljs-attr">terminated-pod-gc-threshold:</span> <span class="hljs-string">"30"</span>
<span class="hljs-attr">scheduler:</span>
  <span class="hljs-attr">extraArgs:</span>
    <span class="hljs-attr">v:</span> <span class="hljs-string">"4"</span>
<span class="hljs-attr">certificatesDir:</span> <span class="hljs-string">"/etc/kubernetes/pki"</span>
<span class="hljs-comment"># gcr.io 被墙，换成微软的镜像地址</span>
<span class="hljs-attr">imageRepository:</span> <span class="hljs-string">"gcr.azk8s.cn/google_containers"</span>
<span class="hljs-attr">clusterName:</span> <span class="hljs-string">"kuberentes"</span>
<span class="hljs-meta">---</span>
<span class="hljs-attr">apiVersion:</span> <span class="hljs-string">kubelet.config.k8s.io/v1beta1</span>
<span class="hljs-attr">kind:</span> <span class="hljs-string">KubeletConfiguration</span>
<span class="hljs-comment"># kubelet specific options here</span>
<span class="hljs-comment"># 此配置保证了 kubelet 能在 swap 开启的情况下启动</span>
<span class="hljs-attr">failSwapOn:</span> <span class="hljs-literal">false</span>
<span class="hljs-attr">nodeStatusUpdateFrequency:</span> <span class="hljs-string">5s</span>
<span class="hljs-comment"># 一些驱逐阀值，具体自行查文档修改</span>
<span class="hljs-attr">evictionSoft:</span>
  <span class="hljs-attr">"imagefs.available":</span> <span class="hljs-string">"15%"</span>
  <span class="hljs-attr">"memory.available":</span> <span class="hljs-string">"512Mi"</span>
  <span class="hljs-attr">"nodefs.available":</span> <span class="hljs-string">"15%"</span>
  <span class="hljs-attr">"nodefs.inodesFree":</span> <span class="hljs-string">"10%"</span>
<span class="hljs-attr">evictionSoftGracePeriod:</span>
  <span class="hljs-attr">"imagefs.available":</span> <span class="hljs-string">"3m"</span>
  <span class="hljs-attr">"memory.available":</span> <span class="hljs-string">"1m"</span>
  <span class="hljs-attr">"nodefs.available":</span> <span class="hljs-string">"3m"</span>
  <span class="hljs-attr">"nodefs.inodesFree":</span> <span class="hljs-string">"1m"</span>
<span class="hljs-attr">evictionHard:</span>
  <span class="hljs-attr">"imagefs.available":</span> <span class="hljs-string">"10%"</span>
  <span class="hljs-attr">"memory.available":</span> <span class="hljs-string">"256Mi"</span>
  <span class="hljs-attr">"nodefs.available":</span> <span class="hljs-string">"10%"</span>
  <span class="hljs-attr">"nodefs.inodesFree":</span> <span class="hljs-string">"5%"</span>
<span class="hljs-attr">evictionMaxPodGracePeriod:</span> <span class="hljs-number">30</span>
<span class="hljs-attr">imageGCLowThresholdPercent:</span> <span class="hljs-number">70</span>
<span class="hljs-attr">imageGCHighThresholdPercent:</span> <span class="hljs-number">80</span>
<span class="hljs-attr">kubeReserved:</span>
  <span class="hljs-attr">"cpu":</span> <span class="hljs-string">"500m"</span>
  <span class="hljs-attr">"memory":</span> <span class="hljs-string">"512Mi"</span>
  <span class="hljs-attr">"ephemeral-storage":</span> <span class="hljs-string">"1Gi"</span>
<span class="hljs-attr">rotateCertificates:</span> <span class="hljs-literal">true</span>
<span class="hljs-meta">---</span>
<span class="hljs-attr">apiVersion:</span> <span class="hljs-string">kubeproxy.config.k8s.io/v1alpha1</span>
<span class="hljs-attr">kind:</span> <span class="hljs-string">KubeProxyConfiguration</span>
<span class="hljs-comment"># kube-proxy specific options here</span>
<span class="hljs-attr">clusterCIDR:</span> <span class="hljs-string">"10.30.0.1/16"</span>
<span class="hljs-comment"># 启用 ipvs 模式</span>
<span class="hljs-attr">mode:</span> <span class="hljs-string">"ipvs"</span>
<span class="hljs-attr">ipvs:</span>
  <span class="hljs-attr">minSyncPeriod:</span> <span class="hljs-string">5s</span>
  <span class="hljs-attr">syncPeriod:</span> <span class="hljs-string">5s</span>
  <span class="hljs-comment"># ipvs 负载策略</span>
  <span class="hljs-attr">scheduler:</span> <span class="hljs-string">"wrr"</span></code></pre></div><p><strong>关于这个配置配置文件的文档还是很不完善，对于不懂 golang 的人来说很难知道具体怎么配置，以下做一下简要说明(请确保你已经拉取了 kubernetes 源码和安装了 Goland)</strong></p><p><strong>kubeadm 配置中每个配置段都会有个 <code>kind</code> 字段，<code>kind</code> 实际上对应了 go 代码中的 <code>struct</code> 结构体；同时从 <code>apiVersion</code> 字段中能够看到具体的版本，比如 <code>v1alpha1</code> 等；有了这两个信息事实上你就可以直接在源码中去找到对应的结构体</strong></p><p><img src="https://cdn.oss.link/markdown/dwo5h.png" srcset="/img/loading.gif" alt="struct_search"></p><p>在结构体中所有的配置便可以一目了然</p><p><img src="https://cdn.oss.link/markdown/0jc9b.png" srcset="/img/loading.gif" alt="struct_detail"></p><p>关于数据类型，如果是 <code>string</code> 的类型，那么意味着你要在 yaml 里写 <code>&quot;xxxx&quot;</code> 带引号这种，当然有些时候不写能兼容，有些时候不行比如 <code>extraArgs</code> 字段是一个 <code>map[string]string</code> 如果 value 不带引号就报错；<strong>如果数据类型为 <code>metav1.Duration</code>(实际上就是 <code>time.Duration</code>)，那么你看着它是个 <code>int64</code> 但实际上你要写 <code>1h2m3s</code> 这种人类可读的格式，这是 go 的特色…</strong></p><p><strong>audit-policy.yaml</strong></p><div class="hljs"><pre><code class="hljs yaml"><span class="hljs-comment"># Log all requests at the Metadata level.</span>
<span class="hljs-attr">apiVersion:</span> <span class="hljs-string">audit.k8s.io/v1</span>
<span class="hljs-attr">kind:</span> <span class="hljs-string">Policy</span>
<span class="hljs-attr">rules:</span>
<span class="hljs-bullet">-</span> <span class="hljs-attr">level:</span> <span class="hljs-string">Metadata</span></code></pre></div><p>可能 <code>Metadata</code> 级别的审计日志比较多，想自行调整审计日志级别的可以参考<a href="https://kubernetes.io/docs/tasks/debug-application-cluster/audit/#audit-policy" target="_blank" rel="noopener">官方文档</a></p><h4 id="5-3-3、拉起-control-plane"><a href="#5-3-3、拉起-control-plane" class="headerlink" title="5.3.3、拉起 control plane"></a>5.3.3、拉起 control plane</h4><p>有了完整的 <code>kubeadm.yaml</code> 和 <code>audit-policy.yaml</code> 配置后，直接一条命令拉起 control plane 既可</p><div class="hljs"><pre><code class="hljs sh"><span class="hljs-comment"># 先将审计配置放到目标位置(3 台 master 都要执行)</span>
cp audit-policy.yaml /etc/kubernetes
<span class="hljs-comment"># 拉起 control plane</span>
kubeadm init --config kubeadm.yaml --upload-certs --ignore-preflight-errors=Swap</code></pre></div><p><strong>control plane 拉起以后注意要保存屏幕输出，方便后续添加其他集群节点</strong></p><div class="hljs"><pre><code class="hljs sh">Your Kubernetes control-plane has initialized successfully!

To start using your cluster, you need to run the following as a regular user:

  mkdir -p <span class="hljs-variable">$HOME</span>/.kube
  sudo cp -i /etc/kubernetes/admin.conf <span class="hljs-variable">$HOME</span>/.kube/config
  sudo chown $(id -u):$(id -g) <span class="hljs-variable">$HOME</span>/.kube/config

You should now deploy a pod network to the cluster.
Run <span class="hljs-string">"kubectl apply -f [podnetwork].yaml"</span> with one of the options listed at:
  https://kubernetes.io/docs/concepts/cluster-administration/addons/

You can now join any number of the control-plane node running the following <span class="hljs-built_in">command</span> on each as root:

  kubeadm join 127.0.0.1:6443 --token r4t3l3.14mmuivm7xbtaeoj \
    --discovery-token-ca-cert-hash sha256:06f49f1f29d08b797fbf04d87b9b0fd6095a4693e9b1d59c429745cfa082b31d \
    --control-plane --certificate-key 7373f829c733b46fb78f0069f90185e0f00254381641d8d5a7c5984b2cf17cd3

Please note that the certificate-key gives access to cluster sensitive data, keep it secret!
As a safeguard, uploaded-certs will be deleted <span class="hljs-keyword">in</span> two hours; If necessary, you can use
<span class="hljs-string">"kubeadm init phase upload-certs --upload-certs"</span> to reload certs afterward.

Then you can join any number of worker nodes by running the following on each as root:

kubeadm join 127.0.0.1:6443 --token r4t3l3.14mmuivm7xbtaeoj \
    --discovery-token-ca-cert-hash sha256:06f49f1f29d08b797fbf04d87b9b0fd6095a4693e9b1d59c429745cfa082b31d</code></pre></div><p><strong>根据屏幕提示配置 kubectl</strong></p><div class="hljs"><pre><code class="hljs sh">mkdir -p <span class="hljs-variable">$HOME</span>/.kube
sudo cp -i /etc/kubernetes/admin.conf <span class="hljs-variable">$HOME</span>/.kube/config
sudo chown $(id -u):$(id -g) <span class="hljs-variable">$HOME</span>/.kube/config</code></pre></div><h3 id="5-4、部署-CNI"><a href="#5-4、部署-CNI" class="headerlink" title="5.4、部署 CNI"></a>5.4、部署 CNI</h3><p>关于网络插件的选择，以前一直喜欢 Calico，因为其性能确实好；到后来 flannel 出了 <code>host-gw</code> 以后现在两者性能也差不多了；但是 <strong>flannel 好处是一个工具通吃所有环境(云环境+裸机2层直通)，坏处是 flannel 缺乏比较好的策略管理(当然可以使用两者结合的 Canal)；</strong>后来思来想去其实我们生产倒是很少需要策略管理，所以这回怂回到 flannel 了(逃…)</p><p>Flannel 部署非常简单，根据官方文档下载配置，根据情况调整 <code>backend</code> 和 pod 的 CIDR，然后 apply 一下既可</p><div class="hljs"><pre><code class="hljs sh"><span class="hljs-comment"># 下载配置文件</span>
wget https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml

<span class="hljs-comment"># 调整 backend 为 host-gw(测试环境 2 层直连)</span>
k1.node ➜  grep -A 35 ConfigMap kube-flannel.yml
kind: ConfigMap
apiVersion: v1
metadata:
  name: kube-flannel-cfg
  namespace: kube-system
  labels:
    tier: node
    app: flannel
data:
  cni-conf.json: |
    &#123;
      <span class="hljs-string">"name"</span>: <span class="hljs-string">"cbr0"</span>,
      <span class="hljs-string">"cniVersion"</span>: <span class="hljs-string">"0.3.1"</span>,
      <span class="hljs-string">"plugins"</span>: [
        &#123;
          <span class="hljs-string">"type"</span>: <span class="hljs-string">"flannel"</span>,
          <span class="hljs-string">"delegate"</span>: &#123;
            <span class="hljs-string">"hairpinMode"</span>: <span class="hljs-literal">true</span>,
            <span class="hljs-string">"isDefaultGateway"</span>: <span class="hljs-literal">true</span>
          &#125;
        &#125;,
        &#123;
          <span class="hljs-string">"type"</span>: <span class="hljs-string">"portmap"</span>,
          <span class="hljs-string">"capabilities"</span>: &#123;
            <span class="hljs-string">"portMappings"</span>: <span class="hljs-literal">true</span>
          &#125;
        &#125;
      ]
    &#125;
  net-conf.json: |
    &#123;
      <span class="hljs-string">"Network"</span>: <span class="hljs-string">"10.30.0.0/16"</span>,
      <span class="hljs-string">"Backend"</span>: &#123;
        <span class="hljs-string">"Type"</span>: <span class="hljs-string">"host-gw"</span>
      &#125;
    &#125;

<span class="hljs-comment"># 调整完成后 apply 一下</span>
kubectl apply -f kube-flannel.yml</code></pre></div><h3 id="5-5、启动其他-control-plane"><a href="#5-5、启动其他-control-plane" class="headerlink" title="5.5、启动其他 control plane"></a>5.5、启动其他 control plane</h3><p>为了保证 HA 架构，还需要在另外两台 master 上启动 control plane；<strong>在启动之前请确保另外两台 master 节点节点上 <code>/etc/kubernetes/audit-policy.yaml</code> 审计配置已经分发完成，确保 <code>127.0.0.1:6443</code> 上监听的 4 层 LB 工作正常(可尝试使用 <code>curl -k https://127.0.0.1:6443</code> 测试)；</strong>根据第一个 control plane 终端输出，其他 control plane 加入命令如下</p><div class="hljs"><pre><code class="hljs sh">kubeadm join 127.0.0.1:6443 --token r4t3l3.14mmuivm7xbtaeoj \
    --discovery-token-ca-cert-hash sha256:06f49f1f29d08b797fbf04d87b9b0fd6095a4693e9b1d59c429745cfa082b31d \
    --control-plane --certificate-key 7373f829c733b46fb78f0069f90185e0f00254381641d8d5a7c5984b2cf17cd3</code></pre></div><p><strong>由于在使用 <code>kubeadm join</code> 时相关选项(<code>--discovery-token-ca-cert-hash</code>、<code>--control-plane</code>)无法与 <code>--config</code> 一起使用，这也就意味着我们必须增加一些附加指令来提供 <code>kubeadm.yaml</code> 配置文件中的一些属性</strong>；最终完整的 control plane 加入命令如下，在其他 master 直接执行既可(<strong><code>--apiserver-advertise-address</code> 的 IP 地址是目标 master 的 IP</strong>)</p><div class="hljs"><pre><code class="hljs sh">kubeadm join 127.0.0.1:6443 --token r4t3l3.14mmuivm7xbtaeoj \
    --discovery-token-ca-cert-hash sha256:06f49f1f29d08b797fbf04d87b9b0fd6095a4693e9b1d59c429745cfa082b31d \
    --control-plane --certificate-key 7373f829c733b46fb78f0069f90185e0f00254381641d8d5a7c5984b2cf17cd3 \
    --apiserver-advertise-address 172.16.10.22 \
    --apiserver-bind-port 5443 \
    --ignore-preflight-errors=Swap</code></pre></div><p><strong>所有 control plane 启动完成后应当通过在每个节点上运行 <code>kubectl get cs</code> 验证各个组件运行状态</strong></p><div class="hljs"><pre><code class="hljs sh">k2.node ➜ kubectl get cs
NAME                 STATUS    MESSAGE             ERROR
scheduler            Healthy   ok
controller-manager   Healthy   ok
etcd-1               Healthy   &#123;<span class="hljs-string">"health"</span>:<span class="hljs-string">"true"</span>&#125;
etcd-0               Healthy   &#123;<span class="hljs-string">"health"</span>:<span class="hljs-string">"true"</span>&#125;
etcd-2               Healthy   &#123;<span class="hljs-string">"health"</span>:<span class="hljs-string">"true"</span>&#125;

k2.node ➜ kubectl get node -o wide
NAME      STATUS   ROLES    AGE   VERSION   INTERNAL-IP    EXTERNAL-IP   OS-IMAGE             KERNEL-VERSION      CONTAINER-RUNTIME
k1.node   Ready    master   28m   v1.17.0   172.16.10.21   &lt;none&gt;        Ubuntu 18.04.3 LTS   4.15.0-74-generic   docker://19.3.5
k2.node   Ready    master   10m   v1.17.0   172.16.10.22   &lt;none&gt;        Ubuntu 18.04.3 LTS   4.15.0-74-generic   docker://19.3.5
k3.node   Ready    master   3m    v1.17.0   172.16.10.23   &lt;none&gt;        Ubuntu 18.04.3 LTS   4.15.0-74-generic   docker://19.3.5</code></pre></div><h3 id="5-6、启动-Node"><a href="#5-6、启动-Node" class="headerlink" title="5.6、启动 Node"></a>5.6、启动 Node</h3><p>node 节点的启动相较于 master 来说要简单得多，只需要增加一个防止 <code>swap</code> 开启拒绝启动的参数既可</p><div class="hljs"><pre><code class="hljs sh">kubeadm join 127.0.0.1:6443 --token r4t3l3.14mmuivm7xbtaeoj \
    --discovery-token-ca-cert-hash sha256:06f49f1f29d08b797fbf04d87b9b0fd6095a4693e9b1d59c429745cfa082b31d \
    --ignore-preflight-errors=Swap</code></pre></div><p>启动成功后在 master 上可以看到所有 node 信息</p><div class="hljs"><pre><code class="hljs sh">k1.node ➜ kubectl get node -o wide
NAME      STATUS   ROLES    AGE     VERSION   INTERNAL-IP    EXTERNAL-IP   OS-IMAGE             KERNEL-VERSION      CONTAINER-RUNTIME
k1.node   Ready    master   32m     v1.17.0   172.16.10.21   &lt;none&gt;        Ubuntu 18.04.3 LTS   4.15.0-74-generic   docker://19.3.5
k2.node   Ready    master   14m     v1.17.0   172.16.10.22   &lt;none&gt;        Ubuntu 18.04.3 LTS   4.15.0-74-generic   docker://19.3.5
k3.node   Ready    master   6m35s   v1.17.0   172.16.10.23   &lt;none&gt;        Ubuntu 18.04.3 LTS   4.15.0-74-generic   docker://19.3.5
k4.node   Ready    &lt;none&gt;   72s     v1.17.0   172.16.10.24   &lt;none&gt;        Ubuntu 18.04.3 LTS   4.15.0-74-generic   docker://19.3.5
k5.node   Ready    &lt;none&gt;   66s     v1.17.0   172.16.10.25   &lt;none&gt;        Ubuntu 18.04.3 LTS   4.15.0-74-generic   docker://19.3.5</code></pre></div><h3 id="5-7、调整及测试"><a href="#5-7、调整及测试" class="headerlink" title="5.7、调整及测试"></a>5.7、调整及测试</h3><p>集群搭建好以后，如果想让 master 节点也参与调度任务，需要在任意一台 master 节点执行以下命令</p><div class="hljs"><pre><code class="hljs sh"><span class="hljs-comment"># node 节点报错属于正常情况</span>
k1.node ➜ kubectl taint nodes --all node-role.kubernetes.io/master-
node/k1.node untainted
node/k2.node untainted
node/k3.node untainted
taint <span class="hljs-string">"node-role.kubernetes.io/master"</span> not found
taint <span class="hljs-string">"node-role.kubernetes.io/master"</span> not found</code></pre></div><p>最后创建一个 deployment 和一个 service，并在不同主机上 ping pod IP 测试网络联通性，在 pod 内直接 curl service 名称测试 dns 解析既可</p><p><strong>test-nginx.deploy.yaml</strong></p><div class="hljs"><pre><code class="hljs yaml"><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">apps/v1</span>
<span class="hljs-attr">kind:</span> <span class="hljs-string">Deployment</span>
<span class="hljs-attr">metadata:</span>
  <span class="hljs-attr">name:</span> <span class="hljs-string">test-nginx</span>
  <span class="hljs-attr">labels:</span>
    <span class="hljs-attr">app:</span> <span class="hljs-string">test-nginx</span>
<span class="hljs-attr">spec:</span>
  <span class="hljs-attr">replicas:</span> <span class="hljs-number">3</span>
  <span class="hljs-attr">selector:</span>
    <span class="hljs-attr">matchLabels:</span>
      <span class="hljs-attr">app:</span> <span class="hljs-string">test-nginx</span>
  <span class="hljs-attr">template:</span>
    <span class="hljs-attr">metadata:</span>
      <span class="hljs-attr">labels:</span>
        <span class="hljs-attr">app:</span> <span class="hljs-string">test-nginx</span>
    <span class="hljs-attr">spec:</span>
      <span class="hljs-attr">containers:</span>
      <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">test-nginx</span>
        <span class="hljs-attr">image:</span> <span class="hljs-string">nginx:1.17.6-alpine</span>
        <span class="hljs-attr">ports:</span>
        <span class="hljs-bullet">-</span> <span class="hljs-attr">containerPort:</span> <span class="hljs-number">80</span></code></pre></div><p><strong>test-nginx.svc.yaml</strong></p><div class="hljs"><pre><code class="hljs yaml"><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">v1</span>
<span class="hljs-attr">kind:</span> <span class="hljs-string">Service</span>
<span class="hljs-attr">metadata:</span>
  <span class="hljs-attr">name:</span> <span class="hljs-string">test-nginx</span>
<span class="hljs-attr">spec:</span>
  <span class="hljs-attr">selector:</span>
    <span class="hljs-attr">app:</span> <span class="hljs-string">test-nginx</span>
  <span class="hljs-attr">ports:</span>
    <span class="hljs-bullet">-</span> <span class="hljs-attr">protocol:</span> <span class="hljs-string">TCP</span>
      <span class="hljs-attr">port:</span> <span class="hljs-number">80</span>
      <span class="hljs-attr">targetPort:</span> <span class="hljs-number">80</span></code></pre></div><h2 id="六、后续处理"><a href="#六、后续处理" class="headerlink" title="六、后续处理"></a>六、后续处理</h2><blockquote><p>说实话使用 kubeadm 后，我更关注的是集群后续的扩展性调整是否能达到目标；搭建其实很简单，大部份时间都在测试后续调整上</p></blockquote><h3 id="6-1、Etcd-迁移"><a href="#6-1、Etcd-迁移" class="headerlink" title="6.1、Etcd 迁移"></a>6.1、Etcd 迁移</h3><p>由于我们采用的是外部的 Etcd，所以迁移起来比较简单怎么折腾都行；需要注意的是换 IP 的时候注意保证老的 3 个节点至少有一个可用，否则可能导致集群崩溃；调整完成后记得分发相关 Etcd 节点的证书，重启时顺序一个一个重启，不要并行操作</p><h3 id="6-2、Master-配置修改"><a href="#6-2、Master-配置修改" class="headerlink" title="6.2、Master 配置修改"></a>6.2、Master 配置修改</h3><p>如果需要修改 conrol plane 上 apiserver、scheduler 等配置，直接修改 <code>kubeadm.yaml</code> 配置文件(<strong>所以集群搭建好后务必保存好</strong>)，然后执行 <code>kubeadm upgrade apply --config kubeadm.yaml</code> 升级集群既可，升级前一定作好相关备份工作；我只在测试环境测试这个命令工作还可以，生产环境还是需要谨慎</p><h3 id="6-3、证书续期"><a href="#6-3、证书续期" class="headerlink" title="6.3、证书续期"></a>6.3、证书续期</h3><p>目前根据我测试的结果，controller manager 的 <strong>experimental-cluster-signing-duration</strong> 参数在 init 的签发证书阶段似乎并未生效；<strong>目前根据文档描述 <code>kubelet</code> client 的证书会自动滚动，其他证书默认 1 年有效期，需要自己使用命令续签；</strong>续签命令如下</p><div class="hljs"><pre><code class="hljs sh"><span class="hljs-comment"># 查看证书过期时间</span>
k1.node ➜ kubeadm alpha certs check-expiration
[check-expiration] Reading configuration from the cluster...
[check-expiration] FYI: You can look at this config file with <span class="hljs-string">'kubectl -n kube-system get cm kubeadm-config -oyaml'</span>

CERTIFICATE                EXPIRES                  RESIDUAL TIME   CERTIFICATE AUTHORITY   EXTERNALLY MANAGED
admin.conf                 Jan 11, 2021 10:06 UTC   364d                                    no
apiserver                  Jan 11, 2021 10:06 UTC   364d            ca                      no
apiserver-kubelet-client   Jan 11, 2021 10:06 UTC   364d            ca                      no
controller-manager.conf    Jan 11, 2021 10:06 UTC   364d                                    no
front-proxy-client         Jan 11, 2021 10:06 UTC   364d            front-proxy-ca          no
scheduler.conf             Jan 11, 2021 10:06 UTC   364d                                    no

CERTIFICATE AUTHORITY   EXPIRES                  RESIDUAL TIME   EXTERNALLY MANAGED
ca                      Jan 09, 2030 10:06 UTC   9y              no
front-proxy-ca          Jan 09, 2030 10:06 UTC   9y              no

<span class="hljs-comment"># 续签证书</span>
k1.node ➜ kubeadm alpha certs renew all
[renew] Reading configuration from the cluster...
[renew] FYI: You can look at this config file with <span class="hljs-string">'kubectl -n kube-system get cm kubeadm-config -oyaml'</span>

certificate embedded <span class="hljs-keyword">in</span> the kubeconfig file <span class="hljs-keyword">for</span> the admin to use and <span class="hljs-keyword">for</span> kubeadm itself renewed
certificate <span class="hljs-keyword">for</span> serving the Kubernetes API renewed
certificate <span class="hljs-keyword">for</span> the API server to connect to kubelet renewed
certificate embedded <span class="hljs-keyword">in</span> the kubeconfig file <span class="hljs-keyword">for</span> the controller manager to use renewed
certificate <span class="hljs-keyword">for</span> the front proxy client renewed
certificate embedded <span class="hljs-keyword">in</span> the kubeconfig file <span class="hljs-keyword">for</span> the scheduler manager to use renewed</code></pre></div><h3 id="6-4、Node-重加入"><a href="#6-4、Node-重加入" class="headerlink" title="6.4、Node 重加入"></a>6.4、Node 重加入</h3><p>默认的 bootstrap token 会在 24h 后失效，所以后续增加新节点需要重新创建 token，重新创建 token 可以通过以下命令完成</p><div class="hljs"><pre><code class="hljs sh"><span class="hljs-comment"># 列出 token</span>
k1.node ➜ kubeadm token list
TOKEN                     TTL         EXPIRES                     USAGES                   DESCRIPTION                                                EXTRA GROUPS
r4t3l3.14mmuivm7xbtaeoj   22h         2020-01-13T18:06:54+08:00   authentication,signing   &lt;none&gt;                                                     system:bootstrappers:kubeadm:default-node-token
zady4i.57f9i2o6zl9vf9hy   45m         2020-01-12T20:06:53+08:00   &lt;none&gt;                   Proxy <span class="hljs-keyword">for</span> managing TTL <span class="hljs-keyword">for</span> the kubeadm-certs secret        &lt;none&gt;

<span class="hljs-comment"># 创建新 token</span>
k1.node ➜ kubeadm token create --<span class="hljs-built_in">print</span>-join-command
W0112 19:21:15.174765   26626 validation.go:28] Cannot validate kube-proxy config - no validator is available
W0112 19:21:15.174836   26626 validation.go:28] Cannot validate kubelet config - no validator is available
kubeadm join 127.0.0.1:6443 --token 2dz4dc.mobzgjbvu0bkxz7j     --discovery-token-ca-cert-hash sha256:06f49f1f29d08b797fbf04d87b9b0fd6095a4693e9b1d59c429745cfa082b31d</code></pre></div><p>如果忘记了 certificate-key 可以通过一下命令重新 upload 并查看</p><div class="hljs"><pre><code class="hljs sh">k1.node ➜ kubeadm init --config kubeadm.yaml phase upload-certs --upload-certs
W0112 19:23:06.466711   28637 validation.go:28] Cannot validate kubelet config - no validator is available
W0112 19:23:06.466778   28637 validation.go:28] Cannot validate kube-proxy config - no validator is available
[upload-certs] Storing the certificates <span class="hljs-keyword">in</span> Secret <span class="hljs-string">"kubeadm-certs"</span> <span class="hljs-keyword">in</span> the <span class="hljs-string">"kube-system"</span> Namespace
[upload-certs] Using certificate key:
7373f829c733b46fb78f0069f90185e0f00254381641d8d5a7c5984b2cf17cd3</code></pre></div><h3 id="6-5、调整-kubelet"><a href="#6-5、调整-kubelet" class="headerlink" title="6.5、调整 kubelet"></a>6.5、调整 kubelet</h3><p>node 节点一旦启动完成后，kubelet 配置便不可再修改；如果想要修改 kubelet 配置，可以通过调整 <code>/etc/systemd/system/kubelet.service.d/10-kubeadm.conf</code> 配置文件完成</p><h2 id="七、其他"><a href="#七、其他" class="headerlink" title="七、其他"></a>七、其他</h2><p>本文参考了许多官方文档，以下是一些个人认为比较有价值并且在使用 kubeadm 后应该阅读的文档</p><ul><li><a href="https://kubernetes.io/docs/reference/setup-tools/kubeadm/implementation-details" target="_blank" rel="noopener">Implementation details</a></li><li><a href="https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/kubelet-integration/" target="_blank" rel="noopener">Configuring each kubelet in your cluster using kubeadm</a></li><li><a href="https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/control-plane-flags/" target="_blank" rel="noopener">Customizing control plane configuration with kubeadm</a></li><li><a href="https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/high-availability/" target="_blank" rel="noopener">Creating Highly Available clusters with kubeadm</a></li><li><a href="https://kubernetes.io/docs/tasks/administer-cluster/kubeadm/kubeadm-certs/" target="_blank" rel="noopener">Certificate Management with kubeadm</a></li><li><a href="https://kubernetes.io/docs/tasks/administer-cluster/kubeadm/kubeadm-upgrade/" target="_blank" rel="noopener">Upgrading kubeadm clusters</a></li><li><a href="https://kubernetes.io/docs/tasks/administer-cluster/reconfigure-kubelet/" target="_blank" rel="noopener">Reconfigure a Node’s Kubelet in a Live Cluster</a></li></ul></div><hr><div><div class="post-metas mb-3"><div class="post-meta mr-3"><i class="iconfont icon-category"></i> <a class="hover-with-bg" href="/categories/kubernetes/">Kubernetes</a></div><div class="post-meta"><i class="iconfont icon-tags"></i> <a class="hover-with-bg" href="/tags/kubernetes/">Kubernetes</a></div></div><p class="note note-warning">本博客所有文章除特别声明外，均采用 <a rel="license noopener" href="http://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 国际许可协议进行许可，转载请注明出处。</p><div class="post-prevnext row"><article class="post-prev col-6"><a href="/2020/01/21/how-to-upgrade-kubeadm-cluster/"><i class="iconfont icon-arrowleft"></i> <span class="hidden-mobile">kubeadm 集群升级</span> <span class="visible-mobile">上一篇</span></a></article><article class="post-next col-6"> <a href="/2020/01/21/how-to-modify-dns-on-ubuntu18-server/"><span class="hidden-mobile">云服务器下 Ubuntu 18 正确的 DNS 修改</span> <span class="visible-mobile">下一篇</span><i class="iconfont icon-arrowright"></i></a></article></div></div><article class="comments" id="comments"><div class="disqus" style="width:100%"><div id="disqus_thread"></div><script type="text/javascript">var disqus_config=function(){this.page.url="https://mritd.com/2020/01/21/set-up-kubernetes-ha-cluster-by-kubeadm/",this.page.identifier="/2020/01/21/set-up-kubernetes-ha-cluster-by-kubeadm/"};function loadDisqus(){var e,t;e=document,(t=e.createElement("script")).src="//bleem.disqus.com/embed.js",t.setAttribute("data-timestamp",new Date),(e.head||e.body).appendChild(t)}waitElementVisible("disqus_thread",loadDisqus)</script><noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript" target="_blank" rel="nofollow noopener noopener">comments powered by Disqus.</a></noscript></div></article></article></div></div></div><div class="d-none d-lg-block col-lg-2 toc-container" id="toc-ctn"><div id="toc"><p class="toc-header"><i class="iconfont icon-list"></i>&nbsp;目录</p><div id="tocbot"></div></div></div></div></div></main><a id="scroll-top-button" href="#" role="button"><i class="iconfont icon-arrowup" aria-hidden="true"></i></a><div class="modal fade" id="modalSearch" tabindex="-1" role="dialog" aria-labelledby="ModalLabel" aria-hidden="true"><div class="modal-dialog modal-dialog-scrollable modal-lg" role="document"><div class="modal-content"><div class="modal-header text-center"><h4 class="modal-title w-100 font-weight-bold">搜索</h4> <button type="button" id="local-search-close" class="close" data-dismiss="modal" aria-label="Close"> <span aria-hidden="true">&times;</span></button></div><div class="modal-body mx-3"><div class="md-form mb-5"> <input type="text" id="local-search-input" class="form-control validate"> <label data-error="x" data-success="v" for="local-search-input">关键词</label></div><div class="list-group" id="local-search-result"></div></div></div></div></div><footer class="mt-5"><div class="text-center py-3"><div> <a href="https://hexo.io" target="_blank" rel="nofollow noopener"><span>Hexo</span></a><i class="iconfont icon-love"></i> <a href="https://github.com/fluid-dev/hexo-theme-fluid" target="_blank" rel="nofollow noopener"><span>Fluid</span></a></div></div></footer><script src="https://cdn.staticfile.org/jquery/3.4.1/jquery.min.js"></script><script src="https://cdn.staticfile.org/twitter-bootstrap/4.4.1/js/bootstrap.min.js"></script><script src="/js/debouncer.js"></script><script src="/js/main.js"></script><script src="/js/lazyload.js"></script><script defer="defer" src="https://cdn.staticfile.org/clipboard.js/2.0.6/clipboard.min.js"></script><script src="/js/clipboard-use.js"></script><script src="https://cdn.staticfile.org/tocbot/4.11.1/tocbot.min.js"></script><script>$(document).ready(function(){var t=$("#board-ctn").offset().top;tocbot.init({tocSelector:"#tocbot",contentSelector:"#post-body",headingSelector:"h1,h2,h3,h4,h5,h6",linkClass:"tocbot-link",activeLinkClass:"tocbot-active-link",listClass:"tocbot-list",isCollapsedClass:"tocbot-is-collapsed",collapsibleClass:"tocbot-is-collapsible",collapseDepth:3,scrollSmooth:!0,headingsOffset:-t}),0<$(".toc-list-item").length&&$("#toc").css("visibility","visible")})</script><script src="https://cdn.staticfile.org/typed.js/2.0.11/typed.min.js"></script><script>var typed=new Typed("#subtitle",{strings:["  ","kubeadm 搭建 HA kubernetes 集群&nbsp;"],cursorChar:"_",typeSpeed:80,loop:!1});typed.stop(),$(document).ready(function(){$(".typed-cursor").addClass("h2"),typed.start()})</script><script src="https://cdn.staticfile.org/anchor-js/4.2.2/anchor.min.js"></script><script>
    anchors.options = {
      placement: "right",
      visible: "hover",
      
      icon: "❡"
      
    };
    var el = "h1,h2,h3,h4,h5,h6".split(",");
    var res = [];
    for (item of el) {
      res.push(".markdown-body > " + item)
    }
    anchors.add(res.join(", "))
  </script><script src="/js/local-search.js"></script><script>var path="/local-search.xml",inputArea=document.querySelector("#local-search-input");inputArea.onclick=function(){searchFunc(path,"local-search-input","local-search-result"),this.onclick=null}</script><script src="https://cdn.staticfile.org/fancybox/3.5.7/jquery.fancybox.min.js"></script><link rel="stylesheet" href="https://cdn.staticfile.org/fancybox/3.5.7/jquery.fancybox.min.css"><script>$("#post img:not(.no-zoom img, img[no-zoom]), img[zoom]").each(function(){var t=document.createElement("a");$(t).attr("data-fancybox","images"),$(t).attr("href",$(this).attr("src")),$(this).wrap(t)})</script></body></html>