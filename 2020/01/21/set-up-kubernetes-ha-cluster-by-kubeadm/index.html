<!DOCTYPE html><html lang="zh-CN" data-default-color-scheme="auto"><head><meta charset="UTF-8"><link rel="apple-touch-icon" sizes="76x76" href="/img/apple-touch-icon.png"><link rel="icon" href="/img/favicon.ico"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=5,shrink-to-fit=no"><meta http-equiv="x-ua-compatible" content="ie=edge"><meta http-equiv="Content-Security-Policy" content="upgrade-insecure-requests"><meta name="theme-color" content="#2f4154"><meta name="author" content="Kovacs"><meta name="keywords" content="Kubernetes,HA,kubeadm"><meta name="description" content="距离上一次折腾 kubeadm 大约已经一两年了(记不太清了)，在很久一段时间内一直采用二进制部署的方式来部署 kubernetes 集群，随着 kubeadm 的不断稳定，目前终于可以重新试试这个不错的工具了"><meta property="og:type" content="article"><meta property="og:title" content="kubeadm 搭建 HA kubernetes 集群"><meta property="og:url" content="https://mritd.com/2020/01/21/set-up-kubernetes-ha-cluster-by-kubeadm/index.html"><meta property="og:site_name" content="Kovacs"><meta property="og:description" content="距离上一次折腾 kubeadm 大约已经一两年了(记不太清了)，在很久一段时间内一直采用二进制部署的方式来部署 kubernetes 集群，随着 kubeadm 的不断稳定，目前终于可以重新试试这个不错的工具了"><meta property="og:locale" content="zh_CN"><meta property="og:image" content="https://mritd.com/img/kubernetes_DigitalOcean.png"><meta property="article:published_time" content="2020-01-21T04:40:36.000Z"><meta property="article:modified_time" content="2020-01-21T04:40:36.000Z"><meta property="article:author" content="Kovacs"><meta property="article:tag" content="Kubernetes"><meta name="twitter:card" content="summary_large_image"><meta name="twitter:image" content="https://mritd.com/img/kubernetes_DigitalOcean.png"><meta name="twitter:creator" content="@kovacs_orz"><meta name="twitter:site" content="https://twitter.com/kovacs_orz"><title>kubeadm 搭建 HA kubernetes 集群 - Kovacs</title><link rel="stylesheet" href="https://lib.baomitu.com/twitter-bootstrap/4.6.1/css/bootstrap.min.css"><link rel="stylesheet" href="https://lib.baomitu.com/github-markdown-css/4.0.0/github-markdown.min.css"><link rel="stylesheet" href="https://lib.baomitu.com/hint.css/2.7.0/hint.min.css"><link rel="stylesheet" href="https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.css"><link rel="stylesheet" href="//at.alicdn.com/t/font_1749284_hj8rtnfg7um.css"><link rel="stylesheet" href="//at.alicdn.com/t/font_1736178_lbnruvf0jn.css"><link rel="stylesheet" href="/css/main.css"><link id="highlight-css" rel="stylesheet" href="/css/highlight.css"><link id="highlight-css-dark" rel="stylesheet" href="/css/highlight-dark.css"><script id="fluid-configs">var Fluid=window.Fluid||{};Fluid.ctx=Object.assign({},Fluid.ctx);var CONFIG={hostname:"mritd.com",root:"/",version:"1.9.7",typing:{enable:!0,typeSpeed:80,cursorChar:"_",loop:!1,scope:[]},anchorjs:{enable:!0,element:"h1,h2,h3,h4,h5,h6",placement:"right",visible:"hover",icon:"❡"},progressbar:{enable:!0,height_px:3,color:"#29d",options:{showSpinner:!0,trickleSpeed:100}},code_language:{enable:!0,default:"TEXT"},copy_btn:!0,image_caption:{enable:!0},image_zoom:{enable:!0,img_url_replace:["",""]},toc:{enable:!0,placement:"right",headingSelector:"h1,h2,h3,h4,h5,h6",collapseDepth:3},lazyload:{enable:!0,loading_img:"/img/loading.gif",onlypost:!1,offset_factor:2},web_analytics:{enable:!0,follow_dnt:!0,baidu:null,google:{measurement_id:"G-H06NPECR0Z"},tencent:{sid:null,cid:null},woyaola:null,cnzz:null,leancloud:{app_id:null,app_key:null,server_url:null,path:"window.location.pathname",ignore_local:!1}},search_path:"/local-search.xml",include_content_in_search:!0};if(CONFIG.web_analytics.follow_dnt){var dntVal=navigator.doNotTrack||window.doNotTrack||navigator.msDoNotTrack;Fluid.ctx.dnt=dntVal&&(dntVal.startsWith("1")||dntVal.startsWith("yes")||dntVal.startsWith("on"))}</script><script src="/js/utils.js"></script><script src="/js/color-schema.js"></script><script async>Fluid.ctx.dnt||Fluid.utils.createScript("https://www.googletagmanager.com/gtag/js?id=G-H06NPECR0Z",(function(){function a(){dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],a("js",new Date),a("config","G-H06NPECR0Z")}))</script><meta name="generator" content="Hexo 7.0.0"><link rel="alternate" href="/atom.xml" title="Kovacs" type="application/atom+xml">
<link rel="alternate" href="/rss.xml" title="Kovacs" type="application/rss+xml">
</head><body><header><div class="header-inner" style="height:70vh"><nav id="navbar" class="navbar fixed-top navbar-expand-lg navbar-dark scrolling-navbar"><div class="container"><a class="navbar-brand" href="/"><strong>Kovacs</strong> </a><button id="navbar-toggler-btn" class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarSupportedContent" aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation"><div class="animated-icon"><span></span><span></span><span></span></div></button><div class="collapse navbar-collapse" id="navbarSupportedContent"><ul class="navbar-nav ml-auto text-center"><li class="nav-item"><a class="nav-link" href="/" target="_self"><i class="iconfont icon-home-fill"></i> <span>首页</span></a></li><li class="nav-item"><a class="nav-link" href="/archives/" target="_self"><i class="iconfont icon-archive-fill"></i> <span>归档</span></a></li><li class="nav-item"><a class="nav-link" href="/categories/" target="_self"><i class="iconfont icon-category-fill"></i> <span>分类</span></a></li><li class="nav-item"><a class="nav-link" href="/tags/" target="_self"><i class="iconfont icon-tags-fill"></i> <span>标签</span></a></li><li class="nav-item"><a class="nav-link" href="/about/" target="_self"><i class="iconfont icon-user-fill"></i> <span>关于</span></a></li><li class="nav-item"><a class="nav-link" href="/friends/" target="_self"><i class="iconfont icon-link-fill"></i> <span>友链</span></a></li><li class="nav-item" id="search-btn"><a class="nav-link" target="_self" href="javascript:;" data-toggle="modal" data-target="#modalSearch" aria-label="Search"><i class="iconfont icon-search"></i></a></li><li class="nav-item" id="color-toggle-btn"><a class="nav-link" target="_self" href="javascript:;" aria-label="Color Toggle"><i class="iconfont icon-dark" id="color-toggle-icon"></i></a></li></ul></div></div></nav><div id="banner" class="banner" parallax="true" style="background:url(/img/default.jpg) no-repeat center center;background-size:cover"><div class="full-bg-img"><div class="mask flex-center" style="background-color:rgba(0,0,0,.3)"><div class="banner-text text-center fade-in-up"><div class="h2"><span id="subtitle" data-typed-text="kubeadm 搭建 HA kubernetes 集群"></span></div><div class="mt-3"><span class="post-meta mr-2"><i class="iconfont icon-author" aria-hidden="true"></i> Kovacs </span><span class="post-meta"><i class="iconfont icon-date-fill" aria-hidden="true"></i> <time datetime="2020-01-21 12:40" pubdate>2020年1月21日 下午</time></span></div><div class="mt-1"><span class="post-meta mr-2"><i class="iconfont icon-chart"></i> 5.1k 字 </span><span class="post-meta mr-2"><i class="iconfont icon-clock-fill"></i> 43 分钟</span></div></div></div></div></div></div></header><main><div class="container-fluid nopadding-x"><div class="row nomargin-x"><div class="side-col d-none d-lg-block col-lg-2"></div><div class="col-lg-8 nopadding-x-md"><div class="container nopadding-x-md" id="board-ctn"><div id="board"><article class="post-content mx-auto"><h1 id="seo-header">kubeadm 搭建 HA kubernetes 集群</h1><div class="markdown-body"><h2 id="一、环境准备"><a href="#一、环境准备" class="headerlink" title="一、环境准备"></a>一、环境准备</h2><p>搭建环境为 5 台虚拟机，每台虚拟机配置为 4 核心 8G 内存，虚拟机 IP 范围为 <code>172.16.10.21~25</code>，其他软件配置如下</p><ul><li>os version: ubuntu 18.04</li><li>kubeadm version: 1.17.0</li><li>kubernetes version: 1.17.0</li><li>etcd version: 3.3.18</li><li>docker version: 19.03.5</li></ul><h2 id="二、HA-方案"><a href="#二、HA-方案" class="headerlink" title="二、HA 方案"></a>二、HA 方案</h2><p>目前的 HA 方案与官方的不同，官方 HA 方案推荐使用类似 haproxy 等工具进行 4 层代理 apiserver，但是同样会有一个问题就是我们还需要对这个 haproxy 做 HA；由于目前我们实际生产环境都是多个独立的小集群，所以单独弄 2 台 haproxy + keeplived 去维持这个 apiserver LB 的 HA 有点不划算；所以还是准备延续老的 HA 方案，将外部 apiserver 的 4 层 LB 前置到每个 node 节点上；<strong>目前是采用在每个 node 节点上部署 nginx 4 层代理所有 apiserver，nginx 本身资源消耗低而且请求量不大，综合来说对宿主机影响很小；</strong>以下为 HA 的大致方案图</p><p><img src="https://cdn.oss.link/markdown/mktld.png" srcset="/img/loading.gif" lazyload alt="ha"></p><h2 id="三、环境初始化"><a href="#三、环境初始化" class="headerlink" title="三、环境初始化"></a>三、环境初始化</h2><h3 id="3-1、系统环境"><a href="#3-1、系统环境" class="headerlink" title="3.1、系统环境"></a>3.1、系统环境</h3><p>由于个人操作习惯原因，目前已经将常用的初始化环境整理到一个小脚本里了，脚本具体参见 <a target="_blank" rel="noopener" href="https://github.com/mritd/shell_scripts/blob/master/init_ubuntu.sh">mritd&#x2F;shell_scripts</a> 仓库，基本上常用的初始化内容为:</p><ul><li>设置 locale(en_US.UTF-8)</li><li>设置时区(Asia&#x2F;Shanghai)</li><li>更新所有系统软件包(system update)</li><li>配置 vim(vim8 + 常用插件、配色)</li><li>ohmyzsh(别跟我说不兼容 bash 脚本，我就是喜欢)</li><li>docker</li><li>ctop(一个 docker 的辅助工具)</li><li>docker-compose</li></ul><p><strong>在以上初始化中，实际对 kubernetes 安装产生影响的主要有三个地方:</strong></p><ul><li><strong>docker 的 cgroup driver 调整为 systemd，具体参考 <a target="_blank" rel="noopener" href="https://github.com/mritd/config/blob/master/docker/docker.service">docker.service</a></strong></li><li><strong>docker 一定要限制 conatiner 日志大小，防止 apiserver 等日志大量输出导致磁盘占用过大</strong></li><li><strong>安装 <code>conntrack</code> 和 <code>ipvsadm</code>，后面可能需要借助其排查问题</strong></li></ul><h3 id="3-2、配置-ipvs"><a href="#3-2、配置-ipvs" class="headerlink" title="3.2、配置 ipvs"></a>3.2、配置 ipvs</h3><p>由于后面 kube-proxy 需要使用 ipvs 模式，所以需要对内核参数、模块做一些调整，调整命令如下:</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><code class="hljs sh"><span class="hljs-built_in">cat</span> &gt;&gt; /etc/sysctl.conf &lt;&lt;<span class="hljs-string">EOF</span><br><span class="hljs-string">net.ipv4.ip_forward=1</span><br><span class="hljs-string">net.bridge.bridge-nf-call-iptables=1</span><br><span class="hljs-string">net.bridge.bridge-nf-call-ip6tables=1</span><br><span class="hljs-string">EOF</span><br><br>sysctl -p<br><br><span class="hljs-built_in">cat</span> &gt;&gt; /etc/modules &lt;&lt;<span class="hljs-string">EOF</span><br><span class="hljs-string">ip_vs</span><br><span class="hljs-string">ip_vs_lc</span><br><span class="hljs-string">ip_vs_wlc</span><br><span class="hljs-string">ip_vs_rr</span><br><span class="hljs-string">ip_vs_wrr</span><br><span class="hljs-string">ip_vs_lblc</span><br><span class="hljs-string">ip_vs_lblcr</span><br><span class="hljs-string">ip_vs_dh</span><br><span class="hljs-string">ip_vs_sh</span><br><span class="hljs-string">ip_vs_fo</span><br><span class="hljs-string">ip_vs_nq</span><br><span class="hljs-string">ip_vs_sed</span><br><span class="hljs-string">ip_vs_ftp</span><br><span class="hljs-string">EOF</span><br></code></pre></td></tr></table></figure><p><strong>配置完成后切记需要重启，重启完成后使用 <code>lsmod | grep ip_vs</code> 验证相关 ipvs 模块加载是否正常，本文将主要使用 <code>ip_vs_wrr</code>，所以目前只关注这个模块既可。</strong></p><p><img src="https://cdn.oss.link/markdown/4irz1.png" srcset="/img/loading.gif" lazyload alt="ipvs_mode"></p><h2 id="四、安装-Etcd"><a href="#四、安装-Etcd" class="headerlink" title="四、安装 Etcd"></a>四、安装 Etcd</h2><h3 id="4-1、方案选择"><a href="#4-1、方案选择" class="headerlink" title="4.1、方案选择"></a>4.1、方案选择</h3><p>官方对于集群 HA 给出了两种有关于 Etcd 的部署方案:</p><ul><li>一种是深度耦合到 <code>control plane</code> 上，即每个 <code>control plane</code> 一个 etcd</li><li>另一种是使用外部的 Etcd 集群，通过在配置中指定外部集群让 apiserver 等组件连接</li></ul><p>在测试深度耦合 <code>control plane</code> 方案后，发现一些比较恶心的问题；比如说开始创建第二个 <code>control plane</code> 时配置写错了需要重建，此时你一旦删除第二个 <code>control plane</code> 会导致第一个 <code>control plane</code> 也会失败，原因是<strong>创建第二个 <code>control plane</code> 时 kubeadm 已经自动完成了 etcd 的集群模式，当删除第二个 <code>control plane</code> 的时候由于集群可用原因会导致第一个 <code>control plane</code> 下的 etcd 发现节点失联从而也不提供服务；</strong>所以综合考虑到后续迁移、灾备等因素，这里选择了将 etcd 放置在外部集群中；同样也方便我以后各种折腾应对一些极端情况啥的。</p><h3 id="4-2、部署-Etcd"><a href="#4-2、部署-Etcd" class="headerlink" title="4.2、部署 Etcd"></a>4.2、部署 Etcd</h3><p>确定了需要在外部部署 etcd 集群后，只需要开干就完事了；查了一下 ubuntu 官方源已经有了 etcd 安装包，但是版本比较老，测试了一下 golang 的 build 版本是 1.10；所以我还是选择了从官方 release 下载最新的版本安装；当然最后还是因为懒，我自己打了一个 deb 包… deb 包可以从这个项目 <a target="_blank" rel="noopener" href="https://github.com/mritd/etcd-deb/releases">mritd&#x2F;etcd-deb</a> 下载，担心安全性的可以利用项目脚本自己打包，以下是安装过程:</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs sh"><span class="hljs-comment"># 下载软件包</span><br>wget https://github.com/mritd/etcd-deb/releases/download/v3.3.18/etcd_3.3.18_amd64.deb<br>wget https://github.com/mritd/etcd-deb/releases/download/v3.3.18/cfssl_1.4.1_amd64.deb<br><span class="hljs-comment"># 安装 etcd(至少在 3 台节点上执行)</span><br>dpkg -i etcd_3.3.18_amd64.deb cfssl_1.4.1_amd64.deb<br></code></pre></td></tr></table></figure><p><strong>既然自己部署 etcd，那么证书签署啥的还得自己来了，证书签署这里借助 cfssl 工具，cfssl 目前提供了 deb 的 make target，但是没找到 deb 包，所以也自己 build 了(担心安全性的可自行去官方下载)；</strong>接着编辑一下 <code>/etc/etcd/cfssl/etcd-csr.json</code> 文件，用 <code>/etc/etcd/cfssl/create.sh</code> 脚本创建证书，并将证书复制到指定目录</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs sh"><span class="hljs-comment"># 创建证书</span><br><span class="hljs-built_in">cd</span> /etc/etcd/cfssl &amp;&amp; ./create.sh<br><span class="hljs-comment"># 复制证书</span><br><span class="hljs-built_in">mv</span> /etc/etcd/cfssl/*.pem /etc/etcd/ssl<br></code></pre></td></tr></table></figure><p>最后在 3 台节点上修改配置，并将刚刚创建的证书同步到其他两台节点启动既可；下面是单台节点的配置样例</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br></pre></td><td class="code"><pre><code class="hljs sh"><span class="hljs-comment"># /etc/etcd/etcd.conf</span><br><span class="hljs-comment"># [member]</span><br>ETCD_NAME=etcd1<br>ETCD_DATA_DIR=<span class="hljs-string">&quot;/var/lib/etcd/data&quot;</span><br>ETCD_WAL_DIR=<span class="hljs-string">&quot;/var/lib/etcd/wal&quot;</span><br>ETCD_SNAPSHOT_COUNT=<span class="hljs-string">&quot;100&quot;</span><br>ETCD_HEARTBEAT_INTERVAL=<span class="hljs-string">&quot;100&quot;</span><br>ETCD_ELECTION_TIMEOUT=<span class="hljs-string">&quot;1000&quot;</span><br>ETCD_LISTEN_PEER_URLS=<span class="hljs-string">&quot;https://172.16.10.21:2380&quot;</span><br>ETCD_LISTEN_CLIENT_URLS=<span class="hljs-string">&quot;https://172.16.10.21:2379,http://127.0.0.1:2379&quot;</span><br>ETCD_MAX_SNAPSHOTS=<span class="hljs-string">&quot;5&quot;</span><br>ETCD_MAX_WALS=<span class="hljs-string">&quot;5&quot;</span><br><span class="hljs-comment">#ETCD_CORS=&quot;&quot;</span><br><br><span class="hljs-comment"># [cluster]</span><br>ETCD_INITIAL_ADVERTISE_PEER_URLS=<span class="hljs-string">&quot;https://172.16.10.21:2380&quot;</span><br><span class="hljs-comment"># if you use different ETCD_NAME (e.g. test), set ETCD_INITIAL_CLUSTER value for this name, i.e. &quot;test=http://...&quot;</span><br>ETCD_INITIAL_CLUSTER=<span class="hljs-string">&quot;etcd1=https://172.16.10.21:2380,etcd2=https://172.16.10.22:2380,etcd3=https://172.16.10.23:2380&quot;</span><br>ETCD_INITIAL_CLUSTER_STATE=<span class="hljs-string">&quot;new&quot;</span><br>ETCD_INITIAL_CLUSTER_TOKEN=<span class="hljs-string">&quot;etcd-cluster&quot;</span><br>ETCD_ADVERTISE_CLIENT_URLS=<span class="hljs-string">&quot;https://172.16.10.21:2379&quot;</span><br><span class="hljs-comment">#ETCD_DISCOVERY=&quot;&quot;</span><br><span class="hljs-comment">#ETCD_DISCOVERY_SRV=&quot;&quot;</span><br><span class="hljs-comment">#ETCD_DISCOVERY_FALLBACK=&quot;proxy&quot;</span><br><span class="hljs-comment">#ETCD_DISCOVERY_PROXY=&quot;&quot;</span><br><span class="hljs-comment">#ETCD_STRICT_RECONFIG_CHECK=&quot;false&quot;</span><br>ETCD_AUTO_COMPACTION_RETENTION=<span class="hljs-string">&quot;24&quot;</span><br><br><span class="hljs-comment"># [proxy]</span><br><span class="hljs-comment">#ETCD_PROXY=&quot;off&quot;</span><br><span class="hljs-comment">#ETCD_PROXY_FAILURE_WAIT=&quot;5000&quot;</span><br><span class="hljs-comment">#ETCD_PROXY_REFRESH_INTERVAL=&quot;30000&quot;</span><br><span class="hljs-comment">#ETCD_PROXY_DIAL_TIMEOUT=&quot;1000&quot;</span><br><span class="hljs-comment">#ETCD_PROXY_WRITE_TIMEOUT=&quot;5000&quot;</span><br><span class="hljs-comment">#ETCD_PROXY_READ_TIMEOUT=&quot;0&quot;</span><br><br><span class="hljs-comment"># [security]</span><br>ETCD_CERT_FILE=<span class="hljs-string">&quot;/etc/etcd/ssl/etcd.pem&quot;</span><br>ETCD_KEY_FILE=<span class="hljs-string">&quot;/etc/etcd/ssl/etcd-key.pem&quot;</span><br>ETCD_CLIENT_CERT_AUTH=<span class="hljs-string">&quot;true&quot;</span><br>ETCD_TRUSTED_CA_FILE=<span class="hljs-string">&quot;/etc/etcd/ssl/etcd-root-ca.pem&quot;</span><br>ETCD_AUTO_TLS=<span class="hljs-string">&quot;true&quot;</span><br>ETCD_PEER_CERT_FILE=<span class="hljs-string">&quot;/etc/etcd/ssl/etcd.pem&quot;</span><br>ETCD_PEER_KEY_FILE=<span class="hljs-string">&quot;/etc/etcd/ssl/etcd-key.pem&quot;</span><br>ETCD_PEER_CLIENT_CERT_AUTH=<span class="hljs-string">&quot;true&quot;</span><br>ETCD_PEER_TRUSTED_CA_FILE=<span class="hljs-string">&quot;/etc/etcd/ssl/etcd-root-ca.pem&quot;</span><br>ETCD_PEER_AUTO_TLS=<span class="hljs-string">&quot;true&quot;</span><br><br><span class="hljs-comment"># [logging]</span><br><span class="hljs-comment">#ETCD_DEBUG=&quot;false&quot;</span><br><span class="hljs-comment"># examples for -log-package-levels etcdserver=WARNING,security=DEBUG</span><br><span class="hljs-comment">#ETCD_LOG_PACKAGE_LEVELS=&quot;&quot;</span><br><br><span class="hljs-comment"># [performance]</span><br>ETCD_QUOTA_BACKEND_BYTES=<span class="hljs-string">&quot;5368709120&quot;</span><br>ETCD_AUTO_COMPACTION_RETENTION=<span class="hljs-string">&quot;3&quot;</span><br></code></pre></td></tr></table></figure><p><strong>注意: 其他两台节点请调整 <code>ETCD_NAME</code> 为不重复的其他名称，调整 <code>ETCD_LISTEN_PEER_URLS</code>、<code>ETCD_LISTEN_CLIENT_URLS</code>、<code>ETCD_INITIAL_ADVERTISE_PEER_URLS</code>、<code>ETCD_ADVERTISE_CLIENT_URLS</code> 为其他节点对应的 IP；同时生产环境请将 <code>ETCD_INITIAL_CLUSTER_TOKEN</code> 替换为复杂的 token</strong></p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs sh"><span class="hljs-comment"># 同步证书</span><br>scp -r /etc/etcd/ssl 172.16.10.22:/etc/etcd/ssl<br>scp -r /etc/etcd/ssl 172.16.10.23:/etc/etcd/ssl<br><span class="hljs-comment"># 修复权限(3台节点都要执行)</span><br><span class="hljs-built_in">chown</span> -R etcd:etcd /etc/etcd<br><span class="hljs-comment"># 最后每个节点依次启动既可</span><br>systemctl start etcd<br></code></pre></td></tr></table></figure><p>启动完成后可以通过以下命令测试是否正常</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs sh"><span class="hljs-comment"># 查看集群成员</span><br>k1.node ➜ etcdctl member list<br><br>3cbbaf77904c6153, started, etcd2, https://172.16.10.22:2380, https://172.16.10.22:2379<br>8eb7652b6bd99c30, started, etcd1, https://172.16.10.21:2380, https://172.16.10.21:2379<br>91f4e10726460d8c, started, etcd3, https://172.16.10.23:2380, https://172.16.10.23:2379<br><br><span class="hljs-comment"># 检测集群健康状态</span><br>k1.node ➜ etcdctl endpoint health --cacert /etc/etcd/ssl/etcd-root-ca.pem --cert /etc/etcd/ssl/etcd.pem --key /etc/etcd/ssl/etcd-key.pem --endpoints https://172.16.10.21:2379,https://172.16.10.22:2379,https://172.16.10.23:2379<br><br>https://172.16.10.21:2379 is healthy: successfully committed proposal: took = 16.632246ms<br>https://172.16.10.23:2379 is healthy: successfully committed proposal: took = 21.122603ms<br>https://172.16.10.22:2379 is healthy: successfully committed proposal: took = 22.592005ms<br></code></pre></td></tr></table></figure><h2 id="五、部署-Kubernetes"><a href="#五、部署-Kubernetes" class="headerlink" title="五、部署 Kubernetes"></a>五、部署 Kubernetes</h2><h3 id="5-1、安装-kueadm"><a href="#5-1、安装-kueadm" class="headerlink" title="5.1、安装 kueadm"></a>5.1、安装 kueadm</h3><p>安装 kubeadm 没什么好说的，国内被墙用阿里的源既可</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs sh">apt-get install -y apt-transport-https<br>curl https://mirrors.aliyun.com/kubernetes/apt/doc/apt-key.gpg | apt-key add -<br><span class="hljs-built_in">cat</span> &lt;&lt;<span class="hljs-string">EOF &gt;/etc/apt/sources.list.d/kubernetes.list</span><br><span class="hljs-string">deb https://mirrors.aliyun.com/kubernetes/apt/ kubernetes-xenial main</span><br><span class="hljs-string">EOF</span><br>apt update<br><br><span class="hljs-comment"># ebtables、ethtool kubelet 可能会用，具体忘了，反正从官方文档上看到的</span><br>apt install kubelet kubeadm kubectl ebtables ethtool -y<br></code></pre></td></tr></table></figure><h3 id="5-2、部署-Nginx"><a href="#5-2、部署-Nginx" class="headerlink" title="5.2、部署 Nginx"></a>5.2、部署 Nginx</h3><p>从上面的 HA 架构图上可以看到，为了维持 apiserver 的 HA，需要在每个机器上部署一个 nginx 做 4 层的 LB；为保证后续的 node 节点正常加入，需要首先行部署 nginx；nginx 安装同样喜欢偷懒，直接 docker 跑了…毕竟都开始 kubeadm 了，那么也没必要去纠结 docker 是否稳定的问题了；以下为 nginx 相关配置</p><p><strong>apiserver-proxy.conf</strong></p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><code class="hljs sh">error_log stderr notice;<br><br>worker_processes auto;<br>events &#123;<br>	multi_accept on;<br>	use epoll;<br>	worker_connections 1024;<br>&#125;<br><br>stream &#123;<br>    upstream kube_apiserver &#123;<br>        least_conn;<br>        <span class="hljs-comment"># 后端为三台 master 节点的 apiserver 地址</span><br>        server 172.16.10.21:5443;<br>        server 172.16.10.22:5443;<br>        server 172.16.10.23:5443;<br>    &#125;<br>    <br>    server &#123;<br>        listen        0.0.0.0:6443;<br>        proxy_pass    kube_apiserver;<br>        proxy_timeout 10m;<br>        proxy_connect_timeout 1s;<br>    &#125;<br>&#125;<br></code></pre></td></tr></table></figure><p><strong>kube-apiserver-proxy.service</strong></p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><code class="hljs sh">[Unit]<br>Description=kubernetes apiserver docker wrapper<br>Wants=docker.socket<br>After=docker.service<br><br>[Service]<br>User=root<br>PermissionsStartOnly=<span class="hljs-literal">true</span><br>ExecStart=/usr/bin/docker run -p 6443:6443 \<br>                          -v /etc/kubernetes/apiserver-proxy.conf:/etc/nginx/nginx.conf \<br>                          --name kube-apiserver-proxy \<br>                          --net=host \<br>                          --restart=on-failure:5 \<br>                          --memory=512M \<br>                          nginx:1.17.6-alpine<br>ExecStartPre=-/usr/bin/docker <span class="hljs-built_in">rm</span> -f kube-apiserver-proxy<br>ExecStop=/usr/bin/docker <span class="hljs-built_in">rm</span> -rf kube-apiserver-proxy<br>Restart=always<br>RestartSec=15s<br>TimeoutStartSec=30s<br><br>[Install]<br>WantedBy=multi-user.target<br></code></pre></td></tr></table></figure><p>启动 nginx 代理(每台机器都要启动，包括 master 节点)</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs sh"><span class="hljs-built_in">cp</span> apiserver-proxy.conf /etc/kubernetes<br><span class="hljs-built_in">cp</span> kube-apiserver-proxy.service /lib/systemd/system<br>systemctl daemon-reload<br>systemctl <span class="hljs-built_in">enable</span> kube-apiserver-proxy.service &amp;&amp; systemctl start kube-apiserver-proxy.service<br></code></pre></td></tr></table></figure><h3 id="5-3、启动-control-plane"><a href="#5-3、启动-control-plane" class="headerlink" title="5.3、启动 control plane"></a>5.3、启动 control plane</h3><h4 id="5-3-1、关于-Swap"><a href="#5-3-1、关于-Swap" class="headerlink" title="5.3.1、关于 Swap"></a>5.3.1、关于 Swap</h4><p>目前 kubelet 为了保证内存 limit，需要在每个节点上关闭 swap；但是说实话我看了这篇文章 <a target="_blank" rel="noopener" href="https://chrisdown.name/2018/01/02/in-defence-of-swap.html">In defence of swap: common misconceptions</a> 以后还是不想关闭 swap；更确切的说其实我们生产环境比较 “富”，pod 都不 limit 内存，所以下面的部署我忽略了 swap 错误检测</p><h4 id="5-3-2、kubeadm-配置"><a href="#5-3-2、kubeadm-配置" class="headerlink" title="5.3.2、kubeadm 配置"></a>5.3.2、kubeadm 配置</h4><p>当前版本的 kubeadm 已经支持了完善的配置管理(当然细节部分还有待支持)，以下为我目前使用的配置，相关位置已经做了注释，更具体的配置自行查阅官方文档</p><p><strong>kubeadm.yaml</strong></p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br></pre></td><td class="code"><pre><code class="hljs yaml"><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">kubeadm.k8s.io/v1beta2</span><br><span class="hljs-attr">kind:</span> <span class="hljs-string">InitConfiguration</span><br><span class="hljs-attr">localAPIEndpoint:</span><br>  <span class="hljs-comment"># 第一个 master 节点 IP</span><br>  <span class="hljs-attr">advertiseAddress:</span> <span class="hljs-string">&quot;172.16.10.21&quot;</span><br>  <span class="hljs-comment"># 6443 留给了 nginx，apiserver 换到 5443</span><br>  <span class="hljs-attr">bindPort:</span> <span class="hljs-number">5443</span><br><span class="hljs-comment"># 这个 token 使用以下命令生成</span><br><span class="hljs-comment"># kubeadm alpha certs certificate-key</span><br><span class="hljs-attr">certificateKey:</span> <span class="hljs-string">7373f829c733b46fb78f0069f90185e0f00254381641d8d5a7c5984b2cf17cd3</span> <br><span class="hljs-meta">---</span><br><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">kubeadm.k8s.io/v1beta2</span><br><span class="hljs-attr">kind:</span> <span class="hljs-string">ClusterConfiguration</span><br><span class="hljs-comment"># 使用外部 etcd 配置</span><br><span class="hljs-attr">etcd:</span><br>  <span class="hljs-attr">external:</span><br>    <span class="hljs-attr">endpoints:</span><br>    <span class="hljs-bullet">-</span> <span class="hljs-string">&quot;https://172.16.10.21:2379&quot;</span><br>    <span class="hljs-bullet">-</span> <span class="hljs-string">&quot;https://172.16.10.22:2379&quot;</span><br>    <span class="hljs-bullet">-</span> <span class="hljs-string">&quot;https://172.16.10.23:2379&quot;</span><br>    <span class="hljs-attr">caFile:</span> <span class="hljs-string">&quot;/etc/etcd/ssl/etcd-root-ca.pem&quot;</span><br>    <span class="hljs-attr">certFile:</span> <span class="hljs-string">&quot;/etc/etcd/ssl/etcd.pem&quot;</span><br>    <span class="hljs-attr">keyFile:</span> <span class="hljs-string">&quot;/etc/etcd/ssl/etcd-key.pem&quot;</span><br><span class="hljs-comment"># 网络配置</span><br><span class="hljs-attr">networking:</span><br>  <span class="hljs-attr">serviceSubnet:</span> <span class="hljs-string">&quot;10.25.0.0/16&quot;</span><br>  <span class="hljs-attr">podSubnet:</span> <span class="hljs-string">&quot;10.30.0.1/16&quot;</span><br>  <span class="hljs-attr">dnsDomain:</span> <span class="hljs-string">&quot;cluster.local&quot;</span><br><span class="hljs-attr">kubernetesVersion:</span> <span class="hljs-string">&quot;v1.17.0&quot;</span><br><span class="hljs-comment"># 全局 apiserver LB 地址，由于采用了 nginx 负载，所以直接指向本地既可</span><br><span class="hljs-attr">controlPlaneEndpoint:</span> <span class="hljs-string">&quot;127.0.0.1:6443&quot;</span><br><span class="hljs-attr">apiServer:</span><br>  <span class="hljs-comment"># apiserver 的自定义扩展参数</span><br>  <span class="hljs-attr">extraArgs:</span><br>    <span class="hljs-attr">v:</span> <span class="hljs-string">&quot;4&quot;</span><br>    <span class="hljs-attr">alsologtostderr:</span> <span class="hljs-string">&quot;true&quot;</span><br>    <span class="hljs-comment"># 审计日志相关配置</span><br>    <span class="hljs-attr">audit-log-maxage:</span> <span class="hljs-string">&quot;20&quot;</span><br>    <span class="hljs-attr">audit-log-maxbackup:</span> <span class="hljs-string">&quot;10&quot;</span><br>    <span class="hljs-attr">audit-log-maxsize:</span> <span class="hljs-string">&quot;100&quot;</span><br>    <span class="hljs-attr">audit-log-path:</span> <span class="hljs-string">&quot;/var/log/kube-audit/audit.log&quot;</span><br>    <span class="hljs-attr">audit-policy-file:</span> <span class="hljs-string">&quot;/etc/kubernetes/audit-policy.yaml&quot;</span><br>    <span class="hljs-attr">authorization-mode:</span> <span class="hljs-string">&quot;Node,RBAC&quot;</span><br>    <span class="hljs-attr">event-ttl:</span> <span class="hljs-string">&quot;720h&quot;</span><br>    <span class="hljs-attr">runtime-config:</span> <span class="hljs-string">&quot;api/all=true&quot;</span><br>    <span class="hljs-attr">service-node-port-range:</span> <span class="hljs-string">&quot;30000-50000&quot;</span><br>    <span class="hljs-attr">service-cluster-ip-range:</span> <span class="hljs-string">&quot;10.25.0.0/16&quot;</span><br>  <span class="hljs-comment"># 由于自行定义了审计日志配置，所以需要将宿主机上的审计配置</span><br>  <span class="hljs-comment"># 挂载到 kube-apiserver 的 pod 容器中</span><br>  <span class="hljs-attr">extraVolumes:</span><br>  <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">&quot;audit-config&quot;</span><br>    <span class="hljs-attr">hostPath:</span> <span class="hljs-string">&quot;/etc/kubernetes/audit-policy.yaml&quot;</span><br>    <span class="hljs-attr">mountPath:</span> <span class="hljs-string">&quot;/etc/kubernetes/audit-policy.yaml&quot;</span><br>    <span class="hljs-attr">readOnly:</span> <span class="hljs-literal">true</span><br>    <span class="hljs-attr">pathType:</span> <span class="hljs-string">&quot;File&quot;</span><br>  <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">&quot;audit-log&quot;</span><br>    <span class="hljs-attr">hostPath:</span> <span class="hljs-string">&quot;/var/log/kube-audit&quot;</span><br>    <span class="hljs-attr">mountPath:</span> <span class="hljs-string">&quot;/var/log/kube-audit&quot;</span><br>    <span class="hljs-attr">pathType:</span> <span class="hljs-string">&quot;DirectoryOrCreate&quot;</span><br>  <span class="hljs-comment"># 这里是 apiserver 的证书地址配置</span><br>  <span class="hljs-comment"># 为了防止以后出特殊情况，我增加了一个泛域名</span><br>  <span class="hljs-attr">certSANs:</span><br>  <span class="hljs-bullet">-</span> <span class="hljs-string">&quot;*.kubernetes.node&quot;</span><br>  <span class="hljs-bullet">-</span> <span class="hljs-string">&quot;172.16.10.21&quot;</span><br>  <span class="hljs-bullet">-</span> <span class="hljs-string">&quot;172.16.10.22&quot;</span><br>  <span class="hljs-bullet">-</span> <span class="hljs-string">&quot;172.16.10.23&quot;</span><br>  <span class="hljs-attr">timeoutForControlPlane:</span> <span class="hljs-string">5m</span><br><span class="hljs-attr">controllerManager:</span><br>  <span class="hljs-attr">extraArgs:</span><br>    <span class="hljs-attr">v:</span> <span class="hljs-string">&quot;4&quot;</span><br>    <span class="hljs-comment"># 宿主机 ip 掩码</span><br>    <span class="hljs-attr">node-cidr-mask-size:</span> <span class="hljs-string">&quot;19&quot;</span><br>    <span class="hljs-attr">deployment-controller-sync-period:</span> <span class="hljs-string">&quot;10s&quot;</span><br>    <span class="hljs-attr">experimental-cluster-signing-duration:</span> <span class="hljs-string">&quot;87600h&quot;</span><br>    <span class="hljs-attr">node-monitor-grace-period:</span> <span class="hljs-string">&quot;20s&quot;</span><br>    <span class="hljs-attr">pod-eviction-timeout:</span> <span class="hljs-string">&quot;2m&quot;</span><br>    <span class="hljs-attr">terminated-pod-gc-threshold:</span> <span class="hljs-string">&quot;30&quot;</span><br><span class="hljs-attr">scheduler:</span><br>  <span class="hljs-attr">extraArgs:</span><br>    <span class="hljs-attr">v:</span> <span class="hljs-string">&quot;4&quot;</span><br><span class="hljs-attr">certificatesDir:</span> <span class="hljs-string">&quot;/etc/kubernetes/pki&quot;</span><br><span class="hljs-comment"># gcr.io 被墙，换成微软的镜像地址</span><br><span class="hljs-attr">imageRepository:</span> <span class="hljs-string">&quot;gcr.azk8s.cn/google_containers&quot;</span><br><span class="hljs-attr">clusterName:</span> <span class="hljs-string">&quot;kuberentes&quot;</span><br><span class="hljs-meta">---</span><br><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">kubelet.config.k8s.io/v1beta1</span><br><span class="hljs-attr">kind:</span> <span class="hljs-string">KubeletConfiguration</span><br><span class="hljs-comment"># kubelet specific options here</span><br><span class="hljs-comment"># 此配置保证了 kubelet 能在 swap 开启的情况下启动</span><br><span class="hljs-attr">failSwapOn:</span> <span class="hljs-literal">false</span><br><span class="hljs-attr">nodeStatusUpdateFrequency:</span> <span class="hljs-string">5s</span><br><span class="hljs-comment"># 一些驱逐阀值，具体自行查文档修改</span><br><span class="hljs-attr">evictionSoft:</span><br>  <span class="hljs-attr">&quot;imagefs.available&quot;:</span> <span class="hljs-string">&quot;15%&quot;</span><br>  <span class="hljs-attr">&quot;memory.available&quot;:</span> <span class="hljs-string">&quot;512Mi&quot;</span><br>  <span class="hljs-attr">&quot;nodefs.available&quot;:</span> <span class="hljs-string">&quot;15%&quot;</span><br>  <span class="hljs-attr">&quot;nodefs.inodesFree&quot;:</span> <span class="hljs-string">&quot;10%&quot;</span><br><span class="hljs-attr">evictionSoftGracePeriod:</span><br>  <span class="hljs-attr">&quot;imagefs.available&quot;:</span> <span class="hljs-string">&quot;3m&quot;</span><br>  <span class="hljs-attr">&quot;memory.available&quot;:</span> <span class="hljs-string">&quot;1m&quot;</span><br>  <span class="hljs-attr">&quot;nodefs.available&quot;:</span> <span class="hljs-string">&quot;3m&quot;</span><br>  <span class="hljs-attr">&quot;nodefs.inodesFree&quot;:</span> <span class="hljs-string">&quot;1m&quot;</span><br><span class="hljs-attr">evictionHard:</span><br>  <span class="hljs-attr">&quot;imagefs.available&quot;:</span> <span class="hljs-string">&quot;10%&quot;</span><br>  <span class="hljs-attr">&quot;memory.available&quot;:</span> <span class="hljs-string">&quot;256Mi&quot;</span><br>  <span class="hljs-attr">&quot;nodefs.available&quot;:</span> <span class="hljs-string">&quot;10%&quot;</span><br>  <span class="hljs-attr">&quot;nodefs.inodesFree&quot;:</span> <span class="hljs-string">&quot;5%&quot;</span><br><span class="hljs-attr">evictionMaxPodGracePeriod:</span> <span class="hljs-number">30</span><br><span class="hljs-attr">imageGCLowThresholdPercent:</span> <span class="hljs-number">70</span><br><span class="hljs-attr">imageGCHighThresholdPercent:</span> <span class="hljs-number">80</span><br><span class="hljs-attr">kubeReserved:</span><br>  <span class="hljs-attr">&quot;cpu&quot;:</span> <span class="hljs-string">&quot;500m&quot;</span><br>  <span class="hljs-attr">&quot;memory&quot;:</span> <span class="hljs-string">&quot;512Mi&quot;</span><br>  <span class="hljs-attr">&quot;ephemeral-storage&quot;:</span> <span class="hljs-string">&quot;1Gi&quot;</span><br><span class="hljs-attr">rotateCertificates:</span> <span class="hljs-literal">true</span><br><span class="hljs-meta">---</span><br><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">kubeproxy.config.k8s.io/v1alpha1</span><br><span class="hljs-attr">kind:</span> <span class="hljs-string">KubeProxyConfiguration</span><br><span class="hljs-comment"># kube-proxy specific options here</span><br><span class="hljs-attr">clusterCIDR:</span> <span class="hljs-string">&quot;10.30.0.1/16&quot;</span><br><span class="hljs-comment"># 启用 ipvs 模式</span><br><span class="hljs-attr">mode:</span> <span class="hljs-string">&quot;ipvs&quot;</span><br><span class="hljs-attr">ipvs:</span><br>  <span class="hljs-attr">minSyncPeriod:</span> <span class="hljs-string">5s</span><br>  <span class="hljs-attr">syncPeriod:</span> <span class="hljs-string">5s</span><br>  <span class="hljs-comment"># ipvs 负载策略</span><br>  <span class="hljs-attr">scheduler:</span> <span class="hljs-string">&quot;wrr&quot;</span><br></code></pre></td></tr></table></figure><p><strong>关于这个配置配置文件的文档还是很不完善，对于不懂 golang 的人来说很难知道具体怎么配置，以下做一下简要说明(请确保你已经拉取了 kubernetes 源码和安装了 Goland)</strong></p><p><strong>kubeadm 配置中每个配置段都会有个 <code>kind</code> 字段，<code>kind</code> 实际上对应了 go 代码中的 <code>struct</code> 结构体；同时从 <code>apiVersion</code> 字段中能够看到具体的版本，比如 <code>v1alpha1</code> 等；有了这两个信息事实上你就可以直接在源码中去找到对应的结构体</strong></p><p><img src="https://cdn.oss.link/markdown/dwo5h.png" srcset="/img/loading.gif" lazyload alt="struct_search"></p><p>在结构体中所有的配置便可以一目了然</p><p><img src="https://cdn.oss.link/markdown/0jc9b.png" srcset="/img/loading.gif" lazyload alt="struct_detail"></p><p>关于数据类型，如果是 <code>string</code> 的类型，那么意味着你要在 yaml 里写 <code>&quot;xxxx&quot;</code> 带引号这种，当然有些时候不写能兼容，有些时候不行比如 <code>extraArgs</code> 字段是一个 <code>map[string]string</code> 如果 value 不带引号就报错；<strong>如果数据类型为 <code>metav1.Duration</code>(实际上就是 <code>time.Duration</code>)，那么你看着它是个 <code>int64</code> 但实际上你要写 <code>1h2m3s</code> 这种人类可读的格式，这是 go 的特色…</strong></p><p><strong>audit-policy.yaml</strong></p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs yaml"><span class="hljs-comment"># Log all requests at the Metadata level.</span><br><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">audit.k8s.io/v1</span><br><span class="hljs-attr">kind:</span> <span class="hljs-string">Policy</span><br><span class="hljs-attr">rules:</span><br><span class="hljs-bullet">-</span> <span class="hljs-attr">level:</span> <span class="hljs-string">Metadata</span><br></code></pre></td></tr></table></figure><p>可能 <code>Metadata</code> 级别的审计日志比较多，想自行调整审计日志级别的可以参考<a target="_blank" rel="noopener" href="https://kubernetes.io/docs/tasks/debug-application-cluster/audit/#audit-policy">官方文档</a></p><h4 id="5-3-3、拉起-control-plane"><a href="#5-3-3、拉起-control-plane" class="headerlink" title="5.3.3、拉起 control plane"></a>5.3.3、拉起 control plane</h4><p>有了完整的 <code>kubeadm.yaml</code> 和 <code>audit-policy.yaml</code> 配置后，直接一条命令拉起 control plane 既可</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs sh"><span class="hljs-comment"># 先将审计配置放到目标位置(3 台 master 都要执行)</span><br><span class="hljs-built_in">cp</span> audit-policy.yaml /etc/kubernetes<br><span class="hljs-comment"># 拉起 control plane</span><br>kubeadm init --config kubeadm.yaml --upload-certs --ignore-preflight-errors=Swap<br></code></pre></td></tr></table></figure><p><strong>control plane 拉起以后注意要保存屏幕输出，方便后续添加其他集群节点</strong></p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><code class="hljs sh">Your Kubernetes control-plane has initialized successfully!<br><br>To start using your cluster, you need to run the following as a regular user:<br><br>  <span class="hljs-built_in">mkdir</span> -p <span class="hljs-variable">$HOME</span>/.kube<br>  sudo <span class="hljs-built_in">cp</span> -i /etc/kubernetes/admin.conf <span class="hljs-variable">$HOME</span>/.kube/config<br>  sudo <span class="hljs-built_in">chown</span> $(<span class="hljs-built_in">id</span> -u):$(<span class="hljs-built_in">id</span> -g) <span class="hljs-variable">$HOME</span>/.kube/config<br><br>You should now deploy a pod network to the cluster.<br>Run <span class="hljs-string">&quot;kubectl apply -f [podnetwork].yaml&quot;</span> with one of the options listed at:<br>  https://kubernetes.io/docs/concepts/cluster-administration/addons/<br><br>You can now <span class="hljs-built_in">join</span> any number of the control-plane node running the following <span class="hljs-built_in">command</span> on each as root:<br><br>  kubeadm <span class="hljs-built_in">join</span> 127.0.0.1:6443 --token r4t3l3.14mmuivm7xbtaeoj \<br>    --discovery-token-ca-cert-hash sha256:06f49f1f29d08b797fbf04d87b9b0fd6095a4693e9b1d59c429745cfa082b31d \<br>    --control-plane --certificate-key 7373f829c733b46fb78f0069f90185e0f00254381641d8d5a7c5984b2cf17cd3<br><br>Please note that the certificate-key gives access to cluster sensitive data, keep it secret!<br>As a safeguard, uploaded-certs will be deleted <span class="hljs-keyword">in</span> two hours; If necessary, you can use<br><span class="hljs-string">&quot;kubeadm init phase upload-certs --upload-certs&quot;</span> to reload certs afterward.<br><br>Then you can <span class="hljs-built_in">join</span> any number of worker nodes by running the following on each as root:<br><br>kubeadm <span class="hljs-built_in">join</span> 127.0.0.1:6443 --token r4t3l3.14mmuivm7xbtaeoj \<br>    --discovery-token-ca-cert-hash sha256:06f49f1f29d08b797fbf04d87b9b0fd6095a4693e9b1d59c429745cfa082b31d<br></code></pre></td></tr></table></figure><p><strong>根据屏幕提示配置 kubectl</strong></p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs sh"><span class="hljs-built_in">mkdir</span> -p <span class="hljs-variable">$HOME</span>/.kube<br>sudo <span class="hljs-built_in">cp</span> -i /etc/kubernetes/admin.conf <span class="hljs-variable">$HOME</span>/.kube/config<br>sudo <span class="hljs-built_in">chown</span> $(<span class="hljs-built_in">id</span> -u):$(<span class="hljs-built_in">id</span> -g) <span class="hljs-variable">$HOME</span>/.kube/config<br></code></pre></td></tr></table></figure><h3 id="5-4、部署-CNI"><a href="#5-4、部署-CNI" class="headerlink" title="5.4、部署 CNI"></a>5.4、部署 CNI</h3><p>关于网络插件的选择，以前一直喜欢 Calico，因为其性能确实好；到后来 flannel 出了 <code>host-gw</code> 以后现在两者性能也差不多了；但是 <strong>flannel 好处是一个工具通吃所有环境(云环境+裸机2层直通)，坏处是 flannel 缺乏比较好的策略管理(当然可以使用两者结合的 Canal)；</strong>后来思来想去其实我们生产倒是很少需要策略管理，所以这回怂回到 flannel 了(逃…)</p><p>Flannel 部署非常简单，根据官方文档下载配置，根据情况调整 <code>backend</code> 和 pod 的 CIDR，然后 apply 一下既可</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><code class="hljs sh"><span class="hljs-comment"># 下载配置文件</span><br>wget https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml<br><br><span class="hljs-comment"># 调整 backend 为 host-gw(测试环境 2 层直连)</span><br>k1.node ➜  grep -A 35 ConfigMap kube-flannel.yml<br>kind: ConfigMap<br>apiVersion: v1<br>metadata:<br>  name: kube-flannel-cfg<br>  namespace: kube-system<br>  labels:<br>    tier: node<br>    app: flannel<br>data:<br>  cni-conf.json: |<br>    &#123;<br>      <span class="hljs-string">&quot;name&quot;</span>: <span class="hljs-string">&quot;cbr0&quot;</span>,<br>      <span class="hljs-string">&quot;cniVersion&quot;</span>: <span class="hljs-string">&quot;0.3.1&quot;</span>,<br>      <span class="hljs-string">&quot;plugins&quot;</span>: [<br>        &#123;<br>          <span class="hljs-string">&quot;type&quot;</span>: <span class="hljs-string">&quot;flannel&quot;</span>,<br>          <span class="hljs-string">&quot;delegate&quot;</span>: &#123;<br>            <span class="hljs-string">&quot;hairpinMode&quot;</span>: <span class="hljs-literal">true</span>,<br>            <span class="hljs-string">&quot;isDefaultGateway&quot;</span>: <span class="hljs-literal">true</span><br>          &#125;<br>        &#125;,<br>        &#123;<br>          <span class="hljs-string">&quot;type&quot;</span>: <span class="hljs-string">&quot;portmap&quot;</span>,<br>          <span class="hljs-string">&quot;capabilities&quot;</span>: &#123;<br>            <span class="hljs-string">&quot;portMappings&quot;</span>: <span class="hljs-literal">true</span><br>          &#125;<br>        &#125;<br>      ]<br>    &#125;<br>  net-conf.json: |<br>    &#123;<br>      <span class="hljs-string">&quot;Network&quot;</span>: <span class="hljs-string">&quot;10.30.0.0/16&quot;</span>,<br>      <span class="hljs-string">&quot;Backend&quot;</span>: &#123;<br>        <span class="hljs-string">&quot;Type&quot;</span>: <span class="hljs-string">&quot;host-gw&quot;</span><br>      &#125;<br>    &#125;<br><br><span class="hljs-comment"># 调整完成后 apply 一下</span><br>kubectl apply -f kube-flannel.yml<br></code></pre></td></tr></table></figure><h3 id="5-5、启动其他-control-plane"><a href="#5-5、启动其他-control-plane" class="headerlink" title="5.5、启动其他 control plane"></a>5.5、启动其他 control plane</h3><p>为了保证 HA 架构，还需要在另外两台 master 上启动 control plane；<strong>在启动之前请确保另外两台 master 节点节点上 <code>/etc/kubernetes/audit-policy.yaml</code> 审计配置已经分发完成，确保 <code>127.0.0.1:6443</code> 上监听的 4 层 LB 工作正常(可尝试使用 <code>curl -k https://127.0.0.1:6443</code> 测试)；</strong>根据第一个 control plane 终端输出，其他 control plane 加入命令如下</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs sh">kubeadm <span class="hljs-built_in">join</span> 127.0.0.1:6443 --token r4t3l3.14mmuivm7xbtaeoj \<br>    --discovery-token-ca-cert-hash sha256:06f49f1f29d08b797fbf04d87b9b0fd6095a4693e9b1d59c429745cfa082b31d \<br>    --control-plane --certificate-key 7373f829c733b46fb78f0069f90185e0f00254381641d8d5a7c5984b2cf17cd3<br></code></pre></td></tr></table></figure><p><strong>由于在使用 <code>kubeadm join</code> 时相关选项(<code>--discovery-token-ca-cert-hash</code>、<code>--control-plane</code>)无法与 <code>--config</code> 一起使用，这也就意味着我们必须增加一些附加指令来提供 <code>kubeadm.yaml</code> 配置文件中的一些属性</strong>；最终完整的 control plane 加入命令如下，在其他 master 直接执行既可(<strong><code>--apiserver-advertise-address</code> 的 IP 地址是目标 master 的 IP</strong>)</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs sh">kubeadm <span class="hljs-built_in">join</span> 127.0.0.1:6443 --token r4t3l3.14mmuivm7xbtaeoj \<br>    --discovery-token-ca-cert-hash sha256:06f49f1f29d08b797fbf04d87b9b0fd6095a4693e9b1d59c429745cfa082b31d \<br>    --control-plane --certificate-key 7373f829c733b46fb78f0069f90185e0f00254381641d8d5a7c5984b2cf17cd3 \<br>    --apiserver-advertise-address 172.16.10.22 \<br>    --apiserver-bind-port 5443 \<br>    --ignore-preflight-errors=Swap <br></code></pre></td></tr></table></figure><p><strong>所有 control plane 启动完成后应当通过在每个节点上运行 <code>kubectl get cs</code> 验证各个组件运行状态</strong></p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs sh">k2.node ➜ kubectl get cs<br>NAME                 STATUS    MESSAGE             ERROR<br>scheduler            Healthy   ok<br>controller-manager   Healthy   ok<br>etcd-1               Healthy   &#123;<span class="hljs-string">&quot;health&quot;</span>:<span class="hljs-string">&quot;true&quot;</span>&#125;<br>etcd-0               Healthy   &#123;<span class="hljs-string">&quot;health&quot;</span>:<span class="hljs-string">&quot;true&quot;</span>&#125;<br>etcd-2               Healthy   &#123;<span class="hljs-string">&quot;health&quot;</span>:<span class="hljs-string">&quot;true&quot;</span>&#125;<br><br>k2.node ➜ kubectl get node -o wide<br>NAME      STATUS   ROLES    AGE   VERSION   INTERNAL-IP    EXTERNAL-IP   OS-IMAGE             KERNEL-VERSION      CONTAINER-RUNTIME<br>k1.node   Ready    master   28m   v1.17.0   172.16.10.21   &lt;none&gt;        Ubuntu 18.04.3 LTS   4.15.0-74-generic   docker://19.3.5<br>k2.node   Ready    master   10m   v1.17.0   172.16.10.22   &lt;none&gt;        Ubuntu 18.04.3 LTS   4.15.0-74-generic   docker://19.3.5<br>k3.node   Ready    master   3m    v1.17.0   172.16.10.23   &lt;none&gt;        Ubuntu 18.04.3 LTS   4.15.0-74-generic   docker://19.3.5<br></code></pre></td></tr></table></figure><h3 id="5-6、启动-Node"><a href="#5-6、启动-Node" class="headerlink" title="5.6、启动 Node"></a>5.6、启动 Node</h3><p>node 节点的启动相较于 master 来说要简单得多，只需要增加一个防止 <code>swap</code> 开启拒绝启动的参数既可</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs sh">kubeadm <span class="hljs-built_in">join</span> 127.0.0.1:6443 --token r4t3l3.14mmuivm7xbtaeoj \<br>    --discovery-token-ca-cert-hash sha256:06f49f1f29d08b797fbf04d87b9b0fd6095a4693e9b1d59c429745cfa082b31d \<br>    --ignore-preflight-errors=Swap<br></code></pre></td></tr></table></figure><p>启动成功后在 master 上可以看到所有 node 信息</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs sh">k1.node ➜ kubectl get node -o wide<br>NAME      STATUS   ROLES    AGE     VERSION   INTERNAL-IP    EXTERNAL-IP   OS-IMAGE             KERNEL-VERSION      CONTAINER-RUNTIME<br>k1.node   Ready    master   32m     v1.17.0   172.16.10.21   &lt;none&gt;        Ubuntu 18.04.3 LTS   4.15.0-74-generic   docker://19.3.5<br>k2.node   Ready    master   14m     v1.17.0   172.16.10.22   &lt;none&gt;        Ubuntu 18.04.3 LTS   4.15.0-74-generic   docker://19.3.5<br>k3.node   Ready    master   6m35s   v1.17.0   172.16.10.23   &lt;none&gt;        Ubuntu 18.04.3 LTS   4.15.0-74-generic   docker://19.3.5<br>k4.node   Ready    &lt;none&gt;   72s     v1.17.0   172.16.10.24   &lt;none&gt;        Ubuntu 18.04.3 LTS   4.15.0-74-generic   docker://19.3.5<br>k5.node   Ready    &lt;none&gt;   66s     v1.17.0   172.16.10.25   &lt;none&gt;        Ubuntu 18.04.3 LTS   4.15.0-74-generic   docker://19.3.5<br></code></pre></td></tr></table></figure><h3 id="5-7、调整及测试"><a href="#5-7、调整及测试" class="headerlink" title="5.7、调整及测试"></a>5.7、调整及测试</h3><p>集群搭建好以后，如果想让 master 节点也参与调度任务，需要在任意一台 master 节点执行以下命令</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs sh"><span class="hljs-comment"># node 节点报错属于正常情况</span><br>k1.node ➜ kubectl taint nodes --all node-role.kubernetes.io/master-<br>node/k1.node untainted<br>node/k2.node untainted<br>node/k3.node untainted<br>taint <span class="hljs-string">&quot;node-role.kubernetes.io/master&quot;</span> not found<br>taint <span class="hljs-string">&quot;node-role.kubernetes.io/master&quot;</span> not found<br></code></pre></td></tr></table></figure><p>最后创建一个 deployment 和一个 service，并在不同主机上 ping pod IP 测试网络联通性，在 pod 内直接 curl service 名称测试 dns 解析既可</p><p><strong>test-nginx.deploy.yaml</strong></p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><code class="hljs yaml"><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">apps/v1</span><br><span class="hljs-attr">kind:</span> <span class="hljs-string">Deployment</span><br><span class="hljs-attr">metadata:</span><br>  <span class="hljs-attr">name:</span> <span class="hljs-string">test-nginx</span><br>  <span class="hljs-attr">labels:</span><br>    <span class="hljs-attr">app:</span> <span class="hljs-string">test-nginx</span><br><span class="hljs-attr">spec:</span><br>  <span class="hljs-attr">replicas:</span> <span class="hljs-number">3</span><br>  <span class="hljs-attr">selector:</span><br>    <span class="hljs-attr">matchLabels:</span><br>      <span class="hljs-attr">app:</span> <span class="hljs-string">test-nginx</span><br>  <span class="hljs-attr">template:</span><br>    <span class="hljs-attr">metadata:</span><br>      <span class="hljs-attr">labels:</span><br>        <span class="hljs-attr">app:</span> <span class="hljs-string">test-nginx</span><br>    <span class="hljs-attr">spec:</span><br>      <span class="hljs-attr">containers:</span><br>      <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">test-nginx</span><br>        <span class="hljs-attr">image:</span> <span class="hljs-string">nginx:1.17.6-alpine</span><br>        <span class="hljs-attr">ports:</span><br>        <span class="hljs-bullet">-</span> <span class="hljs-attr">containerPort:</span> <span class="hljs-number">80</span><br></code></pre></td></tr></table></figure><p><strong>test-nginx.svc.yaml</strong></p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs yaml"><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">v1</span><br><span class="hljs-attr">kind:</span> <span class="hljs-string">Service</span><br><span class="hljs-attr">metadata:</span><br>  <span class="hljs-attr">name:</span> <span class="hljs-string">test-nginx</span><br><span class="hljs-attr">spec:</span><br>  <span class="hljs-attr">selector:</span><br>    <span class="hljs-attr">app:</span> <span class="hljs-string">test-nginx</span><br>  <span class="hljs-attr">ports:</span><br>    <span class="hljs-bullet">-</span> <span class="hljs-attr">protocol:</span> <span class="hljs-string">TCP</span><br>      <span class="hljs-attr">port:</span> <span class="hljs-number">80</span><br>      <span class="hljs-attr">targetPort:</span> <span class="hljs-number">80</span><br></code></pre></td></tr></table></figure><h2 id="六、后续处理"><a href="#六、后续处理" class="headerlink" title="六、后续处理"></a>六、后续处理</h2><blockquote><p>说实话使用 kubeadm 后，我更关注的是集群后续的扩展性调整是否能达到目标；搭建其实很简单，大部份时间都在测试后续调整上</p></blockquote><h3 id="6-1、Etcd-迁移"><a href="#6-1、Etcd-迁移" class="headerlink" title="6.1、Etcd 迁移"></a>6.1、Etcd 迁移</h3><p>由于我们采用的是外部的 Etcd，所以迁移起来比较简单怎么折腾都行；需要注意的是换 IP 的时候注意保证老的 3 个节点至少有一个可用，否则可能导致集群崩溃；调整完成后记得分发相关 Etcd 节点的证书，重启时顺序一个一个重启，不要并行操作</p><h3 id="6-2、Master-配置修改"><a href="#6-2、Master-配置修改" class="headerlink" title="6.2、Master 配置修改"></a>6.2、Master 配置修改</h3><p>如果需要修改 conrol plane 上 apiserver、scheduler 等配置，直接修改 <code>kubeadm.yaml</code> 配置文件(<strong>所以集群搭建好后务必保存好</strong>)，然后执行 <code>kubeadm upgrade apply --config kubeadm.yaml</code> 升级集群既可，升级前一定作好相关备份工作；我只在测试环境测试这个命令工作还可以，生产环境还是需要谨慎</p><h3 id="6-3、证书续期"><a href="#6-3、证书续期" class="headerlink" title="6.3、证书续期"></a>6.3、证书续期</h3><p>目前根据我测试的结果，controller manager 的 <strong>experimental-cluster-signing-duration</strong> 参数在 init 的签发证书阶段似乎并未生效；<strong>目前根据文档描述 <code>kubelet</code> client 的证书会自动滚动，其他证书默认 1 年有效期，需要自己使用命令续签；</strong>续签命令如下</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><code class="hljs sh"><span class="hljs-comment"># 查看证书过期时间</span><br>k1.node ➜ kubeadm alpha certs check-expiration<br>[check-expiration] Reading configuration from the cluster...<br>[check-expiration] FYI: You can look at this config file with <span class="hljs-string">&#x27;kubectl -n kube-system get cm kubeadm-config -oyaml&#x27;</span><br><br>CERTIFICATE                EXPIRES                  RESIDUAL TIME   CERTIFICATE AUTHORITY   EXTERNALLY MANAGED<br>admin.conf                 Jan 11, 2021 10:06 UTC   364d                                    no<br>apiserver                  Jan 11, 2021 10:06 UTC   364d            ca                      no<br>apiserver-kubelet-client   Jan 11, 2021 10:06 UTC   364d            ca                      no<br>controller-manager.conf    Jan 11, 2021 10:06 UTC   364d                                    no<br>front-proxy-client         Jan 11, 2021 10:06 UTC   364d            front-proxy-ca          no<br>scheduler.conf             Jan 11, 2021 10:06 UTC   364d                                    no<br><br>CERTIFICATE AUTHORITY   EXPIRES                  RESIDUAL TIME   EXTERNALLY MANAGED<br>ca                      Jan 09, 2030 10:06 UTC   9y              no<br>front-proxy-ca          Jan 09, 2030 10:06 UTC   9y              no<br><br><span class="hljs-comment"># 续签证书</span><br>k1.node ➜ kubeadm alpha certs renew all<br>[renew] Reading configuration from the cluster...<br>[renew] FYI: You can look at this config file with <span class="hljs-string">&#x27;kubectl -n kube-system get cm kubeadm-config -oyaml&#x27;</span><br><br>certificate embedded <span class="hljs-keyword">in</span> the kubeconfig file <span class="hljs-keyword">for</span> the admin to use and <span class="hljs-keyword">for</span> kubeadm itself renewed<br>certificate <span class="hljs-keyword">for</span> serving the Kubernetes API renewed<br>certificate <span class="hljs-keyword">for</span> the API server to connect to kubelet renewed<br>certificate embedded <span class="hljs-keyword">in</span> the kubeconfig file <span class="hljs-keyword">for</span> the controller manager to use renewed<br>certificate <span class="hljs-keyword">for</span> the front proxy client renewed<br>certificate embedded <span class="hljs-keyword">in</span> the kubeconfig file <span class="hljs-keyword">for</span> the scheduler manager to use renewed<br></code></pre></td></tr></table></figure><h3 id="6-4、Node-重加入"><a href="#6-4、Node-重加入" class="headerlink" title="6.4、Node 重加入"></a>6.4、Node 重加入</h3><p>默认的 bootstrap token 会在 24h 后失效，所以后续增加新节点需要重新创建 token，重新创建 token 可以通过以下命令完成</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs sh"><span class="hljs-comment"># 列出 token</span><br>k1.node ➜ kubeadm token list<br>TOKEN                     TTL         EXPIRES                     USAGES                   DESCRIPTION                                                EXTRA GROUPS<br>r4t3l3.14mmuivm7xbtaeoj   22h         2020-01-13T18:06:54+08:00   authentication,signing   &lt;none&gt;                                                     system:bootstrappers:kubeadm:default-node-token<br>zady4i.57f9i2o6zl9vf9hy   45m         2020-01-12T20:06:53+08:00   &lt;none&gt;                   Proxy <span class="hljs-keyword">for</span> managing TTL <span class="hljs-keyword">for</span> the kubeadm-certs secret        &lt;none&gt;<br><br><span class="hljs-comment"># 创建新 token</span><br>k1.node ➜ kubeadm token create --print-join-command<br>W0112 19:21:15.174765   26626 validation.go:28] Cannot validate kube-proxy config - no validator is available<br>W0112 19:21:15.174836   26626 validation.go:28] Cannot validate kubelet config - no validator is available<br>kubeadm <span class="hljs-built_in">join</span> 127.0.0.1:6443 --token 2dz4dc.mobzgjbvu0bkxz7j     --discovery-token-ca-cert-hash sha256:06f49f1f29d08b797fbf04d87b9b0fd6095a4693e9b1d59c429745cfa082b31d<br></code></pre></td></tr></table></figure><p>如果忘记了 certificate-key 可以通过一下命令重新 upload 并查看</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs sh">k1.node ➜ kubeadm init --config kubeadm.yaml phase upload-certs --upload-certs<br>W0112 19:23:06.466711   28637 validation.go:28] Cannot validate kubelet config - no validator is available<br>W0112 19:23:06.466778   28637 validation.go:28] Cannot validate kube-proxy config - no validator is available<br>[upload-certs] Storing the certificates <span class="hljs-keyword">in</span> Secret <span class="hljs-string">&quot;kubeadm-certs&quot;</span> <span class="hljs-keyword">in</span> the <span class="hljs-string">&quot;kube-system&quot;</span> Namespace<br>[upload-certs] Using certificate key:<br>7373f829c733b46fb78f0069f90185e0f00254381641d8d5a7c5984b2cf17cd3<br></code></pre></td></tr></table></figure><h3 id="6-5、调整-kubelet"><a href="#6-5、调整-kubelet" class="headerlink" title="6.5、调整 kubelet"></a>6.5、调整 kubelet</h3><p>node 节点一旦启动完成后，kubelet 配置便不可再修改；如果想要修改 kubelet 配置，可以通过调整 <code>/etc/systemd/system/kubelet.service.d/10-kubeadm.conf</code> 配置文件完成</p><h2 id="七、其他"><a href="#七、其他" class="headerlink" title="七、其他"></a>七、其他</h2><p>本文参考了许多官方文档，以下是一些个人认为比较有价值并且在使用 kubeadm 后应该阅读的文档</p><ul><li><a target="_blank" rel="noopener" href="https://kubernetes.io/docs/reference/setup-tools/kubeadm/implementation-details">Implementation details</a></li><li><a target="_blank" rel="noopener" href="https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/kubelet-integration/">Configuring each kubelet in your cluster using kubeadm</a></li><li><a target="_blank" rel="noopener" href="https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/control-plane-flags/">Customizing control plane configuration with kubeadm</a></li><li><a target="_blank" rel="noopener" href="https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/high-availability/">Creating Highly Available clusters with kubeadm</a></li><li><a target="_blank" rel="noopener" href="https://kubernetes.io/docs/tasks/administer-cluster/kubeadm/kubeadm-certs/">Certificate Management with kubeadm</a></li><li><a target="_blank" rel="noopener" href="https://kubernetes.io/docs/tasks/administer-cluster/kubeadm/kubeadm-upgrade/">Upgrading kubeadm clusters</a></li><li><a target="_blank" rel="noopener" href="https://kubernetes.io/docs/tasks/administer-cluster/reconfigure-kubelet/">Reconfigure a Node’s Kubelet in a Live Cluster</a></li></ul></div><hr><div><div class="post-metas my-3"><div class="post-meta mr-3 d-flex align-items-center"><i class="iconfont icon-category"></i> <span class="category-chains"><span class="category-chain"><a href="/categories/kubernetes/" class="category-chain-item">Kubernetes</a></span></span></div><div class="post-meta"><i class="iconfont icon-tags"></i> <a href="/tags/kubernetes/" class="print-no-link">#Kubernetes</a></div></div><div class="license-box my-3"><div class="license-title"><div>kubeadm 搭建 HA kubernetes 集群</div><div>https://mritd.com/2020/01/21/set-up-kubernetes-ha-cluster-by-kubeadm/</div></div><div class="license-meta"><div class="license-meta-item"><div>作者</div><div>Kovacs</div></div><div class="license-meta-item license-meta-date"><div>发布于</div><div>2020年1月21日</div></div><div class="license-meta-item"><div>许可协议</div><div><a class="print-no-link" target="_blank" href="https://creativecommons.org/licenses/by-nc-sa/4.0/"><span class="hint--top hint--rounded" aria-label="BY - 署名"><i class="iconfont icon-by"></i> </span></a><a class="print-no-link" target="_blank" href="https://creativecommons.org/licenses/by-nc-sa/4.0/"><span class="hint--top hint--rounded" aria-label="NC - 非商业性使用"><i class="iconfont icon-nc"></i> </span></a><a class="print-no-link" target="_blank" href="https://creativecommons.org/licenses/by-nc-sa/4.0/"><span class="hint--top hint--rounded" aria-label="SA - 相同方式共享"><i class="iconfont icon-sa"></i></span></a></div></div></div><div class="license-icon iconfont"></div></div><div class="post-prevnext my-3"><article class="post-prev col-6"><a href="/2020/01/21/how-to-upgrade-kubeadm-cluster/" title="kubeadm 集群升级"><i class="iconfont icon-arrowleft"></i> <span class="hidden-mobile">kubeadm 集群升级</span> <span class="visible-mobile">上一篇</span></a></article><article class="post-next col-6"><a href="/2020/01/21/how-to-modify-dns-on-ubuntu18-server/" title="云服务器下 Ubuntu 18 正确的 DNS 修改"><span class="hidden-mobile">云服务器下 Ubuntu 18 正确的 DNS 修改</span> <span class="visible-mobile">下一篇</span> <i class="iconfont icon-arrowright"></i></a></article></div></div><article id="comments" lazyload><div class="disqus" style="width:100%"><div id="disqus_thread"></div><script type="text/javascript">var disqus_config=function(){this.page.url="https://mritd.com/2020/01/21/set-up-kubernetes-ha-cluster-by-kubeadm/",this.page.identifier="/2020/01/21/set-up-kubernetes-ha-cluster-by-kubeadm/"};Fluid.utils.loadComments("#disqus_thread",(function(){var e=document,t=e.createElement("script");t.src="//bleem.disqus.com/embed.js",t.setAttribute("data-timestamp",new Date),(e.head||e.body).appendChild(t)}))</script><noscript>Please enable JavaScript to view the comments</noscript></div></article></article></div></div></div><div class="side-col d-none d-lg-block col-lg-2"><aside class="sidebar" style="margin-left:-1rem"><div id="toc"><p class="toc-header"><i class="iconfont icon-list"></i> <span>目录</span></p><div class="toc-body" id="toc-body"></div></div></aside></div></div></div><a id="scroll-top-button" aria-label="TOP" href="#" role="button"><i class="iconfont icon-arrowup" aria-hidden="true"></i></a><div class="modal fade" id="modalSearch" tabindex="-1" role="dialog" aria-labelledby="ModalLabel" aria-hidden="true"><div class="modal-dialog modal-dialog-scrollable modal-lg" role="document"><div class="modal-content"><div class="modal-header text-center"><h4 class="modal-title w-100 font-weight-bold">搜索</h4><button type="button" id="local-search-close" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">&times;</span></button></div><div class="modal-body mx-3"><div class="md-form mb-5"><input type="text" id="local-search-input" class="form-control validate"> <label data-error="x" data-success="v" for="local-search-input">关键词</label></div><div class="list-group" id="local-search-result"></div></div></div></div></div></main><footer><div class="footer-inner"><div class="footer-content"><a href="https://hexo.io" target="_blank" rel="nofollow noopener"><span>Hexo</span></a> <i class="iconfont icon-love"></i> <a href="https://github.com/fluid-dev/hexo-theme-fluid" target="_blank" rel="nofollow noopener"><span>Fluid</span></a> <i class="iconfont icon-docker"></i> <a href="https://bandwagonhost.com/aff.php?aff=61367" target="_blank" rel="nofollow noopener"><span>BandwagonHost</span></a></div></div></footer><script src="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.js"></script><link rel="stylesheet" href="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.css"><script>NProgress.configure({showSpinner:!0,trickleSpeed:100}),NProgress.start(),window.addEventListener("load",(function(){NProgress.done()}))</script><script src="https://lib.baomitu.com/jquery/3.6.0/jquery.min.js"></script><script src="https://lib.baomitu.com/twitter-bootstrap/4.6.1/js/bootstrap.min.js"></script><script src="/js/events.js"></script><script src="/js/plugins.js"></script><script src="https://lib.baomitu.com/typed.js/2.0.12/typed.min.js"></script><script>!function(t,e){var i=Fluid.plugins.typing,n=e.getElementById("subtitle");n&&i&&i(n.getAttribute("data-typed-text"))}(window,document)</script><script src="/js/img-lazyload.js"></script><script>Fluid.utils.createScript("https://lib.baomitu.com/tocbot/4.18.2/tocbot.min.js",(function(){var t=jQuery("#toc");if(0!==t.length&&window.tocbot){var i=jQuery("#board-ctn").offset().top;window.tocbot.init(Object.assign({tocSelector:"#toc-body",contentSelector:".markdown-body",linkClass:"tocbot-link",activeLinkClass:"tocbot-active-link",listClass:"tocbot-list",isCollapsedClass:"tocbot-is-collapsed",collapsibleClass:"tocbot-is-collapsible",scrollSmooth:!0,includeTitleTags:!0,headingsOffset:-i},CONFIG.toc)),t.find(".toc-list-item").length>0&&t.css("visibility","visible"),Fluid.events.registerRefreshCallback((function(){if("tocbot"in window){tocbot.refresh();var t=jQuery("#toc");if(0===t.length||!tocbot)return;t.find(".toc-list-item").length>0&&t.css("visibility","visible")}}))}}))</script><script src="https://lib.baomitu.com/clipboard.js/2.0.11/clipboard.min.js"></script><script>Fluid.plugins.codeWidget()</script><script>Fluid.utils.createScript("https://lib.baomitu.com/anchor-js/4.3.1/anchor.min.js",(function(){window.anchors.options={placement:CONFIG.anchorjs.placement,visible:CONFIG.anchorjs.visible},CONFIG.anchorjs.icon&&(window.anchors.options.icon=CONFIG.anchorjs.icon);var n=(CONFIG.anchorjs.element||"h1,h2,h3,h4,h5,h6").split(","),o=[];for(var s of n)o.push(".markdown-body > "+s.trim());"left"===CONFIG.anchorjs.placement&&(window.anchors.options.class="anchorjs-link-left"),window.anchors.add(o.join(", ")),Fluid.events.registerRefreshCallback((function(){if("anchors"in window){anchors.removeAll();var n=(CONFIG.anchorjs.element||"h1,h2,h3,h4,h5,h6").split(","),o=[];for(var s of n)o.push(".markdown-body > "+s.trim());"left"===CONFIG.anchorjs.placement&&(anchors.options.class="anchorjs-link-left"),anchors.add(o.join(", "))}}))}))</script><script>Fluid.utils.createScript("https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.js",(function(){Fluid.plugins.fancyBox()}))</script><script>Fluid.plugins.imageCaption()</script><script src="/js/local-search.js"></script><script src="/js/boot.js"></script><noscript><div class="noscript-warning">博客在允许 JavaScript 运行的环境下浏览效果更佳</div></noscript></body></html>